---
title: 1차시 14(빅데이터 분석):데이터 엔지니어링 기초 맛보기
layout: single
classes: wide
categories:
  - 데이터 엔지니어링
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 1. 데이터 엔지니어링 
### **1.1 데이터 엔지니어링 개요**

**데이터 엔지니어링이란?**  
데이터 엔지니어링은 **데이터 분석가와 데이터 과학자가 요리할 수 있는 깨끗하고 신선한 식재료(데이터)를 준비하는 일**과 같습니다. 셰프(분석가/과학자)가 요리를 시작하기 전에 식재료를 손질하고 다듬어 놓는 것처럼, 데이터 엔지니어는 분석가가 바로 사용할 수 있도록 데이터를 수집하고, 저장하며, 가공하는 모든 과정을 담당합니다.

* **데이터 분석가/과학자와의 차이:**  
  * **데이터 엔지니어:** "어떻게" 데이터를 안정적이고 효율적으로 준비할 것인가를 고민합니다. 데이터 수집 파이프라인, 데이터베이스, 데이터 웨어하우스를 구축하고 관리하는 데 초점을 맞춥니다.  
  * **데이터 분석가/과학자:** 준비된 데이터를 사용하여 "무엇을" 분석할 것인가를 고민합니다. 데이터 속에서 패턴을 찾고, 의미 있는 인사이트를 도출하거나 예측 모델을 만듭니다.

### **1.2 ETL/ELT 프로세스**

데이터를 준비하는 가장 기본적인 작업 흐름을 **ETL** 또는 **ELT**라고 부릅니다.

* **E**xtract (추출): 다양한 소스(예: 웹사이트 API, 파일, 다른 데이터베이스)로부터 데이터를 가져오는 과정입니다.  
  * **예시:** 쇼핑몰의 일별 매출 데이터를 API를 통해 가져오거나, 고객 문의 내용을 CSV 파일 형태로 다운로드하는 것.  
* **T**ransform (변환): 추출한 데이터를 분석에 적합한 형태로 가공하는 과정입니다.  
  * **예시:** 날짜 형식을 통일하고, 누락된 값(결측치)을 채우거나 제거하며, 중복된 데이터를 정리하는 것. 예를 들어, 2025/08/21과 2025-08-21로 흩어져 있는 날짜 데이터를 모두 2025-08-21 형식으로 통일하는 작업이 여기에 속합니다.  
* **L**oad (적재): 변환까지 마친 데이터를 최종 목적지(데이터 웨어하우스, 데이터 레이크 등)에 저장하는 과정입니다.  
  * **예시:** 깨끗하게 정리된 쇼핑몰 매출 데이터를 분석용으로 구축한 데이터 웨어하우스에 저장하는 것.  
* **ETL vs. ELT:**  
  * **ETL:** 데이터 소스에서 데이터를 **추출(Extract)**한 뒤, 중간 서버에서 **변환(Transform)** 작업을 거쳐 최종 저장소에 **적재(Load)**하는 방식입니다.  
  * **ELT:** 데이터 소스에서 데이터를 **추출(Extract)**하여 일단 최종 저장소에 **적재(Load)**한 후, 저장소 내에서 **변환(Transform)** 작업을 수행하는 방식입니다. 클라우드 기반 환경에서는 대용량 데이터를 빠르게 적재한 뒤 클라우드 플랫폼의 강력한 컴퓨팅 자원을 활용해 변환 작업을 처리하는 **ELT 방식이 주로 사용**됩니다.


### **1.3 왜 데이터 엔지니어링이 필요한가?**

현대 사회에서는 데이터의 양이 기하급수적으로 늘어나고, 그 형태도 매우 다양해졌습니다. 이러한 **빅데이터** 시대에는 단순히 데이터를 수집하는 것만으로는 부족합니다.

* **안정적인 파이프라인 구축:** 데이터를 자동으로, 그리고 정기적으로 수집하고 가공하여 분석가에게 제공하는 **파이프라인**을 구축하는 것이 중요합니다. 이 파이프라인이 불안정하면 분석가들은 매번 수동으로 데이터를 가져와서 정리해야 하는 번거로움을 겪게 되고, 이는 결국 비즈니스 의사결정의 지연으로 이어집니다.  
* **고품질 데이터 확보:** 데이터 엔지니어링은 데이터의 품질을 보장하는 핵심 역할을 합니다. 데이터가 정확하고, 최신 상태이며, 신뢰할 수 있어야만 그에 기반한 분석과 예측의 가치가 높아집니다.

## **2. 데이터 웨어하우스(DW) 종류와 기본 아키텍처**
### **2.1 데이터 웨어하우스(DW) 종류 소개**

* **전통적 DW (데이터 웨어하우스):잘 정리된 도서관**  
  * **특징:** 미리 정해진 서가(테이블)에 책(정형 데이터)을 종류별로 정리해 놓은 형태입니다. 덕분에 원하는 책(분석 데이터)을 아주 빠르게 찾아볼 수 있습니다.  
  * **주요 기술:** **BigQuery, Snowflake**, **Amazon Redshift**.  
  * **장점:** 데이터가 잘 구조화되어 있어 **분석 쿼리 속도가 매우 빠릅니다**. 이미 정해진 규칙대로 데이터가 들어와야 하므로 품질 관리가 용이합니다.  
  * **단점:** 정해진 서가에 넣을 수 없는 **새로운 종류의 책(비정형/반정형 데이터)**은 보관하기 어렵습니다.  
* **데이터 레이크 (Data Lake):모든 물건을 담는 거대한 호수**  
  * **특징:** 물건의 종류나 형태(정형, 반정형, 비정형 데이터)에 상관없이 일단 한곳에 모두 쌓아두는 형태입니다.  
  * **주요 기술:** **S3(Amazon Simple Storage Service)**, **Hadoop**.  
  * **장점:** **모든 종류의 데이터**를 **저렴한 비용**으로 저장할 수 있습니다. 데이터를 어떻게 쓸지 미리 고민하지 않고 일단 쌓아둘 수 있다는 것이 큰 장점입니다.  
  * **단점:** 데이터가 정돈되어 있지 않아 **원하는 데이터를 찾기 어렵고**, 복잡한 쿼리 처리 속도가 느립니다. 데이터를 사용하기 전에 직접 정제하는 과정이 필요합니다.  
* **데이터 레이크하우스 (Data Lakehouse):정돈된 호수가 있는 복합 물류 창고**  
  * **특징:** 데이터 레이크의 **저장 유연성**과 데이터 웨어하우스의 **빠른 쿼리 성능**을 결합한 최신 아키텍처입니다. 비정형 데이터도 저장하면서, 필요한 데이터는 마치 DW처럼 빠르게 쿼리할 수 있도록 만들어줍니다.  
  * **주요 기술:** **Databricks**, **Delta Lake**.  
  * **장점:** 모든 데이터 타입을 수용하면서, DW처럼 **SQL 쿼리**를 사용해 빠르게 분석할 수 있습니다. 데이터 엔지니어링, 분석, 머신러닝/AI를 위한 통합 환경을 제공합니다.

### **2.2 Databricks 소개: 왜 데이터 분석가가 알아야 할까?**

* **Databricks란? Apache Spark 기반의 올인원(All-in-one) 데이터 플랫폼**  
  * 대용량 데이터(빅데이터)를 처리하고 분석하는 데 최적화된 클라우드 서비스입니다. 데이터 레이크하우스를 대표하는 기술이며, 데이터 엔지니어링, 데이터 분석, 그리고 AI/ML 개발까지 **하나의 플랫폼**에서 모두 가능합니다.  
* **Databricks가 '최신' 기술인 이유**  
  * **AI/ML 통합:** 데이터 분석가가 파이썬, R로 머신러닝 모델을 개발하고 관리하는 환경을 쉽게 제공합니다.  
  * **협업 기능:** 여러 사용자가 노트북 환경에서 코드를 공유하고 협업할 수 있어 효율적입니다.  
  * **서버리스 컴퓨팅:** 사용한 만큼만 비용을 지불하는 구조로 효율적인 비용 관리가 가능합니다.  
* **무료 계정으로 데모 가능:** Databricks는 **커뮤니티 에디션**을 통해 무료로 기능을 체험해 볼 수 있습니다. 간단한 데이터 분석 실습을 통해 직접 플랫폼을 경험해 볼 수 있다는 점을 강조해 주세요.

### **2.3 기본 파이프라인 구성 요소**

* **데이터 흐름 관리:데이터를 자동으로 옮겨주는 수도관**  
  * 데이터 분석에 필요한 데이터는 매일, 매시간 새롭게 생성됩니다. 이 데이터를 자동으로 데이터 웨어하우스로 옮기고, 분석에 맞게 가공하는 일련의 과정을 **데이터 파이프라인**이라고 합니다.  
  * **Airflow(에어플로우)**는 이 데이터 파이프라인의 **일정(스케줄링)**을 관리해주는 도구입니다. "매일 새벽 3시에 특정 데이터베이스의 데이터를 가져와서, 정제 후 최종 분석 테이블에 넣어라"와 같은 작업을 **자동으로 실행**하도록 설정할 수 있습니다.  
* **왜 알아야 할까?**  
  * 데이터 분석가도 이제 데이터를 직접 가져오고 처리하는 파이프라인의 기본 개념을 이해해야 합니다. 최신 데이터로 분석하고, 반복적인 데이터 수집/정제 작업을 자동화하기 위함입니다.

## **3. 실전 맛보기: 간단한 ETL/ELT 파이프라인 구축**

데이터 분석가에게 있어, 데이터는 항상 깨끗하고 완벽하게 준비되어 있지 않습니다. 이 섹션에서는 **'분석에 필요한 데이터를 직접 가져오고 가공하는 과정'**을 체험하며 데이터 파이프라인의 핵심을 이해합니다.

### 3.1  **필수 도구**: Python/SQL (기본) 🐍, Apache Spark (선택, 빅데이터 처리용)  
* **파이썬과 SQL:** 데이터 분석가에게 가장 익숙한 도구로 소규모 데이터 파이프라인을 구축하는 데 필수적입니다.  
* **아파치 스파크**: 일반 PC로 처리하기 어려운 **대용량 데이터(수백 GB 이상)**를 다룰 때 사용하는 분산 처리 엔진

### 3.2 **데모 예시**:  
1. **파이썬으로 간단한 ETL 파이프라인 구축**  
    * **목표**: CSV 파일 형태의 원천 데이터를 읽어와 Pandas를 이용해 데이터를 정제하고, 로컬 데이터베이스인 **SQLite**에 저장하는 실습을 합니다.  
    * **실습 내용**:  
      * Pandas 라이브러리를 활용해 CSV 파일을 로드합니다.  
      * 데이터의 결측치를 확인하고 적절한 값으로 채웁니다. (ex: df.fillna())  
      * 이상치나 오류 데이터를 제거하거나 수정합니다.  
      * 정제된 데이터를 **SQLite** 데이터베이스에 테이블로 저장합니다. (pd.to\_sql())  
      * **핵심**: 이 과정은 데이터 분석가가 흔히 겪는 데이터 전처리 과정을 자동화하는 파이프라인의 가장 기본적인 형태입니다.  
2. **클라우드 기반 데이터 레이크/웨어하우스 맛보기: Databricks 데모**  
    * **목표**: 대용량 데이터를 다루는 현대적인 **클라우드 데이터 플랫폼**을 체험합니다. **Databricks**는 클라우드에서 **Spark**를 가장 쉽게 사용할 수 있는 도구입니다.  
    * **데모 내용**:  
      * Databricks 노트북에서 SparkSession을 통해 대용량 데이터가 저장된 **데이터 레이크**(S3, ADLS 등)에서 직접 파일을 읽어옵니다.  
      * **Spark SQL**을 사용해 SQL만으로 수십억 건의 데이터를 빠르게 집계하는 쿼리를 실행합니다.  
      * **Unity Catalog**: 데이터 거버넌스 도구인 Unity Catalog를 간단히 시연합니다. 이를 통해 **데이터에 누가, 언제, 어떻게 접근했는지**를 관리하는 개념을 소개합니다. 이는 기업에서 데이터 보안과 신뢰성을 확보하는 데 매우 중요합니다.  
3. **코딩 없는 ETL: 클라우드 도구 맛보기 (AWS Glue 또는 Azure Data Factory)**  
    * **목표**: 데이터 파이프라인을 코딩 없이 **GUI(그래픽 사용자 인터페이스)**로 구축하는 방법을 경험합니다.  
    * **설명 내용**:  
      * AWS Glue나 Azure Data Factory와 같은 **클라우드 ETL 서비스**는 드래그 앤 드롭 방식으로 데이터 소스, 변환 로직, 목적지를 연결하여 파이프라인을 시각적으로 만듭니다.  
      * **장점**: 코딩 지식이 없어도 파이프라인을 만들 수 있어, 데이터 분석가가 직접 간단한 파이프라인을 구축할 때 유용합니다.  

### 3.3 **데이터 엔지니어링의 최신 트렌드: 자동화와 ELT**  
* **AutoML**이 머신러닝을 자동화하는 것처럼, 데이터 엔지니어링 역시 자동화되고 있습니다.  
* **dbt(data build tool)**: dbt는 데이터 웨어하우스(빅쿼리, 스노우플레이크 등)에서 SQL을 사용해 데이터를 **변환(Transform)**하는 과정을 자동화하는 도구입니다. 데이터가 이미 적재된 후에 변환 작업을 수행하는 **ELT(Extract-Load-Transform)** 트렌드의 핵심입니다.  
* **핵심**: 데이터 엔지니어링은 이제 무작정 코드를 짜는 것이 아니라, 효율적으로 데이터를 관리하고 분석가에게 전달하는 자동화된 시스템을 구축하는 방향으로 발전하고 있습니다.

## **4. 최신 트렌드**

### 4.1 **AI 통합: 데이터 분석과 AI/ML의 경계가 허물어지다**
과거에는 데이터 분석가와 AI/ML 엔지니어의 역할이 명확히 구분되었습니다. 하지만 최근에는 데이터 파이프라인 구축 단계에서부터 AI/ML 모델 개발 및 배포까지 통합적으로 다루는 추세입니다.

* **Databricks와 MLflow:** **Databricks**는 대규모 데이터 처리와 머신러닝을 통합한 플랫폼입니다. 특히 **MLflow**라는 오픈소스 도구를 활용하면 모델의 개발, 실험 추적, 배포 과정을 한곳에서 관리할 수 있습니다.  
  * **왜 중요한가?** 데이터 분석가가 모델을 만들 때, 어떤 데이터를 사용했고, 어떤 파라미터로 학습했는지 기록하는 것이 중요합니다. MLflow는 이러한 **실험 과정(Experiment Tracking)**을 체계적으로 관리하고, 최적의 모델을 **모델 레지스트리(Model Registry)**에 등록하여 다른 팀원과 공유하거나 실제 서비스에 쉽게 배포(**Model Deployment**)할 수 있도록 돕습니다.

### 4.2 **실시간 스트리밍: 실시간으로 데이터에 반응하기**

전통적인 데이터 분석은 정해진 시간에 데이터를 모아 처리하는 배치(Batch) 방식이었습니다. 하지만 이제는 고객 행동, 센서 데이터 등 끊임없이 발생하는 데이터를 즉시 처리하고 분석하는 **실시간 스트리밍(Real-time Streaming)**이 중요해지고 있습니다.

* **Apache Kafka:** **Apache Kafka**는 실시간 스트리밍 데이터를 효율적으로 처리하는 분산 메시지 큐 시스템
  * **왜 중요한가?** 온라인 쇼핑몰에서 고객이 상품을 클릭할 때마다 실시간으로 데이터를 수집하여, 즉시 개인화된 추천 상품을 보여주는 시스템을 구축할 수 있습니다. 이는 고객 경험을 향상시키고 비즈니스 의사결정 속도를 높이는 데 결정적인 역할을 합니다.  
  * **어떻게 동작하나?** 데이터가 발생하는 **생산자(Producer)**와 데이터를 가져다 쓰는 **소비자(Consumer)** 사이에서 Kafka가 데이터를 중간에 저장하고 전달하는 역할을 합니다. 이를 통해 데이터 소스와 분석 시스템이 독립적으로 동작할 수 있어 시스템의 유연성이 높아집니다.