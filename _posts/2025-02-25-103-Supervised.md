---
title: 8차시 3:머신러닝(지도학습) 
layout: single
classes: wide
categories:
  - 데이터 시각화
tags:
  - 지도학습
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

![지도학습](/assets/images/supervised.png)


- 지도학습(Supervised Learning)은 입력 데이터와 출력 데이터가 주어졌을 때, 입력과 출력 간의 관계를 학습하는 방법입니다. 이를 통해 새로운 입력 데이터에 대해 정확한 출력을 예측할 수 있습니다. 



## **1. 회귀 (Regression)**

### **1-1. Simple Linear Regression**
**핵심 개념**:  
단일 독립 변수(x)와 종속 변수(y) 간의 선형 관계를 모델링합니다. `y = wx + b` 형태로 표현됩니다.

**샘플 예제**:
```python
import numpy as np
from sklearn.linear_model import LinearRegression

# 데이터 생성
X = np.array([[1], [2], [3], [4], [5]])  # 독립 변수
y = np.array([2, 4, 6, 8, 10])           # 종속 변수

# 모델 학습
model = LinearRegression()
model.fit(X, y)

# 예측
print("기울기(w):", model.coef_[0])
print("절편(b):", model.intercept_)
print("예측값:", model.predict([[6]]))  # x=6일 때 y 예측
```

---

### **1-2. Multiple Linear Regression**
**핵심 개념**:  
여러 개의 독립 변수(x1, x2, ...)와 종속 변수(y) 간의 선형 관계를 모델링합니다. `y = w1x1 + w2x2 + ... + b` 형태로 표현됩니다.

**샘플 예제**:
```python
from sklearn.linear_model import LinearRegression

# 데이터 생성
X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # 다중 독립 변수
y = np.array([3, 5, 7, 9])                      # 종속 변수

# 모델 학습
model = LinearRegression()
model.fit(X, y)

# 예측
print("계수(w):", model.coef_)
print("절편(b):", model.intercept_)
print("예측값:", model.predict([[5, 6]]))  # x1=5, x2=6일 때 y 예측
```

---

### **1-3. Polynomial Regression**
**핵심 개념**:  
독립 변수를 다항식으로 변환하여 비선형 관계를 모델링합니다. `y = w1x + w2x^2 + ... + b` 형태로 표현됩니다.

**샘플 예제**:
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# 데이터 생성
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([1, 4, 9, 16, 25])  # y = x^2 관계

# 다항식 변환
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# 모델 학습
model = LinearRegression()
model.fit(X_poly, y)

# 예측
print("계수(w):", model.coef_)
print("절편(b):", model.intercept_)
print("예측값:", model.predict(poly.transform([[6]])))  # x=6일 때 y 예측
```

---

### **1-4. Lasso Regression**
**핵심 개념**:  
L1 정규화를 사용하여 계수의 크기를 제한하고, 일부 계수를 0으로 만듭니다. 특성 선택(feature selection)에 유용합니다.

**샘플 예제**:
```python
from sklearn.linear_model import Lasso

# 데이터 생성
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

# 모델 학습
model = Lasso(alpha=0.1)  # alpha: 정규화 강도
model.fit(X, y)

# 예측
print("계수(w):", model.coef_)
print("절편(b):", model.intercept_)
print("예측값:", model.predict([[6]]))
```

---

### **1-5. Ridge Regression**
**핵심 개념**:  
L2 정규화를 사용하여 계수의 크기를 제한합니다. 과적합(overfitting)을 방지하는 데 효과적입니다.

**샘플 예제**:
```python
from sklearn.linear_model import Ridge

# 데이터 생성
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

# 모델 학습
model = Ridge(alpha=0.1)  # alpha: 정규화 강도
model.fit(X, y)

# 예측
print("계수(w):", model.coef_)
print("절편(b):", model.intercept_)
print("예측값:", model.predict([[6]]))
```


## **2. 분류 (Classification)**

### **2-1. Logistic Regression**
**핵심 개념**:  
이진 분류 문제에서 확률을 기반으로 클래스를 예측합니다. 시그모이드 함수를 사용합니다.

**샘플 예제**:
```python
from sklearn.linear_model import LogisticRegression

# 데이터 생성
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([0, 0, 1, 1, 1])  # 이진 분류

# 모델 학습
model = LogisticRegression()
model.fit(X, y)

# 예측
print("예측 클래스:", model.predict([[6]]))
print("예측 확률:", model.predict_proba([[6]]))
```

---

### **2-2. K-Nearest Neighbors (KNN)**
**핵심 개념**:  
새로운 데이터 포인트를 가장 가까운 k개의 이웃 데이터 포인트로 분류합니다.

**샘플 예제**:
```python
from sklearn.neighbors import KNeighborsClassifier

# 데이터 생성
X = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]])
y = np.array([0, 0, 0, 1, 1])

# 모델 학습
model = KNeighborsClassifier(n_neighbors=3)
model.fit(X, y)

# 예측
print("예측 클래스:", model.predict([[5, 6]]))
```

---

### **2-3. Support Vector Machine (SVM)**
**핵심 개념**:  
데이터를 최대 마진으로 분리하는 초평면(hyperplane)을 찾습니다.

**샘플 예제**:
```python
from sklearn.svm import SVC

# 데이터 생성
X = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]])
y = np.array([0, 0, 0, 1, 1])

# 모델 학습
model = SVC(kernel='linear')
model.fit(X, y)

# 예측
print("예측 클래스:", model.predict([[5, 6]]))
```

---

### **2-4. Kernel SVM**
**핵심 개념**:  
비선형 데이터를 고차원 공간으로 매핑하여 선형 분리가 가능하도록 합니다.

**샘플 예제**:
```python
from sklearn.svm import SVC

# 데이터 생성
X = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]])
y = np.array([0, 0, 0, 1, 1])

# 모델 학습
model = SVC(kernel='rbf')  # RBF 커널 사용
model.fit(X, y)

# 예측
print("예측 클래스:", model.predict([[5, 6]]))
```

---

### **2-5. Naive Bayes**
**핵심 개념**:  
베이지안 정리를 기반으로 조건부 확률을 계산하여 분류합니다. 독립성 가정을 사용합니다.

**샘플 예제**:
```python
from sklearn.naive_bayes import GaussianNB

# 데이터 생성
X = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]])
y = np.array([0, 0, 0, 1, 1])

# 모델 학습
model = GaussianNB()
model.fit(X, y)

# 예측
print("예측 클래스:", model.predict([[5, 6]]))
```



## **3. 앙상블 (Ensemble)**

### **3-1. Decision Tree**
**핵심 개념**:  
데이터를 규칙 기반으로 분할하여 결정 트리를 구성합니다.

**샘플 예제**:
```python
from sklearn.tree import DecisionTreeClassifier

# 데이터 생성
X = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]])
y = np.array([0, 0, 0, 1, 1])

# 모델 학습
model = DecisionTreeClassifier()
model.fit(X, y)

# 예측
print("예측 클래스:", model.predict([[5, 6]]))
```

---

### **3-2. Random Forest**
**핵심 개념**:  
여러 개의 결정 트리를 앙상블하여 일반화 성능을 향상시킵니다.

**샘플 예제**:
```python
from sklearn.ensemble import RandomForestClassifier

# 데이터 생성
X = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]])
y = np.array([0, 0, 0, 1, 1])

# 모델 학습
model = RandomForestClassifier(n_estimators=10)
model.fit(X, y)

# 예측
print("예측 클래스:", model.predict([[5, 6]]))
```

---

### **3-3. XGBoost**
**핵심 개념**:  
Gradient Boosting 알고리즘을 사용하여 순차적으로 모델을 학습합니다.

**샘플 예제**:
```python
from xgboost import XGBClassifier

# 데이터 생성
X = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]])
y = np.array([0, 0, 0, 1, 1])

# 모델 학습
model = XGBClassifier()
model.fit(X, y)

# 예측
print("예측 클래스:", model.predict([[5, 6]]))
```

---

### **3-4. CatBoost**
**핵심 개념**:  
범주형 데이터 처리에 특화된 Gradient Boosting 알고리즘입니다.

**샘플 예제**:
```python
from catboost import CatBoostClassifier

# 데이터 생성
X = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]])
y = np.array([0, 0, 0, 1, 1])

# 모델 학습
model = CatBoostClassifier(silent=True)
model.fit(X, y)

# 예측
print("예측 클래스:", model.predict([[5, 6]]))
```



## **4. 딥러닝 (Deep Learning)**

### **4-1. Artificial Neural Network (ANN)**
**핵심 개념**:  
신경망을 통해 복잡한 비선형 관계를 학습합니다.

**샘플 예제**:
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# 데이터 생성
X = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]])
y = np.array([0, 0, 0, 1, 1])

# 모델 정의
model = Sequential([
    Dense(4, activation='relu', input_shape=(2,)),
    Dense(1, activation='sigmoid')
])

# 모델 컴파일 및 학습
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X, y, epochs=100, verbose=0)

# 예측
print("예측 확률:", model.predict([[5, 6]]))
```

---

### **4-2. Convolutional Neural Network (CNN)**
**핵심 개념**:  
이미지 데이터를 처리하기 위한 신경망으로, 컨볼루션 연산을 사용합니다.

**샘플 예제**:
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Flatten, Dense

# 데이터 생성 (간단한 이미지 데이터)
X = np.random.rand(5, 8, 8, 1)  # 8x8 크기의 흑백 이미지 5장
y = np.array([0, 0, 1, 1, 0])

# 모델 정의
model = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(8, 8, 1)),
    Flatten(),
    Dense(1, activation='sigmoid')
])

# 모델 컴파일 및 학습
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X, y, epochs=10, verbose=0)

# 예측
print("예측 확률:", model.predict(np.random.rand(1, 8, 8, 1)))
```

---

### **4-3. Recurrent Neural Network (RNN)**
**핵심 개념**:  
시계열 데이터나 순차적 데이터를 처리하기 위한 신경망입니다.

**샘플 예제**:
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

# 데이터 생성 (시계열 데이터)
X = np.random.rand(5, 10, 1)  # 10개의 타임스텝, 1개의 피처
y = np.array([0, 1, 0, 1, 0])

# 모델 정의
model = Sequential([
    SimpleRNN(10, activation='relu', input_shape=(10, 1)),
    Dense(1, activation='sigmoid')
])

# 모델 컴파일 및 학습
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X, y, epochs=10, verbose=0)

# 예측
print("예측 확률:", model.predict(np.random.rand(1, 10, 1)))
```

---

### **4-4. Transformers + Attention**
**핵심 개념**:  
자연어 처리(NLP)에서 주로 사용되며, 어텐션 메커니즘을 통해 중요한 정보에 집중합니다.

**샘플 예제**:
```python
from transformers import pipeline

# 사전 학습된 Transformer 모델 로드
classifier = pipeline("sentiment-analysis")

# 예측
result = classifier("I love this product!")
print(result)
```

---

**결론**:  
각 모델은 특정 문제에 적합한 강점과 약점이 있습니다. 문제의 성격에 따라 적절한 모델을 선택하고, 데이터 전처리 및 하이퍼파라미터 튜닝을 통해 성능을 최적화해야 합니다.