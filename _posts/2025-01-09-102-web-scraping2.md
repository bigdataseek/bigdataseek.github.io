---
title: 1차시 2(빅데이터 분석):웹 스크래핑(실습 예제)
layout: single
classes: wide
categories:
  - web scraping
tags:
  - html
  - beautifulsoup
  - selenium
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## **1. BeautifulSoup 예제**

### **1.1 "Quotes to Scrape"를 활용**
* 웹 스크래핑 연습용 사이트
    * [http://quotes.toscrape.com/](http://quotes.toscrape.com/)
    * [http://books.toscrape.com/](http://books.toscrape.com/)

```python
import requests
from bs4 import BeautifulSoup

# 1. 스크래핑할 웹사이트의 URL 설정
url = "http://quotes.toscrape.com/"

# 2. 웹 서버에 접속하여 HTML 내용 가져오기
# User-Agent 설정은 필수는 아니지만, 일부 사이트에서 필요할 수 있습니다.
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
}
response = requests.get(url, headers=headers)

# HTTP 요청이 성공했는지 확인 (상태 코드 200은 성공을 의미)
if response.status_code == 200:
    print("웹 페이지에 성공적으로 접속했습니다.")
    
    # 3. BeautifulSoup을 사용하여 HTML 내용 분석 준비
    # response.text에는 웹 페이지의 HTML 코드가 문자열 형태로 들어있습니다.
    # soup = BeautifulSoup(response.text, "lxml")  # lxml 설치 필요 (pip install lxml)
    soup = BeautifulSoup(response.text, "html.parser")

    # 4. 원하는 데이터 찾기 (HTML 구조 분석)
    # 웹 브라우저의 개발자 도구 (F12)를 열어 보면,
    # 각 명언은 <div class="quote"> 태그 안에 있다는 것을 알 수 있습니다.
    quotes = soup.find_all('div', class_='quote') # 'quote' 클래스를 가진 모든 div 태그를 찾음

    # 5. 추출한 데이터 출력
    print("\n--- 스크래핑 결과 ---")
    for quote in quotes:
        # 각 quote div 안에서 명언 텍스트를 찾습니다.
        # 명언 텍스트는 <span class="text"> 태그 안에 있습니다.
        text_element = quote.find('span', class_='text')
        quote_text = text_element.get_text(strip=True) if text_element else "N/A"

        # 각 quote div 안에서 작가 이름을 찾습니다.
        # 작가 이름은 <small class="author"> 태그 안에 있습니다.
        author_element = quote.find('small', class_='author')
        author_name = author_element.get_text(strip=True) if author_element else "N/A"

        print(f"명언: {quote_text}")
        print(f"작가: {author_name}")
        print("-" * 30) # 구분을 위한 선

else:
    print(f"웹 페이지 접속에 실패했습니다. 상태 코드: {response.status_code}")
```

<br>


### 1.2 "Quotes to Scrape" 페이지네이션 스크래핑 예제

```python
import requests
from bs4 import BeautifulSoup
import time # 요청 사이에 지연 시간을 주기 위해 추가

def scrape_quotes_with_pagination(base_url):
    """
    Quotes to Scrape 웹사이트에서 페이지네이션을 처리하여 모든 명언과 작가를 스크래핑합니다.
    """
    all_quotes_data = [] # 모든 페이지에서 스크래핑한 데이터를 저장할 리스트
    current_url = base_url # 현재 스크래핑할 페이지의 URL

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
    }

    page_num = 1
    while True: # 'Next' 버튼이 없을 때까지 무한 루프
        print(f"\n--- 페이지 {page_num} 스크래핑 시작: {current_url} ---")
        
        try:
            response = requests.get(current_url, headers=headers)
            response.raise_for_status() # HTTP 오류가 발생하면 예외 발생

            soup = BeautifulSoup(response.text, "html.parser")

            # 현재 페이지의 모든 명언 블록 찾기
            quotes_on_page = soup.find_all('div', class_='quote')

            if not quotes_on_page: # 현재 페이지에 명언이 없으면 루프 종료 (마지막 페이지이거나 오류)
                print(f"페이지 {page_num}에서 더 이상 명언을 찾을 수 없습니다. 스크래핑을 종료합니다.")
                break

            for quote in quotes_on_page:
                text_element = quote.find('span', class_='text')
                author_element = quote.find('small', class_='author')

                quote_text = text_element.get_text(strip=True) if text_element else "N/A"
                author_name = author_element.get_text(strip=True) if author_element else "N/A"

                all_quotes_data.append({"명언": quote_text, "작가": author_name})

            # 다음 페이지 링크 찾기
            # <li class="next"><a href="/page/2/">Next &rarr;</a></li>
            next_link_element = soup.find('li', class_='next')

            if next_link_element and next_link_element.find('a'):
                # 'next' 클래스를 가진 li 태그 안에 있는 a 태그의 'href' 속성 값 가져오기                
                # relative_next_url = next_link_element.find('a')['href']
                relative_next_url = next_link_element.find('a').get('href') #속성이 없을 경우 None을 반환
                
                # 상대 경로를 절대 경로로 변환 (예: /page/2/ -> http://quotes.toscrape.com/page/2/)
                # urljoin(): 웹 스크래핑 시 상대 URL을 절대 URL로 변환할 때 반드시 사용
                current_url = requests.compat.urljoin(base_url, relative_next_url)
                page_num += 1
                
                # 다음 요청 전에 잠시 기다림 (서버에 부담을 주지 않기 위함)
                time.sleep(1) 
            else:
                # 'Next' 링크를 찾을 수 없으면 마지막 페이지에 도달한 것이므로 루프 종료
                print(f"페이지 {page_num}에서 'Next' 링크를 찾을 수 없습니다. 스크래핑을 종료합니다.")
                break

        except requests.exceptions.RequestException as e:
            print(f"요청 오류 발생: {e}")
            break
        except Exception as e:
            print(f"스크래핑 중 예상치 못한 오류 발생: {e}")
            break

    return all_quotes_data

# --- 실행 부분 ---
if __name__ == "__main__":
    base_url = "http://quotes.toscrape.com/"
    scraped_data = scrape_quotes_with_pagination(base_url)

    print("\n\n--- 모든 페이지의 스크래핑 최종 결과 (일부만 출력) ---")
    for i, data in enumerate(scraped_data[:25]): # 처음 25개 데이터만 출력
        print(f"{i+1}. 명언: {data['명언']}")
        print(f"   작가: {data['작가']}")
        print("-" * 40)

    print(f"\n총 {len(scraped_data)}개의 명언이 스크래핑되었습니다.")
```

## **2. Selenium 예제**
### 2.1 실제 뉴스 사이트에서 헤드라인을 수집

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

def news_scraping():
    """
    실제 뉴스 사이트에서 헤드라인을 수집하는 실용적인 예제
    """
    print("\n=== 뉴스 헤드라인 스크래핑 ===")
    
    driver = webdriver.Chrome()
    
    try:
        # Hacker News 사이트 접속 (스크래핑이 허용되는 사이트)
        driver.get("https://news.ycombinator.com/")
        
        # 페이지 로딩 대기
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "titleline"))
        )
        
        # 뉴스 제목들 수집
        headlines = driver.find_elements(By.CSS_SELECTOR, "span.titleline > a")
        
        print("=== Hacker News 상위 헤드라인 ===")
        for i, headline in enumerate(headlines[:10], 1):  # 상위 10개만
            print(f"{i}. {headline.text}")
            if headline.get_attribute("href"):
                print(f"   링크: {headline.get_attribute('href')}")
        
        time.sleep(3)
        
    finally:
        driver.quit()


if __name__ == '__main__':
	news_scraping()
```

### 2.2 duckduckgo 검색 사이트 이용하기
*   Google 검색창에서 검색어를 입력하고 결과값을 확인하려 했으나 **봇 감지(CAPTCHA)**에 걸림
*   DuckDuckGo는 Google이나 Bing처럼 정보를 검색할 수 있는 웹 검색 엔진입니다. 하지만 다음과 같은 점에서 차별화
    *   봇 감지 없음:	Google과 달리 Selenium 등 자동화 도구에 CAPTCHA를 거의 띄우지 않음
    *   빠른 로딩:	JavaScript 로딩이 적어 자동화 테스트에 적합
    *   간단한 구조:	검색 결과의 HTML 구조가 단순해서 크롤링/스크래핑 용이

```python
def search_duckduckgo():

	# 드라이버 실행
	driver = webdriver.Chrome()
	# 구글 홈페이지로 이동!

	driver.get("https://duckduckgo.com/")
	print(f"현재 페이지 제목: {driver.title}") # 로봇이 접속한 웹사이트의 제목을 확인해요.


	# 1. 검색창 찾기
	# 웹사이트의 각 부분(버튼, 입력창 등)은 고유한 이름(ID, class, name 등)이나 위치(XPath)를 가지고 있어요.
	# 구글 검색창은 'name' 속성이 'q'인 경우가 많아요.
	try:
		search_box = driver.find_element(By.NAME, "q") # By.NAME을 이용해서 'q'라는 이름을 가진 요소를 찾아요.
		print("검색창을 찾았습니다!")

		# 2. 검색어 입력하기
		search_query = "Selenium Python"
		search_box.send_keys(search_query) # 찾은 검색창에 'Selenium Python'이라고 입력해요.
		print(f"검색어 '{search_query}'를 입력했습니다.")

		# 3. 검색 실행 (엔터 키 누르기)
		search_box.submit() # 입력한 검색어를 제출(엔터)해서 검색을 시작해요.
		print("검색을 실행했습니다.")

		# 4. 검색 결과 페이지가 나타날 때까지 잠시 기다리기
		time.sleep(3) # 웹 페이지가 로딩될 시간을 3초 정도 줘요. (인터넷 속도에 따라 조절)

		# 5. 검색 결과 페이지의 제목 확인
		print(f"검색 결과 페이지 제목: {driver.title}")

		# 6. 첫 번째 검색 결과 제목 가져오기 (예시)
		# 검색 결과는 보통 <h2> 태그에 제목이 있어요.
		first_result_title = driver.find_element(By.CSS_SELECTOR, 'h2')
		print(f"첫 번째 검색 결과 제목: {first_result_title.text}")

	except Exception as e:
		print(f"웹 요소를 찾거나 상호작용 중 오류가 발생했습니다: {e}")

	driver.quit()

if __name__ == '__main__':
	search_duckduckgo()
```