---
title: 4차시 4 :StatQuest(Machine Learning 2)
layout: single
classes: wide
categories:
  - Machine Learning
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 26. 정규화(Regularization) 기법의 이해: 라쏘 회귀 (L1 정규화)

**정규화(Regularization)** 기법 중 하나인 **라쏘 회귀(Lasso Regression)**에 대해 설명합니다. 라쏘 회귀는 모델의 **과적합(overfitting)**을 방지하고 예측 성능을 향상시키며, 특히 모델을 **단순화**하는 데 강력한 장점을 가집니다. 이 내용은 **릿지 회귀(Ridge Regression)**와 매우 유사하지만 중요한 차이점을 가지고 있습니다.

### **26.1 릿지 회귀(Ridge Regression)와의 유사점 및 배경**

라쏘 회귀를 이해하기 위해 먼저 릿지 회귀와 공통점들을 살펴봅니다.

*   **과적합 문제**: 최소 제곱법을 사용한 선형 회귀는 훈련 데이터에 매우 잘 맞을 수 있지만, 새로운 데이터에 대한 예측에서는 **높은 분산(high variance)**을 보여 성능이 저하될 수 있습니다. 이를 **과적합**이라고 합니다.
*   **정규화의 목표**: 릿지 회귀와 라쏘 회귀 모두 훈련 데이터에 약간의 **편향(bias)**을 도입하는 대신 **분산(variance)**을 크게 줄여 장기적으로 더 나은 예측을 제공하는 것을 목표로 합니다.
*   **릿지 회귀의 작동 원리**: 릿지 회귀는 '잔차 제곱의 합'을 최소화하는 대신 '잔차 제곱의 합 + 람다(λ) × (기울기(slope)의 제곱)'을 최소화합니다. 여기서 `람다 × (기울기의 제곱)`이 릿지 회귀 패널티(L2 패널티)입니다.

### **26.2 라쏘 회귀(Lasso Regression)의 작동 원리**

라쏘 회귀는 릿지 회귀와 거의 동일한 접근 방식을 따르지만, 패널티 항에서 차이를 보입니다.

*   **패널티 항의 차이**: 릿지 회귀가 기울기의 **제곱**을 사용하는 반면, 라쏘 회귀는 기울기의 **절댓값**을 사용합니다.
    *   라쏘 회귀는 다음 식을 최소화합니다: **잔차 제곱의 합 + 람다(λ) × |기울기(slope)|**.
    *   여기서 **`람다(λ) × |기울기|`** 부분이 **라쏘 회귀 패널티(Lasso Regression penalty)** 또는 L1 패널티라고 불립니다.
*   **람다(λ)의 역할**: 람다(λ)는 패널티의 강도를 조절하는 값입니다. 람다 값이 커질수록 패널티의 영향이 강해지며, 이는 기울기를 0으로 축소하는 데 더 큰 영향을 미칩니다. 람다는 0부터 무한대까지의 값을 가질 수 있으며, **교차 검증(cross-validation)**을 통해 최적의 값을 결정합니다.
*   **효과**: 릿지 회귀와 마찬가지로 라쏘 회귀도 약간의 편향을 도입하여 최소 제곱법보다 낮은 분산을 가진 예측을 제공합니다. 이는 예측이 훈련 데이터에 덜 민감하도록 만듭니다.

### **26.3 릿지 회귀와의 결정적인 차이: 변수 제거 능력 (Feature Selection)**

라쏘 회귀의 가장 중요한 특징이자 릿지 회귀와의 **큰 차이점**은 바로 **매개변수를 완전히 0으로 축소할 수 있다는 것**입니다.

*   **릿지 회귀의 한계**: 릿지 회귀는 기울기를 0에 **점근적으로(asymptotically)** 가깝게 축소할 수는 있지만, **결코 완전히 0으로 만들지는 못합니다**. 즉, 모든 변수는 여전히 모델에 남아 있습니다.
*   **라쏘 회귀의 강력함**: 라쏘 회귀는 람다 값이 충분히 커지면 특정 기울기(매개변수)를 **완전히 0으로 만들 수 있습니다**.
    *   예를 들어, 크기를 예측하는 모델에 체중, 식단(유용한 변수) 외에 점성술 별자리, 제비의 비행 속도(쓸모없는 변수)가 포함되어 있다고 가정해 봅시다.
    *   릿지 회귀는 쓸모없는 변수의 매개변수를 작게 만들 수는 있지만 0으로 만들지는 못합니다.
    *   반면, 라쏘 회귀는 람다 값을 늘려 쓸모없는 변수의 매개변수를 **완전히 0으로 만들어 해당 항을 방정식에서 제외**시킬 수 있습니다.
*   **장점**:
    *   **모델 단순화 및 해석 용이성**: 쓸모없는 변수를 제거함으로써 최종 방정식을 **더 간단하고 해석하기 쉽게 만듭니다**.
    *   **성능 향상**: 쓸모없는 변수가 많은 모델의 경우, 라쏘 회귀는 이러한 변수를 제거함으로써 릿지 회귀보다 분산을 줄이는 데 더 효과적일 수 있습니다.
    *   **변수 선택(Feature Selection)** 기능: 라쏘 회귀는 이러한 능력 덕분에 자동으로 중요한 변수를 선택(feature selection)하는 효과를 가집니다.

### **26.4 라쏘 회귀의 적용 및 특징**

*   **적용 분야**: 릿지 회귀와 마찬가지로 라쏘 회귀도 다양한 맥락에서 적용될 수 있습니다. 연속형 변수나 이산형 변수를 예측에 사용하는 경우, 또는 로지스틱 회귀와 같이 복잡한 모델에도 적용 가능합니다.
*   **패널티 적용 범위**: y절편을 제외한 모든 예측 변수의 매개변수에 패널티가 적용됩니다.
*   **매개변수 축소의 불균형**: 라쏘 회귀 역시 모든 매개변수를 동일하게 축소할 필요는 없으며, 특정 매개변수를 다른 매개변수보다 훨씬 더 많이 축소할 수 있습니다.
*   **릿지 회귀와의 비교**: 대부분의 변수가 유용할 때는 릿지 회귀가 라쏘 회귀보다 약간 더 나은 성능을 보이는 경향이 있습니다.

요약하자면, **라쏘 회귀**는 **L1 패널티(기울기 절댓값)**를 사용하여 모델의 과적합을 방지하고 분산을 줄이는 강력한 정규화 기법입니다. 특히, 쓸모없는 변수의 매개변수를 **완전히 0으로 축소하여 모델에서 해당 변수를 제거**함으로써, 모델을 **단순화하고 해석을 용이하게 하며** 쓸모없는 변수가 많은 상황에서 예측 성능을 향상시키는 데 탁월한 효과를 발휘합니다. 이는 **릿지 회귀가 매개변수를 0에 가깝게만 축소할 수 있다는 점**과 대조되는 라쏘 회귀만의 독특한 강점입니다.


## 27. Ridge vs Lasso 회귀분석, 시각적으로 설명
**릿지(Ridge) 회귀 분석**과 **라쏘(Lasso) 회귀 분석**의 핵심적인 차이를 시각적으로 이해할 수 있도록 돕는 자료입니다.

가장 적합한 선을 찾기 위해 **잔차제곱합(sum of squared residuals)**을 계산하는 기본적인 개념부터 시작합니다. 잔차제곱합이 가장 작은 지점이 최적의 기울기를 나타내며, 이는 그래프 상에서 포물선의 가장 낮은 지점으로 표현됩니다.

이제 릿지 회귀와 라쏘 회귀가 이 기본적인 최적화 과정에 어떤 **패널티**를 추가하는지 설명합니다.

### 27.1  **릿지(Ridge) 회귀 분석 (L2 Norm)**
*   릿지 회귀는 **L2 norm** 또는 "**패널티 제곱**"이라고 불리는 패널티를 사용합니다.
*   이 패널티는 `lambda(λ) * 기울기^2` 형태로 잔차제곱합에 추가됩니다. 여기서 `lambda`는 패널티의 강도를 조절하는 값입니다.
*   **람다(λ) 값이 증가할수록**, 최적의 기울기 값은 0에 가까워집니다.
*   하지만 중요한 점은, 릿지 회귀의 패널티 커브는 **항상 부드러운 포물선 모양**을 유지하기 때문에, 아무리 람다 값을 크게 설정하더라도 최적의 기울기가 정확히 0이 되지는 않습니다. 즉, 모든 변수들의 계수를 0에 가깝게 만들지만, 완전히 제거하지는 않습니다.

### 27.2  **라쏘(Lasso) 회귀 분석 (L1 Norm)**
*   라쏘 회귀는 **L1 norm** 또는 "**절대값 패널티**"라고 불리는 패널티를 사용합니다.
*   이 패널티는 `lambda * |기울기|` 형태로 잔차제곱합에 추가됩니다.
*   **람다(λ) 값이 증가할수록**, 최적의 기울기 값은 0에 가까워집니다.
*   릿지와 달리, 라쏘의 패널티 커브는 기울기 0인 지점에서 **'구부러진' 형태**를 보입니다. 이 '구부러진' 특성 때문에, 람다 값이 특정 임계점 이상으로 커지면 **최적의 기울기 값이 정확히 0이 될 수 있습니다**.
*   최적의 기울기가 0이 된다는 것은 해당 변수가 모델에서 **제거되어 예측에 사용되지 않음**을 의미합니다 (예: 키 예측 시 몸무게 변수를 무시하게 됨). 이는 라쏘 회귀가 **특성 선택(Feature Selection)** 기능을 수행할 수 있게 하는 중요한 특징입니다.

**핵심 요약:**
*   **릿지 회귀** (L2 패널티): 기울기를 0에 가깝게 만들지만, 0으로 만들지는 않습니다. 모든 변수를 유지하면서 계수를 축소하는 효과가 있습니다.
*   **라쏘 회귀** (L1 패널티): 기울기를 0에 가깝게 만들 수 있으며, 충분히 강한 패널티가 주어지면 **특정 변수의 기울기를 정확히 0으로 만들어 해당 변수를 모델에서 제거**할 수 있습니다. 이는 특성 선택에 유용합니다.

이 두 회귀 기법의 패널티가 최적의 선(기울기)에 어떻게 다른 영향을 미치는지를 시각적으로 잘 설명하여, AI 모델의 **과적합(overfitting)을 방지**하고 **모델의 복잡성을 관리**하는 중요한 방법을 이해하는 데 큰 도움이 될 것입니다.

## 28. 엘라스틱 넷(Elastic Net) 회귀 분석
엘라스틱 넷 회귀는 앞서 다뤘던 릿지(Ridge) 회귀와 라쏘(Lasso) 회귀의 장점을 결합한 강력한 정규화 기법입니다.

### 28.1 릿지(Ridge) 및 라쏘(Lasso) 회귀 간략 복습

엘라스틱 넷을 이해하기 전에 릿지와 라쏘 회귀의 핵심 특징을 다시 상기하는 것이 좋습니다:

*   **라쏘(Lasso) 회귀**: 모델에 **불필요한 변수가 많을 때** 가장 효과적입니다. 예를 들어, 키를 예측하는 모델에서 "점성술적 별자리"나 "제비의 비행 속도"와 같은 쓸모없는 변수들은 **모델에서 제거**하여 더 단순하고 해석하기 쉬운 모델을 만듭니다. 이는 **특성 선택(Feature Selection)**에 유용합니다.
*   **릿지(Ridge) 회귀**: 모델의 대부분 변수가 **유용할 때** 가장 효과적입니다. 릿지 회귀는 모든 변수의 **계수(parameters)를 축소**하지만, 어떤 변수도 완전히 제거하지는 않습니다. 모델의 모든 변수에 대해 많은 정보를 알고 있을 때 유용합니다.

### 28.2 엘라스틱 넷(Elastic Net) 회귀의 필요성

최근 딥러닝 모델들은 수백만 개의 매개변수(variables)를 포함하는 경우가 많습니다. 이러한 모델에서는 어떤 변수가 유용한지, 어떤 변수가 불필요한지 미리 알기 어렵습니다. 이럴 때 릿지나 라쏘 중 하나를 선택하기 어렵기 때문에 **엘라스틱 넷 회귀**가 필요합니다. 엘라스틱 넷은 겉으로는 복잡해 보이지만, 릿지와 라쏘를 이해했다면 매우 간단합니다.

### 28.3 엘라스틱 넷 회귀의 작동 방식

엘라스틱 넷 회귀는 다음과 같은 특징을 가집니다:

*   **최소 제곱법(Least Squares) 기반**: 라쏘 및 릿지 회귀와 마찬가지로, 엘라스틱 넷 회귀도 최소 제곱법에서 시작합니다.
*   **두 가지 패널티의 결합**: 엘라스틱 넷은 **라쏘 회귀 패널티**와 **릿지 회귀 패널티**를 함께 사용합니다.
*   **독립적인 람다(Lambda) 값**: 라쏘 회귀 패널티(`lambda_1`)와 릿지 회귀 패널티(`lambda_2`)는 각각 **고유한 람다 값**을 가집니다. 이 두 람다 값의 최적 조합은 **교차 검증(cross-validation)**을 통해 찾아냅니다.

### 28.4 람다 값에 따른 엘라스틱 넷의 변화

`lambda_1`과 `lambda_2` 값의 조합에 따라 엘라스틱 넷은 다른 형태의 회귀 분석이 될 수 있습니다:

*   **`lambda_1 = 0` 이고 `lambda_2 = 0`**: 원래의 **최소 제곱 추정**이 됩니다.
*   **`lambda_1 > 0` 이고 `lambda_2 = 0`**: **라쏘 회귀**와 동일해집니다.
*   **`lambda_1 = 0` 이고 `lambda_2 > 0`**: **릿지 회귀**와 동일해집니다.
*   **`lambda_1 > 0` 이고 `lambda_2 > 0`**: 라쏘와 릿지의 **하이브리드** 형태, 즉 **순수한 엘라스틱 넷 회귀**가 됩니다.

### 28.5 엘라스틱 넷의 주요 장점: 상관관계 처리

엘라스틱 넷 회귀는 특히 **변수들 사이에 상관관계가 있을 때** 뛰어난 성능을 보입니다:

*   **라쏘 회귀의 한계**: 라쏘는 상관관계가 있는 여러 변수들 중에서 **하나만 선택**하고 나머지는 제거하는 경향이 있습니다.
*   **릿지 회귀의 특성**: 릿지는 상관관계가 있는 모든 변수의 **계수를 함께 축소**하는 경향이 있습니다.
*   **엘라스틱 넷의 강점**: 엘라스틱 넷은 라쏘와 릿지의 장점을 결합하여, 상관관계가 있는 변수들의 계수들을 **그룹화하여 함께 축소**하고, 필요에 따라 이들을 모두 모델에 유지하거나 **모두 한 번에 제거**할 수 있습니다.

결론적으로, 엘라스틱 넷 회귀는 라쏘와 릿지 회귀의 패널티를 결합하여 **두 기법의 장점을 모두 취하며**, 특히 **상관관계가 있는 매개변수를 더 잘 처리**한다는 장점을 가집니다. 이는 복잡한 AI 모델에서 과적합을 방지하고, 중요한 특성을 선택하며, 모델의 안정성을 높이는 데 매우 유용한 기법입니다.

## 29. R에서 릿지(Ridge),라쏘(Lasso), 그리고 **엘라스틱 넷(Elastic Net)**
**릿지(Ridge) 회귀 분석**, **라쏘(Lasso) 회귀 분석**, 그리고 **엘라스틱 넷(Elastic Net) 회귀 분석**을 R 프로그래밍 언어의 `glmnet` 라이브러리를 사용하여 구현하는 방법을 설명합니다. 특히 `glmnet`이 이 세 가지 정규화 기법을 어떻게 통합하여 사용하는지에 초점을 맞춥니다.

### 29.1 `glmnet` 라이브러리의 핵심 파라미터: `lambda`와 `alpha`

`glmnet` 라이브러리는 릿지, 라쏘, 엘라스틱 넷 회귀를 위해 두 가지 주요 파라미터를 사용합니다:

*   **`alpha`**: 이 파라미터는 0에서 1 사이의 값을 가지며, 릿지 패널티와 라쏘 패널티의 혼합 정도를 조절합니다.
    *   **`alpha = 0`**: 이 경우 라쏘 패널티는 사라지고, **릿지 회귀**만 남습니다.
    *   **`alpha = 1`**: 이 경우 릿지 패널티는 사라지고, **라쏘 회귀**만 남습니다.
    *   **`alpha`가 0과 1 사이의 값**: 이 경우 릿지와 라쏘 패널티가 혼합되어 **엘라스틱 넷 회귀**가 됩니다. 엘라스틱 넷은 특히 **상관관계가 있는 변수들을 더 잘 축소**하는 데 강점을 가집니다.
*   **`lambda`**: 이 파라미터는 회귀에 적용되는 **패널티의 전체 강도**를 제어합니다.
    *   **`lambda = 0`**: 패널티가 전혀 적용되지 않아, 선형 회귀의 경우 표준 **최소 제곱법**과 동일해집니다.
    *   **`lambda > 0`**: 엘라스틱 넷 패널티가 적용되어 매개변수 추정치를 축소하기 시작합니다.

`glmnet` 패키지를 사용할 때는 **`lambda`와 `alpha`의 다양한 값을 테스트**하여 최적의 모델을 찾습니다.

### 29.2 `glmnet`을 사용한 실제 구현 (R 예시)

영상은 가상의 데이터를 사용하여 릿지, 라쏘, 엘라스틱 넷 회귀를 구현하는 구체적인 R 코드를 보여줍니다.

1.  **데이터 준비**:
    *   `glmnet` 라이브러리를 로드하고, 재현 가능한 결과를 위해 `set.seed()`를 설정합니다.
    *   **N=1,000 샘플, P=5,000 매개변수**를 가진 가상 데이터 세트를 생성하며, 이 중 **15개 매개변수만이 결과 예측에 실제로 도움**이 되고 나머지는 노이즈입니다.
    *   데이터를 **훈련 세트 (2/3)**와 **테스트 세트 (1/3)**로 나눕니다.
2.  **릿지 회귀 (`alpha = 0`)**:
    *   `cv.glmnet()` 함수를 사용하여 훈련 데이터에 모델을 적합시킵니다. `cv`는 **교차 검증(cross-validation)**을 사용하여 최적의 `lambda` 값을 찾는다는 의미입니다 (기본값은 10겹 교차 검증).
    *   `type.measure`는 교차 검증 평가 방식으로 **MSE (Mean Squared Error)**를 사용하고, `family`는 선형 회귀를 나타내는 `gaussian`으로 설정합니다.
    *   `alpha`를 `0`으로 설정하여 릿지 회귀를 수행합니다.
    *   `predict()` 함수로 테스트 데이터에 대한 예측을 수행하고, 이때 `lambda.1se` (가장 간단한 모델을 만드는 람다 값)를 사용합니다.
    *   계산된 테스트 MSE는 14.47 정도였습니다.
3.  **라쏘 회귀 (`alpha = 1`)**:
    *   릿지 회귀와 동일한 방식으로 `cv.glmnet()`을 호출하지만, `alpha`를 `1`로 설정합니다.
    *   테스트 MSE를 계산합니다. 이 예시에서는 1.19 정도로, 릿지 회귀 (14.47)보다 훨씬 작아 **라쏘 회귀가 이 데이터에 더 적합함**을 보여줍니다.
4.  **엘라스틱 넷 회귀 (`alpha = 0.5`)**:
    *   다시 `cv.glmnet()`을 호출하지만, `alpha`를 `0.5`로 설정하여 릿지와 라쏘 패널티의 혼합을 적용합니다.
    *   테스트 MSE를 계산합니다. 이 예시에서는 1.24 정도로, 라쏘 회귀 (1.19)보다는 약간 높았지만 릿지 회귀 (14.47)보다는 훨씬 낮았습니다.
5.  **다양한 `alpha` 값 테스트**:
    *   `for` 루프를 사용하여 `alpha` 값을 0부터 1까지 0.1 단위로 변경하며 **모든 가능한 엘라스틱 넷 조합** (릿지 포함, 라쏘 포함)을 테스트합니다.
    *   각 `alpha` 값에 대한 테스트 MSE를 계산하여 결과를 데이터 프레임에 저장합니다.
    *   이 예시에서는 **`alpha = 1` (라쏘 회귀)**일 때 가장 작은 MSE를 얻어 **가장 좋은 성능**을 보였습니다. 이는 5,000개의 매개변수 중 15개만 유용하고 나머지는 노이즈인 데이터셋의 특성상, 불필요한 변수를 효과적으로 제거하는 라쏘의 강점이 발휘된 결과입니다.

**결론적으로, 이 영상은 `glmnet` 라이브러리의 `alpha`와 `lambda` 파라미터를 조절하여 릿지, 라쏘, 엘라스틱 넷 회귀를 구현하고, 특정 데이터셋에서 어떤 정규화 방법이 가장 효과적인지 비교하는 과정을 AI 입문자들에게 실용적인 예시를 통해 보여줍니다.** 이는 과적합을 방지하고 모델의 복잡성을 관리하는 중요한 기법들을 이해하는 데 필수적인 내용입니다.

## 30. 주성분 분석(PCA)

### **30.1 PCA란 무엇인가요?**
PCA는 **복잡한 데이터에서 핵심 정보를 추출하여 시각화하고 더 깊은 통찰력을 얻는 데 사용되는 통계 기법**입니다. 특히, 많은 변수(유전자, 시험 점수, 시가총액 등)를 가진 데이터를 낮은 차원(주로 2차원)으로 **차원 축소**하여 비슷한 샘플(쥐, 학생, 기업 등)들이 함께 군집하는 경향을 보여주고, 어떤 변수가 데이터 클러스터링에 가장 중요한지 알려줍니다. 특이값 분해(SVD)를 통해 단계별로 진행됩니다.

### **30.2 다차원 데이터와 시각화의 한계**
*   **하나의 유전자**: 데이터를 숫자 선에 표시할 수 있으며, 비슷한 샘플(쥐 1,2,3과 쥐 4,5,6)이 함께 군집합니다.
*   **두 개의 유전자**: 2차원 X-Y 그래프에 표시할 수 있으며, 각 유전자가 하나의 차원을 구성합니다. 샘플 군집을 시각적으로 확인할 수 있습니다.
*   **세 개의 유전자**: 3차원 그래프를 사용하여 표시할 수 있습니다.
*   **네 개 이상의 유전자**: 데이터를 더 이상 직접 시각화할 수 없습니다. 4개의 유전자를 위해서는 4개의 차원이 필요하기 때문입니다. **PCA는 이 시점에서 4개 이상의 유전자 측정값(차원)을 가진 데이터에 대해 2차원 PCA 플롯을 생성하여 시각화를 가능하게 합니다**.

### **30.3 PCA의 핵심 원리 (2차원 예시)**

*   **데이터 중심화**: 먼저, 각 유전자(변수)의 평균 측정값을 계산하고, 데이터의 중심을 파악합니다. 그 다음, 이 중심이 그래프의 원점(0,0)에 오도록 **데이터를 이동**시킵니다. 이 이동은 데이터 점들의 상대적인 분포를 변화시키지 않습니다.
*   **최적의 선 찾기 (주요 성분 1, PC1)**:
    *   데이터를 원점을 통과하는 무작위 선에 **투영**합니다.
    *   PCA는 **원점에서부터 투영된 점들까지의 거리 제곱의 합을 최대화**하는 선을 찾습니다. (이는 데이터 점들에서 선까지의 수직 거리를 최소화하는 것과 동일합니다).
    *   이렇게 찾아진 선이 **주요 성분 1 (PC1)**입니다.
*   **PC1의 의미**:
    *   PC1은 원본 유전자(변수)들의 **선형 결합**("칵테일 레시피")으로 표현될 수 있습니다. 예를 들어, PC1은 '유전자 1의 4파트와 유전자 2의 1파트를 섞은 것'으로 설명될 수 있으며, 이는 **유전자 1이 데이터 분산에 더 중요함**을 나타냅니다.
    *   이 선형 결합의 계수(예: 0.97, 0.242)는 **적재 점수(loading scores)**라고 불리며, **고유 벡터(eigenvector)** 또는 **단위 벡터(unit vector)**라고도 합니다. 고유 벡터의 길이는 1로 조정됩니다.
    *   PC1 고유벡터의 거리 제곱합(SS 거리)을 **고유값(eigenvalue)**이라고 부르며, 고유값의 제곱근은 **특이값(singular value)**입니다.
*   **주요 성분 2 (PC2)**:
    *   2차원 데이터의 경우, PC2는 PC1과 **원점을 통과하며 직교하는** 선입니다. 별도의 최적화 작업이 필요 없습니다.
    *   PC2 또한 유전자들의 선형 결합(고유 벡터)과 적재 점수를 가지며, PC2가 데이터 분산에 어떤 변수가 더 중요한지 보여줍니다.

### **30.4 PCA 플롯 생성 및 분산 설명력**
*   **플롯 회전**: PC1이 수평이 되고 PC2가 수직이 되도록 모든 것을 회전시킵니다.
*   **샘플 투영**: 각 샘플을 PC1과 PC2에 투영한 점들을 사용하여 최종 **2차원 PCA 플롯**에 샘플의 위치를 결정합니다. 이 플롯에서 비슷한 샘플들이 함께 군집하는 것을 볼 수 있습니다.
*   **분산 설명력**: 각 PC는 전체 데이터 분산 중 일부를 설명합니다.
    *   **고유값**은 원점으로부터의 투영된 점들까지의 거리 제곱합으로 계산되며, 이를 샘플 수에서 1을 뺀 값으로 나누어 **분산도**로 변환합니다.
    *   PC1이 83%, PC2가 17%를 설명한다면, 이는 PC1이 전체 분산의 대부분을 차지한다는 의미입니다.
    *   **스크리 플롯(scree plot)**은 각 PC가 차지하는 분산도의 비율을 시각적으로 보여줍니다. PC1과 PC2가 분산의 대부분(예: 94%)을 설명한다면, 2차원 PCA 그래프만으로도 원래의 고차원 데이터를 매우 정확하게 근사할 수 있습니다.

### **30.5 고차원 데이터에 대한 PCA (3개 이상의 유전자)**
*   **PC 확장**: 3개 이상의 유전자를 가진 데이터도 2차원 예시와 유사하게 진행됩니다. 데이터를 중심에 두고 PC1을 찾고, 그 다음 PC1에 직교하는 PC2를 찾습니다. 더 많은 유전자가 있다면, PC1과 PC2에 모두 직교하는 PC3 등 추가적인 주성분을 찾습니다.
*   **PC의 수**: 이론적으로 PC의 수는 변수(유전자)의 수와 같거나, 샘플의 수 중 더 작은 값과 같습니다.
*   **시각화**: 스크리 플롯을 통해 PC1과 PC2가 데이터 분산의 대부분을 설명하는 경우(예: 90% 이상), 고차원 데이터를 이 두 개의 주성분에 투영하여 2차원 PCA 그래프를 그릴 수 있습니다. 이 2차원 플롯은 많은 정보를 담고 있으며 데이터 군집을 식별하는 데 사용될 수 있습니다.
*   **주의사항**: 만약 PC3, PC4 등 낮은 순위의 PC들도 상당한 분산도를 설명한다면, 처음 두 개의 PC만을 사용하는 것이 데이터를 정확하게 대표하지 못할 수도 있습니다. 하지만 그럼에도 불구하고 군집 식별에는 유용할 수 있습니다.


PCA는 **아주 복잡하고 많은 정보를 가진 데이터를 단순하게 만들어서 우리가 눈으로 보고 이해할 수 있게 해주는 마법 같은 도구**입니다. 특히, 마치 '칵테일 레시피'처럼 어떤 변수가 데이터의 특징을 가장 잘 설명하는지 알려주어 **데이터 분석의 핵심을 파악**하는 데 큰 도움을 줍니다. 이를 통해 데이터 속의 숨겨진 그룹을 찾아내고, 중요한 요소를 식별하는 등 인공지능 모델을 개발하기 전에 데이터를 탐색하고 전처리하는 데 필수적인 기법입니다.

## 31. 주성분 분석(PCA)의 핵심 아이디어

### **31.1 PCA란 무엇인가요? (핵심 목표)**
PCA는 **겉으로 보기에는 비슷해 보이지만 사실은 중요한 차이를 가지고 있는 복잡한 데이터를 분석하고 시각화하기 위한 통계 기법**입니다. 예를 들어, 건강한 세포들처럼 보이지만 실제로는 다른 "유형"의 세포들이 존재할 수 있는데, 이러한 내부적인 차이를 파악하는 데 사용됩니다. 세포뿐만 아니라 사람, 자동차, 도시 등 다양한 대상에 적용될 수 있습니다. AI 분야에서는 방대한 양의 데이터에서 **핵심적인 특징을 추출**하고 **데이터의 구조를 이해**하는 데 필수적인 전처리 과정으로 활용됩니다.

### **31.2 왜 PCA가 필요한가요? (다차원 데이터의 문제점)**
데이터를 분석할 때, 우리는 각 대상(예: 세포)의 다양한 특성(예: 유전자가 얼마나 활성화되었는지)을 측정합니다.
*   **2개의 특성 (예: 유전자 1과 유전자 9)**: 두 특성 간의 관계는 2D 산포도에 쉽게 그려볼 수 있습니다. 예를 들어, 두 유전자가 서로 반대로 활성화된다면 음의 상관관계를 보여주며, 이는 두 세포가 서로 다른 일을 하고 있음을 암시할 수 있습니다.
*   **3개의 특성**: 3D 그래프를 사용하여 관계를 시각화할 수 있습니다. 그래프를 회전시켜가며 각 대상이 서로 어떤 관계를 갖는지 파악할 수 있습니다.
*   **4개 이상의 특성**: 하지만 4개 이상의 특성을 가진 데이터는 더 이상 직접 시각화할 수 없습니다. 단순히 2개씩 짝지어 그림을 많이 그리거나 여러 축을 가진 복잡한 그래프를 그리는 것은 "바보 같은 짓"입니다.

**이러한 문제를 해결하기 위해 PCA가 등장합니다.** PCA는 이러한 **다차원 데이터를 우리가 이해하고 볼 수 있는 2차원(또는 3차원) 그래프로 변환**시켜 줍니다.

### **31.3 PCA의 핵심 아이디어: 복잡한 관계를 2D 플롯으로!**

*   **모든 관계를 2D 점으로**: PCA는 모든 대상(예: 세포)들 간의 복잡한 상관관계를 파악하여, 그 결과를 **2차원 그래프 상의 점**으로 변환시킵니다.
*   **군집(Cluster) 형성**: 이 2차원 PCA 그래프에서 **서로 높은 상관성을 갖는 대상들은 하나의 군집(cluster)으로 묶이는 경향**을 보입니다. 이 군집들을 통해 겉으로는 알 수 없었던 "다른 일을 하고 있는 다른 유형의 세포들"을 쉽게 식별할 수 있습니다. 예를 들어, 색깔을 입혀보니 3가지 유형의 세포들이 명확히 구분되는 것을 볼 수 있습니다.
*   **축의 중요도**: PCA 그래프의 각 축은 중요한 순서대로 설정됩니다.
    *   **첫 번째 주성분 (PC1)**: 가장 중요한 축으로, 데이터에 존재하는 가장 큰 차이를 설명합니다. PC1을 따라 멀리 떨어져 있는 군집들은 그 차이가 더욱 극명하다는 의미입니다.
    *   **두 번째 주성분 (PC2)**: 두 번째로 중요한 축으로, PC1 다음으로 큰 차이를 설명합니다.

### **31.4 PCA는 "차원 축소"의 한 방법**
PCA는 복잡하고 많은 변수(차원)를 가진 데이터를 더 적은 수의 핵심적인 변수(차원)로 줄여주는 **"차원 축소(Dimension Reduction)" 기법 중 하나**입니다. 차원 축소는 AI 모델의 학습 효율성을 높이고, 데이터의 본질적인 특성을 시각적으로 이해하는 데 큰 도움을 줍니다.

### **31.5 다른 차원 축소 기법들**
PCA 외에도 데이터를 이해하고 차원을 축소하는 다양한 방법들이 있습니다.
*   **히트맵(Heatmap)**
*   **t-SNE (t-distributed Stochastic Neighbor Embedding)**
*   **MDS (Multidimensional Scaling)**

PCA는 단순히 숫자로만 가득했던 복잡한 데이터를 **우리가 눈으로 보고 이해할 수 있는 그림으로 바꿔주는 강력한 도구**입니다. 이 그림을 통해 **데이터 속에 숨겨진 그룹(군집)을 찾아내고, 데이터의 가장 중요한 특징이 무엇인지 파악**할 수 있습니다. 

## 32. 주성분 분석(PCA) 실용적인 팁

PCA는 데이터의 차원을 축소하고, 데이터 내의 가장 중요한 변동성을 파악하는 데 사용되는 강력한 통계 기법입니다. 효과적인 PCA를 위해 다음 팁들을 고려해야 합니다:

### 32.1  **변수들의 스케일(Scale) 조정**:
*   **문제점**: 만약 여러 변수들이 서로 다른 스케일을 가지고 있다면 (예: 수학 점수가 0-100점, 독해 점수가 0-10점), PCA는 스케일이 더 큰 변수에 편향될 수 있습니다. 예를 들어, 스케일이 큰 수학 점수가 독해 점수보다 10배 더 중요하게 간주되어, 첫 번째 주성분(PC1)이 대부분 수학 점수로 구성될 수 있습니다.
*   **해결책**: 각 변수의 스케일을 **동일하게 조정**해야 합니다. **표준 편차로 각 변수를 나누는 것**이 일반적인 방법입니다. 이렇게 하면 범위가 넓은 변수는 많이 스케일링되고, 범위가 좁은 변수는 최소한으로 스케일링되어 변수들이 동등하게 기여할 수 있습니다.

### 32.2  **데이터 중심화(Centering)**:
*   **중요성**: 데이터를 중심화하는 것(각 변수에서 평균을 빼는 것)은 PCA의 **가장 첫 번째이자 중요한 단계**입니다.
*   **주의사항**: 모든 PCA 프로그램이 기본적으로 이 작업을 수행하는 것은 아니므로, 사용하는 프로그램이 데이터를 중심화하는지 **반드시 확인**하거나 직접 중심화해야 합니다. 중심화하지 않고 PCA를 수행하면 예상과 다른 결과가 나올 수 있습니다.

### 32.3  **예상되는 주성분(Principal Components, PCs)의 개수**:
*   **기술적인 최대 개수**: 기술적으로는 데이터 세트에 있는 **각 변수마다 하나의 주성분**을 얻을 수 있습니다.
*   **실질적인 유효 개수**: 하지만 유의미한(고유값이 0보다 큰) 주성분의 개수는 다음과 같은 경우에 달라질 수 있습니다:
    *   **변수 간 높은 상관관계**: 만약 변수들이 100% 상관관계를 가진다면 (예: 수학 점수와 독해 점수가 완벽하게 일치), 실제로는 하나의 주성분(PC1)만으로 전체 변동성을 100% 설명할 수 있으며, 다른 주성분은 고유값이 0이 되어 무의미해집니다.
    *   **표본(샘플) 수가 변수 수보다 적은 경우**: 이 경우, **표본의 수가 유효 주성분의 개수에 상한선**을 둡니다.
        *   예를 들어, 두 개의 표본만 있다면, 두 점은 하나의 직선을 정의하므로 **하나의 주성분**만 가집니다.
        *   세 개의 표본이 있다면, 세 점은 하나의 평면을 정의하며, 평면은 두 개의 축을 가지므로 **두 개의 주성분**만 가집니다.
        *   따라서, 변수의 수가 많더라도 표본 수가 적으면 유효한 주성분의 개수는 표본 수보다 많을 수 없습니다.

## 33. R 프로그래밍 언어를 사용하여 주성분 분석(PCA)

PCA는 복잡한 데이터를 시각화하고, 데이터 내의 주요 변동성을 이해하는 데 매우 유용한 기법입니다. R에서 PCA를 수행하는 단계는 다음과 같습니다.

### 33.1  **데이터 준비**:
*   데모를 위해 가상의 데이터 세트를 생성합니다. 이 데이터 세트는 10개의 샘플(wild-type 5개, knockout 5개)과 각 샘플에서 측정된 100개의 유전자(gene)로 구성됩니다.
*   원본 데이터 행렬은 **샘플이 열(columns)이고 유전자(변수)가 행(rows)**으로 구성됩니다.

### 33.2  **`prcomp` 함수를 사용한 PCA 수행**:
*   R에서 PCA를 수행하기 위해 **`prcomp` 함수**를 호출합니다.
*   **중요**: `prcomp` 함수는 기본적으로 **샘플을 행(rows)으로, 유전자(변수)를 열(columns)로 기대**합니다.
*   따라서, 만약 원본 데이터가 샘플이 열이고 유전자가 행인 경우, `t` 함수를 사용하여 **데이터 행렬을 전치(transpose)**해야 합니다. 전치하지 않으면 유전자가 서로 어떻게 관련되어 있는지를 보여주는 그래프가 생성되어 샘플 간의 관계를 보는 목표와 달라질 수 있습니다.
*   `prcomp` 함수는 세 가지 결과를 반환합니다: `X`(주성분), `sdev`(표준 편차), `rotation`(로딩 점수).

### 33.3  **PCA 플롯 그리기**:
*   `prcomp` 함수의 결과 중 `X`는 그래프를 그리기 위한 **주성분(Principal Components, PCs)**을 포함합니다.
*   데이터 세트에 10개의 샘플이 있다면 10개의 주성분이 존재합니다.
*   **첫 번째 주성분(PC1)**은 원본 데이터에서 가장 많은 변동성을 설명하며, **두 번째 주성분(PC2)**은 두 번째로 많은 변동성을 설명합니다.
*   일반적으로 2차원 PCA 그래프를 그리기 위해 **PC1을 x축에, PC2를 y축에 사용**합니다. 때로는 PC2와 PC3을 사용할 수도 있습니다.
*   **기본 그래픽(base graphics) 함수 `plot`**을 사용하여 간단한 2D 플롯을 그릴 수 있습니다.
*   **`ggplot2` 패키지**를 사용하여 더 정교하고 많은 정보를 담는 PCA 플롯을 만들 수 있습니다.
    *   `ggplot2`를 사용하기 위해 데이터를 `ggplot2`가 선호하는 형식(샘플 ID와 XY 좌표를 포함하는 데이터 프레임)으로 먼저 포맷해야 합니다.
    *   `ggplot2`는 각 주성분이 원본 데이터에서 설명하는 **변동성의 백분율을 축 레이블에 표시**하여 그래프의 의미를 쉽게 파악할 수 있도록 해줍니다.

### 33.4  **PCA 결과 해석**:
*   **각 주성분이 설명하는 변동성 파악**:
    *   각 주성분이 원본 데이터에서 설명하는 변동성의 양을 파악하려면 `sdev` (표준 편차)의 **제곱**을 사용합니다.
    *   이 값을 백분율로 변환하면 각 주성분이 전체 변동성 중 몇 퍼센트를 설명하는지 알 수 있어 훨씬 유용합니다.
    *   **`barplot` 함수**를 사용하여 이러한 백분율을 시각화할 수 있습니다.
    *   만약 PC1이 데이터 변동성의 거의 전부를 설명한다면, 이는 그룹(예: wild-type과 knockout) 간에 **큰 차이가 존재함**을 의미합니다.
*   **로딩 점수를 통한 주요 변수(유전자) 확인**:
    *   `prcomp` 함수가 반환하는 `rotation`은 **로딩 점수(loading scores)**를 포함합니다.
    *   로딩 점수는 **어떤 유전자(변수)가 PCA 플롯에서 샘플의 위치에 가장 큰 영향**을 미치는지 알려줍니다.
    *   **큰 음수 값**을 가진 유전자는 샘플을 그래프의 왼쪽으로 밀어내고, **큰 양수 값**을 가진 유전자는 샘플을 오른쪽으로 밀어냅니다.
    *   가장 큰 영향을 미치는 유전자를 찾기 위해 로딩 점수의 **절대값**을 사용하여 크기 순으로 정렬할 수 있습니다.
    *   이를 통해 특정 그룹(예: knockout 샘플)을 오른쪽으로 밀어내는 유전자 또는 다른 그룹(예: wild-type 샘플)을 왼쪽으로 밀어내는 유전자를 식별할 수 있습니다.

## 34. 파이썬에서 주성분 분석(PCA)을 수행하는 방법

### 34.1  **필수 패키지 임포트**:
*   **Pandas**: 데이터를 쉽게 조작하기 위해 사용됩니다.
*   **Numpy**: 무작위 숫자 생성 및 다양한 수학적 연산을 수행합니다.
*   **Scikit-learn**: PCA 기능을 제공하며, 특히 `PCA` 함수가 가장 일반적으로 사용됩니다. 또한 `preprocessing` 패키지를 통해 데이터 스케일링 기능을 제공합니다.
*   **Matplotlib**: 멋진 그래프를 그리는 데 사용됩니다.
*   파이썬은 범용 프로그래밍 언어이므로 데이터 테이블, 난수 생성, 그래프 작성 기능이 내장되어 있지 않기 때문에 필요한 기능들을 패키지로 임포트합니다.

### 34.2  **예제 데이터 생성**:
*   PCA를 적용할 수 있는 가상의 데이터셋을 만듭니다.
*   100개의 유전자 이름과 10개의 샘플(5개의 야생형(WT) 및 5개의 녹아웃(KO) 샘플)로 구성된 데이터를 생성합니다.
*   `pandas` 데이터 프레임에 데이터를 저장하고, Poisson 분포를 사용하여 무작위 데이터를 채웁니다.
*   `head` 메서드와 `shape` 속성을 통해 데이터의 형태(100개의 유전자 x 10개의 샘플)를 확인합니다.

### 34.3  **데이터 전처리: 중앙 맞추기 및 스케일링**:
*   PCA를 수행하기 전, 데이터를 **중앙에 맞추고(centering)** **스케일링(scaling)**하는 과정은 매우 중요합니다.
*   중앙에 맞추면 각 유전자의 평균값이 0이 되고, 스케일링하면 각 유전자의 표준 편차가 1이 됩니다. 이는 모든 변수가 동등한 기여를 하도록 만듭니다.
*   `scikit-learn`의 `scale` 함수를 사용하며, 이 함수는 샘플이 행에 있어야 하므로, 샘플이 열에 있는 경우 데이터를 **전치(transpose)**해야 합니다.
*   `StandardScaler`의 `fit_transform` 메서드 또한 머신러닝에서 일반적으로 사용되는 스케일링 방법으로 소개됩니다.

### 34.4  **PCA 수행**:
*   `scikit-learn`은 PCA를 수행하기 위해 `PCA` 객체를 사용합니다. 이 객체는 하나의 데이터셋으로 학습된 후 다른 데이터셋에 적용될 수 있습니다.
*   스케일링된 데이터에 `fit` 메서드를 호출하여 PCA의 모든 수학적 계산을 수행합니다. 이 과정에서 **로딩 점수(loading scores)**와 각 **주성분(principal component)**이 설명하는 변동량이 계산됩니다.
*   마지막으로, 로딩 점수와 스케일링된 데이터를 기반으로 PCA 그래프를 위한 좌표를 생성합니다.

### 34.5  **PCA 결과 시각화 및 해석**:
*   **스크리 플롯(Scree Plot)**: 각 주성분이 설명하는 변동량의 비율을 계산하여 막대 그래프로 그립니다. 이 플롯을 통해 몇 개의 주성분을 최종 그래프에 포함할지 결정할 수 있습니다. 동영상 예시에서는 첫 번째 주성분이 거의 모든 변동을 설명하므로 PC1과 PC2를 사용한 2D 그래프가 데이터의 좋은 표현이 됩니다.
*   **PCA 플롯(PCA Plot)**: `matplotlib`을 사용하여 새로운 좌표를 기반으로 산점도를 그립니다. 샘플 이름이 그래프에 추가됩니다.
*   결과 해석: 예시에서 WT 샘플은 왼쪽에, KO 샘플은 오른쪽에 군집을 형성하여 서로 다른 특성을 가짐을 보여줍니다. 이는 두 샘플 그룹이 x축을 따라 명확하게 분리되어 있다는 것을 의미합니다.
*   **로딩 점수 분석**: PC1에 대한 로딩 점수를 확인하여 어떤 변수(여기서는 유전자)가 두 군집을 분리하는 데 가장 큰 영향을 미쳤는지 파악합니다. 로딩 점수의 절대값을 기준으로 정렬하여 상위 10개 유전자와 해당 점수를 출력합니다. 예시에서는 많은 유전자들이 샘플 분리에 유사하게 기여했음을 알 수 있습니다.

## 35. 선형 판별 분석(LDA): 다차원 데이터 분류를 위한 강력한 도구

선형 판별 분석(LDA)은 **지도 학습(Supervised Learning)**에서 사용되는 강력한 통계 기법으로, **다차원 데이터의 차원을 축소하면서 알려진 범주들 간의 분리 가능성을 최대화**하는 데 중점을 둡니다. 이는 복잡한 데이터를 시각화하고 분류하는 데 매우 유용합니다.

### **35.1 LDA의 필요성: 왜 LDA가 필요한가?**
*   **단일 유전자의 한계**: 특정 암 치료제의 효과를 예측하기 위해 하나의 유전자(예: 유전자 X) 발현량만으로는 치료 효과가 있는 사람(녹색 점)과 없는 사람(빨간 점)을 명확하게 구분하기 어렵습니다. 겹치는 부분이 많아 명확한 기준점을 찾기 어렵습니다.
*   **여러 유전자의 활용**: 두 개의 유전자(예: 유전자 X와 유전자 Y)를 사용하면 두 범주를 더 잘 분리할 수 있는 선을 그릴 수 있지만, 여전히 완벽하지는 않습니다.
*   **고차원 데이터의 시각화 문제**: 세 개 이상의 유전자(차원)를 사용하면 시각화가 매우 어려워집니다. 예를 들어, 3차원 그래프는 평면으로 범주를 분리하지만, 컴퓨터 화면에서 정확히 파악하기 어렵고, 4차원 또는 10,000차원 그래프는 그리는 것 자체가 불가능합니다.
*   **PCA와의 차이점**: 주성분 분석(PCA)도 차원 축소를 하지만, PCA는 데이터의 가장 큰 변동성에 초점을 맞춥니다. 반면, LDA는 **알려진 범주들 간의 분리 가능성을 최대화**하는 데 중점을 둡니다. 따라서 분류 문제를 해결할 때는 LDA가 더 적합할 수 있습니다.

### **35.2 LDA의 핵심 원리: 어떻게 범주를 분리하는가?**
LDA는 다음 두 가지 기준을 동시에 고려하여 새로운 축을 생성하고 데이터를 이 축에 투영합니다.
*   **두 범주 평균 간의 거리 최대화**: 새로운 축에 데이터가 투영되었을 때, 각 범주의 평균(예: 녹색 평균과 빨간 평균) 사이의 거리를 최대한 멀리 벌립니다.
*   **각 범주 내의 산포도(Scatter) 최소화**: 각 범주 내에서 데이터 포인트들이 평균 주변에 얼마나 퍼져 있는지를 나타내는 산포도를 최소화합니다.
*   **최적화 공식**: LDA는 '두 평균 간 거리의 제곱 / 산포도의 합'이라는 비율을 최대화하는 방식으로 이 두 기준의 균형을 최적화합니다. 분자를 제곱하는 이유는 거리가 음수가 되는 것을 방지하여 항상 양수로 만들기 위함입니다.
*   **균형의 중요성**: 두 평균 사이의 거리가 아무리 멀어도 각 범주 내의 산포도가 크면 여전히 많은 중복이 발생할 수 있습니다. LDA는 거리 최대화와 산포도 최소화를 동시에 최적화하여 **명확한 범주 분리**를 달성합니다.

### **35.3 세 개 이상의 범주 처리 방법**
범주가 세 개 이상일 경우 (예: 두 유전자와 세 가지 범주) LDA는 다음 두 가지 방식으로 작동합니다.
*   **거리 측정 방식 변경**: 단순히 두 평균 사이의 거리를 측정하는 대신, 먼저 모든 데이터의 **중심점(Central Point)**을 찾습니다. 그리고 각 범주의 중심점과 이 전체 중심점 사이의 거리를 측정하여 이를 최대화합니다. 동시에 각 범주 내의 산포도를 최소화하는 것은 동일합니다.
*   **여러 개의 새로운 축 생성**: 세 개의 범주는 기하학적으로 평면을 정의하므로, LDA는 데이터를 분리하기 위해 **두 개의 새로운 축**을 생성합니다. 이는 10,000개의 유전자(차원)를 가진 데이터에서 세 범주를 분리해야 할 때, 단 두 개의 축만으로 시각화할 수 있게 되어 매우 강력한 장점입니다. 실제 데이터 예시에서 10,000개의 유전자를 두 개의 축으로 줄여 세 범주를 시각적으로 쉽게 구별할 수 있음을 보여줍니다.

### **35.4 LDA와 PCA의 비교 및 유사점**
*   **차이점**:
    *   **PCA**: 데이터의 **가장 큰 변동성**을 설명하는 유전자에 초점을 맞춰 차원을 축소합니다.
    *   **LDA**: **알려진 범주들 간의 분리 가능성**을 최대화하는 데 초점을 맞춥니다. 실제 데이터를 사용한 비교에서 LDA가 PCA보다 범주 분리를 훨씬 더 잘 수행하는 것을 볼 수 있습니다.
*   **유사점**:
    *   **차원 축소**: 두 방법 모두 고차원 데이터를 저차원으로 축소합니다.
    *   **축의 중요도 순위**: 두 방법 모두 생성된 새로운 축들을 중요도에 따라 순위를 매깁니다 (예: LD1/PC1이 가장 큰 변동/분리를 설명).
    *   **주도 유전자 확인**: 두 방법 모두 어떤 유전자나 변수가 새로운 축을 주도하는지 확인할 수 있습니다 (PCA에서는 로딩 스코어, LDA에서는 새로운 축과 변수의 상관관계).

**결론적으로, LDA는 다차원 데이터에서 알려진 범주들을 가장 효과적으로 분리하고 분류하고자 할 때 매우 유용한 차원 축소 기법입니다.** 특히 고차원 데이터를 시각화하고 이해하기 쉽게 만들면서도 분류 성능을 극대화하는 데 탁월한 성능을 발휘합니다.

## 36. BAM

## 37. 다차원 척도법(MDS)과 주성분 좌표 분석(PCoA)
데이터 과학에서 중요한 두 가지 차원 축소 기법인 **다차원 척도법(MDS)과 주성분 좌표 분석(PCoA)**에 대해 다룹니다.

### 37.1 MDS와 PCoA의 기본 개념 및 목적
*   **MDS와 PCoA의 목표:** 샘플 간의 **거리를 2차원 그래프로 변환**하는 것입니다. 이를 통해 샘플들이 어떻게 서로 관련되어 있는지 시각적으로 파악할 수 있습니다.
*   **MDS의 종류:** 다차원 척도법에는 **고전적(Classical) 또는 메트릭(Metric) 다차원 척도법**과 **비메트릭(Non-metric) 다차원 척도법**이 있습니다. 이 동영상에서는 **고전적 다차원 척도법만을 다루며, 이는 주성분 좌표 분석(PCoA)과 정확히 동일**하다고 설명합니다.

### 37.2 PCA와의 비교 및 차이점
*   **PCA와의 유사성:** MDS와 PCoA는 주성분 분석(PCA)과 매우 유사하며, PCA의 기본 개념을 이해하고 있다면 더 쉽게 이해할 수 있습니다.
*   **핵심 차이점:**
    *   **PCA:** 샘플 간의 **상관관계(correlations)를 기반**으로 2차원 그래프를 생성합니다.
    *   **MDS 및 PCoA:** 샘플 간의 **거리(distances)를 기반**으로 2차원 그래프를 생성합니다.
*   **결과 그래프의 특징:** 두 기법 모두 최종 플롯에서 **서로 가까운 샘플일수록 더 밀접하게 군집(cluster)을 형성**합니다.

### 37.3 데이터 및 거리 계산 방법
*   **데이터 예시:** 동영상에서는 정상 세포의 유전자 전사량 데이터를 예시로 들지만, 사람, 자동차, 도시 등 다양한 유형의 데이터에도 적용될 수 있습니다. 예를 들어, 사람이라면 키, 혈압, 독서 수준 등을 측정할 수 있습니다.
*   **거리 계산의 필요성:** MDS 또는 PCoA를 수행하려면 **모든 샘플 쌍(pair of cells) 사이의 거리를 계산**해야 합니다.
*   **거리 계산 방법:**
    *   **유클리드 거리(Euclidean Distance):** 두 지점 사이의 직선 거리를 측정하는 매우 일반적인 방법입니다. 여러 유전자(변수)가 있을 경우 피타고라스의 정리를 확장하여 계산합니다.
    *   **유클리드 거리 사용 시 주의점:** 유클리드 거리를 사용할 경우, **생성되는 그래프는 PCA 그래프와 동일**하게 됩니다. 이는 선형 거리를 최소화하는 클러스터링이 선형 상관관계를 최대화하는 것과 같기 때문입니다.
    *   **다른 거리 측정 방법:** 유클리드 거리 외에도 다양한 거리 측정 방법이 있습니다.
        *   **로그 배율 변화(Log Fold Change)의 절댓값 평균:** 생물학자들이 유전자 간의 로그 배율 변화에 관심이 많기 때문에 자주 사용하는 방법입니다. 이 방법을 사용하면 음의 배율 변화가 양의 배율 변화를 상쇄하지 않도록 절댓값을 취합니다.
        *   **기타 거리:** 맨해튼 거리(Manhattan distance), 해밍 거리(Hamming distance), 대원 거리(Great Circle distance) 등이 있으며, 웹에서 찾아볼 수 있습니다.
*   **거리 선택의 중요성:** **최적의 거리를 선택하는 것은 데이터 과학의 예술(art of data science)의 일부**입니다.

### 37.4 MDS/PCoA와 PCA의 수학적 과정 및 결과
*   **수학적 과정:**
    *   **PCA:** 샘플 간의 상관관계를 계산한 다음, **아이겐 분해(eigen decomposition)**와 같은 복잡한 수학적 과정을 통해 그래프의 좌표를 얻습니다.
    *   **MDS/PCoA:** 샘플 간의 거리를 계산하는 것으로 시작한다는 점을 제외하면, **나머지 복잡한 수학적 과정은 PCA와 정확히 동일**합니다.
*   **결과물(Output):** 두 기법 모두 유사한 결과물을 제공합니다.
    *   **그래프의 좌표(coordinates for a graph)**.
    *   **각 축이 설명하는 변동의 비율(percent of variation each axis accounts for)**.
    *   **어떤 변수가 가장 큰 영향을 미치는지 결정하는 로딩 점수(loading scores)**.

결론적으로, 이 동영상은 MDS와 PCoA가 샘플 간의 거리를 기반으로 데이터의 숨겨진 구조를 2차원 공간에 시각화하는 강력한 도구임을 설명합니다. 이는 PCA와 밀접하게 관련되어 있지만, 분석의 시작점이 '상관관계'에서 '거리'로 바뀐다는 중요한 차이점이 있다.

## 38. R에서 MDS 및 PCoA 
R에서 다차원 척도법(Multi-Dimensional Scaling, MDS)과 주 좌표 분석(Principal Coordinate Analysis, PCoA)을 수행하는 방법을 설명합니다. 특히 **고전적 또는 메트릭 MDS는 PCoA와 정확히 동일하다**는 점을 강조합니다.

### **38.1 데이터 준비 및 PCA (주성분 분석) 비교**

*   **데이터 구성**: 100개의 유전자에서 측정된 값(100개 행)을 포함하며, 10개의 샘플(10개 열)로 이루어진 가상의 매트릭스를 사용합니다.
*   **샘플 유형**: 처음 5개 열은 야생형(Wild-Type, WT) 샘플이고, 마지막 5개 열은 녹아웃(Knockout, KO) 샘플입니다.
*   **PCA 수행**: 먼저 데이터셋에 대한 PCA를 수행하여 MDS와 비교합니다.
    *   PCA 플롯에서는 야생형 샘플이 그래프의 왼쪽에, 녹아웃 샘플이 오른쪽에 위치합니다.
    *   X축(PC1)은 데이터 변동의 91%를 설명하고, Y축(PC2)은 2.7%를 설명하여 야생형과 녹아웃 샘플 간에 대부분의 차이가 있음을 보여줍니다.

### **38.2 MDS/PCoA 수행: 유클리드 거리 (Euclidean Distance) 사용**

MDS/PCoA를 수행하는 주요 단계는 다음과 같습니다:

*   **1단계: 거리 행렬 생성**:
    *   `dist` 함수를 사용하여 거리 행렬을 만듭니다.
    *   샘플이 행이 되도록 매트릭스를 **전치(transpose)**하고, 각 유전자에 대한 측정값을 **중심화(center)하고 스케일링(scale)**합니다.
    *   `dist` 함수에 **유클리드 거리(Euclidean distance)** 측정법을 사용하도록 지시합니다. (참고: `dist` 함수는 6가지 다른 거리 측정법을 선택할 수 있습니다).
*   **2단계: 거리 행렬에 MDS 수행**:
    *   `CMD scale` 함수를 사용하여 거리 행렬에 대해 다차원 척도법을 수행합니다. `CMD scale`은 고전적 다차원 척도법(Classical Multi-Dimensional Scaling)을 의미합니다.
    *   고유값(eigenvalues)을 반환하도록 설정합니다. 이 고유값은 최종 MDS 플롯에서 각 축이 거리 행렬의 변동을 얼마나 설명하는지 계산하는 데 사용됩니다.
*   **3단계: 각 축이 설명하는 변동량 계산**:
    *   반환된 고유값을 사용하여 MDS 플롯의 각 축이 설명하는 변동량을 계산합니다.
*   **4단계: `ggplot2`를 위한 데이터 포맷팅**:
    *   그래프 생성을 위해 데이터를 `ggplot2` 형식으로 정리합니다.
*   **5단계: 그래프 생성**:
    *   `ggplot2`를 호출하여 시각적으로 멋진 그래프를 만듭니다.
*   **결과**: 유클리드 거리를 사용한 MDS 그래프는 PCA 그래프와 **정확히 동일**하게 나타납니다.
    *   야생형 샘플은 왼쪽에, 녹아웃 샘플은 오른쪽에 위치하며, X축은 데이터 변동의 91%, Y축은 2.7%를 설명합니다.

### **38.3 MDS/PCoA 수행: 사용자 정의 거리 측정법 사용 (예: 로그 폴드 변화의 절대값 평균)**

다른 거리 측정법을 사용할 때 어떤 변화가 있는지 살펴봅니다:

*   **목표**: 유클리드 거리 대신 **로그 폴드 변화(log fold change)의 절대값 평균**을 거리 측정법으로 사용합니다. 이는 `edgeR`의 `plotMDS` 함수가 사용하는 방식과 유사합니다.
*   **1단계: 로그2 값 계산**: 각 유전자에 대한 측정값의 로그2 값을 계산합니다.
*   **2단계: 사용자 정의 거리 행렬 생성**:
    *   `dist` 함수에 내장되어 있지 않으므로, 수동으로 빈 매트릭스를 생성하고 채웁니다.
    *   전체 행렬은 대칭이므로, 아래쪽 삼각형(lower triangle) 부분만 계산하면 됩니다.
    *   이 매트릭스에 로그 폴드 변화의 절대값 평균을 채워 넣습니다.
*   **3단계: 새로운 거리 행렬에 MDS 수행**:
    *   수동으로 만든 매트릭스를 `CMD scale`이 인식할 수 있는 '진정한 거리 행렬(true distance matrix)'로 변환합니다. (진정한 거리 행렬은 아래쪽 삼각형만 계산하면 됩니다).
    *   나머지 단계(고유값 사용, 변동량 계산, `ggplot2` 포맷팅, 그래프 생성)는 유클리드 거리를 사용했을 때와 동일하게 진행합니다.
*   **결과**: 유클리드 거리와 로그 폴드 변화의 절대값 평균을 사용한 두 MDS 플롯은 **유사하지만 동일하지는 않습니다**.
    *   새로운 그래프에서는 X축이 변동의 99.2%를 설명하여 (유클리드 거리의 91%에 비해) 더 많은 변동을 설명합니다.

MDS/PCoA는 데이터 내의 샘플 간의 관계를 시각화하는 강력한 도구입니다. 특히 **어떤 거리 측정법을 사용하는지가 플롯의 결과에 큰 영향**을 미친다는 것을 보여줍니다. 유클리드 거리를 사용하면 PCA와 동일한 결과를 얻을 수 있지만, 특정 문제에 더 적합한 다른 거리 측정법을 사용하면 데이터의 다른 중요한 측면을 드러낼 수 있습니다.

## 39. t-SNE (티스니)
고차원 데이터를 저차원 그래프로 효과적으로 시각화하는 강력한 비지도 학습 알고리즘인 t-SNE (t-distributed Stochastic Neighbor Embedding)에 대해 명확하게 설명합니다. 특히, **고차원 데이터셋의 클러스터링 정보를 저차원 공간에서도 최대한 보존하는 방법**을 이해하는 데 중점을 둡니다.

### **39.1 t-SNE란 무엇인가요?**

*   **목표**: t-SNE는 **고차원 데이터셋을 가져와서 원본 정보의 많은 부분을 유지하는 저차원 그래프(예: 2D 또는 3D)로 축소**하는 역할을 합니다.
*   **활용**: 복잡한 데이터셋(예: 유전자 발현 데이터, 이미지 데이터)에서 데이터 포인트 간의 유사성을 시각적으로 파악하고 숨겨진 패턴이나 그룹을 발견하는 데 유용합니다.
*   **PCA와의 관계**: 고차원 데이터를 저차원 그래프로 축소한다는 점에서 PCA(주성분 분석)와 유사하지만, t-SNE는 특히 데이터의 **국소적인 구조(local structure)**, 즉 가까이 있는 데이터 포인트 간의 관계를 보존하는 데 더 중점을 둡니다. PCA에 대한 이해가 있다면 t-SNE를 더 잘 이해할 수 있습니다.

### **39.2 t-SNE의 핵심 아이디어**

t-SNE는 고차원 공간(예: 2차원 산점도)에 있는 데이터의 클러스터링을 저차원 공간(예: 1차원 숫자 선)에서도 보존하는 방법을 찾습니다. 이를 위해 다음 과정을 따릅니다:

*   **초기 설정**: 데이터 포인트를 저차원 공간에 무작위로 배치합니다.
*   **점들의 이동**: t-SNE는 이 점들을 조금씩 움직여 클러스터가 형성되도록 합니다.
    *   **끌어당김(Attraction)**: 고차원 공간에서 서로 가까운 점들은 저차원 공간에서 서로에게 끌려갑니다.
    *   **밀어냄(Repulsion)**: 고차원 공간에서 서로 멀리 떨어진 점들은 저차원 공간에서 서로를 밀어냅니다.
    *   이러한 끌어당김과 밀어냄의 균형을 통해 점들이 적절한 위치를 찾게 됩니다.

### **39.3 t-SNE의 상세 작동 방식**

t-SNE는 크게 세 단계로 작동합니다:

1. **단계 1: 고차원 공간(원본 데이터)에서 모든 점 간의 유사성 결정**
*   **거리 측정**: 먼저, 두 점 사이의 거리를 측정합니다.
*   **비정규화된 유사성 계산**:
    *   관심 지점을 중심으로 하는 **정규 분포 곡선**에 거리를 플로팅합니다.
    *   관심 지점에서 곡선까지의 선 길이를 "비정규화된 유사성(unscaled similarity)"으로 정의합니다.
    *   정규 분포를 사용하므로, **멀리 떨어진 점들은 매우 낮은 유사성 값을 갖고, 가까운 점들은 높은 유사성 값**을 갖게 됩니다.
*   **유사성 점수 스케일링**:
    *   비정규화된 유사성 점수들이 합계가 1이 되도록 스케일링합니다.
    *   이 스케일링 과정은 데이터의 **밀도(density)**를 고려합니다. 데이터 밀도가 낮은 영역은 더 넓은 정규 분포 곡선을 사용합니다.
    *   t-SNE는 "혼란도(perplexity)"라는 파라미터를 사용하여 각 점 주위의 예상 밀도를 정의합니다.
*   **유사성 행렬 생성**: 최종적으로, 모든 점에 대해 이러한 유사성 점수를 계산하여 유사성 행렬을 만듭니다.
    *   유사성은 방향에 따라 다를 수 있으므로, t-SNE는 양방향 유사성 점수를 **평균**하여 대칭적인 유사성을 만듭니다.
    *   점 자신에 대한 유사성은 클러스터링에 도움이 되지 않으므로 **0으로 설정**합니다.

2. **단계 2: 저차원 공간(축소된 그래프)에서 점 간의 유사성 계산**
*   **무작위 투영**: 원본 데이터를 저차원 공간(예: 숫자 선)에 무작위로 투영합니다.
*   **유사성 계산**: 고차원 공간에서와 유사하게, 저차원 공간에 있는 점들 간의 유사성 점수를 계산합니다.
*   **T-분포 사용**: 하지만, 이때는 정규 분포 대신 **t-분포(t-distribution)**를 사용합니다.
    *   t-분포는 정규 분포보다 가운데가 낮고 꼬리 부분이 두껍습니다 (fat tails).
    *   **t-분포를 사용하는 이유**: t-분포는 저차원 공간에서 멀리 떨어진 점들 간에 약한 반발력을 발생시켜, **클러스터들이 한가운데로 뭉쳐서 알아보기 어렵게 되는 현상을 방지**하고 더 잘 분리되도록 돕습니다. 이것이 t-SNE의 이름에 't'가 붙은 이유입니다.
*   **유사성 행렬 생성**: 저차원 공간의 유사성 행렬을 만듭니다.

3. **단계 3: 두 유사성 행렬을 일치시키기 위한 점 이동**
*   t-SNE의 목표는 **저차원 공간의 유사성 행렬이 고차원 공간의 유사성 행렬과 최대한 비슷하게** 만드는 것입니다.
*   **반복적인 최적화**: t-SNE는 각 단계에서 저차원 공간의 점들을 조금씩 움직여 두 행렬이 더 유사해지도록 합니다. 이는 한 번에 해결할 수 없는 체스 게임처럼, 한 번에 한 수씩 나아가는 과정과 같습니다.

t-SNE는 고차원 데이터의 숨겨진 클러스터링 구조를 효과적으로 시각화하는 강력한 방법입니다. 특히, **t-분포를 사용하여 저차원 공간에서 클러스터들이 명확하게 분리되도록 돕는다는 점**이 중요합니다. 이 설명은 간단한 예시를 사용했지만, 복잡한 데이터셋에도 동일한 개념이 적용됩니다.

## 40. 계층적 클러스터링(Hierarchical Clustering)

계층적 클러스터링은 **데이터의 유사성을 기반으로 행 또는 열의 순서를 재정렬하여 데이터 간의 상관관계를 쉽게 파악할 수 있도록 돕는 기법**입니다. 주로 히트맵(Heat Map)과 함께 사용되며, 클러스터링을 통해 데이터 시각화가 크게 개선됩니다.

### 40.1 히트맵(Heat Map)이란?
*   **구성**: 일반적으로 열(columns)은 서로 다른 샘플을 나타내고, 행(rows)은 서로 다른 유전자(genes)의 측정값을 나타냅니다.
*   **색상**: 빨간색은 유전자의 높은 발현(High expression)을, 파란색 또는 보라색은 낮은 발현(Lower expression)을 의미합니다.
*   계층적 클러스터링은 이 히트맵의 행 또는 열을 유사성에 따라 정렬하여 데이터 패턴을 명확히 보여줍니다.

### 40.2 계층적 클러스터링의 기본 개념 및 단계
간단한 히트맵(예: 3개 샘플, 4개 유전자)을 통해 계층적 클러스터링 과정을 이해할 수 있습니다.

1.  **가장 유사한 요소 찾기**: 특정 유전자(예: 유전자 1)와 가장 유사한 다른 유전자를 찾습니다. 유사성은 샘플별 발현 패턴(색상)을 비교하여 판단합니다. 예를 들어, 유전자 1과 유전자 3은 샘플 1에서 모두 빨간색(높은 발현), 샘플 3에서 모두 파란색(낮은 발현)으로 유사합니다.
2.  **클러스터 형성**: 가장 유사한 두 유전자(예: 유전자 1과 3)를 찾아 **하나의 클러스터로 병합**합니다.
3.  **클러스터를 단일 요소처럼 취급**: 새로 형성된 클러스터를 하나의 유전자처럼 간주하고, 이 클러스터와 다른 유전자/클러스터 간의 유사성을 다시 평가합니다. 이 과정을 반복하여 가장 유사한 쌍을 계속 병합해 나갑니다.
4.  **최종 병합**: 모든 유전자가 하나의 큰 클러스터로 병합될 때까지 이 과정을 반복합니다.

### 40.3 덴드로그램(Dendrogram)
*   덴드로그램은 계층적 클러스터링 결과와 함께 제공되는 트리(tree) 구조의 다이어그램입니다.
*   **유사성과 클러스터 형성 순서**를 시각적으로 보여줍니다.
*   **가지의 길이**: 가지의 길이가 **짧을수록 더 유사**하며 먼저 형성된 클러스터를 의미합니다. 가지의 길이가 **길수록 덜 유사**하며 나중에 형성된 클러스터(예: 모든 유전자가 병합된 최종 클러스터)를 나타냅니다.

### 40.4 유사성 측정 방법 (거리 측정법)
"가장 유사하다"는 것의 정의는 임의로 선택될 수 있습니다.

*   **유클리드 거리(Euclidean Distance)**:
    *   두 유전자 간의 **직선 거리**를 측정하는 방법으로, 매우 일반적으로 사용됩니다.
    *   두 샘플에 대한 두 유전자(유전자 1, 유전자 2)의 값이 주어졌을 때, 유클리드 거리는 `sqrt((샘플 1 값의 차이)^2 + (샘플 2 값의 차이)^2)`로 계산됩니다.
    *   이는 기하학적으로 각 샘플에서의 값 차이를 변으로 하는 삼각형의 빗변 길이와 같습니다. 샘플이 많아져도 이 공식은 쉽게 확장됩니다.
*   **맨해튼 거리(Manhattan Distance)**:
    *   두 유전자 간의 **수직 및 수평 거리의 합**, 즉 "택시 거리"를 측정하는 방법입니다.
    *   차이 값들의 절댓값을 단순히 더합니다 (차이 값을 제곱하고 제곱근을 취하는 대신).
    *   기하학적으로는 각 차이값을 나타내는 선분들을 머리부터 꼬리까지 이어 붙였을 때의 총 길이에 해당합니다.
*   **선택의 중요성**: 유클리드 거리와 맨해튼 거리 중 어떤 방법을 선택하느냐에 따라 히트맵의 결과가 유사하면서도 다르게 나타날 수 있습니다. 특정 거리 측정법을 선택해야 할 생물학적 또는 물리적 이유는 없으며, **데이터에 대한 더 많은 통찰력을 제공하는 방법을 선택**하는 것이 중요합니다.

### 40.5 클러스터 비교 방법
클러스터가 형성된 후, 이 클러스터를 다른 유전자나 클러스터와 비교하는 방법에도 여러 가지가 있습니다.

*   **센트로이드(Centroid) 방법**: 각 클러스터 내 샘플 측정값의 **평균(average)**과 비교하는 방법입니다.
*   **단일 연결(Single Linkage) 방법**: 각 클러스터에서 **가장 가까운 점(closest point)**을 기준으로 비교하는 방법입니다.
*   **완전 연결(Complete Linkage) 방법**: 각 클러스터에서 **가장 먼 점(furthest point)**을 기준으로 비교하는 방법입니다. (R의 `hclust` 함수에서는 이것이 기본 설정입니다).
*   **선택의 중요성**: 이러한 클러스터 비교 방법의 선택 또한 클러스터링 결과와 데이터 표현 방식에 영향을 미칩니다. 대부분의 프로그램에는 합리적인 기본 설정이 있습니다.

계층적 클러스터링은 데이터의 유사성에 기반하여 계층적인 구조로 클러스터를 형성하는 과정입니다. 이때 유사성을 정의하는 '거리 측정법'과 클러스터들을 비교하는 '연결 방법'의 선택이 결과에 영향을 미치므로, 데이터의 특성을 고려하여 적절한 방법을 선택하는 것이 중요합니다. 덴드로그램은 이러한 클러스터 형성 과정과 유사성 정도를 시각적으로 보여주는 유용한 도구입니다.

## 41. K-평균 클러스터링(K-means Clustering)

K-평균 클러스터링은 **데이터를 미리 정한 K개의 클러스터로 나누는 알고리즘**입니다. 이 기법은 데이터의 패턴을 파악하고 유사한 데이터 포인트들을 그룹화하는 데 사용됩니다.

### 41.1 K-평균 클러스터링의 목적
*   측정값을 가진 샘플 데이터를 시각적으로 명확한 클러스터로 분류하고자 할 때 사용됩니다. 예를 들어, 다른 유형의 종양이나 세포 유형에서 얻은 측정값을 분류할 수 있습니다.
*   사람의 눈으로 명확하게 보이는 클러스터뿐만 아니라, 컴퓨터가 자동으로 클러스터를 식별하도록 합니다.

### 41.2 K-평균 클러스터링 단계
K-평균 클러스터링은 다음과 같은 단계를 반복하여 클러스터를 형성합니다.

1.  **K 값 선택**: 데이터에서 식별하고자 하는 클러스터의 개수 `K`를 선택합니다. (예: K=3).
2.  **초기 클러스터 무작위 선택**: 무작위로 K개의 서로 다른 데이터 포인트를 초기 클러스터(중심점)로 선택합니다.
3.  **거리 측정 및 할당**: 각 데이터 포인트와 K개의 초기 클러스터 간의 거리를 측정합니다.
    *   첫 번째 데이터 포인트부터 시작하여, 각 클러스터 중심점과의 거리를 계산합니다.
    *   해당 데이터 포인트를 **가장 가까운 클러스터에 할당**합니다.
    *   모든 데이터 포인트에 대해 이 과정을 반복합니다.
4.  **클러스터 평균 계산**: 각 클러스터에 할당된 모든 데이터 포인트들의 **평균(mean)을 계산하여 새로운 클러스터 중심점**으로 설정합니다.
5.  **재클러스터링 및 반복**:
    *   새로운 클러스터 중심점을 기준으로 3단계와 4단계를 반복합니다. 즉, 각 데이터 포인트를 새로운 중심점에 따라 다시 가장 가까운 클러스터에 할당하고, 각 클러스터의 새로운 평균을 계산합니다.
    *   이 과정은 **클러스터 할당이 더 이상 변경되지 않을 때까지 반복**됩니다.

### 41.3 클러스터링 품질 평가 및 반복 실행
*   초기 무작위 클러스터 선택에 따라 결과가 달라질 수 있습니다.
*   클러스터링의 품질은 각 클러스터 내의 **총 변동(total variation)을 합산**하여 평가할 수 있습니다. 변동이 낮을수록 더 좋은 클러스터링으로 간주됩니다.
*   K-평균 클러스터링은 여러 번 **다른 시작점(initial clusters)으로 전체 과정을 반복 수행**합니다. 그리고 이들 중 가장 좋은(변동이 가장 적은) 클러스터링 결과를 선택하여 반환합니다.

### 41.4 최적의 K 값 선택 방법 (엘보우 플롯 - Elbow Plot)
데이터에 따라 최적의 K 값을 명확하게 알 수 없을 때가 있습니다.

*   **다양한 K 값 시도**: `K=1`부터 시작하여 `K=2`, `K=3` 등 다양한 K 값으로 클러스터링을 시도합니다.
*   **클러스터 내 총 변동 계산**: 각 K 값에 대해 클러스터 내의 총 변동(total variation)을 계산합니다.
    *   K=1일 때는 최악의 시나리오로 변동이 가장 큽니다.
    *   K 값이 증가할수록 클러스터 내의 총 변동은 감소합니다. 궁극적으로 각 클러스터에 하나의 포인트만 있을 경우 변동은 0이 됩니다.
*   **엘보우 플롯**: K 값의 증가에 따른 **변동 감소량**을 그래프로 그립니다. 이 그래프를 **엘보우 플롯(Elbow Plot)**이라고 합니다.
    *   변동 감소량이 급격하게 줄어들다가, 어느 시점부터는 감소율이 크게 둔화되는 "팔꿈치(elbow)" 지점이 나타납니다.
    *   이 **팔꿈치 지점의 K 값을 최적의 K 값으로 선택**하는 것이 일반적입니다.

### 41.5 K-평균 클러스터링과 계층적 클러스터링의 차이점
*   **K-평균 클러스터링**: 사용자가 지정한 K개의 클러스터로 데이터를 명확하게 나누려고 시도합니다.
*   **계층적 클러스터링**: 두 데이터 포인트가 얼마나 유사한지를 쌍으로 알려주며, 계층적인 구조를 보여줍니다.

### 41.6 다양한 데이터 유형에 적용
K-평균 클러스터링은 다양한 형태의 데이터에 적용될 수 있습니다.

*   **1차원 데이터**: 선 위에 플로팅된 데이터.
*   **XY 그래프 데이터**: 2차원 공간에 플로팅된 데이터. 이 경우 유클리드 거리(Euclidean distance)를 사용하여 거리를 측정합니다.
*   **히트맵 데이터**: 여러 샘플(열)과 여러 유전자(행)를 가진 히트맵 데이터도 클러스터링할 수 있습니다.
    *   두 샘플이 있다면, 이들을 X축과 Y축으로 간주하고 XY 그래프처럼 데이터를 플로팅한 후 클러스터링할 수 있습니다.
    *   실제로 데이터를 플로팅하지 않아도, 데이터 포인트 간의 거리를 계산할 수 있다면 클러스터링이 가능합니다.
*   **유클리드 거리**:
    *   **2차원 데이터**: 피타고라스 정리와 동일하게 `sqrt((X2-X1)^2 + (Y2-Y1)^2)`로 거리를 계산합니다.
    *   **다차원 데이터**: 3개 이상의 샘플(축)이 있을 경우에도 `sqrt((차이_1)^2 + (차이_2)^2 + (차이_3)^2 + ...)`와 같이 각 차원에서의 차이 제곱의 합의 제곱근을 사용하여 유클리드 거리를 계산할 수 있습니다.

K-평균 클러스터링은 사용자가 지정한 K개의 클러스터로 데이터를 분할하는 알고리즘입니다. 무작위 초기 클러스터에서 시작하여 데이터 포인트를 가장 가까운 클러스터에 할당하고, 클러스터 중심을 재계산하는 과정을 클러스터 할당이 안정화될 때까지 반복합니다. 최적의 K 값은 엘보우 플롯을 통해 결정할 수 있으며, 유클리드 거리와 같은 거리 측정법을 사용하여 다양한 차원의 데이터에 적용될 수 있습니다.

## 42. DBSCAN(Density-Based Spatial Clustering of Applications with Noise) 클러스터링

DBSCAN은 **밀도 기반 클러스터링 알고리즘으로, 사람의 눈처럼 밀집된 데이터 영역을 클러스터로 식별하고, 밀도가 낮은 영역의 포인트를 이상치(outlier)로 분류**합니다. 특히, K-평균 클러스터링과 같은 다른 방법으로는 어려운 **중첩된(nested) 클러스터나 고차원 데이터에서 클러스터를 식별하는 데 효과적**입니다.

### 42.1 DBSCAN의 필요성 및 장점
*   **중첩된 클러스터 처리**: K-평균 클러스터링과 같은 표준 방법은 중첩된 클러스터(하나의 클러스터가 다른 클러스터를 감싸는 형태)를 식별하는 데 어려움을 겪을 수 있습니다. DBSCAN은 이러한 복잡한 구조를 잘 처리합니다.
*   **고차원 데이터**: 무게, 키와 같은 2차원 데이터를 시각적으로 클러스터링하는 것은 쉽지만, 나이, 소득 등 4개 이상의 특징(feature)을 포함하는 고차원 그래프는 시각적으로 불가능합니다. DBSCAN은 눈으로 볼 수 없는 고차원 데이터에서도 중첩된 클러스터를 식별할 수 있습니다.
*   **밀도 기반**: 클러스터를 점들의 밀집도(density)에 따라 식별합니다. 밀집도가 높은 영역을 클러스터로, 낮은 영역을 이상치로 간주합니다.

### 42.2 DBSCAN의 핵심 개념 및 매개변수
DBSCAN 알고리즘은 **두 가지 사용자 정의 매개변수**를 사용합니다.

*   **반지름 (Radius of the orange circle)**: 각 포인트 주변에 그리는 원의 반경을 결정합니다. 이 원 안에 있는 점들을 "가까운 점"으로 간주합니다.
*   **최소 이웃 수 (Minimum number of close points for a core point)**: **코어 포인트(Core Point)**를 정의하는 데 사용됩니다. 어떤 점이 코어 포인트가 되려면, 그 점의 반지름 내에 최소한 이 매개변수 값 이상의 다른 점들이 존재해야 합니다.

### 42.3 DBSCAN 알고리즘 단계

1.  **가까운 점 개수 계산**: 각 데이터 포인트 주변에 사용자 정의 반지름의 원을 그리고, 해당 원 안에 들어오는 다른 점들의 개수를 셉니다.
2.  **포인트 유형 분류**: 계산된 가까운 점 개수를 기준으로 모든 포인트를 세 가지 유형 중 하나로 분류합니다.
    *   **코어 포인트(Core Point)**: 자신의 반지름 내에 **최소 이웃 수 이상의 다른 점들을 포함**하는 점입니다. (예: 최소 4개 이상의 점을 포함하는 점)
    *   **비-코어 포인트(Non-Core Point)**: 코어 포인트가 아닌 점들을 의미합니다.
    *   **이상치(Outlier)**: 클러스터에 속하지 않는 비-코어 포인트입니다.
3.  **클러스터 형성**:
    *   **무작위 코어 포인트 선택**: 분류된 코어 포인트 중 하나를 무작위로 선택하여 첫 번째 클러스터를 시작합니다.
    *   **클러스터 확장**: 선택된 코어 포인트에 가까운 다른 코어 포인트들을 모두 해당 클러스터에 추가합니다. 이들 새로운 코어 포인트들도 클러스터를 계속 확장하는 데 사용됩니다.
    *   **비-코어 포인트 추가**: 클러스터 확장이 더 이상 불가능할 때, 클러스터 내의 코어 포인트에 가까운 **모든 비-코어 포인트**를 해당 클러스터에 추가합니다.
        *   **중요**: 비-코어 포인트는 클러스터에 합류할 수는 있지만, **클러스터를 더 이상 확장하는 데 사용되지는 않습니다**. 즉, 비-코어 포인트에 가까운 다른 비-코어 포인트는 해당 클러스터에 추가되지 않습니다 (코어 포인트에 가까워야만 추가됨).
    *   **새로운 클러스터 생성**: 모든 코어 포인트가 클러스터에 할당될 때까지 이 과정을 반복합니다. 아직 클러스터에 할당되지 않은 다른 코어 포인트가 있다면, 이를 시작점으로 새로운 클러스터를 형성합니다.
4.  **이상치 처리**: 모든 코어 포인트가 클러스터에 할당되고 클러스터 생성이 완료된 후에도, **어떤 클러스터의 코어 포인트에도 가깝지 않은 비-코어 포인트**는 최종적으로 **이상치(outliers)**로 분류됩니다.

### 42.4 클러스터 생성 순서의 중요성
*   DBSCAN은 클러스터를 순차적으로 생성합니다.
*   만약 한 비-코어 포인트가 두 개의 다른 클러스터의 코어 포인트 모두에 가까울 경우, 이 비-코어 포인트는 **먼저 생성된 클러스터에 할당**됩니다. 일단 한 클러스터에 할당되면, 다른 클러스터에 속할 자격이 없어집니다.

DBSCAN은 밀도 기반 클러스터링 기법으로, 사용자 정의 반지름과 최소 이웃 수를 통해 코어 포인트를 정의하고, 이를 기반으로 밀집된 데이터 영역을 클러스터로 식별합니다. 특히 중첩되거나 불규칙한 형태의 클러스터, 그리고 고차원 데이터에서 강력한 성능을 발휘하며, 밀도가 낮은 영역의 점들은 이상치로 분류합니다.

## 43. K-최근접 이웃(K-nearest neighbors, KNN) 알고리즘

**K-최근접 이웃(KNN) 알고리즘**은 데이터를 분류하는 매우 간단하면서도 효과적인 방법입니다. 이미 범주가 알려진 많은 데이터가 있을 때, **새로운 미분류 데이터 포인트가 어떤 범주에 속하는지 결정하는 데 사용**됩니다.

### 43.1 **작동 방식 (단계별 설명):**

1.  **1단계: 알려진 범주의 데이터셋 준비**
    *   먼저, **미리 범주가 알려진 데이터셋**(예: 장 종양의 다양한 세포 유형)으로 시작합니다. 이 데이터를 **훈련 데이터(training data)**라고 부릅니다.
    *   이 데이터는 주성분 분석(PCA)이나 계층적 클러스터링과 같은 방법을 사용하여 시각화(예: 클러스터 플롯, 히트맵)될 수 있습니다.

2.  **2단계: 미분류 새 데이터 추가**
    *   범주를 알 수 없는 새로운 데이터 포인트(예: 다른 종양에서 가져온 분류되지 않은 세포)를 플롯에 추가합니다. 목표는 이 새로운 세포가 어떤 유형의 세포와 가장 유사한지 파악하여 범주를 할당하는 것입니다.

3.  **3단계: 새 데이터 분류 (가장 가까운 이웃 찾기)**
    *   새로운 데이터 포인트를 분류하기 위해, 해당 데이터에 **가장 가까운(가장 유사한) 이웃 데이터 포인트들**을 찾아봅니다.
    *   **K 값의 의미**: KNN에서 'K'는 새로운 데이터 포인트를 분류하는 데 사용될 **가장 가까운 이웃의 수**를 의미합니다.

    *   **K=1인 경우**: 가장 가까운 이웃 하나만을 사용하여 범주를 결정합니다. 만약 가장 가까운 이웃이 녹색 범주라면, 새로운 데이터도 녹색으로 분류됩니다.

    *   **K가 1보다 큰 경우 (예: K=11)**: K개의 가장 가까운 이웃들의 범주를 조사하고 **"다수결 투표"**를 통해 가장 많은 표를 얻은 범주로 새로운 데이터를 분류합니다.
        *   예를 들어, K=11일 때 7개의 이웃이 빨간색, 3개가 주황색, 1개가 녹색이라면, 빨간색이 가장 많은 표를 얻었으므로 새로운 데이터는 빨간색으로 할당됩니다.

    *   **동점 처리**:
        *   K를 **홀수**로 설정하면 동점 상황을 피할 수 있는 경우가 많습니다.
        *   만약 여전히 동점이 발생하면, 동전을 던지거나 해당 세포에 범주를 할당하지 않기로 결정할 수 있습니다.

### 43.2 **K 값 선택에 대한 고려사항:**

*   **최적의 K 값 결정**: K의 최적 값을 결정하는 물리적 또는 생물학적인 방법은 없습니다. 여러 값을 시도해봐야 할 수 있습니다.
*   **K 값 선택 방법**: 훈련 데이터의 일부를 마치 알 수 없는 데이터처럼 취급하고, KNN 알고리즘을 사용하여 분류한 다음, 실제 범주와 얼마나 잘 일치하는지 평가하여 K 값을 선택할 수 있습니다.
*   **K 값의 영향**:
    *   **낮은 K 값 (예: K=1 또는 K=2)**: 노이즈에 취약하며, 이상치(outliers)의 영향을 크게 받을 수 있습니다.
    *   **높은 K 값**: 데이터를 평활화(smooth over)하는 효과가 있지만, 너무 높으면 샘플 수가 적은 특정 범주가 다른 범주에 의해 항상 다수결에서 밀릴 수 있습니다.

## 44. 나이브 베이즈(Naive Bayes) 알고리즘

**나이브 베이즈 분류기**는 텍스트 분류와 같은 작업에 널리 사용되는 확률 기반의 머신러닝 알고리즘입니다. 특히 **다항 나이브 베이즈(Multinomial Naive Bayes)** 분류기가 흔히 사용되며, 이 설명은 해당 분류기에 초점을 맞춥니다. **가우시안 나이브 베이즈(Gaussian Naive Bayes)** 분류기라는 또 다른 버전도 있습니다.

**주요 활용 분야**: 스팸 메시지를 일반 메시지에서 걸러내는 스팸 필터링과 같은 분류 문제에 활용됩니다.

### 44.1 **작동 방식 (단계별 설명):**

1.  **1단계: 훈련 데이터 준비 및 단어 빈도 분석 (히스토그램 생성)**
    *   먼저, **이미 분류된 메시지 데이터**(예: 친구/가족으로부터 온 '일반 메시지'와 '스팸' 메시지)를 준비합니다.
    *   각 범주(일반 메시지, 스팸) 내에 나타나는 모든 단어의 **히스토그램**을 만듭니다.
        *   예를 들어, 일반 메시지에 "dear"라는 단어가 8번, "friend"가 5번 나타나고, 스팸에는 "dear"가 2번 나타나는 식입니다.

2.  **2단계: 단어별 조건부 확률(Likelihoods) 계산**
    *   각 단어가 특정 범주에 속할 **확률**을 계산합니다. 이는 **우도(likelihood)**라고도 불립니다. 이 경우에는 확률과 우도라는 용어가 상호 교환적으로 사용될 수 있습니다.
    *   **계산 방법**: 해당 단어가 특정 범주에서 나타난 총 횟수를 그 범주 내의 전체 단어 수로 나눕니다.
        *   예: 일반 메시지에 "dear"가 나타날 확률 = (일반 메시지 내 "dear"의 총 횟수) / (일반 메시지 내 총 단어 수).
        *   예: 일반 메시지 내 "dear" 확률은 0.47, "friend" 확률은 0.29, "lunch"는 0.18, "money"는 0.06입니다.
        *   스팸 메시지 내 "dear" 확률은 0.29입니다.

3.  **3단계: 새로운 메시지 분류 (예: "dear friend")**
    *   **사전 확률(Prior Probability) 설정**: 분류되지 않은 새로운 메시지(예: "dear friend")가 주어졌을 때, 먼저 특정 메시지가 일반 메시지일 **초기 추측 확률(prior probability)**을 설정합니다. 스팸일 확률에 대해서도 동일하게 설정합니다.
        *   이 초기 추측은 훈련 데이터에서 추정할 수 있습니다. 예를 들어, 12개의 메시지 중 8개가 일반 메시지라면, 일반 메시지일 사전 확률은 8/12 = 0.67입니다. 스팸일 사전 확률은 4/12 = 0.33입니다.

    *   **스코어(Score) 계산**:
        *   **일반 메시지 스코어**: 일반 메시지일 사전 확률에 메시지에 포함된 각 단어가 일반 메시지일 확률(우도)을 곱합니다.
            *   예: (일반 메시지 사전 확률) \* (일반 메시지 내 "dear" 확률) \* (일반 메시지 내 "friend" 확률) = 0.67 \* 0.47 \* 0.29 = 0.09.
            *   이 0.09는 메시지가 "dear friend"일 때 일반 메시지일 확률에 비례하는 값입니다.
        *   **스팸 스코어**: 스팸일 사전 확률에 메시지에 포함된 각 단어가 스팸일 확률을 곱합니다.
            *   예: (스팸 사전 확률) \* (스팸 내 "dear" 확률) \* (스팸 내 "friend" 확률) = 0.33 \* 0.29 \* 0.14 = 0.01.
            *   이 0.01은 메시지가 "dear friend"일 때 스팸일 확률에 비례하는 값입니다.

    *   **분류 결정**: 계산된 스코어 중 더 높은 스코어를 가진 범주로 메시지를 분류합니다.
        *   "dear friend"의 경우, 일반 메시지 스코어(0.09)가 스팸 스코어(0.01)보다 높으므로, 이 메시지는 **일반 메시지**로 분류됩니다.

### 44.2 **"0 확률 문제"와 라플라스 스무딩(Laplace Smoothing)**

*   **문제점**: 만약 새로운 메시지("lunch money money money money")에 포함된 단어("lunch")가 특정 범주(예: 스팸)의 훈련 데이터에 **전혀 나타나지 않았다면**, 해당 단어의 확률은 0이 됩니다. 이 경우, 해당 범주에 대한 전체 스코어가 0이 되어, 다른 단어의 빈도와 관계없이 무조건 해당 범주로 분류되지 않는 문제가 발생합니다.
    *   예: 스팸 내 "lunch" 확률이 0이면, 메시지에 "lunch"가 포함된 경우 스팸 스코어는 무조건 0이 되어, 해당 메시지는 스팸으로 분류될 수 없습니다.

*   **해결책 (Add-one Smoothing 또는 라플라스 스무딩)**: 이 문제를 해결하기 위해, 각 단어의 히스토그램에 **1(알파, α)**을 더합니다. 즉, 모든 단어가 적어도 한 번은 나타났다고 가정하는 것입니다.
    *   이렇게 하면 어떤 단어의 확률도 0이 되지 않습니다.
    *   예: 스팸 내 "lunch" 확률은 (0+1) / (스팸 내 총 단어 수 + 추가된 총 카운트 수) = 1/(7+4) = 0.09가 됩니다.
    *   이 방법은 메시지가 일반 메시지이거나 스팸일 사전 확률에는 영향을 미치지 않습니다.
    *   이 수정 덕분에, "lunch money money money money"는 이제 스팸 스코어가 0보다 커지고, 실제 스팸으로 올바르게 분류될 수 있습니다.

### 44.3 **나이브 베이즈가 "나이브(Naive)"한 이유**

*   **단순한 가정**: 나이브 베이즈는 메시지의 모든 단어 순서를 동일하게 취급하며, 문법 규칙이나 일반적인 구문을 무시합니다.
    *   예: "dear friend"와 "friend dear"의 일반 메시지 스코어는 정확히 같습니다.
*   **"Bag of Words" 모델**: 언어를 단순히 단어들의 주머니(bag of words)로 간주하며, 각 메시지는 이 주머니에서 무작위로 뽑은 단어들의 묶음이라고 생각합니다.
*   **실용적인 이유**: 언어의 모든 합리적인 구문이나 관계를 추적하는 것은 불가능하기 때문에 이러한 단순화된 가정을 사용합니다.
*   **성능**: 이러한 "나이브한" 가정에도 불구하고, 나이브 베이즈는 스팸 필터링과 같은 실제 분류 문제에서 **놀랍도록 잘 작동하는 경향**이 있습니다.
*   **머신러닝 용어**: 단어 간의 관계를 무시하므로 **높은 편향(high bias)**을 가지지만, 실제로는 잘 작동하기 때문에 **낮은 분산(low variance)**을 가진다고 할 수 있습니다.

## 45. 가우시안 나이브 베이즈(Gaussian Naive Bayes)

가우시안 나이브 베이즈는 특정 범주를 예측하는 데 사용되는 머신러닝 분류 알고리즘입니다. 이 방법은 훈련 데이터 세트의 데이터를 나타내는 **가우시안 분포(Gaussian distributions)**의 이름을 따서 명명되었습니다.

**이론 학습을 위한 사전 지식**:
*   **다항 나이브 베이즈(Multinomial Naive Bayes)**의 주요 아이디어
*   **로그 함수(Log function)**
*   **정규 또는 가우시안 분포(Normal or Gaussian distribution)**
*   **확률(Probability)과 가능도(Likelihood)의 차이**

### 45.1 **가우시안 나이브 베이즈 작동 방식 (예시: 영화 'Troll 2' 선호도 예측)**

동영상에서는 1990년 영화 'Troll 2'를 좋아할지 여부를 예측하는 시나리오를 통해 가우시안 나이브 베이즈를 설명합니다.

1.  **데이터 수집 및 특징 정의**:
    *   'Troll 2'를 좋아하는 사람들과 좋아하지 않는 사람들로부터 데이터를 수집합니다.
    *   측정된 특징(feature)은 다음과 같습니다: **매일 먹는 팝콘 양, 마시는 탄산음료 양, 먹는 사탕 양**.

2.  **가우시안 분포 생성**:
    *   각 특징(팝콘, 탄산음료, 사탕)에 대해, 'Troll 2'를 좋아하는 그룹과 좋아하지 않는 그룹 각각의 **평균(mean)과 표준편차(standard deviation)**를 계산합니다.
    *   이 평균과 표준편차를 사용하여 각 특징과 그룹에 해당하는 **가우시안 분포**를 그립니다. 예를 들어, 'Troll 2'를 좋아하는 사람들의 팝콘 평균은 24이고 표준편차는 4입니다.

3.  **새로운 데이터 예측 과정**:
    *   새로운 사람이 나타나 매일 팝콘 20g, 탄산음료 500ml, 사탕 25g을 섭취한다고 가정해 봅시다. 이 사람이 'Troll 2'를 좋아할지 예측하는 과정은 다음과 같습니다:

    *   **a. 초기 추정 (사전 확률 - Prior Probabilities)**:
        *   먼저 "이 사람이 'Troll 2'를 좋아한다"는 초기 추정(사전 확률)을 합니다.
        *   이 초기 추정은 훈련 데이터에서 추정되며, 예를 들어 훈련 데이터 16명 중 8명이 'Troll 2'를 좋아했다면 초기 추정은 0.5가 됩니다.
        *   "이 사람이 'Troll 2'를 좋아하지 않는다"는 초기 추정도 마찬가지로 0.5가 됩니다.

    *   **b. 점수 계산**:
        *   **"Troll 2를 좋아한다"는 점수**는 **초기 추정**에 각 특징의 **가능도(likelihood)**를 모두 **곱한 값**입니다.
        *   가능도란, 예를 들어 "이 사람이 'Troll 2'를 좋아한다고 가정했을 때, 팝콘 20g을 먹을 가능도"를 의미합니다.
        *   **가능도**는 해당하는 데이터 값(x축 좌표)에서 해당 가우시안 분포 곡선의 **y축 좌표**입니다.
        *   수식으로 표현하면 다음과 같습니다:
            `점수(좋아함) = 사전확률(좋아함) × 가능도(팝콘 | 좋아함) × 가능도(탄산음료 | 좋아함) × 가능도(사탕 | 좋아함)`
        *   마찬가지로 **"Troll 2를 좋아하지 않는다"는 점수**도 동일한 방식으로 계산합니다.

    *   **c. 언더플로우(Underflow) 문제 방지 및 로그 함수 사용**:
        *   가능도 값들이 종종 **매우 작은 숫자**가 될 수 있습니다. 컴퓨터는 0에 너무 가까운 숫자를 정확하게 추적하는 데 한계가 있으며, 이로 인해 **언더플로우(underflow) 문제**와 오류가 발생할 수 있습니다.
        *   이 문제를 해결하기 위해 **모든 값에 로그 함수를 적용**하는 것이 좋습니다.
        *   로그 함수를 사용하면 **곱셈이 개별 로그 값들의 합으로 변환**됩니다. (예: `log(a*b*c) = log(a) + log(b) + log(c)`).
        *   자연로그(로그 밑이 e인 로그)가 통계 및 머신러닝에서 가장 흔히 사용됩니다.
        *   따라서 각 가능도의 로그 값을 계산한 후, 이 값들을 모두 더하여 최종 로그 점수를 얻습니다.

    *   **d. 점수 비교 및 분류**:
        *   **"Troll 2를 좋아한다"의 최종 로그 점수**와 **"Troll 2를 좋아하지 않는다"의 최종 로그 점수**를 비교합니다.
        *   **더 큰 로그 점수**를 가진 쪽으로 새로운 사람을 분류합니다.
        *   예시에서는 "Troll 2를 좋아하지 않는다"의 로그 점수(-48)가 "Troll 2를 좋아한다"의 로그 점수(-124)보다 크므로, 이 사람은 'Troll 2'를 좋아하지 않는다고 분류됩니다.

### 45.2 **핵심 통찰 및 특징의 중요성**

*   예시의 새로운 사람은 평균적인 'Troll 2' 비선호자보다 훨씬 많은 팝콘을 먹고, 선호자만큼의 탄산음료를 마셨지만, **사탕 섭취량이 'Troll 2' 선호자보다 훨씬 많았기 때문에** 최종적으로 'Troll 2'를 좋아하지 않는다고 분류되었습니다.
*   이는 **특정 특징(예: 사탕)**이 다른 특징(팝콘, 탄산음료)보다 분류에 **훨씬 더 큰 영향**을 미칠 수 있음을 보여줍니다.
*   어떤 특징(팝콘, 탄산음료 또는 사탕)이 가장 좋은 분류를 돕는지 결정하기 위해 **교차 검증(cross-validation)**을 사용할 수 있습니다.

## 46. 의사결정 및 분류 트리(Decision and Classification Trees)

의사결정 트리는 **어떤 진술의 참 또는 거짓 여부에 따라 결정을 내리는** 모델입니다. 특정 카테고리로 분류하는 경우 **분류 트리(Classification Tree)**라고 하며, 수치 값을 예측하는 경우 **회귀 트리(Regression Tree)**라고 합니다.

### **46.1 분류 트리의 기본 구조 및 용어**

*   **루트 노드(Root Node)**: 트리의 가장 위에 있는 노드입니다.
*   **내부 노드(Internal Nodes) 또는 브랜치(Branches)**: 화살표가 들어오고 나가는 중간 노드입니다.
*   **리프 노드(Leaf Nodes) 또는 리프(Leaves)**: 화살표가 들어오지만 나가는 화살표가 없는 최종 노드입니다. 최종 분류 또는 예측 결과가 여기에 도출됩니다.
*   **데이터 유형 혼합 가능**: 숫자 데이터와 예/아니오 데이터를 한 트리에서 혼합하여 사용할 수 있습니다.
*   **반복되는 질문 및 임계값**: 트리는 동일한 질문을 여러 번 할 수 있으며, 숫자 데이터의 임계값은 항상 같지 않을 수 있습니다.

### **46.2 원시 데이터로 분류 트리 구축하기**

분류 트리를 구축하는 과정은 다음과 같습니다:

*   **최상위 질문 결정**: 트리 맨 위에 어떤 질문을 할지 결정하는 것부터 시작합니다. 이는 각 질문이 예측에 얼마나 효과적인지 평가하여 이루어집니다.
*   **불순도(Impurity) 개념**:
    *   **불순한(Impure) 리프**: 여러 카테고리의 사람들이 섞여 있는 리프를 의미합니다 (예: 팝콘을 좋아하는 사람과 싫어하는 사람이 섞여 있음).
    *   **순수한(Pure) 리프**: 한 가지 카테고리의 사람들만 있는 리프를 의미합니다 (예: 팝콘을 싫어하는 사람들만 있음).
*   **불순도 정량화**:
    *   **지니 불순도(Gini Impurity)**: 리프의 불순도를 정량화하는 가장 인기 있는 방법 중 하나입니다. 엔트로피(Entropy)나 정보 이득(Information Gain)과 같은 다른 방법들도 있지만, 계산상 유사한 결과를 보여줍니다.
    *   **지니 불순도 계산**: 각 리프의 지니 불순도는 `1 - (YES 확률)^2 - (NO 확률)^2`로 계산합니다.
    *   **가중 평균**: 리프에 포함된 사람들의 수가 다를 수 있으므로, 전체 지니 불순도는 각 리프의 불순도를 가중 평균하여 계산합니다.
*   **숫자 데이터 처리**:
    *   나이(Age)와 같은 숫자 데이터는 먼저 **정렬**됩니다.
    *   인접한 값들의 **평균**을 계산하여 잠재적인 분할 임계값을 만듭니다.
    *   각 평균 연령 임계값에 대해 지니 불순도를 계산하고, 가장 낮은 불순도를 갖는 임계값을 선택합니다.
*   **최고의 분할 선택**: 여러 특징(예: 팝콘, 탄산음료, 나이) 중에서 **가장 낮은 지니 불순도**를 가지는 특징을 트리의 최상위 질문으로 선택합니다.
*   **재귀적 분할**: 선택된 노드가 여전히 불순한 경우, 해당 노드 내의 데이터를 기준으로 추가적인 질문(예: 팝콘을 좋아하는지, 나이가 특정 값보다 적은지)을 통해 불순도를 더 낮출 수 있는지 확인하고 **반복적으로 분할**합니다.
*   **리프 노드의 최종 값 할당**: 리프 노드에서는 **가장 많은 수의 사람들이 속하는 카테고리**가 해당 리프의 최종 출력 값(예: "Cool as Ice를 좋아한다" 또는 "좋아하지 않는다")이 됩니다.

### **46.3 구축된 트리 사용하기**
새로운 사람이 왔을 때, 구축된 트리의 질문에 따라 데이터를 내려가면서 최종 리프 노드에 도달하면 해당 리프의 출력 값이 그 사람에 대한 예측이 됩니다.

### **46.4 과적합(Overfitting) 문제 및 해결책**
*   **과적합**: 트리의 리프에 소수의 사람들만 속할 경우, 해당 리프의 예측 신뢰도가 낮아져 **과적합**이 발생할 수 있습니다.
*   **해결 방법**:
    *   **가지치기(Pruning)**: 트리의 복잡도를 줄여 과적합을 방지하는 방법입니다.
    *   **트리 성장 제한**: 리프당 최소 인원 수(예: 리프당 3명 이상)와 같은 제약을 두어 트리의 성장을 제한합니다. 이 경우 리프가 불순할 수 있지만, 예측의 정확성에 대한 더 나은 감각을 얻을 수 있습니다.
    *   **교차 검증(Cross Validation)**: 트리의 성장 제한 값을 결정하기 위해 교차 검증을 사용하여 가장 잘 작동하는 값을 선택합니다.

## 47. 의사결정 트리: 특징 선택 및 결측 데이터 처리

의사결정 트리 구축 시 발생하는 몇 가지 중요한 주제를 다룹니다.

### **47.1 특징 선택 (Feature Selection)**

의사결정 트리는 본질적으로 **자동 특징 선택** 기능을 가지고 있습니다.

*   **작동 방식**: 트리를 구축하는 과정에서 특정 질문(특징)이 **불순도(impurity) 감소에 기여하지 않는다면**, 해당 특징은 트리에 포함되지 않습니다.
    *   예를 들어, "흉통(chest pain)" 데이터가 있어도, 이것이 환자를 분리하는 데 불순도 감소에 전혀 도움이 되지 않는다면, 흉통에 대한 질문은 트리에 사용되지 않습니다. 결과적으로 트리는 "좋은 혈액 순환"과 "동맥 폐색"과 같은 더 효과적인 특징들로만 구성됩니다.
*   **과적합(Overfitting) 방지**:
    *   **과적합**이란 트리가 원본 데이터(트리 생성에 사용된 데이터)에는 잘 작동하지만, 다른 새로운 데이터셋에는 잘 작동하지 않는 현상을 말합니다. 의사결정 트리는 종종 과적합되는 경향이 있습니다.
    *   이를 방지하기 위해, **불순도 감소가 충분히 커야만 분할(split)을 허용하는 임계값(threshold)을 설정**할 수 있습니다. 즉, 분할이 충분히 큰 차이를 만들어야만 트리가 해당 특징을 사용하도록 하는 것입니다.
    *   이러한 방식을 통해 **더 단순한 트리를 얻고 과적합을 방지**할 수 있습니다. 요약하자면, 특징 선택은 트리를 과적합되지 않도록 하는 데 도움이 됩니다.

### **47.2 결측 데이터 (Missing Data) 처리**

의사결정 트리 구축 시 데이터에 결측값(missing values)이 있을 때 이를 처리하는 다양한 방법이 있습니다.

*   **가장 흔한 옵션으로 대체**:
    *   만약 어떤 특징(예: "동맥 폐색")의 값이 누락되었다면, 전체 데이터에서 해당 특징의 **가장 흔한 값** (예: '예'가 '아니오'보다 많으면 '예'로 대체)으로 채울 수 있습니다.
*   **다른 특징과의 상관관계 활용**:
    *   결측값이 있는 특징과 **가장 높은 상관관계(correlation)**를 보이는 다른 특징을 찾아 이를 기준으로 결측값을 추정할 수 있습니다.
    *   예를 들어, "동맥 폐색"과 "흉통"이 높은 상관관계를 보인다면, 흉통 여부를 통해 동맥 폐색 여부를 유추할 수 있습니다. 첫 번째, 두 번째, 세 번째 환자가 두 범주 모두에서 유사한 값을 가질 때, 네 번째 환자의 흉통이 '예'라면 동맥 폐색도 '예'로 추정할 수 있습니다.
*   **평균 또는 중앙값으로 대체**:
    *   몸무게(weight)와 같은 **수치형 데이터**에 결측값이 있는 경우, 해당 특징의 **평균(mean) 또는 중앙값(median)**으로 결측값을 대체할 수 있습니다.
*   **회귀 분석(Linear Regression) 활용**:
    *   결측값이 있는 수치형 데이터(예: 몸무게)와 **높은 상관관계를 보이는 다른 수치형 데이터**(예: 키)가 있다면, 두 특징 간의 선형 회귀 분석을 수행할 수 있습니다.
    *   **최소 제곱선(least squares line)**을 사용하여 결측값을 예측하고 채울 수 있습니다.

## 48. 회귀 트리(Regression Trees)

**회귀 트리**는 **수치형 값**을 예측하는 데 사용되는 **의사결정 트리(Decision Tree)의 한 종류**입니다. 일반적인 의사결정 트리는 나뭇잎(leaf) 노드에 참/거짓 또는 특정 이산형 범주와 같은 이산적인 범주를 가지는 **분류 트리(Classification Tree)**와 달리, **회귀 트리의 각 리프는 수치형 값을 나타냅니다**.

### **48.1 회귀 트리가 필요한 이유**

*   **선형 관계가 아닌 데이터**: 데이터가 선형 관계를 보이지 않을 때(예: 용량에 따른 약물 효과가 특정 범위에서 높고 다른 범위에서 낮은 경우), **직선(선형 회귀)으로는 정확한 예측을 하기 어렵습니다**. 이럴 때 회귀 트리가 더 나은 예측을 제공합니다.
*   **다수의 예측 변수 처리**: 용량, 나이, 성별 등 **세 가지 이상의 예측 변수를 사용하여 약물 효과를 예측해야 할 때, 그래프를 그리는 것은 매우 어렵거나 불가능합니다**. 회귀 트리는 이러한 **추가 예측 변수를 쉽게 수용**할 수 있습니다.

### **48.2 회귀 트리의 예측 방식**

*   회귀 트리는 데이터를 여러 그룹으로 나누고, **각 리프(leaf)에 해당하는 관측치들의 평균 값**을 해당 그룹의 예측 값으로 사용합니다. 예를 들어, 특정 용량 범위에 속하는 환자들의 평균 약물 효과를 예측값으로 사용하는 식입니다.

### **48.3 회귀 트리 구축 과정**

회귀 트리는 **상향식(top-down) 방식**으로 구축됩니다.

1.  **데이터 분할 기준 찾기 (단일 예측 변수)**:
    *   **잔차 제곱합(Sum of Squared Residuals, SSR) 최소화**: 데이터를 두 그룹으로 나누는 최적의 임계값(threshold)을 찾기 위해 **잔차 제곱합(SSR)**이라는 측정 기준을 사용합니다.
        *   **잔차(Residual)**: 관측된 값과 예측된 값의 차이를 의미합니다.
        *   **SSR 계산**: 각 관측치의 (관측 값 - 예측 값)의 제곱을 모두 더하여 계산합니다. 이 값이 작을수록 예측의 품질이 좋다는 것을 의미합니다.
    *   **임계값 후보 선정**: 데이터의 연속된 두 관측치들의 평균을 계산하여 임계값 후보로 사용합니다.
    *   **최적의 임계값 선택**: 모든 임계값 후보에 대해 SSR을 계산하고, **SSR이 가장 작은 임계값을 찾아 데이터 분할의 기준으로 선택**합니다. 이 임계값이 트리의 루트(root)가 됩니다.

2.  **데이터 분할 기준 찾기 (다수의 예측 변수)**:
    *   **각 예측 변수에 대한 최적의 임계값 찾기**: 각 예측 변수(예: 용량, 나이, 성별)에 대해 SSR을 최소화하는 최적의 임계값을 각각 찾습니다.
    *   **가장 낮은 SSR을 가진 후보 선택**: **모든 예측 변수의 후보들 중 가장 낮은 SSR을 가진 예측 변수와 그 임계값을 선택하여 트리의 루트로 삼습니다**.
    *   이후 과정은 단일 예측 변수일 때와 마찬가지로, 각 노드에서 남은 관측치들을 다시 분할하는 과정을 반복합니다.

3.  **과적합(Overfitting) 방지**:
    *   트리가 훈련 데이터를 완벽하게 학습하면 새로운 데이터에 대한 성능이 좋지 않을 수 있습니다. 이를 **과적합(Overfitting)**이라고 합니다.
    *   과적합을 방지하는 간단한 방법 중 하나는 **노드를 분할할 최소 관측치 수(minimum number of observations)**를 설정하는 것입니다. 일반적으로 이 최소 수는 20개이지만, 예시에서는 7개로 설정되었습니다.
    *   어떤 노드의 관측치 수가 이 최소 관측치 수보다 적으면, **더 이상 분할하지 않고 해당 노드를 리프(leaf)로 만듭니다**. 이 리프의 출력 값은 해당 노드에 있는 관측치들의 평균 약물 효과가 됩니다.

### **48.4 회귀 트리 구축 완료**

*   더 이상 분할할 수 있는 노드(리프의 관측치 수가 최소 관측치 수보다 적거나, 모든 관측치의 예측 변수 값이 같아 추가 분할이 불필요한 경우)가 없을 때까지 위 과정을 반복하면 트리 구축이 완료됩니다.

회귀 트리는 복잡한 데이터 관계를 이해하고 예측하는 강력한 도구이며, 특히 여러 예측 변수가 있을 때 그 유용성이 더욱 커집니다.

## 49. 회귀 트리 가지치기 (Regression Tree Pruning)

회귀 트리 가지치기는 **과적합(Overfitting)을 방지하고 트리의 성능을 향상**시키기 위한 중요한 과정입니다. 이번 동영상에서는 **비용 복잡도 가지치기(Cost Complexity Pruning, 일명 Weakest Link Pruning)** 방법에 대해 다룹니다.

### **49.1 가지치기가 필요한 이유: 과적합 방지**

*   **과적합(Overfitting) 문제**: 트리가 훈련 데이터(training data)에 너무 완벽하게 맞춰지면(예: 모든 작은 패턴까지 학습), 새로운 데이터(testing data)에 대한 예측 성능이 저하될 수 있습니다. 훈련 데이터에 대한 **잔차 제곱합(SSR)**은 낮을지라도, 테스트 데이터에 대한 잔차는 오히려 커질 수 있습니다.
*   **해결책**: 가지치기는 트리의 일부 잎(leaves)을 제거하고 해당 분할을 더 많은 관측치의 평균을 나타내는 단일 잎으로 대체하여 과적합을 방지합니다. 이렇게 하면 훈련 데이터에는 덜 완벽하게 맞더라도 **테스트 데이터에서는 훨씬 더 나은 성능**을 보여줄 수 있습니다.

### **49.2 비용 복잡도 가지치기(Cost Complexity Pruning)의 원리**

비용 복잡도 가지치기는 여러 가지치기 방법 중 하나로, **트리 점수(Tree Score)**를 계산하여 최적의 트리를 선택합니다.

*   **트리 점수(Tree Score) 계산**:
    *   **잔차 제곱합(Sum of Squared Residuals, SSR)**: 각 트리의 예측 정확도를 나타내는 값으로, 값이 작을수록 훈련 데이터에 더 잘 맞는다는 의미입니다. 가지를 제거할수록 SSR은 커지게 됩니다.
    *   **트리 복잡도 페널티(Tree Complexity Penalty)**: 트리의 복잡성(잎의 수)에 대한 벌점입니다. 트리의 잎 수(T)와 튜닝 매개변수 `알파(α)`의 곱으로 계산됩니다. 즉, `알파(α) * T`. 잎이 많을수록 페널티가 커집니다.
    *   **트리 점수 = SSR + (알파 * T)**. 이 점수가 가장 낮은 트리가 최적의 트리로 간주됩니다.

### **49.3 최적의 `알파(α)` 값 찾기**

`알파(α)` 값은 트리 복잡도 페널티의 가중치를 조절하는 **튜닝 매개변수(tuning parameter)**이며, 최적의 `알파` 값을 찾는 것이 중요합니다. `알파` 값에 따라 선택되는 트리가 달라질 수 있습니다.

1.  **전체 데이터로 풀 사이즈 트리 구축**: 먼저 모든 데이터를 사용하여 가지치기 되지 않은 **완전한 크기의 회귀 트리(full-sized regression tree)**를 구축합니다.
    *   `알파`가 0일 때는 트리 복잡도 페널티가 0이 되므로, SSR만으로 트리 점수를 계산하며, 이 경우 풀 사이즈 트리가 항상 가장 낮은 트리 점수를 가집니다.
2.  **`알파` 값의 시퀀스 생성**: `알파` 값을 점진적으로 증가시키면서, 각 `알파` 값에서 가장 낮은 트리 점수를 갖는 트리의 시퀀스(풀 사이즈 트리부터 하나의 잎만 남은 트리까지)를 찾습니다.
3.  **교차 검증(Cross-Validation)을 통한 최적 `알파` 선정**:
    *   **데이터 분할**: 전체 데이터셋을 훈련 데이터(training data)와 테스트 데이터(testing data)로 나눕니다.
    *   **트리 시퀀스 구축**: 앞서 찾은 `알파` 값들을 사용하여 **훈련 데이터만으로** 각 `알파`에 해당하는 최적의 가지치기된 트리 시퀀스를 구축합니다.
    *   **테스트 데이터로 SSR 계산**: 각 가지치기된 트리에 대해 **테스트 데이터만으로** 잔차 제곱합(SSR)을 계산합니다. 이 SSR 값이 가장 낮은 트리가 해당 분할에서 가장 좋은 성능을 보인 트리입니다.
    *   **반복**: 이 과정을 10-겹 교차 검증(10-fold cross-validation)과 같이 여러 번 반복합니다.
    *   **최종 `알파` 선정**: 여러 번의 반복 결과, **테스트 데이터에 대한 평균 SSR이 가장 낮은 `알파` 값을 최종 최적 `알파`로 선택**합니다. (예시에서는 `알파` = 10,000일 때 평균 SSR이 가장 낮았습니다.)

### **49.4 최종 가지치기된 트리 선택**

*   최적의 `알파` 값을 선택한 후, 이전에 전체 데이터로 구축했던 트리 시퀀스 중에서 **해당 `알파` 값에 해당하는 트리를 최종 가지치기된 트리(final pruned tree)로 선택**합니다.

비용 복잡도 가지치기는 회귀 트리가 새로운 데이터에 대해 일반화 능력을 갖추도록 돕는 강력한 방법입니다. 이를 통해 모델의 복잡성을 관리하고 예측 성능을 최적화할 수 있습니다.

## 50. 이산형 변수 인코딩 방법 핵심 정리: One-Hot, Label, Target, K-Fold Target Encoding

많은 머신러닝 알고리즘, 특히 신경망(Neural Networks)과 같은 인기 있는 알고리즘들은 이산형 변수(예: 좋아하는 색상, 우편번호)를 직접 처리하는 데 어려움을 겪습니다. 따라서 이러한 이산형 데이터를 **수치형 값으로 변환(인코딩)**하는 과정이 필수적입니다.

### **50.1 One-Hot Encoding (원-핫 인코딩)**

*   **개념**: 이산형 변수가 세 개 이상의 선택지를 가질 때 (예: 파란색, 빨간색, 초록색), **각 선택지마다 새로운 열(column)을 생성**합니다. 원래 값이 해당 선택지인 경우 1을, 그렇지 않은 경우 0을 할당합니다.
    *   **예시**: '좋아하는 색상'이 '파란색'인 경우, '파란색' 열에 1을, '빨간색'과 '초록색' 열에는 0을 넣습니다.
*   **적용 대상**: 'Troll 2를 사랑하는지 여부'(Yes/No)와 같이 두 가지 선택지만 있는 이산형 변수는 간단히 Yes를 1로, No를 0으로 대체할 수 있습니다.
*   **장점**: 대부분의 머신러닝 알고리즘이 숫자 데이터를 잘 처리할 수 있게 합니다.
*   **단점**: 선택지가 매우 많을 경우(예: 미국의 41,683개 우편번호), **너무 많은 새로운 열을 생성하여 데이터를 다루기 어렵게 만들 수 있습니다**.

### **50.2 Label Encoding (레이블 인코딩)**

*   **개념**: 이산형 변수에 많은 선택지가 있을 때, 각 선택지에 단순히 **임의의 숫자(예: 0, 1, 2)를 순서대로 할당**합니다.
    *   **예시**: 파란색을 0으로, 빨간색을 1로, 초록색을 2로 지정할 수 있습니다.
*   **장점**: 원-핫 인코딩처럼 많은 열을 생성하지 않습니다.
*   **단점**: 할당된 숫자가 **임의적**이며, 일부 머신러닝 알고리즘(예: 의사결정 트리)은 이 숫자의 순서가 어떤 의미를 가진다고 오해하여 문제가 발생할 수 있습니다. 예를 들어, 빨간색(1)과 초록색(2)을 파란색(0)보다 가깝게 묶으려 할 수 있습니다.

### **50.3 Target Encoding (타겟 인코딩)**

*   **개념**: 임의의 숫자를 할당하는 대신, **우리가 예측하려는 대상(Target) 변수의 평균 값**을 사용하여 이산형 선택지를 대체합니다.
    *   **예시**: '파란색'을 좋아하는 사람들의 'Troll 2 사랑 여부'의 평균값(예: 0.33)으로 '파란색'을 대체합니다.
*   **장점**: 임의의 순서 문제를 피하고, 타겟 변수와의 관계를 반영한 의미 있는 숫자를 할당합니다.

*   **3.1. 가중 평균(Weighted Mean)을 사용한 Target Encoding (고급 버전)**
    *   **문제점**: 특정 선택지에 대한 데이터가 적을 경우 (예: 빨간색을 좋아하는 사람이 한 명뿐일 때), 그 선택지의 평균값은 신뢰도가 낮을 수 있습니다.
    *   **해결책**: 해당 선택지의 평균값과 전체 타겟 변수의 평균값(전체 평균)을 결합한 **가중 평균**을 사용합니다.
    *   **계산**: `(옵션 평균 * n) + (전체 평균 * m) / (n + m)`.
        *   `n`: 특정 옵션에 대한 데이터 수.
        *   `m`: 전체 평균에 대한 가중치(사용자가 정의하는 **하이퍼파라미터**). `m`이 클수록 데이터가 적을 때 전체 평균에 더 가깝게 인코딩됩니다.
    *   **효과**: 데이터가 적은 옵션의 인코딩 값은 전체 평균에 가까워지고, 데이터가 많은 옵션은 원래 평균에 가까워집니다. 이 방법은 베이지안 방식과 유사하여 "베이지안 평균 인코딩(Bayesian mean encoding)"이라고도 불립니다.

### **50.4 K-Fold Target Encoding (K-겹 타겟 인코딩)**

*   **문제점: 데이터 누출(Data Leakage)**
    *   타겟 인코딩은 **예측하려는 타겟 변수를 사용하여 입력 변수(피처)의 값을 수정**합니다. 이는 **데이터 누출**이라는 "데이터 과학의 금기(no-no)"로 이어질 수 있습니다.
    *   데이터 누출은 모델이 훈련 데이터에서는 훌륭하게 작동하지만, 새로운 테스트 데이터에서는 성능이 크게 저하되는 **과적합(Overfitting)**을 유발합니다.
*   **해결책: K-Fold Target Encoding**
    *   **데이터 분할**: 전체 데이터를 `K`개의 동일한 크기의 부분집합(fold)으로 나눕니다.
    *   **인코딩 과정**:
        *   특정 부분집합(예: 부분집합 A)의 특정 옵션(예: 파란색)을 인코딩할 때, **해당 부분집합 A의 타겟 값은 무시**합니다.
        *   대신, **나머지 모든 부분집합(예: 부분집합 B)의 타겟 값만을 사용하여 가중 평균을 계산**하고, 이 값을 부분집합 A의 해당 옵션에 할당합니다.
        *   이 과정을 모든 부분집합과 모든 옵션에 대해 반복합니다.
    *   **효과**: 각 행이 자신의 타겟 값을 사용하여 인코딩 값을 계산하지 않으므로 **데이터 누출을 줄이고 과적합을 방지**합니다.
*   **Leave-One-Out Target Encoding**: `K`를 전체 데이터 행 수와 동일하게 설정하는 특별한 경우로, 각 행의 인코딩은 해당 행을 제외한 모든 다른 행의 타겟 값을 사용합니다.
*   **실용적 팁**: `K=5` 또는 Leave-One-Out 방식이 흔히 사용되며 좋은 결과를 보입니다.

이러한 인코딩 방법들은 이산형 데이터를 머신러닝 모델에 적합하게 변환하여 모델의 성능과 일반화 능력을 향상시키는 데 매우 중요합니다.

## 51. 파이썬으로 분류 트리 구축하기

**사이킷런(scikit-learn)** 라이브러리와 **비용 복잡도 가지치기(Cost Complexity Pruning)**를 사용하여 분류 트리를 처음부터 끝까지 구축하는 방법을 다룹니다. 목표는 **UCI 머신러닝 저장소(UCI Machine Learning Repository)**의 심장병 데이터를 활용하여 환자의 심장병 유무를 예측하는 것입니다. 

### 51.1 **분류 트리의 중요성**:
*   분류 트리는 의사 결정 과정을 쉽게 이해할 수 있어, 예측 결과를 상사에게 설명해야 하는 경우와 같이 의사 결정의 근거를 알아야 할 때 매우 유용합니다.
*   특히 의료 분야에서 의사 결정의 논리를 정확히 추적할 수 있어 자주 활용됩니다.
*   데이터 탐색 및 어떤 특성(feature)이나 변수가 가장 중요한지 파악하는 데도 유용합니다.

### 51.2 **분류 트리 구축 과정 주요 단계**:

1.  **데이터 임포트 및 모듈 로드**:
    *   데이터 조작을 위한 Pandas.
    *   평균 및 표준 편차 계산을 위한 NumPy.
    *   그래프를 그리기 위한 Matplotlib.
    *   분류 트리, 혼동 행렬(confusion matrix), 교차 검증을 위한 다양한 scikit-learn 모듈을 로드합니다.
    *   UCI 머신러닝 저장소의 심장병 데이터셋을 로드하며, 이는 성별, 연령, 혈압 등 여러 지표를 기반으로 심장병을 예측하는 데 사용됩니다. 데이터는 판다스(Pandas)의 `read_csv` 함수를 통해 스프레드시트와 유사한 데이터프레임(DataFrame) 형태로 로드됩니다.
    *   로드된 데이터의 열 번호를 UCI 웹사이트에서 가져온 열 이름으로 대체하여 데이터 조작을 용이하게 합니다.

2.  **결측치(Missing Data) 처리**:
    *   데이터 분석 프로젝트의 가장 큰 부분은 데이터 서식을 올바르게 지정하고 수정하는 것입니다. 이 과정의 첫 단계는 결측 데이터를 식별하고 처리하는 것입니다.
    *   `df.dtypes`를 사용하여 데이터프레임의 데이터 유형을 확인하고, `object` 유형을 가진 `ca`와 `fal` 컬럼에 문제가 있음을 식별합니다. 이 컬럼들은 숫자 대신 물음표('?')를 포함하고 있어 결측치를 나타냅니다.
    *   `unique()` 함수를 사용하여 각 컬럼의 고유값을 확인하여 물음표의 존재를 확인합니다.
    *   scikit-learn의 분류 트리는 결측치가 있는 데이터셋을 지원하지 않으므로, 이를 처리해야 합니다.
    *   결측치가 있는 행의 개수(`len` 함수 사용)를 파악한 결과, 전체 303개 행 중 6개(2%)에 불과하여 해당 행들을 제거하는 방식을 택합니다. 결측치가 있는 행을 제거한 새로운 데이터프레임(`df_no_missing`)이 생성됩니다.

3.  **데이터 형식 지정 (X와 Y 분할 및 원-핫 인코딩)**:
    *   데이터를 분류에 사용할 컬럼들(특성, **X**)과 예측하고자 하는 하나의 컬럼(타겟, **y**)으로 나눕니다. `hd`(심장병) 컬럼이 예측 대상인 **y**가 됩니다.
    *   scikit-learn 의사결정 트리는 연속형 데이터(예: 혈압, 심박수)를 기본적으로 지원하지만, 범주형 데이터(예: 흉통 유형)는 직접 지원하지 않습니다.
    *   범주형 데이터를 사용하려면 **원-핫 인코딩(One-Hot Encoding)**이라는 기법을 사용하여 범주형 데이터를 여러 개의 이진(0 또는 1) 값 컬럼으로 변환해야 합니다. 예를 들어, 흉통(chest pain) 컬럼의 4가지 유형(1, 2, 3, 4)을 각각 별도의 컬럼(`chest pain_1.0`, `chest pain_2.0` 등)으로 변환합니다.
    *   Pandas의 `get_dummies()` 함수를 사용하여 흉통, 심전도, 경사, 탈륨 심장 스캔 컬럼에 대해 원-핫 인코딩을 적용합니다.
    *   성별, 공복 혈당, 운동 유발 협심증과 같이 이미 이진(0과 1) 값을 가지는 범주형 컬럼은 추가적인 처리가 필요 없습니다.
    *   예측 대상인 **y**(심장병 여부)는 0(심장병 없음)과 1~4(다양한 정도의 심장병)의 값을 가지는데, 이 튜토리얼에서는 0보다 큰 모든 값을 1로 변환하여 단순 이진 분류 문제로 만듭니다.

4.  **예비 분류 트리 구축**:
    *   데이터를 훈련 데이터셋과 테스트 데이터셋으로 분할합니다 (`train_test_split` 함수 사용, 기본 비율 70:30).
    *   **Decision Tree Classifier**를 초기화하고 훈련 데이터에 맞춰 모델을 적합(fit)시킵니다.
    *   `plot_tree` 함수를 사용하여 생성된 트리를 시각화합니다. 초기 트리는 매우 복잡하고 거대한 형태를 가집니다.
    *   테스트 데이터셋에 대한 **혼동 행렬(Confusion Matrix)**을 그려 트리의 성능을 평가합니다. 초기 트리는 과적합(overfit)되었을 가능성이 높습니다. 과적합은 훈련 데이터에 너무 잘 맞춰져 실제 시험 데이터에서는 성능이 떨어지는 현상입니다.

5.  **비용 복잡도 가지치기를 통한 트리 최적화**:
    *   의사결정 트리는 훈련 데이터에 과적합되는 경향이 강하며, **비용 복잡도 가지치기(Cost Complexity Pruning)**는 과적합을 줄이고 정확도를 향상시키는 데 효과적인 방법입니다.
    *   가지치기 매개변수인 **알파(alpha)**의 최적값을 찾는 것이 핵심입니다. 알파 값은 가지치기 정도를 제어합니다.
    *   훈련 데이터셋과 테스트 데이터셋에 대한 트리의 정확도를 알파 값의 함수로 플로팅하여 최적의 알파 값을 시각적으로 추정할 수 있습니다. 알파가 0일 때(가지치기 없음) 훈련 데이터셋에서 가장 좋은 성능을 보이지만, 가지치기를 통해 알파 값을 늘리면 테스트 데이터셋의 정확도가 향상되는 것을 확인할 수 있습니다.
    *   단일 훈련/테스트 분할에 의존하지 않고, **교차 검증(Cross-validation)**을 사용하여 알파 값에 대한 모델의 민감도를 확인하고 보다 견고한 최적의 알파 값을 찾습니다.
    *   교차 검증을 통해 최적의 알파 값을 결정한 후, 이를 `float` 형식으로 변환하여 최종 트리를 구축하는 데 사용합니다.

6.  **최종 분류 트리 구축, 평가 및 해석**:
    *   최적의 알파 값(`ccp_alpha`)을 설정하여 최종 분류 트리를 구축하고 훈련 데이터에 적합시킵니다.
    *   가지치기된 트리로 생성된 **혼동 행렬(Confusion Matrix)**은 예비 트리보다 훨씬 향상된 분류 정확도를 보여줍니다. 이는 거대한 예비 트리가 훈련 데이터에 과적합되어 있었음을 의미합니다.
    *   최종 가지치기된 트리를 시각화하고 해석합니다.
        *   트리의 각 노드는 데이터를 분할하는 데 사용된 컬럼 이름(예: `ca <= 0.5`)을 가집니다. 조건이 참이면 왼쪽으로, 거짓이면 오른쪽으로 이동합니다.
        *   각 노드에는 **Gini 불순도(Gini impurity)**, 해당 노드의 샘플 수, 분류 수(예: 심장병 없음 118명, 심장병 있음 104명)가 표시됩니다.
        *   `class`는 다수 클래스를 나타내며, 노드 색상(주황색: 심장병 없음 다수, 파란색: 심장병 있음 다수)의 진하기는 Gini 불순도가 낮을수록(더 나은 분할) 진해집니다.
        *   **리프(leaf) 노드**는 더 이상 데이터를 분할하지 않는 최종 노드입니다.


데이터 임포트부터 결측치 처리, 원-핫 인코딩을 통한 데이터 형식 지정, 예비 분류 트리 구축, 비용 복잡도 가지치기를 통한 트리 최적화, 그리고 최종 트리의 평가 및 해석까지 **분류 트리(Classification Tree)**를 만드는 전체 과정을 상세히 다루고 있습니다. 