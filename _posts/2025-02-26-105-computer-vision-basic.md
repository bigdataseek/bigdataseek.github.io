---
title: 3ì°¨ì‹œ 5:Computer Vision Basic
layout: single
classes: wide
categories:
  - Computer Vision
toc: true # ì´ í¬ìŠ¤íŠ¸ì—ì„œ ëª©ì°¨ë¥¼ í™œì„±í™”
toc_sticky: true # ëª©ì°¨ë¥¼ ê³ ì •í• ì§€ ì—¬ë¶€ (ì„ íƒ ì‚¬í•­)
---

## 1. **ì´ë¯¸ì§€ ë°ì´í„° ì´í•´**

**1. ì´ë¯¸ì§€ í‘œí˜„ ë°©ì‹ ì‹¬í™”**
* ì´ë¯¸ì§€ëŠ” ì–´ë–»ê²Œ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì €ì¥ë ê¹Œìš”? ê¸°ë³¸ì ìœ¼ë¡œ RGBì™€ Grayscale ì™¸ì—ë„ ë‹¤ì–‘í•œ ìƒ‰ ê³µê°„(Color Space)ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìš°ë¦¬ëŠ” ì´ë¯¸ì§€ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    - **HSV(Hue, Saturation, Value):**
    HSVëŠ” ì¸ê°„ì˜ ì‹œê° ì²´ê³„ì— ë§ì¶°ì§„ ìƒ‰ìƒ í‘œí˜„ ë°©ì‹ì…ë‹ˆë‹¤.  
        - **Hue(ìƒ‰ìƒ):** ìƒ‰ì˜ ì¢…ë¥˜ (ì˜ˆ: ë¹¨ê°•, íŒŒë‘)
        - **Saturation(ì±„ë„):** ìƒ‰ì˜ ì„ ëª…ë„
        - **Value(ëª…ë„):** ìƒ‰ì˜ ë°ê¸°  

    - **YCrCb:**
        - YCrCbëŠ” ì˜ìƒ ì••ì¶• ë° ë°©ì†¡ í‘œì¤€ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìƒ‰ ê³µê°„ì…ë‹ˆë‹¤.  
        - **Y:** ë°ê¸° ì •ë³´ (Luminance)
        - **Cr, Cb:** ìƒ‰ì°¨ ì •ë³´ (Chrominance)

    - **ì´ë¯¸ì§€ ì–‘ìí™”:**
        - ì–‘ìí™”ëŠ” ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ ìˆ˜ë¥¼ ì¤„ì—¬ ì €ì¥ ê³µê°„ì„ ìµœì í™”í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. 
        - ì˜ˆë¥¼ ë“¤ì–´, 24ë¹„íŠ¸ RGB ì´ë¯¸ì§€ë¥¼ 8ë¹„íŠ¸ë¡œ ë³€í™˜í•˜ë©´ íŒŒì¼ í¬ê¸°ë¥¼ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ, ì‹œê°ì  í’ˆì§ˆì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    - **ì‹¤ìŠµ: OpenCVë¥¼ í™œìš©í•œ ìƒ‰ ê³µê°„ ë³€í™˜**

        ```python
        # !pip install opencv-python
        import cv2
        import matplotlib.pyplot as plt

        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread('example.jpg')
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # OpenCVëŠ” BGRë¡œ ë¡œë“œí•¨

        # RGB -> HSV ë³€í™˜
        image_hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)

        # ê²°ê³¼ ì‹œê°í™”
        plt.figure(figsize=(10, 5))
        plt.subplot(1, 2, 1), plt.imshow(image_rgb), plt.title('Original (RGB)')
        plt.subplot(1, 2, 2), plt.imshow(image_hsv[:, :, 0], cmap='hsv'), plt.title('HSV (Hue Channel)')
        plt.show()
        ```
    ![hue.png](/assets/images/HSV.png)


**2. ì´ë¯¸ì§€ ë°ì´í„°ì˜ ì €ì¥ í˜•ì‹ ì‹¬ì¸µ ë¶„ì„**
- JPEG, PNG, GIF ë“± ë‹¤ì–‘í•œ ì´ë¯¸ì§€ íŒŒì¼ í˜•ì‹ì´ ì¡´ì¬í•˜ë©°, ê° í˜•ì‹ì€ íŠ¹ì • ìš©ë„ì— ì í•©í•©ë‹ˆë‹¤.  
    - **JPEG:** ì†ì‹¤ ì••ì¶•, ì‚¬ì§„ì— ì í•©  
    - **PNG:** ë¬´ì†ì‹¤ ì••ì¶•, íˆ¬ëª…ë„ ì§€ì›  
    - **GIF:** ì• ë‹ˆë©”ì´ì…˜ ì§€ì›  

- **ì‹¤ìŠµ: PILì„ í™œìš©í•œ ì´ë¯¸ì§€ í˜•ì‹ ë³€í™˜**

    ```python
    from PIL import Image

    # ì´ë¯¸ì§€ ë¡œë“œ
    img = Image.open('example.jpg')

    # JPEG -> PNG ë³€í™˜
    img.save('example_converted.png', 'PNG')
    print("ì´ë¯¸ì§€ í˜•ì‹ ë³€í™˜ ì™„ë£Œ!")
    ```

## 2. **ë°ì´í„°ì…‹: ì»´í“¨í„° ë¹„ì „ì˜ í•µì‹¬ ìì›**
* ì»´í“¨í„° ë¹„ì „ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì´ í•„ìš”í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ëª‡ ê°€ì§€ ì¤‘ìš”í•œ ê³µê°œ ë°ì´í„°ì…‹ì„ ì†Œê°œí•©ë‹ˆë‹¤.

    - **ImageNet**
        - ImageNetì€ 1,000ê°œ ì´ìƒì˜ í´ë˜ìŠ¤ë¡œ êµ¬ì„±ëœ ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë¶„ë¥˜ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ë”¥ëŸ¬ë‹ ì—°êµ¬ì˜ ê¸°ë°˜ì´ ë˜ì—ˆìœ¼ë©°, ResNet, VGG ë“±ì˜ ëª¨ë¸ ê°œë°œì— í° ì—­í• ì„ í–ˆìŠµë‹ˆë‹¤.

    - **COCO**
        - COCO(Common Objects in Context)ëŠ” ê°ì²´ íƒì§€, ì„¸ê·¸ë©˜í…Œì´ì…˜, ì´ë¯¸ì§€ ìº¡ì…”ë‹ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì„ ìœ„í•œ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. ê° ì´ë¯¸ì§€ì—ëŠ” ê°ì²´ ê²½ê³„ ìƒì(Bounding Box)ì™€ ì„¸ë¶€ ë ˆì´ë¸”ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

    - **Cityscapes**
        - CityscapesëŠ” ììœ¨ ì£¼í–‰ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë„ì‹œ í™˜ê²½ ë°ì´í„°ì…‹ìœ¼ë¡œ, ì°¨ì„  ì¸ì‹, ë³´í–‰ì íƒì§€ ë“±ì— í™œìš©ë©ë‹ˆë‹¤.

- **ì‹¤ìŠµ: PyTorchë¥¼ í™œìš©í•œ ë°ì´í„°ì…‹ ë¡œë”©**
    
    ```python
    import torch
    from torchvision import datasets, transforms

    # ë°ì´í„° ì „ì²˜ë¦¬ ì •ì˜
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])

    # CIFAR-10 ë°ì´í„°ì…‹ ë¡œë”©
    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    
    # ë°ì´í„°ì…‹ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê°ì²´
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)

    # ë°ì´í„° ì‹œê°í™”
    # ë°ì´í„° ë¡œë”ë¥¼ ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´ë¡œ ë³€í™˜ í›„ nextë¡œ ì²«ë²ˆì§¸ ë°°ì¹˜ ê°€ì ¸ì˜´
    images, labels = next(iter(train_loader))
    print(f"ë°°ì¹˜ í¬ê¸°: {images.shape}, ë ˆì´ë¸”: {labels}")
    ```


## 3. **ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (Image Preprocessing)**  
- **ì‹¤ìŠµ: ë…¸ì´ì¦ˆ ì œê±° ë° ë°ê¸° ì¡°ì •(OpenCV)**  

```python
import cv2
import numpy as np

# ì´ë¯¸ì§€ ë¡œë“œ
image = cv2.imread('noisy_image.jpg')

# ë…¸ì´ì¦ˆ ì œê±° (ë¸”ëŸ¬ ì²˜ë¦¬)
denoised_image = cv2.GaussianBlur(image, (5, 5), 0)

# ë°ê¸° ì¡°ì •
brightness_adjusted = cv2.convertScaleAbs(denoised_image, alpha=1.5, beta=30)

# ê²°ê³¼ í‘œì‹œ
cv2.imshow('Original', image)
cv2.imshow('Denoised', denoised_image)
cv2.imshow('Brightness Adjusted', brightness_adjusted)
cv2.waitKey(0)
```

<img src="{{ site.url }}{{ site.baseurl }}/assets/images/denoised.png" alt="denoised image" width="500">

- **ê³¼ì œ**: ìì‹ ì˜ ì‚¬ì§„ì„ ê°€ì ¸ì™€ ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ê³  ë°ê¸°ë¥¼ ì¡°ì •í•´ë³´ê¸°.


## 4. **ê¸°ë³¸ ì´ë¯¸ì§€ ë¶„ì„ (Low-Level Vision)**  
- **ì‹¤ìŠµ: ì—ì§€ ê°ì§€(OpenCV)**  

```python
import cv2

# ì´ë¯¸ì§€ ë¡œë“œ ë° ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
image = cv2.imread('object.jpg')
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# ì—ì§€ ê°ì§€
edges = cv2.Canny(gray, threshold1=100, threshold2=200)

# ê²°ê³¼ í‘œì‹œ
cv2.imshow('Original', image)
cv2.imshow('Edges', edges)
cv2.waitKey(0)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/edge_profile.png" alt="denoised image" width="500">

- **ê³¼ì œ**: ë‹¤ì–‘í•œ ì„ê³„ê°’(threshold)ì„ ì‹¤í—˜í•˜ë©° ì—ì§€ ê°ì§€ ê²°ê³¼ë¥¼ ë¹„êµí•´ë³´ê¸°.



## 5. **íŒ¨í„´ ì¸ì‹ ë° ë¶„ë¥˜ (Pattern Recognition & Classification)**  
- **ì‹¤ìŠµ ì˜ˆì œ: ì†ê¸€ì”¨ ìˆ«ì ì¸ì‹ (MNIST ë°ì´í„°ì…‹,TensorFlow/Keras)**  

```python
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# ëª¨ë¸ ì •ì˜
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=5)

# í‰ê°€
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test Accuracy: {test_acc}")
```

- **ê³¼ì œ**: ìì‹ ì´ ì§ì ‘ ì“´ ìˆ«ì ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ë¶„ë¥˜ ê²°ê³¼ í™•ì¸í•˜ê¸°.



## 6. **ê°ì²´ íƒì§€ (Object Detection)**  
- **ì‹¤ìŠµ ì˜ˆì œ: YOLOë¥¼ ì‚¬ìš©í•œ ê°ì²´ íƒì§€**  

```python
import cv2

# YOLO ëª¨ë¸ ë¡œë“œ
net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]

# ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
image = cv2.imread('street.jpg')
height, width, channels = image.shape
blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
net.setInput(blob)
outs = net.forward(output_layers)

# ê°ì²´ íƒì§€ ê²°ê³¼ ì‹œê°í™”
for out in outs:
    for detection in out:
        scores = detection[5:]
        class_id = np.argmax(scores)
        confidence = scores[class_id]
        if confidence > 0.5:
            center_x, center_y, w, h = (detection[0:4] * np.array([width, height, width, height])).astype(int)
            x, y = int(center_x - w / 2), int(center_y - h / 2)
            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2.imshow('Detected Objects', image)
cv2.waitKey(0)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/yolo_street.png" alt="yolo street" width="400">

- **ê³¼ì œ**: ë‹¤ì–‘í•œ ì´ë¯¸ì§€ì—ì„œ ê°ì²´ íƒì§€ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  ê²½ê³„ ìƒìì˜ ì •í™•ë„ë¥¼ í‰ê°€í•˜ê¸°.



## 7. **ì‹œë§¨í‹± ì„¸ë¶„í™” (Semantic Segmentation)**  
- **ì‹¤ìŠµ: U-Netì„ ì‚¬ìš©í•œ ë„ë¡œ/ë³´í–‰ì ì„¸ë¶„í™”**  

```python
import torch
from torchvision import models, transforms
from PIL import Image

# Pretrained U-Net ëª¨ë¸ ë¡œë“œ
model = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()

# ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
image = Image.open('road.jpg')
preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
input_tensor = preprocess(image).unsqueeze(0)

# ì¶”ë¡ 
with torch.no_grad():
    output = model(input_tensor)['out'][0]
output_predictions = output.argmax(0)

# ê²°ê³¼ ì‹œê°í™”
import matplotlib.pyplot as plt
plt.imshow(output_predictions.byte().cpu().numpy())
plt.show()
```

<img src="{{ site.url }}{{ site.baseurl }}/assets/images/semantic_road.png" alt="semantic road" width="400">

- **ê³¼ì œ**: ë„ë¡œ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì™€ ë„ë¡œì™€ ë³´í–‰ìë¥¼ êµ¬ë¶„í•˜ëŠ” ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ í™•ì¸í•˜ê¸°.

## 8. **ë„ë©”ì¸ ì ì‘: ìƒˆë¡œìš´ í™˜ê²½ì—ì„œì˜ ì ìš©**
- ë„ë©”ì¸ ì ì‘(Domain Adaptation)ì€ í•œ í™˜ê²½ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì„ ë‹¤ë¥¸ í™˜ê²½ì—ì„œë„ ì˜ ì‘ë™í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ì „ í•™ìŠµëœ ImageNet ëª¨ë¸ì„ ì˜ë£Œ ì´ë¯¸ì§€ ë°ì´í„°ì— ì ìš©í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.

- **Transfer Learning**
    - Transfer Learningì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ íŠ¹ì§•ì„ í™œìš©í•˜ì—¬ ìƒˆë¡œìš´ ì‘ì—…ì— ì ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. Fine-tuningê³¼ Feature Extractionì´ ì£¼ìš” ì „ëµì…ë‹ˆë‹¤.

- **ì‹¤ìŠµ: ì‚¬ì „ í•™ìŠµëœ ResNet ëª¨ë¸ì„ í™œìš©í•œ ì „ì´ í•™ìŠµ**

```python
import torch
import torchvision.models as models
from torchvision import transforms

# ì‚¬ì „ í•™ìŠµëœ ResNet ëª¨ë¸ ë¡œë“œ
model = models.resnet18(pretrained=True)

# ë§ˆì§€ë§‰ ë ˆì´ì–´ ìˆ˜ì • (ìƒˆë¡œìš´ í´ë˜ìŠ¤ ìˆ˜ì— ë§ì¶¤)
num_ftrs = model.fc.in_features
model.fc = torch.nn.Linear(num_ftrs, 10)  # 10ê°œ í´ë˜ìŠ¤ë¡œ ë³€ê²½

# ë°ì´í„° ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
```


## 9. **3D ì´í•´ ë° ì¬êµ¬ì„± (3D Vision)**  
- **ì‹¤ìŠµ ì˜ˆì œ: Stereo Matchingì„ ì‚¬ìš©í•œ ê¹Šì´ ë§µ ìƒì„±(OpenCV)**  
- ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ
    - Middlebury ë°ì´í„°ì…‹ì—ì„œ Tsukuba ì´ë¯¸ì§€ ìŒì„ ì¶”ì²œí•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ê°„ë‹¨í•˜ê³  ì‘ì€ í¬ê¸°ì˜ ì´ë¯¸ì§€(left.png, right.png)ë¥¼ ì œê³µí•˜ë©°, ì‹¤ìŠµì— ì í•©í•©ë‹ˆë‹¤.
    - êµ¬ì²´ì ì¸ ë§í¬: [Tsukuba Stereo Pair](https://vision.middlebury.edu/stereo/data/scenes2001/) (ì—¬ê¸°ì„œ "Tsukuba" ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ).
    - scene1.row3.col2.ppm (ì™¼ìª½ ì‹œì )ì„ left.jpgë¡œ ì €ì¥.
    - scene1.row3.col4.ppm (ì˜¤ë¥¸ìª½ ì‹œì )ì„ right.jpgë¡œ ì €ì¥
    
```python
import cv2
import numpy as np

# ìŠ¤í…Œë ˆì˜¤ ì´ë¯¸ì§€ ë¡œë“œ
imgL = cv2.imread("left.jpg", 0)
imgR = cv2.imread("right.jpg", 0)

# StereoBM ì•Œê³ ë¦¬ì¦˜ ì„¤ì •
stereo = cv2.StereoBM_create(numDisparities=64, blockSize=13)
disparity = stereo.compute(imgL, imgR)

plt.figure(figsize=(10, 5))
plt.subplot(1, 3, 1), plt.imshow(imgL), plt.title('Left')
plt.subplot(1, 3, 2), plt.imshow(imgR), plt.title('Right')
plt.subplot(1, 3, 3), plt.imshow(disparity, "gray")
plt.show()
```

![stereo BM](/assets/images/stereo_bm.png)




## 10. **ë™ì‘ ì¸ì‹ ë° ë¹„ë””ì˜¤ ë¶„ì„ (Motion Analysis)**  
- **ì‹¤ìŠµ: ë°°ê²½ ì°¨ë¶„ë²•ì„ ì‚¬ìš©í•œ ì›€ì§ì„ ê°ì§€(OpenCV)**  
    - [ìƒ˜í”Œ ë™ì˜ìƒ](https://pixabay.com/ko/videos/%EC%82%AC%EB%9E%8C%EB%93%A4-%EA%B1%B0%EB%A6%AC-%EC%9A%B0%ED%81%AC%EB%9D%BC%EC%9D%B4%EB%82%98-39836/)

```python
import cv2

cap = cv2.VideoCapture('video.mp4')
fgbg = cv2.createBackgroundSubtractorMOG2()

while True:
    ret, frame = cap.read()
    if not ret:
        break

    fgmask = fgbg.apply(frame)
    cv2.imshow('Motion Detection', fgmask)

    if cv2.waitKey(30) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

- **ê²°ê³¼ ì˜ˆì‹œ**:

<video controls width="640" height="360">
    <source src="{{ site.url }}{{ site.baseurl }}/assets/videos/output_motion.mp4" type="video/mp4">   
    Your browser does not support the video tag.     
</video>

## 11. **í‰ê°€ ì§€í‘œ: ëª¨ë¸ ì„±ëŠ¥ í‰ê°€**
- ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ì§€í‘œê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.

- **IoU(Intersection over Union)**
    - IoUëŠ” ê°ì²´ íƒì§€ ë° ì„¸ê·¸ë©˜í…Œì´ì…˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì§€í‘œë¡œ, ì˜ˆì¸¡ëœ ê²½ê³„ ìƒìì™€ ì‹¤ì œ ê²½ê³„ ìƒìì˜ ê²¹ì¹˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.

    ```python
    import numpy as np

    def calculate_iou(boxA, boxB):
        """
        ë‘ ê²½ê³„ ìƒì(boxA, boxB)ì˜ IoUë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        boxA, boxB: [x_min, y_min, x_max, y_max]
        """
        # ê²¹ì¹˜ëŠ” ì˜ì—­ì˜ ì¢Œí‘œ ê³„ì‚°
        xA = max(boxA[0], boxB[0])
        yA = max(boxA[1], boxB[1])
        xB = min(boxA[2], boxB[2])
        yB = min(boxA[3], boxB[3])

        # ê²¹ì¹˜ëŠ” ì˜ì—­ì˜ ë„“ì´ ê³„ì‚°
        inter_width = max(0, xB - xA + 1)
        inter_height = max(0, yB - yA + 1)
        inter_area = inter_width * inter_height

        # ê° ë°•ìŠ¤ì˜ ë„“ì´ ê³„ì‚°
        boxA_area = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
        boxB_area = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)

        # IoU ê³„ì‚°
        iou = inter_area / float(boxA_area + boxB_area - inter_area)
        return iou

    # ì˜ˆì‹œ ê²½ê³„ ìƒì
    boxA = [50, 50, 150, 150]  # ì‹¤ì œ ê²½ê³„ ìƒì
    boxB = [70, 60, 170, 140]  # ì˜ˆì¸¡ ê²½ê³„ ìƒì

    iou_score = calculate_iou(boxA, boxB)
    print(f"IoU: {iou_score:.2f}")
    ```


- **mAP(mean Average Precision)**
    - mAPëŠ” ê°ì²´ íƒì§€ì—ì„œ ì‚¬ìš©ë˜ë©°, ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ê°ì²´ë¥¼ íƒì§€í–ˆëŠ”ì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

    ```python
    from sklearn.metrics import average_precision_score

    # ì˜ˆì¸¡ í™•ë¥ ê³¼ ì‹¤ì œ ë ˆì´ë¸”
    y_true = [1, 0, 1, 1, 0, 1]  # ì‹¤ì œ í´ë˜ìŠ¤ (1: Positive, 0: Negative)
    y_scores = [0.9, 0.4, 0.8, 0.7, 0.2, 0.6]  # ì˜ˆì¸¡ í™•ë¥ 

    # Average Precision ê³„ì‚°
    ap = average_precision_score(y_true, y_scores)
    print(f"Average Precision: {ap:.2f}")

    # mAPëŠ” ì—¬ëŸ¬ í´ë˜ìŠ¤ì— ëŒ€í•´ APë¥¼ í‰ê· í•œ ê°’ì…ë‹ˆë‹¤.
    # ë‹¨ì¼ í´ë˜ìŠ¤ì˜ ê²½ìš° APì™€ ë™ì¼í•©ë‹ˆë‹¤.
    ```

- **F1-score**
    - F1-scoreëŠ” ì •ë°€ë„(Precision)ì™€ ì¬í˜„ìœ¨(Recall)ì˜ ì¡°í™” í‰ê· ìœ¼ë¡œ, ë¶ˆê· í˜•í•œ ë°ì´í„°ì…‹ì—ì„œ ìœ ìš©

    ```python
    from sklearn.metrics import f1_score

    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’
    y_true = [0, 1, 0, 1, 1, 0]
    y_pred = [0, 1, 0, 0, 1, 0]

    # F1-score ê³„ì‚°
    f1 = f1_score(y_true, y_pred)
    print(f"F1-score: {f1:.2f}")
    ```

- **Confusion Matrix**

    ```python
    from sklearn.metrics import confusion_matrix, classification_report
    import seaborn as sns

    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’
    y_true = [0, 1, 0, 1, 1, 0]
    y_pred = [0, 1, 0, 0, 1, 0]

    # Confusion Matrix ê³„ì‚°
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

    # ë¶„ë¥˜ ë³´ê³ ì„œ ì¶œë ¥
    print(classification_report(y_true, y_pred))
    ```


## 12. **ì‘ìš© ë° ìµœì‹  íŠ¸ë Œë“œ**  
- **ì‹¤ìŠµ: GANì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìƒì„±(DCGAN, PyTorch)**  
- dcganì€ ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì•„ë‹˜ â†’ ë°˜ë“œì‹œ ê°™ì€ í´ë”ì— dcgan.py íŒŒì¼ì´ ìˆì–´ì•¼ í•¨ 

```python

import torch
from torchvision.utils import make_grid
from dcgan import Generator  # ì‚¬ì „ ì •ì˜ëœ DCGAN Generator ëª¨ë¸

# Generator ëª¨ë¸ ë¡œë“œ
netG = Generator().eval()
netG.load_state_dict(torch.load('generator.pth'))# í•™ìŠµ í›„ ì €ì¥ëœ ëª¨ë¸ íŒŒì¼

# ëœë¤ ë…¸ì´ì¦ˆ ìƒì„± ë° ì´ë¯¸ì§€ ìƒì„±
noise = torch.randn(16, 100, 1, 1)
fake_images = netG(noise)

# ê²°ê³¼ ì‹œê°í™”
grid = make_grid(fake_images, nrow=4, normalize=True)
plt.imshow(grid.permute(1, 2, 0).detach().numpy())
plt.show()
```

![fake_image](/assets/images/fake_image.png)

## **13.ì „ì´í•™ìŠµ**

- ì•„ë˜ëŠ” **CIFAR-10 ë°ì´í„°ì…‹**ì„ í™œìš©í•˜ì—¬ ì‚¬ì „ í•™ìŠµëœ ResNet ëª¨ë¸ì„ Fine-tuningí•˜ëŠ” ì „ì²´ ì½”ë“œì…ë‹ˆë‹¤. í•™ìŠµ í›„ì—ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , Confusion Matrixì™€ ë¶„ë¥˜ ë³´ê³ ì„œë¥¼ ì¶œë ¥

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models, transforms
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# 1. ë°ì´í„° ì „ì²˜ë¦¬ ì •ì˜
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # ResNet ì…ë ¥ í¬ê¸°ë¡œ ì¡°ì •
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet ì •ê·œí™”
])

# 2. ë°ì´í„°ì…‹ ë¡œë”© (CIFAR-10)

train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

# 10% ë°ì´í„° ì‚¬ìš©ğŸš€
# train_dataset = torch.utils.data.Subset(train_dataset, indices=range(0, len(train_dataset), 10))  

# 10% ë°ì´í„° ì‚¬ìš©ğŸš€
# test_dataset = torch.utils.data.Subset(test_dataset, indices=range(0, len(test_dataset), 10))  

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)

# 3. ì‚¬ì „ í•™ìŠµëœ ResNet ëª¨ë¸ ë¡œë“œ ë° ìˆ˜ì •
model = models.resnet18(pretrained=True)

# ë§ˆì§€ë§‰ ë ˆì´ì–´ ìˆ˜ì • (CIFAR-10ì€ 10ê°œ í´ë˜ìŠ¤)
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 10)  # ì¶œë ¥ ë ˆì´ì–´ë¥¼ 10ê°œ í´ë˜ìŠ¤ë¡œ ë³€ê²½

# GPU ì‚¬ìš© ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# 4. ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜
criterion = nn.CrossEntropyLoss()

# ëª¨ë¸ ì „ì²´ í•™ìŠµğŸš€(Fine-Tuning)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ í•™ìŠµğŸš€(Feature Extraction)
# optimizer = optim.Adam(model.fc.parameters(), lr=0.001)  

# 5. í•™ìŠµ ë£¨í”„
def train_model(model, train_loader, criterion, optimizer, num_epochs=5):
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

    print("í•™ìŠµ ì™„ë£Œ!")

# 6. í‰ê°€ í•¨ìˆ˜
def evaluate_model(model, test_loader):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        correct = 0
        total = 0
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            total += labels.size(0)
            correct += (preds == labels).sum().item()

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    accuracy = 100 * correct / total
    print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.2f}%")

    # Confusion Matrix ì‹œê°í™”
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # ë¶„ë¥˜ ë³´ê³ ì„œ ì¶œë ¥
    print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))

# 7. í•™ìŠµ ë° í‰ê°€ ì‹¤í–‰
train_model(model, train_loader, criterion, optimizer, num_epochs=5)
evaluate_model(model, test_loader)
```

1.  **ë°ì´í„° ì „ì²˜ë¦¬**
- `transforms.Resize((224, 224))`: ResNetì€ 224x224 í¬ê¸°ì˜ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
- `transforms.Normalize`: ImageNet ë°ì´í„°ì…‹ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.

2.  **ë°ì´í„°ì…‹ ë¡œë”©**
- CIFAR-10 ë°ì´í„°ì…‹ì€ 10ê°œì˜ í´ë˜ìŠ¤ë¡œ êµ¬ì„±ëœ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.
- `train_loader`ì™€ `test_loader`ë¥¼ í†µí•´ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì œê³µí•©ë‹ˆë‹¤.

3.  **ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ìˆ˜ì •**
- ResNetì˜ ë§ˆì§€ë§‰ Fully Connected Layer(`fc`)ë¥¼ CIFAR-10ì˜ 10ê°œ í´ë˜ìŠ¤ì— ë§ê²Œ ìˆ˜ì •í•©ë‹ˆë‹¤.

4.  **í•™ìŠµ ë£¨í”„**
- `train_model` í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ì—í¬í¬ ë™ì•ˆ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
- ê° ë°°ì¹˜ë§ˆë‹¤ ì†ì‹¤ì„ ê³„ì‚°í•˜ê³ , ì—­ì „íŒŒë¥¼ í†µí•´ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

5.  **í‰ê°€ í•¨ìˆ˜**
- `evaluate_model` í•¨ìˆ˜ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì„ í‰ê°€í•˜ê³ , ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
- Confusion Matrixì™€ ë¶„ë¥˜ ë³´ê³ ì„œë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:** 
- **í…ŒìŠ¤íŠ¸ ì •í™•ë„**
```
Epoch [1/5], Loss: 0.5625
Epoch [2/5], Loss: 0.5484
Epoch [3/5], Loss: 0.5357
Epoch [4/5], Loss: 0.5289
Epoch [5/5], Loss: 0.5363
í•™ìŠµ ì™„ë£Œ!
í…ŒìŠ¤íŠ¸ ì •í™•ë„: 75.50%
```

- **Confusion Matrix**
![Confusion Matrix](/assets/images/feature_extraction_cm.png)

- **ë¶„ë¥˜ ë³´ê³ ì„œ**
```
           precision    recall  f1-score   support

    airplane       0.60      0.89      0.72        87
  automobile       0.89      0.90      0.90       100
        bird       0.78      0.63      0.70       108
         cat       0.63      0.64      0.64       107
        deer       0.68      0.73      0.70        95
         dog       0.62      0.72      0.66        95
        frog       0.81      0.77      0.79       100
       horse       0.83      0.68      0.75       102
        ship       0.88      0.79      0.84       102
       truck       0.94      0.84      0.88       104

    accuracy                           0.76      1000
   macro avg       0.77      0.76      0.76      1000
weighted avg       0.77      0.76      0.76      1000
```

