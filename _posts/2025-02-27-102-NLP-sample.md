---
title: 11차시 2:NLP(실습예제)
layout: single
classes: wide
categories:
  - NLP
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---


## 1. WordCloud
- **WordCloud**는 텍스트 데이터에서 빈도수가 높은 단어를 크게, 빈도수가 낮은 단어를 작게 표현한 시각화 도구입니다.
- 텍스트 데이터의 주요 키워드를 한눈에 파악할 수 있어, 데이터 분석의 첫 단계로 많이 사용됩니다.

```python

from wordcloud import WordCloud
from konlpy.tag import Okt
from collections import Counter
import matplotlib.pyplot as plt

# 한국어 텍스트 데이터
text = """
자연어 처리(NLP)는 인공지능의 한 분야로, 컴퓨터가 인간의 언어를 이해하고 처리할 수 있게 하는 기술입니다.
NLP는 텍스트 분석, 감정 분석, 기계 번역, 챗봇 등 다양한 분야에서 활용됩니다.
한국어는 띄어쓰기와 조사 처리에 주의해야 합니다.
"""

# 형태소 분석기 초기화
okt = Okt()

# 명사 추출
nouns = okt.nouns(text)

# 불용어 제거
stopwords = ["는", "을", "를", "이", "가", "의", "에", "와", "과", "입니다", "합니다"]
words = [word for word in nouns if word not in stopwords]

# 단어 빈도수 계산
word_count = Counter(words)

# WordCloud 객체 생성
wordcloud = WordCloud(
    font_path='C:/Windows/Fonts/malgun.ttf',  # 한국어 폰트 경로
    width=800,
    height=400,
    background_color='white'
).generate_from_frequencies(word_count)

# WordCloud 시각화
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
```

🔹 **출력 예시:**  
![wordcloud](/assets/images/wordcloud_image.png)


## 2. **감성분석**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# 1. 샘플 데이터 생성 (실제로는 파일이나 API에서 가져올 수 있음)
data = {
    'text': [
        '이 영화는 정말 재미있었어요!', 
        '연기가 너무 좋았습니다.', 
        '스토리가 흥미롭고 감동적이었어요.',
        '시간 낭비였습니다. 정말 별로였어요.',
        '연기도 스토리도 모두 실망스러웠습니다.',
        '돈과 시간이 아까웠어요.',
        '배우들의 연기가 인상적이었습니다.',
        '다시는 보고 싶지 않은 영화입니다.',
        '음악과 영상미가 아름다웠어요.',
        '기대했던 것보다 훨씬 재미없었습니다.'
    ],
    'sentiment': [1, 1, 1, 0, 0, 0, 1, 0, 1, 0]  # 1: 긍정, 0: 부정
}

df = pd.DataFrame(data)
print("데이터 샘플:")
print(df.head())

# 2. 텍스트 전처리 함수
def preprocess_text(text):
    # 소문자 변환 (영어의 경우)
    text = text.lower()
    # 특수문자 제거
    text = re.sub(r'[^\w\s]', '', text)
    # 불필요한 공백 제거
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# 데이터에 전처리 적용
df['clean_text'] = df['text'].apply(preprocess_text)
print("\n전처리된 텍스트 샘플:")
print(df[['text', 'clean_text']].head())

# 3. 데이터 분할 (학습 및 테스트용)
X_train, X_test, y_train, y_test = train_test_split(
    df['clean_text'], df['sentiment'], test_size=0.3, random_state=42)

print(f"\n학습 데이터 크기: {len(X_train)}")
print(f"테스트 데이터 크기: {len(X_test)}")

# 4. 특성 추출: TF-IDF 벡터화
tfidf_vectorizer = TfidfVectorizer(min_df=2)  # 최소 2개 문서에서 등장하는 단어만 포함
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# 주요 특성(단어) 시각화
feature_names = tfidf_vectorizer.get_feature_names_out()
print(f"\n추출된 특성(단어) 수: {len(feature_names)}")
print(f"주요 특성(단어): {', '.join(feature_names[:10])}")

# 5. 모델 학습: 나이브 베이즈 분류기
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train_tfidf, y_train)

# 6. 모델 평가
y_pred = nb_classifier.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
print(f"\n모델 정확도: {accuracy:.4f}")
print("\n분류 보고서:")
print(classification_report(y_test, y_pred, target_names=['부정', '긍정']))

# 7. 새로운 텍스트에 대한 예측
def predict_sentiment(text):
    # 전처리
    clean_text = preprocess_text(text)
    # 벡터화
    text_tfidf = tfidf_vectorizer.transform([clean_text])
    # 예측
    prediction = nb_classifier.predict(text_tfidf)[0]
    prob = nb_classifier.predict_proba(text_tfidf)[0]
    
    sentiment = "긍정" if prediction == 1 else "부정"
    confidence = prob[prediction]
    
    return sentiment, confidence

# 예제 텍스트로 테스트
test_texts = [
    "이 제품은 가격 대비 성능이 매우 좋습니다.",
    "배송이 너무 늦고 서비스가 불친절했어요.",
    "디자인은 괜찮지만 성능이 기대에 미치지 못했습니다."
]

print("\n새로운 텍스트에 대한 감성 예측:")
for text in test_texts:
    sentiment, confidence = predict_sentiment(text)
    print(f"텍스트: '{text}'")
    print(f"예측 감성: {sentiment} (확률: {confidence:.4f})")
    print("-" * 50)
```

## 3. **단어 임베딩 및 텍스트 유사도 분석**

```python
# 필요한 라이브러리 설치 (최초 실행 시)
# pip install gensim numpy scikit-learn

from gensim.models import Word2Vec
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# 1. 샘플 텍스트 데이터 준비
# 간단한 문장 리스트로 단어 임베딩 학습
sentences = [
    ["고양이", "강아지", "좋아해"],
    ["고양이", "귀여워"],
    ["강아지", "충성스럽다"],
    ["고양이", "독립적이다"],
    ["강아지", "친구"]
]

# 2. Word2Vec 모델 학습
# 단어 임베딩 생성 (벡터 크기: 10, 윈도우: 2, 최소 빈도: 1)
model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=0)

# 3. 단어 벡터 확인
word1 = "고양이"
word2 = "강아지"
vector1 = model.wv[word1]  # "고양이"의 벡터
vector2 = model.wv[word2]  # "강아지"의 벡터

print(f"'{word1}'의 벡터: {vector1}")
print(f"'{word2}'의 벡터: {vector2}")

# 4. 코사인 유사도 계산
# 벡터를 2D 배열로 reshaping
vector1_reshaped = vector1.reshape(1, -1)
vector2_reshaped = vector2.reshape(1, -1)
similarity = cosine_similarity(vector1_reshaped, vector2_reshaped)[0][0]

print(f"'{word1}'와 '{word2}'의 유사도: {similarity:.4f}")

# 5. 모델에서 비슷한 단어 찾기
similar_words = model.wv.most_similar(word1, topn=3)
print(f"'{word1}'와 유사한 단어: {similar_words}")
```

🔹 **출력 예시:**  
```
'고양이'의 벡터: [ 0.07379206 -0.01533812 -0.04534608  0.06552739 -0.0486109  -0.01816626
  0.02878772  0.00990492 -0.08285812 -0.09450678]
'강아지'의 벡터: [-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809
  0.06458873  0.08972988 -0.05015428 -0.03763372]
'고양이'와 '강아지'의 유사도: 0.5437
'고양이'와 유사한 단어: [('강아지', 0.5436561107635498), ('친구', 0.3293631672859192), ('좋아해', -0.18002544343471527)]
```

## 4. **토픽 모델링**
- LDA를 활용한 토픽 모델링

```python
# 필요한 라이브러리 설치 (최초 실행 시)
# pip install gensim nltk

import gensim
from gensim import corpora
from nltk.tokenize import word_tokenize
import nltk

# NLTK 데이터 다운로드 (최초 실행 시)
nltk.download('punkt')

# 1. 샘플 텍스트 데이터 준비
# 간단한 문서 리스트 (한국어로 예시)
documents = [
    "고양이가 귀여워서 좋아해요 매일 고양이와 놀아요",
    "강아지는 충성스럽고 친구 같은 동물이에요",
    "고양이는 독립적인 성격을 가지고 있어요",
    "강아지와 산책하는 게 정말 재밌어요",
    "고양이와 강아지 둘 다 사랑스러워요"
]

# 2. 텍스트 전처리
# 토큰화 및 불필요한 단어 제거 (간단히 공백 기준으로 분리)
tokenized_docs = [word_tokenize(doc) for doc in documents]

# 3. 사전(Dictionary) 생성
dictionary = corpora.Dictionary(tokenized_docs)

# 4. 문서-단어 행렬(Bag of Words) 생성
corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]

# 5. LDA 모델 학습
# 토픽 수: 2, 반복 횟수: 10
lda_model = gensim.models.LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=2,
    passes=10,
    random_state=42
)

# 6. 토픽 출력
topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(f"토픽 {topic[0] + 1}: {topic[1]}")

# 7. 각 문서의 토픽 분포 확인
print("\n문서별 토픽 분포:")
for i, doc_bow in enumerate(corpus):
    doc_topics = lda_model.get_document_topics(doc_bow)
    print(f"문서 {i + 1}: {doc_topics}")
```

🔹 **출력 예시:**  
```
토픽 1: 0.065*"고양이와" + 0.063*"매일" + 0.063*"좋아해요" + 0.063*"고양이가" + 0.063*"놀아요"
토픽 2: 0.054*"충성스럽고" + 0.054*"친구" + 0.054*"강아지와" + 0.054*"재밌어요" + 0.054*"정말"

문서별 토픽 분포:
문서 1: [(0, 0.9229462), (1, 0.077053785)]
문서 2: [(0, 0.087666884), (1, 0.9123331)]
문서 3: [(0, 0.9136158), (1, 0.0863842)]
문서 4: [(0, 0.08766774), (1, 0.9123323)]
문서 5: [(0, 0.09521585), (1, 0.90478414)]
```
