---
title: 11차시 2:NLP(실습예제)
layout: single
classes: wide
categories:
  - NLP
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---


## 1. WordCloud
- **WordCloud**는 텍스트 데이터에서 빈도수가 높은 단어를 크게, 빈도수가 낮은 단어를 작게 표현한 시각화 도구입니다.
- 텍스트 데이터의 주요 키워드를 한눈에 파악할 수 있어, 데이터 분석의 첫 단계로 많이 사용됩니다.

```python

from wordcloud import WordCloud
from konlpy.tag import Okt
from collections import Counter
import matplotlib.pyplot as plt

# 한국어 텍스트 데이터
text = """
자연어 처리(NLP)는 인공지능의 한 분야로, 컴퓨터가 인간의 언어를 이해하고 처리할 수 있게 하는 기술입니다.
NLP는 텍스트 분석, 감정 분석, 기계 번역, 챗봇 등 다양한 분야에서 활용됩니다.
한국어는 띄어쓰기와 조사 처리에 주의해야 합니다.
"""

# 형태소 분석기 초기화
okt = Okt()

# 명사 추출
nouns = okt.nouns(text)

# 불용어 제거
stopwords = ["는", "을", "를", "이", "가", "의", "에", "와", "과", "입니다", "합니다"]
words = [word for word in nouns if word not in stopwords]

# 단어 빈도수 계산
word_count = Counter(words)

# WordCloud 객체 생성
wordcloud = WordCloud(
    font_path='C:/Windows/Fonts/malgun.ttf',  # 한국어 폰트 경로
    width=800,
    height=400,
    background_color='white'
).generate_from_frequencies(word_count)

# WordCloud 시각화
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
```

## 2. **감성분석**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# 1. 샘플 데이터 생성 (실제로는 파일이나 API에서 가져올 수 있음)
data = {
    'text': [
        '이 영화는 정말 재미있었어요!', 
        '연기가 너무 좋았습니다.', 
        '스토리가 흥미롭고 감동적이었어요.',
        '시간 낭비였습니다. 정말 별로였어요.',
        '연기도 스토리도 모두 실망스러웠습니다.',
        '돈과 시간이 아까웠어요.',
        '배우들의 연기가 인상적이었습니다.',
        '다시는 보고 싶지 않은 영화입니다.',
        '음악과 영상미가 아름다웠어요.',
        '기대했던 것보다 훨씬 재미없었습니다.'
    ],
    'sentiment': [1, 1, 1, 0, 0, 0, 1, 0, 1, 0]  # 1: 긍정, 0: 부정
}

df = pd.DataFrame(data)
print("데이터 샘플:")
print(df.head())

# 2. 텍스트 전처리 함수
def preprocess_text(text):
    # 소문자 변환 (영어의 경우)
    text = text.lower()
    # 특수문자 제거
    text = re.sub(r'[^\w\s]', '', text)
    # 불필요한 공백 제거
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# 데이터에 전처리 적용
df['clean_text'] = df['text'].apply(preprocess_text)
print("\n전처리된 텍스트 샘플:")
print(df[['text', 'clean_text']].head())

# 3. 데이터 분할 (학습 및 테스트용)
X_train, X_test, y_train, y_test = train_test_split(
    df['clean_text'], df['sentiment'], test_size=0.3, random_state=42)

print(f"\n학습 데이터 크기: {len(X_train)}")
print(f"테스트 데이터 크기: {len(X_test)}")

# 4. 특성 추출: TF-IDF 벡터화
tfidf_vectorizer = TfidfVectorizer(min_df=2)  # 최소 2개 문서에서 등장하는 단어만 포함
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# 주요 특성(단어) 시각화
feature_names = tfidf_vectorizer.get_feature_names_out()
print(f"\n추출된 특성(단어) 수: {len(feature_names)}")
print(f"주요 특성(단어): {', '.join(feature_names[:10])}")

# 5. 모델 학습: 나이브 베이즈 분류기
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train_tfidf, y_train)

# 6. 모델 평가
y_pred = nb_classifier.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
print(f"\n모델 정확도: {accuracy:.4f}")
print("\n분류 보고서:")
print(classification_report(y_test, y_pred, target_names=['부정', '긍정']))

# 7. 새로운 텍스트에 대한 예측
def predict_sentiment(text):
    # 전처리
    clean_text = preprocess_text(text)
    # 벡터화
    text_tfidf = tfidf_vectorizer.transform([clean_text])
    # 예측
    prediction = nb_classifier.predict(text_tfidf)[0]
    prob = nb_classifier.predict_proba(text_tfidf)[0]
    
    sentiment = "긍정" if prediction == 1 else "부정"
    confidence = prob[prediction]
    
    return sentiment, confidence

# 예제 텍스트로 테스트
test_texts = [
    "이 제품은 가격 대비 성능이 매우 좋습니다.",
    "배송이 너무 늦고 서비스가 불친절했어요.",
    "디자인은 괜찮지만 성능이 기대에 미치지 못했습니다."
]

print("\n새로운 텍스트에 대한 감성 예측:")
for text in test_texts:
    sentiment, confidence = predict_sentiment(text)
    print(f"텍스트: '{text}'")
    print(f"예측 감성: {sentiment} (확률: {confidence:.4f})")
    print("-" * 50)
```

## 3. **단어 임베딩 및 텍스트 유사도 분석**

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
from sklearn.decomposition import PCA
from matplotlib import font_manager, rc

# 한글 폰트 설정 (matplotlib에서 한글 표시를 위해)
# Windows: font_name = 'Malgun Gothic'
# Mac: font_name = 'AppleGothic'
# Linux: font_name = 'NanumGothic'
# 시스템에 맞는 폰트로 변경 필요

try:
    font_name = 'Malgun Gothic'  # Windows 기본 한글 폰트
    font_path = font_manager.findfont(font_manager.FontProperties(family=font_name))
    rc('font', family=font_name)
except:
    print("한글 폰트 설정에 문제가 있습니다. 시스템에 맞는 폰트로 변경해주세요.")
    pass

# 1. 샘플 데이터 생성
documents = [
    "자연어 처리는 텍스트 데이터를 분석하는 기술입니다.",
    "컴퓨터가 인간의 언어를 이해하기 위한 인공지능의 한 분야입니다.",
    "텍스트 마이닝은 비정형 텍스트 데이터에서 가치 있는 정보를 추출하는 과정입니다.",
    "단어 임베딩은 단어를 벡터 공간에 표현하는 방법입니다.",
    "Word2Vec은 단어 간의 의미적 관계를 학습할 수 있는 모델입니다.",
    "문서 분류는 텍스트를 미리 정의된 카테고리로 분류하는 작업입니다.",
    "감성 분석은 텍스트에서 감정이나 의견을 추출하는 기술입니다.",
    "개체명 인식은 텍스트에서 인명, 지명, 조직명 등을 식별하는 작업입니다.",
    "토큰화는 텍스트를 작은 단위로 나누는 과정입니다.",
    "불용어 제거는 분석에 큰 의미가 없는 단어를 제거하는 과정입니다."
]

# 2. 텍스트 전처리
def preprocess(text):
    # 정제 (특수 문자 제거)
    text = re.sub(r'[^\w\s]', ' ', text)
    # 토큰화 (단어 단위로 분리)
    tokens = simple_preprocess(text)
    return tokens

# 문서를 토큰으로 변환
tokenized_docs = [preprocess(doc) for doc in documents]
print("토큰화된 문서 샘플:")
for i, tokens in enumerate(tokenized_docs[:3]):
    print(f"문서 {i+1}: {tokens}")

# 3. Word2Vec 모델 학습
# min_count: 최소 등장 횟수, window: 컨텍스트 윈도우 크기, vector_size: 벡터 차원
w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)
w2v_model.train(tokenized_docs, total_examples=len(tokenized_docs), epochs=100)

# 단어 벡터 확인
word_vectors = w2v_model.wv
print(f"\n어휘 크기: {len(word_vectors)}")

# 4. 비슷한 단어 찾기
similar_words = word_vectors.most_similar("텍스트", topn=5)
print("\n'텍스트'와 가장 유사한 단어:")
for word, score in similar_words:
    print(f"{word}: {score:.4f}")

# 5. 2D 시각화를 위한 차원 축소
def plot_words(words, model):
    # 단어 벡터 추출
    word_vectors = [model.wv[word] for word in words if word in model.wv]
    word_labels = [word for word in words if word in model.wv]
    
    # PCA로 차원 축소
    pca = PCA(n_components=2)
    coordinates = pca.fit_transform(word_vectors)
    
    # 그래프 그리기
    plt.figure(figsize=(10, 8))
    plt.scatter(coordinates[:, 0], coordinates[:, 1], marker='o')
    
    # 단어 라벨 표시
    for i, word in enumerate(word_labels):
        plt.annotate(word, xy=(coordinates[i, 0], coordinates[i, 1]), fontsize=12)
    
    plt.title('Word2Vec으로 표현한 단어 벡터의 2D 시각화')
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.grid(True)
    plt.show()

# 모든 단어 목록에서 상위 20개 단어 선택하여 시각화
all_words = list(word_vectors.index_to_key)
selected_words = all_words[:20]  # 또는 특정 단어 리스트 지정
print(f"\n시각화할 단어 목록: {selected_words}")
# plot_words(selected_words, w2v_model)  # 실행 환경에 따라 주석 처리

# 6. 문서 벡터 생성 및 유사도 분석
def document_vector(doc, model):
    # 문서에 있는 모든 단어 벡터의 평균으로 문서 벡터 생성
    doc_vector = np.zeros(model.vector_size)
    valid_word_count = 0
    
    for word in doc:
        if word in model.wv:
            doc_vector += model.wv[word]
            valid_word_count += 1
    
    if valid_word_count > 0:
        doc_vector /= valid_word_count
    
    return doc_vector

# 각 문서의 벡터 계산
doc_vectors = [document_vector(doc, w2v_model) for doc in tokenized_docs]

# 문서 간 유사도 계산 (코사인 유사도)
similarity_matrix = cosine_similarity(doc_vectors)

# 유사도 행렬을 DataFrame으로 변환하여 표시
similarity_df = pd.DataFrame(similarity_matrix, index=[f"문서{i+1}" for i in range(len(documents))],
                            columns=[f"문서{i+1}" for i in range(len(documents))])
print("\n문서 간 유사도 행렬 (상위 5x5만 표시):")
print(similarity_df.iloc[:5, :5])

# 7. 특정 문서와 가장 유사한 문서 찾기
def find_similar_documents(query_index, similarity_matrix, top_n=3):
    # 자기 자신을 제외하고 가장 유사한 문서 찾기
    query_similarities = similarity_matrix[query_index]
    most_similar_indices = np.argsort(query_similarities)[::-1][1:top_n+1]  # 자기 자신 제외
    
    return [(i, query_similarities[i]) for i in most_similar_indices]

# 예제: 첫 번째 문서와 가장 유사한 문서 3개 찾기
query_index = 0
similar_docs = find_similar_documents(query_index, similarity_matrix)

print(f"\n문서 {query_index+1}과 가장 유사한 문서:")
print(f"기준 문서: '{documents[query_index]}'")
for doc_idx, similarity in similar_docs:
    print(f"유사 문서 {doc_idx+1} (유사도: {similarity:.4f}): '{documents[doc_idx]}'")

# 8. 새로운 텍스트의 유사도 분석
def process_new_text(text, model, doc_vectors):
    # 텍스트 전처리
    tokens = preprocess(text)
    
    # 문서 벡터 생성
    new_vector = document_vector(tokens, model).reshape(1, -1)
    
    # 기존 문서와의 유사도 계산
    similarities = cosine_similarity(new_vector, doc_vectors)[0]
    
    return similarities

# 새로운 텍스트 예시
new_text = "자연어 처리 기술은 텍스트에서 의미를 추출하는 인공지능 기법입니다."
similarities = process_new_text(new_text, w2v_model, doc_vectors)

# 가장 유사한 문서 찾기
most_similar_idx = np.argmax(similarities)
print(f"\n새로운 텍스트: '{new_text}'")
print(f"가장 유사한 문서 {most_similar_idx+1} (유사도: {similarities[most_similar_idx]:.4f}):")
print(f"'{documents[most_similar_idx]}'")
```

## 4. **토픽 모델링**

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
from sklearn.decomposition import PCA
from matplotlib import font_manager, rc

# 한글 폰트 설정 (matplotlib에서 한글 표시를 위해)
# Windows: font_name = 'Malgun Gothic'
# Mac: font_name = 'AppleGothic'
# Linux: font_name = 'NanumGothic'
# 시스템에 맞는 폰트로 변경 필요

try:
    font_name = 'Malgun Gothic'  # Windows 기본 한글 폰트
    font_path = font_manager.findfont(font_manager.FontProperties(family=font_name))
    rc('font', family=font_name)
except:
    print("한글 폰트 설정에 문제가 있습니다. 시스템에 맞는 폰트로 변경해주세요.")
    pass

# 1. 샘플 데이터 생성
documents = [
    "자연어 처리는 텍스트 데이터를 분석하는 기술입니다.",
    "컴퓨터가 인간의 언어를 이해하기 위한 인공지능의 한 분야입니다.",
    "텍스트 마이닝은 비정형 텍스트 데이터에서 가치 있는 정보를 추출하는 과정입니다.",
    "단어 임베딩은 단어를 벡터 공간에 표현하는 방법입니다.",
    "Word2Vec은 단어 간의 의미적 관계를 학습할 수 있는 모델입니다.",
    "문서 분류는 텍스트를 미리 정의된 카테고리로 분류하는 작업입니다.",
    "감성 분석은 텍스트에서 감정이나 의견을 추출하는 기술입니다.",
    "개체명 인식은 텍스트에서 인명, 지명, 조직명 등을 식별하는 작업입니다.",
    "토큰화는 텍스트를 작은 단위로 나누는 과정입니다.",
    "불용어 제거는 분석에 큰 의미가 없는 단어를 제거하는 과정입니다."
]

# 2. 텍스트 전처리
def preprocess(text):
    # 정제 (특수 문자 제거)
    text = re.sub(r'[^\w\s]', ' ', text)
    # 토큰화 (단어 단위로 분리)
    tokens = simple_preprocess(text)
    return tokens

# 문서를 토큰으로 변환
tokenized_docs = [preprocess(doc) for doc in documents]
print("토큰화된 문서 샘플:")
for i, tokens in enumerate(tokenized_docs[:3]):
    print(f"문서 {i+1}: {tokens}")

# 3. Word2Vec 모델 학습
# min_count: 최소 등장 횟수, window: 컨텍스트 윈도우 크기, vector_size: 벡터 차원
w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)
w2v_model.train(tokenized_docs, total_examples=len(tokenized_docs), epochs=100)

# 단어 벡터 확인
word_vectors = w2v_model.wv
print(f"\n어휘 크기: {len(word_vectors)}")

# 4. 비슷한 단어 찾기
similar_words = word_vectors.most_similar("텍스트", topn=5)
print("\n'텍스트'와 가장 유사한 단어:")
for word, score in similar_words:
    print(f"{word}: {score:.4f}")

# 5. 2D 시각화를 위한 차원 축소
def plot_words(words, model):
    # 단어 벡터 추출
    word_vectors = [model.wv[word] for word in words if word in model.wv]
    word_labels = [word for word in words if word in model.wv]
    
    # PCA로 차원 축소
    pca = PCA(n_components=2)
    coordinates = pca.fit_transform(word_vectors)
    
    # 그래프 그리기
    plt.figure(figsize=(10, 8))
    plt.scatter(coordinates[:, 0], coordinates[:, 1], marker='o')
    
    # 단어 라벨 표시
    for i, word in enumerate(word_labels):
        plt.annotate(word, xy=(coordinates[i, 0], coordinates[i, 1]), fontsize=12)
    
    plt.title('Word2Vec으로 표현한 단어 벡터의 2D 시각화')
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.grid(True)
    plt.show()

# 모든 단어 목록에서 상위 20개 단어 선택하여 시각화
all_words = list(word_vectors.index_to_key)
selected_words = all_words[:20]  # 또는 특정 단어 리스트 지정
print(f"\n시각화할 단어 목록: {selected_words}")
# plot_words(selected_words, w2v_model)  # 실행 환경에 따라 주석 처리

# 6. 문서 벡터 생성 및 유사도 분석
def document_vector(doc, model):
    # 문서에 있는 모든 단어 벡터의 평균으로 문서 벡터 생성
    doc_vector = np.zeros(model.vector_size)
    valid_word_count = 0
    
    for word in doc:
        if word in model.wv:
            doc_vector += model.wv[word]
            valid_word_count += 1
    
    if valid_word_count > 0:
        doc_vector /= valid_word_count
    
    return doc_vector

# 각 문서의 벡터 계산
doc_vectors = [document_vector(doc, w2v_model) for doc in tokenized_docs]

# 문서 간 유사도 계산 (코사인 유사도)
similarity_matrix = cosine_similarity(doc_vectors)

# 유사도 행렬을 DataFrame으로 변환하여 표시
similarity_df = pd.DataFrame(similarity_matrix, index=[f"문서{i+1}" for i in range(len(documents))],
                            columns=[f"문서{i+1}" for i in range(len(documents))])
print("\n문서 간 유사도 행렬 (상위 5x5만 표시):")
print(similarity_df.iloc[:5, :5])

# 7. 특정 문서와 가장 유사한 문서 찾기
def find_similar_documents(query_index, similarity_matrix, top_n=3):
    # 자기 자신을 제외하고 가장 유사한 문서 찾기
    query_similarities = similarity_matrix[query_index]
    most_similar_indices = np.argsort(query_similarities)[::-1][1:top_n+1]  # 자기 자신 제외
    
    return [(i, query_similarities[i]) for i in most_similar_indices]

# 예제: 첫 번째 문서와 가장 유사한 문서 3개 찾기
query_index = 0
similar_docs = find_similar_documents(query_index, similarity_matrix)

print(f"\n문서 {query_index+1}과 가장 유사한 문서:")
print(f"기준 문서: '{documents[query_index]}'")
for doc_idx, similarity in similar_docs:
    print(f"유사 문서 {doc_idx+1} (유사도: {similarity:.4f}): '{documents[doc_idx]}'")

# 8. 새로운 텍스트의 유사도 분석
def process_new_text(text, model, doc_vectors):
    # 텍스트 전처리
    tokens = preprocess(text)
    
    # 문서 벡터 생성
    new_vector = document_vector(tokens, model).reshape(1, -1)
    
    # 기존 문서와의 유사도 계산
    similarities = cosine_similarity(new_vector, doc_vectors)[0]
    
    return similarities

# 새로운 텍스트 예시
new_text = "자연어 처리 기술은 텍스트에서 의미를 추출하는 인공지능 기법입니다."
similarities = process_new_text(new_text, w2v_model, doc_vectors)

# 가장 유사한 문서 찾기
most_similar_idx = np.argmax(similarities)
print(f"\n새로운 텍스트: '{new_text}'")
print(f"가장 유사한 문서 {most_similar_idx+1} (유사도: {similarities[most_similar_idx]:.4f}):")
print(f"'{documents[most_similar_idx]}'")
```

