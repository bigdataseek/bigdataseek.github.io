---
title: 26차시 2:AI Research Papers(Basic)
layout: single
classes: wide
categories:
  - AI Research Papers
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---


## 9. **Playing Atari with Deep Reinforcement Learning (2013, Mnih et al.)**

심층 강화 학습 스터디 가이드

### 9.1 퀴즈
1. 이 논문에서 제시하는 심층 학습 모델은 어떤 종류의 신경망이며, 어떤 강화 학습 알고리즘의 변형을 사용하는가? 
   - 이 모델은 컨볼루션 신경망(Convolutional Neural Network, CNN)이며, Q-러닝 알고리즘의 변형을 사용하여 학습됩니다. CNN의 입력은 원시 픽셀이며, 출력은 미래 보상을 예측하는 가치 함수입니다.

2. 기존의 강화 학습 접근 방식과 비교하여 이 논문의 방법이 갖는 주요 차이점은 무엇인가? 
   - 기존의 강화 학습은 주로 수동으로 설계한 특징과 선형 가치 함수 또는 정책 표현에 의존했습니다. 반면, 이 논문은 원시 감각 입력으로부터 직접 특징을 추출하는 심층 학습 모델을 사용하여 특징 엔지니어링의 필요성을 줄였습니다.

3. 심층 학습 관점에서 강화 학습이 제시하는 주요 어려움 세 가지는 무엇인가? 
   - 첫째, 강화 학습은 종종 희소하고 잡음이 많으며 지연된 스칼라 보상 신호로부터 학습해야 합니다. 둘째, 강화 학습 데이터는 일반적으로 순차적이고 상관관계가 높습니다. 셋째, 알고리즘이 새로운 행동을 학습함에 따라 데이터 분포가 변하므로 심층 학습 방법이 가정하는 고정된 분포와 상반됩니다.

4. 이 논문에서 제안하는 심층 Q-러닝(Deep Q-learning) 알고리즘이 이러한 어려움을 해결하기 위해 사용하는 주요 기술 두 가지는 무엇인가? 
   - 첫째, 경험 재생(experience replay) 메커니즘을 사용하여 이전의 전환(transition)을 무작위로 샘플링하여 훈련 분포를 평활화하고 데이터의 상관관계를 줄입니다. 둘째, 대상 Q 값을 주기적으로 업데이트하여 학습의 안정성을 높입니다.

5. 경험 재생(experience replay) 메커니즘은 강화 학습에서 어떤 이점을 제공하는가? 
   - 경험 재생은 각 경험 단계를 여러 번의 가중치 업데이트에 재사용하여 데이터 효율성을 높입니다. 또한, 연속적인 샘플 간의 높은 상관관계를 깨뜨려 업데이트의 분산을 줄이고, 학습 과정에서 발생할 수 있는 원치 않는 피드백 루프를 방지하여 학습을 안정화합니다.

6. Q-함수를 매개변화하는 두 가지 주요 방법과 이 논문에서 채택한 방법의 장점은 무엇인가? 
   - 한 가지 방법은 히스토리와 행동 쌍을 입력으로 사용하여 Q-값을 예측하는 것이고, 다른 방법은 상태 표현을 입력으로 사용하고 각 가능한 행동에 대한 Q-값을 개별 출력으로 예측하는 것입니다. 이 논문에서는 후자를 채택하여 단일 순방향 패스만으로 주어진 상태의 모든 가능한 행동에 대한 Q-값을 계산할 수 있다는 장점이 있습니다.

7. Atari 2600 게임 환경에서 에이전트의 입력으로 사용되는 데이터는 어떻게 전처리되는가? 
   - 원시 RGB 프레임은 먼저 회색조로 변환되고 110x84 이미지로 다운샘플링됩니다. 최종 입력 표현은 게임 영역을 대략적으로 캡처하는 84x84 영역을 잘라내고, 마지막 4개의 프레임을 쌓아서 84x84x4 이미지를 만듭니다.

8. 이 논문의 실험에서 모든 Atari 게임에 걸쳐 일관되게 유지된 요소 세 가지는 무엇인가? 
   - 신경망 아키텍처, 학습 알고리즘, 하이퍼파라미터 설정은 모든 일곱 가지 Atari 게임에 걸쳐 동일하게 유지되었습니다. 이는 제안된 방법의 견고성을 보여줍니다.

9. 훈련 중 에이전트의 성능을 평가하기 위해 사용된 두 가지 주요 지표는 무엇이며, 각각의 특징은 무엇인가? 
   - 첫 번째 지표는 에피소드당 평균 총 보상으로, 실제 게임 성능을 나타내지만 정책 가중치의 작은 변화에도 크게 변동하는 경향이 있습니다. 두 번째 지표는 고정된 상태 집합에 대한 평균 최대 예측 Q-값으로, 학습 진행 상황을 더 안정적으로 보여주지만 직접적인 게임 성능 측정은 아닙니다.

10. 이 연구 결과는 몇몇 Atari 2600 게임에서 인간 전문가의 성능을 능가했지만, 그렇지 못한 게임도 있다. 그 이유는 무엇이라고 저자들은 추측하는가? 
   - Q*bert, Seaquest, Space Invaders와 같이 인간 성능에 미치지 못하는 게임은 네트워크가 장기간에 걸쳐 전략을 찾아야 하기 때문에 더 어렵다고 저자들은 추측합니다.

### 9.2 에세이 형식 질문
1. 이 논문에서 제시된 심층 강화 학습 접근 방식이 전통적인 강화 학습 방법론에 비해 갖는 혁신적인 측면과 잠재적인 영향에 대해 논하시오.

2. 경험 재생(experience replay)이 심층 Q-러닝의 안정성과 성능 향상에 기여하는 구체적인 방식과 그 한계점에 대해 분석하시오.

3. 이 논문에서 사용된 컨볼루션 신경망 아키텍처가 Atari 게임 플레이라는 강화 학습 문제에 적합한 이유를 설명하고, 다른 신경망 아키텍처와의 비교를 통해 논하시오.

4. 이 연구 결과가 보인 성공에도 불구하고, 여전히 인간 수준의 성능에 도달하지 못한 Atari 게임들을 분석하고, 향후 연구 방향에 대한 아이디어를 제시하시오.

5. 이 논문의 연구 결과가 인공 지능 및 관련 분야에 미치는 더 넓은 의미와 윤리적 고려 사항에 대해 논하시오.

### 9.3 용어 해설

1. 강화 학습 (Reinforcement Learning, RL): 에이전트가 환경과 상호작용하면서 보상을 최대화하는 방법을 학습하는 머신 러닝의 한 분야.

2. 심층 학습 (Deep Learning): 여러 계층으로 구성된 인공 신경망을 사용하여 복잡한 데이터의 패턴을 학습하는 머신 러닝의 한 분야.

3. 컨볼루션 신경망 (Convolutional Neural Network, CNN): 이미지 처리, 비디오 분석 등 시각적 데이터를 처리하는 데 특히 효과적인 심층 신경망의 한 종류. 컨볼루션 계층, 풀링 계층, 완전 연결 계층 등으로 구성됨.

4. Q-러닝 (Q-learning): 행동-가치 함수 (Q-함수)를 학습하는 모델-프리 강화 학습 알고리즘. 주어진 상태에서 특정 행동을 취했을 때 얻을 수 있는 미래 보상의 기댓값을 추정함.

5. 가치 함수 (Value Function): 특정 상태 또는 상태-행동 쌍이 얼마나 "좋은"지 나타내는 함수. 미래에 얻을 것으로 예상되는 보상의 양을 예측함.

6. 정책 (Policy): 에이전트가 주어진 상태에서 어떤 행동을 취할지 결정하는 규칙 또는 전략.

7. 경험 재생 (Experience Replay): 에이전트의 경험 (상태, 행동, 보상, 다음 상태)을 메모리에 저장하고, 학습 시에 이 메모리에서 무작위로 샘플링하여 학습하는 기법. 데이터 효율성을 높이고 샘플 간의 상관관계를 줄이는 데 도움을 줌.

8. 행동-가치 함수 (Action-Value Function) 또는 Q-함수: 특정 상태에서 특정 행동을 취한 후 최적의 정책을 따랐을 때 얻을 수 있는 미래 할인된 보상의 기댓값.

9. 벨만 방정식 (Bellman Equation): 최적 가치 함수가 만족하는 재귀적인 방정식. 현재 상태의 가치는 다음 상태의 최적 가치와 즉각적인 보상으로 표현될 수 있음을 나타냄.

10. ε-탐욕 정책 (Epsilon-Greedy Policy): 탐험과 활용 사이의 균형을 맞추기 위해 사용되는 정책. ε의 확률로 무작위 행동을 선택하고 (1-ε)의 확률로 현재 최적이라고 믿는 행동을 선택함.

11. 보상 신호 (Reward Signal): 에이전트가 환경과 상호작용한 결과로 받는 스칼라 값. 에이전트의 행동이 얼마나 바람직했는지 나타냄.

12. 상태 공간 (State Space): 에이전트가 처할 수 있는 모든 가능한 상태의 집합.

13. 행동 공간 (Action Space): 에이전트가 특정 상태에서 취할 수 있는 모든 가능한 행동의 집합.

14. 모델-프리 (Model-free): 환경의 모델 (상태 전환 확률 및 보상 함수)을 명시적으로 학습하지 않고 직접 경험으로부터 최적 정책 또는 가치 함수를 학습하는 강화 학습 방법.

15. 오프-폴리시 (Off-policy): 행동을 생성하는 정책과 평가 또는 학습하는 정책이 다른 강화 학습 방식. Q-러닝은 대표적인 오프-폴리시 알고리즘임.

16. 확률적 경사 하강법 (Stochastic Gradient Descent, SGD): 머신 러닝 모델의 파라미터를 최적화하기 위해 사용되는 반복적인 알고리즘. 전체 데이터셋 대신 무작위로 선택된 일부 데이터 (미니배치)에 대한 손실 함수의 기울기를 사용하여 파라미터를 업데이트함.

17. RMSProp: 학습률을 개별 파라미터에 적응적으로 조정하는 경사 하강법의 한 종류.

18. 미니배치 (Minibatch): 전체 데이터셋의 작은 부분 집합. 확률적 경사 하강법에서 각 업데이트 단계에 사용됨.

## 10. **Mastering the game of Go with deep neural networks and tree search (2016, Silver et al.)** 

딥러닝 기반 바둑 게임 마스터하기

### 10.1 핵심 개념

•완전 정보 게임: 모든 플레이어가 게임의 모든 시점에서 완벽한 정보를 알고 있는 게임.
•**최적 가치 함수 (v(s)):** 모든 보드 상태(s)에서 완벽한 플레이를 가정했을 때 게임의 결과를 결정하는 함수.
•탐색 트리: 가능한 모든 움직임의 순서를 나타내는 트리 구조.
•넓이 (b): 각 포지션에서 가능한 합법적인 움직임의 수.
•깊이 (d): 게임의 평균 길이 (총 움직임 수).
•위치 평가 (Position Evaluation): 탐색 트리의 특정 상태(s)에서 탐색을 중단하고, 그 아래 하위 트리를 예측된 게임 결과 (근사 가치 함수 v(s))로 대체하여 탐색 깊이를 줄이는 방법.
•정책 (Policy) (p(a|s)): 특정 보드 상태(s)에서 가능한 각 움직임(a)에 대한 확률 분포.
•몬테카를로 롤아웃 (Monte Carlo Rollouts): 정책(p)에 따라 양쪽 플레이어가 무작위로 행동을 선택하여 게임의 끝까지 진행하는 시뮬레이션. 여러 번의 롤아웃 결과를 평균하여 특정 포지션의 가치를 평가하는 방법.
•몬테카를로 트리 탐색 (MCTS): 몬테카를로 롤아웃을 사용하여 탐색 트리의 각 상태 가치를 추정하는 알고리즘. 시뮬레이션 횟수가 증가함에 따라 트리가 확장되고 가치 추정의 정확도가 향상됨.
•심층 컨볼루션 신경망 (Deep Convolutional Neural Networks): 여러 계층의 뉴런으로 구성된 신경망으로, 이미지와 같은 시각적 데이터를 처리하는 데 뛰어난 성능을 보임. 바둑판의 현재 상태를 입력으로 받아 특징을 추출하는 데 사용됨.
•정책 네트워크 (Policy Network) (pσ, pρ): 바둑 보드 상태를 입력으로 받아 가능한 다음 수에 대한 확률 분포를 출력하는 심층 신경망. 지도 학습 (SL 정책) 또는 강화 학습 (RL 정책)을 통해 학습됨.
•가치 네트워크 (Value Network) (vθ): 바둑 보드 상태를 입력으로 받아 현재 플레이어의 승리 확률을 예측하는 단일 값을 출력하는 심층 신경망. 자기 대국 데이터를 사용하여 회귀 학습을 통해 학습됨.
•감독 학습 (Supervised Learning - SL): 인간 전문가의 기보 데이터를 사용하여 정책 네트워크가 전문가의 수를 예측하도록 학습시키는 방법.
•강화 학습 (Reinforcement Learning - RL): 에이전트가 스스로 게임을 플레이하면서 승리라는 보상을 최대화하도록 정책 네트워크를 학습시키는 방법.
•자기 대국 (Self-Play): 학습 중인 에이전트가 자신 또는 이전 버전의 에이전트와 게임을 플레이하는 방식.
•정책 경사 (Policy Gradient): 강화 학습 알고리즘의 하나로, 정책의 매개변수를 직접 업데이트하여 기대되는 보상을 최대화하는 방향으로 학습하는 방법.
•회귀 (Regression): 가치 네트워크를 학습시키는 데 사용되는 머신러닝 방법으로, 입력 상태와 실제 게임 결과 사이의 평균 제곱 오차를 최소화하는 방향으로 네트워크의 가중치를 조정함.
•MCTS에서의 탐색 단계:
   ◦선택 (Selection): 트리 루트에서 시작하여 액션 가치(Q)와 탐색 보너스(u(P))를 기반으로 자식 노드를 재귀적으로 선택하여 리프 노드에 도달하는 단계.
   ◦확장 (Expansion): 리프 노드가 특정 방문 횟수 임계값을 넘으면, 정책 네트워크(pσ)를 사용하여 가능한 합법적인 수에 대한 사전 확률(P)을 계산하고 새로운 노드를 트리에 추가하는 단계.
   ◦평가 (Evaluation): 확장된 리프 노드의 가치를 평가하는 단계. AlphaGo에서는 가치 네트워크(vθ)를 사용한 평가와 빠른 롤아웃 정책(pπ)을 사용한 몬테카를로 롤아웃 결과를 혼합하여 사용함.
   ◦역전파 (Backup): 시뮬레이션 결과를 사용하여 탐색 경로에 있는 각 에지의 액션 가치(Q)와 방문 횟수(N)를 업데이트하는 단계.
•행동 가치 (Q(s, a)): 특정 상태(s)에서 특정 행동(a)을 취했을 때 기대되는 미래 보상.
•방문 횟수 (N(s, a)): 탐색 중에 특정 상태(s)에서 특정 행동(a)을 취한 횟수.
•사전 확률 (P(s, a)): 정책 네트워크(pσ)가 특정 상태(s)에서 특정 행동(a)을 선택할 초기 확률.
•가상 손실 (Virtual Loss): 병렬 탐색 시 여러 스레드가 동일한 노드를 동시에 탐색하는 것을 방지하기 위해 일시적으로 노드의 가치를 낮추는 기법.
•가장 많이 방문한 수 (Most Visited Move): MCTS 탐색이 완료된 후 루트 노드에서 가장 많이 방문한 행동을 최종 수로 선택하는 전략.


### 10.2 퀴즈 (2-3 문장 답안)

1. 완전 정보 게임과 불완전 정보 게임의 주요 차이점은 무엇이며, 바둑은 어떤 유형의 게임에 속하나요?

2. 탐색 트리의 넓이(b)와 깊이(d)가 클 때 완전 탐색이 비현실적인 이유는 무엇이며, 바둑 게임의 넓이와 깊이는 어느 정도인가요?

3. 정책 네트워크와 가치 네트워크는 바둑 AI에서 각각 어떤 역할을 수행하며, 이 두 네트워크는 어떻게 학습되나요?

4. 몬테카를로 트리 탐색(MCTS)의 기본적인 작동 원리는 무엇이며, 탐색 과정의 주요 네 단계를 간략하게 설명해주세요.

5. AlphaGo가 인간 전문가의 기보를 사용하여 초기 정책 네트워크를 학습시키는 이유는 무엇이며, 이 단계의 학습 방법을 무엇이라고 부르나요?

6. 강화 학습 단계에서 AlphaGo는 자기 대국을 통해 정책 네트워크를 개선합니다. 이 과정의 목표와 주요 이점은 무엇인가요?

7. 가치 네트워크를 학습시킬 때 전체 게임 데이터를 사용하는 대신 자기 대국에서 추출한 고유한 포지션을 사용하는 이유는 무엇인가요?

8. AlphaGo의 MCTS 알고리즘에서 정책 네트워크와 가치 네트워크의 평가는 어떻게 결합되어 최종 수 선택에 영향을 미치나요?

9. AlphaGo가 이전의 최고 바둑 프로그램들을 압도하고 인간 프로 기사를 이길 수 있었던 주요 요인들은 무엇이라고 생각하나요?

10. AlphaGo 연구의 의의는 바둑 분야를 넘어 인공지능의 다른 도전 과제에 대해 어떤 시사점을 던져주나요?

### 10.3 퀴즈 정답

1. 완전 정보 게임에서는 모든 플레이어가 게임의 모든 시점에서 완벽한 정보를 알 수 있는 반면, 불완전 정보 게임에서는 일부 정보가 가려져 있습니다. 바둑은 모든 돌의 위치가 공개되어 있고 운 요소가 없으므로 완전 정보 게임에 속합니다.

2. 탐색 트리의 넓이와 깊이가 클수록 탐색해야 할 가능한 움직임의 수가 기하급수적으로 증가하여 현실적인 시간 안에 모든 경우를 탐색하는 것이 불가능해집니다. 바둑은 넓이가 약 250, 깊이가 약 150으로 매우 큰 탐색 공간을 가집니다.

3. 정책 네트워크는 주어진 바둑판 상태에서 유망한 다음 수를 예측하는 역할을 하며, 가치 네트워크는 주어진 바둑판 상태에서 현재 플레이어의 승리 확률을 예측하는 역할을 합니다. 정책 네트워크는 지도 학습과 강화 학습을 통해, 가치 네트워크는 자기 대국 데이터를 이용한 회귀 학습을 통해 학습됩니다.

4. MCTS는 몬테카를로 롤아웃을 반복적으로 수행하여 탐색 트리의 각 노드 가치를 통계적으로 추정하는 알고리즘입니다. 주요 네 단계는 선택, 확장, 평가, 역전파입니다.

5. 인간 전문가의 기보는 바둑의 기본적인 전략과 유망한 수의 경향을 담고 있으므로, 초기 정책 네트워크가 좋은 성능을 빠르게 학습하도록 돕습니다. 이 단계의 학습 방법을 감독 학습이라고 부릅니다.

6. 강화 학습 단계의 목표는 정책 네트워크가 단순히 인간의 수를 흉내 내는 것이 아니라 게임에서 승리하는 것을 최적화하도록 개선하는 것입니다. 자기 대국을 통해 다양한 게임 상황을 경험하고 승패를 통해 스스로 학습할 수 있다는 이점이 있습니다.

7. 전체 게임 데이터 내의 연속적인 포지션들은 매우 높은 상관관계를 가지므로, 이를 사용하여 가치 네트워크를 학습시키면 새로운 포지션에 대한 일반화 능력이 떨어지는 과적합 문제가 발생하기 쉽습니다. 자기 대국에서 추출한 고유한 포지션들은 데이터의 다양성을 높여 과적합을 방지하고 가치 네트워크의 일반화 능력을 향상시킵니다.

8. AlphaGo의 MCTS는 탐색 과정에서 정책 네트워크가 제공하는 사전 확률을 활용하여 탐색의 방향을 설정하고, 리프 노드에 도달했을 때 가치 네트워크가 예측하는 가치와 롤아웃 시뮬레이션 결과를 혼합하여 해당 노드의 가치를 평가합니다. 이 평가는 역전파를 통해 트리의 액션 가치를 업데이트하고 최종적으로 가장 유망한 수를 선택하는 데 기여합니다.

9. 심층 신경망을 이용한 강력한 정책 네트워크와 가치 네트워크, 그리고 이들을 효과적으로 결합한 몬테카를로 트리 탐색 알고리즘 덕분에 AlphaGo는 넓고 깊은 탐색 공간과 복잡한 가치 판단이라는 바둑의 어려움을 극복하고 뛰어난 성능을 보일 수 있었습니다.

10. AlphaGo의 성공은 심층 학습과 트리 검색의 결합이 바둑과 같이 오랫동안 인공지능의 난제로 여겨졌던 영역에서 인간 수준의 성능을 달성할 수 있음을 보여주었으며, 이는 다른 복잡한 의사 결정 문제에 대한 인공지능 접근 방식에 대한 희망을 제시합니다.

### 10.4 논술형 질문
1. AlphaGo가 바둑에서 인간 프로 기사를 넘어선 것은 인공지능 역사에서 어떤 의미를 가지며, 이 성과가 미래의 인공지능 연구에 미칠 영향에 대해 논하시오.

2. AlphaGo의 핵심 요소 기술인 심층 신경망 (정책 네트워크와 가치 네트워크)과 몬테카를로 트리 탐색 (MCTS)은 각각 어떤 강점과 약점을 가지며, 이 두 가지 접근 방식을 결합함으로써 얻는 시너지 효과는 무엇인지 분석하시오.

3. AlphaGo의 학습 과정은 감독 학습, 강화 학습, 자기 대국 등 여러 단계를 거칩니다. 각 학습 단계의 목표와 중요성을 설명하고, 이 순서로 학습하는 것이 효과적인 이유에 대해 논하시오.

4. AlphaGo가 바둑의 복잡성을 해결하기 위해 사용한 방법 (예: 깊이와 넓이 축소, 위치 평가, 정책 기반 행동 샘플링)을 분석하고, 이러한 방법들이 다른 어려운 탐색 문제 해결에 어떻게 적용될 수 있을지 논하시오.

5. AlphaGo의 성공 이후, 바둑 인공지능 분야는 어떻게 발전해 왔으며, AlphaGo의 방법론이 다른 게임 인공지능 또는 일반 인공지능 분야에 미친 구체적인 영향을 설명하시오.

### 10.5 용어 해설

1. 게임 이론 (Game Theory): 전략적 상호작용을 수학적으로 분석하는 학문 분야.

2. 마르코프 결정 과정 (Markov Decision Process - MDP): 상태, 행동, 보상, 상태 전이 확률로 정의되는 수학적 프레임워크로, 강화 학습 문제의 모델링에 사용됨.

3. 알파-베타 가지치기 (Alpha-Beta Pruning): 탐색 트리의 모든 노드를 탐색하지 않고 최적의 수를 찾는 데 불필요한 부분을 제거하는 탐색 알고리즘의 최적화 기법.

4. 신경망 (Neural Network): 인간 뇌의 구조를 모방한 정보 처리 시스템으로, 여러 개의 연결된 노드(뉴런)로 구성되어 복잡한 패턴을 학습할 수 있음.

5. 컨볼루션 신경망 (Convolutional Neural Network - CNN): 이미지 처리, 음성 인식 등 공간적 계층 구조를 가지는 데이터 처리에 특화된 신경망의 한 종류.

6. 활성화 함수 (Activation Function): 신경망의 각 뉴런에서 입력 신호에 대한 출력 신호를 결정하는 비선형 함수.

7. 소프트맥스 함수 (Softmax Function): 벡터의 값들을 확률 분포로 변환하는 함수로, 정책 네트워크의 최종 출력 계층에서 사용됨.

8. 가중치 (Weights): 신경망 내의 연결 강도를 나타내는 파라미터로, 학습 과정에서 조정됨.

9. 편향 (Bias): 신경망 뉴런의 활성화 임계값을 조절하는 파라미터.

10. 손실 함수 (Loss Function): 예측 값과 실제 값의 차이를 측정하는 함수로, 학습의 목표는 이 손실 함수의 값을 최소화하는 것임.

11. 경사 하강법 (Gradient Descent): 손실 함수의 경사를 따라 파라미터를 반복적으로 업데이트하여 손실 함수를 최소화하는 최적화 알고리즘.

12. 학습률 (Learning Rate): 경사 하강법에서 파라미터를 업데이트하는 단계의 크기를 결정하는 하이퍼파라미터.

13. 과적합 (Overfitting): 학습 데이터에만 지나치게 잘 맞고 새로운 데이터에 대한 예측 성능이 떨어지는 현상.

14. 규제화 (Regularization): 과적합을 방지하기 위해 모델의 복잡성을 줄이는 기법.

15. 하이퍼파라미터 (Hyperparameter): 학습 과정 이전에 사용자가 직접 설정해야 하는 파라미터 (예: 학습률, 신경망 계층 수 등).

15. ELO 레이팅 시스템 (Elo Rating System): 체스나 바둑과 같은 경쟁 게임에서 플레이어의 상대적인 실력을 평가하는 시스템.

16. 코미 (Komi): 바둑에서 백돌을 쥔 플레이어에게 주어지는 추가 점수로, 선후수 불균형을 보정하는 역할을 함.

17. 핸디캡 (Handicap): 실력 차이가 큰 플레이어 간의 대국에서 약한 플레이어가 유리하도록 미리 돌을 놓거나 기타 이점을 주는 것.


## 11. **Generative Adversarial Networks (2014, Goodfellow et al.)** 

생성적 적대 신경망 연구 

### 11.1 퀴즈
1. 생성적 적대 신경망(GAN) 프레임워크의 핵심 아이디어는 무엇입니까? 이 프레임워크에는 어떤 두 가지 모델이 동시에 학습됩니까?

2. 판별자 모델(D)의 학습 목표는 무엇이며, 생성자 모델(G)의 학습 목표는 무엇입니까? 이 두 목표는 어떤 관계를 갖습니까?

3. GAN 학습을 "미니맥스 두 플레이어 게임"이라고 설명하는 이유는 무엇입니까? 이 게임의 목표는 무엇이며, 언제 종료됩니까?

4. GAN 학습 시 마르코프 연쇄나 펼쳐진 근사 추론 네트워크가 필요 없는 이유는 무엇입니까? 이것이 이전의 심층 생성 모델과 비교했을 때 어떤 이점을 제공합니까?

5. GAN 논문에서 저자들이 제안하는 실제적인 학습 알고리즘은 무엇입니까? 판별자와 생성자는 학습 과정에서 어떻게 업데이트됩니까?

6. GAN 학습 초기에 생성자가 제대로 학습되지 않았을 때 발생할 수 있는 문제점은 무엇이며, 저자들은 이 문제를 해결하기 위해 어떤 대안적인 목표 함수를 제안합니까?

7. GAN의 이론적 분석에서 $p_g = p_{data}$일 때 전역 최적점에 도달한다는 것을 어떻게 증명합니까? 이때 판별자 $D^*(x)$의 값은 얼마입니까?

8. GAN 프레임워크가 명시적인 확률 밀도 함수 $p_g(x)$를 갖지 않는 것은 어떤 단점을 가질 수 있습니까?

9. GAN의 잠재적인 장점 중 하나는 판별자를 통해 역전파되는 그래디언트만으로 생성자가 업데이트된다는 점입니다. 이는 어떤 통계적 이점을 제공할 수 있습니까?

10. 논문에서 제시된 GAN 프레임워크의 몇 가지 확장 또는 응용 분야는 무엇입니까?

### 11.2 퀴즈 정답

1. GAN 프레임워크의 핵심 아이디어는 적대적 프로세스를 통해 생성 모델을 추정하는 것입니다. 이 프레임워크에서는 데이터 분포를 포착하는 생성 모델 G와 샘플이 G에서 생성된 것인지 훈련 데이터에서 온 것인지를 판별하는 판별 모델 D라는 두 가지 모델이 동시에 학습됩니다.

2. 판별자 모델 D의 학습 목표는 훈련 데이터 샘플과 생성자 G가 생성한 가짜 샘플을 올바르게 분류할 확률을 최대화하는 것입니다. 생성자 모델 G의 학습 목표는 판별자 D가 자신의 가짜 샘플을 실제 데이터로 잘못 분류할 확률을 최대화하는 것입니다. 즉, G는 D를 속이려고 하고 D는 G를 разоблачить하려고 합니다.

3. GAN 학습을 미니맥스 두 플레이어 게임이라고 하는 이유는 G는 $log(1 - D(G(z)))$를 최소화하려고 하고, D는 $log(D(x)) + log(1 - D(G(z)))$를 최대화하려고 하기 때문입니다. 이 게임의 목표는 G가 훈련 데이터 분포를 완벽하게 복구하고 D가 모든 곳에서 1/2이 되는 균형점(안장점)에 도달하는 것입니다.

4. GAN 학습 시에는 생성 모델과 판별 모델 모두 다층 퍼셉트론과 역전파 알고리즘만을 사용하여 학습할 수 있기 때문에 마르코프 연쇄나 펼쳐진 근사 추론 네트워크가 필요하지 않습니다. 이는 이전의 많은 심층 생성 모델들이 복잡한 확률 계산과 근사 추론에 의존했던 것과 비교했을 때 학습 과정을 단순화하고 효율성을 높이는 이점을 제공합니다.

5. 저자들이 제시하는 실제적인 학습 알고리즘은 미니배치 확률적 경사 하강법을 사용하며, 판별자 D를 $k$번 업데이트한 후 생성자 G를 1번 업데이트하는 방식을 번갈아 수행합니다. 판별자는 실제 데이터와 생성된 데이터의 로그 확률을 최대화하는 방향으로 업데이트되고, 생성자는 판별자가 생성된 데이터를 실제 데이터로 잘못 분류할 로그 확률을 최대화하는 방향으로 (또는 $log(1 - D(G(z)))$를 최소화하는 방향으로) 업데이트됩니다.

6. GAN 학습 초기에 생성자가 제대로 학습되지 않았을 때 판별자는 생성된 샘플을 실제 데이터와 매우 쉽게 구별할 수 있게 되어 $D(G(z))$ 값이 0에 가까워지고 $log(1 - D(G(z)))$가 포화되어 생성자에게 충분한 그래디언트가 제공되지 않는 문제가 발생할 수 있습니다. 저자들은 이 문제를 해결하기 위해 생성자를 $log(1 - D(G(z)))$를 최소화하는 대신 $log(D(G(z)))$를 최대화하는 방향으로 학습시키는 대안적인 목표 함수를 제안합니다.

7. GAN의 이론적 분석에서 $p_g = p_{data}$일 때 전역 최적점에 도달한다는 것은 판별자 $D_G^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$가 되고, 이를 통해 $C(G) = \mathbb{E}{x \sim p{data}}[\log D_G^*(x)] + \mathbb{E}{x \sim p_g}[\log(1 - D_G^*(x))]$가 $-\log 4 + 2 \cdot JSD(p{data} | p_g)$로 표현될 수 있음을 보임으로써 증명됩니다. 젠슨-섀넌 발산 $JSD(p_{data} | p_g)$는 항상 0 이상이며 $p_{data} = p_g$일 때만 0이 되므로, $C(G)$의 최솟값은 $-\log 4$이며 이는 $p_g = p_{data}$일 때만 달성됩니다. 이때 판별자 $D^*(x)$의 값은 1/2입니다.

8. GAN 프레임워크가 명시적인 확률 밀도 함수 $p_g(x)$를 갖지 않는 것은 생성 모델의 성능을 정량적으로 평가하기 어렵게 만들 수 있다는 단점을 가집니다. 논문에서는 생성된 샘플에 가우시안 파젠 창을 피팅하여 로그 우도를 추정하는 방법을 사용했지만, 이는 분산이 높고 고차원 공간에서 잘 작동하지 않는다는 한계가 있습니다.

9. GAN의 잠재적인 장점 중 하나는 생성자가 실제 데이터 샘플을 직접 사용하는 대신 판별자를 통해 역전파되는 그래디언트만으로 업데이트된다는 점입니다. 이는 입력의 특정 구성 요소가 생성자의 파라미터에 직접 복사되는 것을 방지하여 생성자가 훈련 데이터를 단순히 암기하는 것이 아니라 데이터의 근본적인 분포를 학습하는 데 도움이 될 수 있다는 통계적 이점을 제공할 수 있습니다.

10. 논문에서 제시된 GAN 프레임워크의 몇 가지 확장 또는 응용 분야로는 조건부 생성 모델 (레이블 또는 다른 정보를 입력으로 사용하여 특정 유형의 데이터를 생성), 학습된 근사 추론 (입력 데이터 $x$가 주어졌을 때 잠재 변수 $z$를 예측하는 보조 네트워크 학습), 준지도 학습 (판별자 또는 추론 네트워크의 특징을 사용하여 제한된 레이블 데이터로 분류기의 성능 향상) 등이 있습니다.

### 11.3 에세이 형식 질문

1. 생성적 적대 신경망(GAN) 프레임워크의 창의적인 측면과 이전의 심층 생성 모델 접근 방식과 비교했을 때 주요 혁신 사항은 무엇인지 논의하십시오.

2. GAN 학습 과정에서 생성자와 판별자 사이의 균형을 유지하는 것의 중요성을 설명하고, 이 균형이 깨졌을 때 발생할 수 있는 문제점과 이를 해결하기 위한 전략을 제시하십시오.

3. GAN이 다양한 데이터셋에서 인상적인 생성 결과를 보여주었음에도 불구하고, 이 모델의 학습 및 평가에는 여전히 몇 가지 어려움이 존재합니다. 이러한 어려움들을 구체적으로 설명하고, 앞으로 GAN 연구가 어떤 방향으로 나아가야 할지 제안하십시오.

4. 판별자 모델이 학습 과정에서 어떤 역할을 수행하는지 상세히 분석하고, 잘 학습된 판별자가 좋은 생성자를 만드는 데 어떻게 기여하는지 설명하십시오. 판별자의 설계 및 학습 전략이 GAN의 전체 성능에 미치는 영향에 대해 논하십시오.

5. GAN 프레임워크가 갖는 이론적 의미와 실제적인 한계를 논하고, 이론적 결과가 실제 딥러닝 모델 학습에 어떻게 적용될 수 있는지, 그리고 이론과 실제 사이의 간극을 줄이기 위해 어떤 연구들이 더 필요하다고 생각하는지 설명하십시오.

### 11.4 용어 해설

1. 생성 모델 (Generative Model): 데이터의 확률 분포를 학습하여 새로운 데이터를 생성할 수 있는 모델.

2. 판별 모델 (Discriminative Model): 주어진 입력이 실제 데이터인지 생성된 데이터인지 판별하는 모델 (분류 모델).

3. 적대적 프로세스 (Adversarial Process): 두 개 이상의 모델이 서로 경쟁하면서 학습하는 방식. GAN에서는 생성자와 판별자가 서로의 성능을 향상시키기 위해 경쟁한다.

4. 미니맥스 게임 (Minimax Game): 두 플레이어가 서로의 이익을 최대화하고 상대방의 이익을 최소화하려고 하는 게임 이론의 개념. GAN 학습은 생성자는 판별자의 오류를 최대화하고 판별자는 자신의 정확도를 최대화하는 미니맥스 게임으로 볼 수 있다.

5. 역전파 (Backpropagation): 신경망의 출력층에서 입력층으로 오차를 전파하여 각 가중치를 업데이트하는 알고리즘. GAN의 생성 모델과 판별 모델 모두 역전파를 사용하여 학습된다.

6. 마르코프 연쇄 (Markov Chain): 이전 상태에만 의존하는 일련의 무작위 변수들의 시퀀스. 이전의 일부 생성 모델에서는 샘플링 과정에 마르코프 연쇄가 사용되었지만, GAN은 이를 필요로 하지 않는다.

7. 근사 추론 (Approximate Inference): 복잡한 확률 모델에서 정확한 추론이 어려울 때, 확률 분포를 근사하거나 중요한 통계적 속성을 추정하는 방법. GAN은 학습 과정에서 근사 추론을 필요로 하지 않는다.

8. 다층 퍼셉트론 (Multilayer Perceptron, MLP): 여러 개의 은닉층을 가진 기본적인 형태의 피드포워드 신경망. GAN 논문에서는 생성 모델과 판별 모델 모두 MLP로 구현되는 특별한 경우를 주로 다룬다.

9. 드롭아웃 (Dropout): 신경망 학습 시 과적합을 방지하기 위해 무작위로 일부 뉴런의 출력을 0으로 만드는 규제 기법. 판별 모델 학습에 적용된다.

10. 정류 선형 유닛 (Rectified Linear Unit, ReLU): $f(x) = \max(0, x)$ 형태의 활성화 함수. 생성 모델과 판별 모델에 사용된다.

11. 시그모이드 함수 (Sigmoid Function): 출력을 0과 1 사이의 값으로 변환하는 활성화 함수. 주로 판별 모델의 최종 출력층에 사용되어 확률을 나타낸다.

12. 맥스아웃 (Maxout): 여러 선형 함수의 최댓값을 출력으로 하는 활성화 함수. 판별 모델에 사용된다.

13. 노이즈 대비 추정 (Noise-Contrastive Estimation, NCE): 생성 모델을 학습하기 위한 방법 중 하나로, 모델 분포와 고정된 노이즈 분포를 구별하도록 학습시킨다. GAN과 유사한 경쟁 메커니즘을 갖지만, 판별자가 확률 밀도의 비율로 정의된다는 차이가 있다.

14. 예측 가능성 최소화 (Predictability Minimization): 신경망의 각 은닉 유닛이 다른 모든 은닉 유닛의 값을 예측하는 두 번째 네트워크의 출력과 달라지도록 학습시키는 방법. GAN과는 경쟁의 성격과 학습 목표가 다르다.

15. 적대적 예제 (Adversarial Examples): 분류기가 오분류하도록 의도적으로 미세한 섭동을 가한 입력 예제. GAN 학습과는 다른 개념이지만, 적대적 예제의 존재는 GAN 학습의 효율성에 영향을 미칠 수 있음을 시사한다.

16. 로그 우도 (Log-likelihood): 주어진 데이터에 대해 특정 확률 모델이 얼마나 적합한지를 나타내는 척도. GAN은 명시적인 로그 우도를 갖지 않으므로, 파젠 창 추정 등의 방법을 사용하여 간접적으로 평가한다.

17. 파젠 창 (Parzen Window): 비모수적 밀도 추정 방법 중 하나. 생성 모델이 생성한 샘플들을 사용하여 확률 밀도를 추정하고 로그 우도를 계산하는 데 사용된다.

18. 젠슨-섀넌 발산 (Jensen-Shannon Divergence, JSD): 두 확률 분포 간의 유사성을 측정하는 방법. GAN의 이론적 분석에서 생성된 분포와 실제 데이터 분포 간의 거리를 나타내는 데 사용된다.

19. 쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL Divergence): 한 확률 분포가 다른 확률 분포와 얼마나 다른지를 측정하는 비대칭적인 척도. JSD의 유도에 사용된다.

20. 조건부 생성 모델 (Conditional Generative Model): 특정 조건(예: 클래스 레이블)이 주어졌을 때 해당 조건에 맞는 데이터를 생성하는 모델. GAN에 조건을 입력으로 추가하여 구현할 수 있다.

21. 준지도 학습 (Semi-supervised Learning): 레이블이 있는 데이터와 레이블이 없는 데이터를 모두 사용하여 학습하는 방법. GAN의 판별자 또는 추론 네트워크의 특징을 활용하여 분류기의 성능을 향상시킬 수 있다.

## 12. **Auto-Encoding Variational Bayes (2013, Kingma & Welling)** 

자기 부호화 변분 베이즈 학습 가이드

### 12.1 퀴즈

1. 자기 부호화 변분 베이즈(AEVB) 알고리즘은 어떤 종류의 확률 모델에 적합하며, 기존 방법들과 비교했을 때 주요 장점은 무엇인가요? (2-3 문장)

2. 변분 하한(variational lower bound)은 주변 우도(marginal likelihood)와 어떤 관계를 가지며, AEVB 알고리즘에서 이 하한을 최적화하는 것이 중요한 이유는 무엇인가요? (2-3 문장)

3. 재매개변수화 기법(reparameterization trick)은 AEVB에서 어떻게 활용되며, 이 기법이 변분 하한의 기울기를 추정하는 데 왜 중요한 역할을 하나요? (2-3 문장)

4.AEVB 알고리즘에서 인식 모델(recognition model) 또는 확률적 인코더(probabilistic encoder)의 역할은 무엇이며, 이는 전통적인 평균장 변분 추론(mean-field variational inference)과 어떻게 다른가요? (2-3 문장)

5. 생성 모델(generative model) 또는 확률적 디코더(probabilistic decoder)는 AEVB에서 어떤 기능을 수행하며, 학습된 잠재 변수(latent variable) 표현을 통해 무엇을 할 수 있게 되나요? (2-3 문장)

6. AEVB 알고리즘의 목적 함수(objective function)는 크게 두 가지 항으로 구성됩니다. 각 항은 무엇을 의미하며, 전체적으로 어떤 균형을 이루도록 학습이 진행되나요? (2-3 문장)

7. 본 논문에서 소개된 SGVB(Stochastic Gradient Variational Bayes) 추정량에는 두 가지 버전(L̃A와 L̃B)이 있습니다. 이 두 추정량의 주요 차이점과 각각의 장단점을 간략하게 설명해주세요. (2-3 문장)

8. 미니배치(minibatch)를 사용한 AEVB 알고리즘의 학습 과정은 어떻게 이루어지며, 대규모 데이터셋에 대한 효율적인 학습이 가능한 이유는 무엇인가요? (2-3 문장)

9. 깨어-잠 알고리즘(Wake-Sleep algorithm)은 본 논문에서 AEVB와 비교되는 기존의 온라인 학습 방법입니다. 깨어-잠 알고리즘의 주요 특징과 AEVB와의 차이점을 설명해주세요. (2-3 문장)

10. AEVB 알고리즘을 학습하여 얻은 인코더(encoder)는 고차원 데이터를 저차원 잠재 공간으로 투영하는 데 활용될 수 있습니다. 이러한 투영은 어떤 종류의 분석이나 작업에 유용하게 사용될 수 있나요? (2-3 문장)

### 12.2 퀴즈 정답

1. AEVB 알고리즘은 연속적인 잠재 변수를 가지며 사후 분포가 다루기 어려운 유향 확률 모델에 적합합니다. 주요 장점은 대규모 데이터셋에 대해 효율적인 추론 및 학습이 가능하며, 다루기 어려운 경우에도 확률적 기울기 방법을 사용하여 최적화할 수 있다는 것입니다.

2. 변분 하한은 항상 주변 우도보다 작거나 같은 값을 가지는 하한이며, AEVB에서는 이 하한을 최대화함으로써 실제 주변 우도를 간접적으로 높이는 것을 목표로 합니다. 주변 우도를 직접 계산하거나 최적화하기 어려운 경우에 변분 하한을 활용합니다.

3. 재매개변수화 기법은 잠재 변수 z를 결정론적 함수 gφ(ε,x)로 표현하여, 독립적인 노이즈 변수 ε로부터 샘플링할 수 있도록 합니다. 이를 통해 변분 하한의 기댓값을 몬테카를로 추정할 때 기울기를 해석적으로 계산할 수 있게 되어 효율적인 최적화가 가능해집니다.

4. 인식 모델은 관측된 데이터 x를 기반으로 잠재 변수 z의 사후 분포 qφ(z|x)를 근사하는 모델입니다. 이는 전통적인 평균장 변분 추론에서 사후 분포의 파라미터를 해석적인 기댓값 계산을 통해 얻는 것과 달리, AEVB에서는 인식 모델 자체를 학습한다는 점에서 차이가 있습니다.

5. 생성 모델은 잠재 변수 z가 주어졌을 때 관측 데이터 x의 조건부 분포 pθ(x|z)를 정의하는 모델입니다. 학습된 잠재 변수 표현을 통해 새로운 데이터를 생성하거나, 데이터의 내부 구조를 이해하고 표현하는 데 활용될 수 있습니다.

6. AEVB의 목적 함수는 KL 발산 항 (DKL(qφ(z|x)||pθ(z)))과 재구성 오차 항 (Eqφ(z|x)[log pθ(x|z)])으로 구성됩니다. KL 발산 항은 근사 사후 분포가 사전 분포와 유사하도록 규제하는 역할을 하며, 재구성 오차 항은 잠재 변수로부터 원본 데이터를 잘 복원하도록 학습합니다.

7. L̃A 추정량은 결합 분포 pθ(x, z)와 근사 사후 분포 qφ(z|x)를 직접 샘플링하여 계산하는 일반적인 추정량이며, L̃B 추정량은 KL 발산 항을 해석적으로 계산할 수 있는 경우에 사용되며 재구성 오차 항만 샘플링하여 계산하므로 일반적으로 분산이 더 작습니다.

8. AEVB는 전체 데이터셋 대신 무작위로 추출한 작은 미니배치를 사용하여 각 파라미터 업데이트를 수행합니다. 각 배치에 대한 기울기를 계산하고 이를 사용하여 모델 파라미터를 점진적으로 업데이트하므로, 전체 데이터셋을 한 번에 처리하는 것보다 훨씬 효율적으로 대규모 데이터셋을 학습할 수 있습니다.

9. 깨어-잠 알고리즘 역시 인식 모델을 사용하여 사후 분포를 근사하지만, 주변 우도의 하한을 직접 최적화하는 대신 두 개의 별도 목적 함수를 동시에 최적화합니다. 또한, 이 알고리즘은 이산 잠재 변수 모델에도 적용 가능하다는 장점이 있습니다.

10. 인코더를 통해 얻은 저차원 잠재 표현은 데이터의 군집화, 시각화, 이상 감지, 특징 추출 등 다양한 작업에 활용될 수 있습니다. 특히 2차원 또는 3차원 잠재 공간으로 투영하면 고차원 데이터의 구조를 직관적으로 이해하는 데 도움이 됩니다.

### 12.3 에세이 형식 질문

1. 자기 부호화기(Autoencoder)의 개념과 변분 추론(Variational Inference)의 원리를 설명하고, AEVB가 이 두 가지 아이디어를 어떻게 결합하여 효율적인 생성 모델 학습을 가능하게 하는지 논하시오.

2. AEVB 알고리즘에서 재매개변수화 기법의 중요성을 자세히 설명하고, 이 기법이 없었다면 확률적 경사 하강법(Stochastic Gradient Descent)을 변분 하한 최적화에 적용하는 데 어떤 어려움이 있었을지 분석하시오.

3. 본 논문에서 AEVB 알고리즘과 비교된 깨어-잠 알고리즘 및 몬테카를로 EM 알고리즘의 장단점을 비교 분석하고, 각 알고리즘이 어떤 상황에 더 적합할지 논하시오.

4. AEVB 알고리즘의 잠재 공간(latent space)이 학습 과정에서 어떤 역할을 수행하며, 학습된 잠재 공간의 특성이 생성 모델의 성능과 데이터 표현 능력에 미치는 영향에 대해 심층적으로 논하시오.

5. 본 논문에서 제시된 AEVB 알고리즘의 확장 가능성 및 향후 연구 방향을 바탕으로, 이 방법론이 머신러닝 및 인공지능 분야에 기여할 수 있는 잠재력과 앞으로 해결해야 할 과제에 대해 자신의 견해를 제시하시오.

### 12.4 용어 해설
1. 변분 베이즈 (Variational Bayes, VB): 다루기 어려운 사후 분포를 더 간단한 분포족으로 근사하고, 이 근사 분포의 파라미터를 최적화하여 실제 사후 분포를 찾는 방법.

2. 변분 하한 (Variational Lower Bound): 주변 우도(marginal likelihood)의 하한으로, 변분 추론에서 최적화의 대상이 되는 함수. 이 하한을 최대화하면 근사 사후 분포가 실제 사후 분포에 더 가까워지고 주변 우도도 증가하게 된다.

3. 잠재 변수 (Latent Variable): 관측되지 않았지만 관측된 데이터의 생성 과정을 설명하는 데 사용되는 확률 변수.

4. 생성 모델 (Generative Model): 데이터의 분포를 학습하여 새로운 데이터를 생성할 수 있는 모델. AEVB에서는 잠재 변수에서 데이터를 생성하는 확률 분포 pθ(x|z)로 표현된다.

5. 인식 모델 (Recognition Model): 관측된 데이터 x를 기반으로 잠재 변수 z의 사후 분포 qφ(z|x)를 근사하는 모델. AEVB에서는 확률적 인코더라고도 불린다.

6. 재매개변수화 기법 (Reparameterization Trick): 잠재 변수 z를 독립적인 노이즈 변수 ε의 결정론적 함수 gφ(ε,x)로 표현하는 방법. 이를 통해 변분 하한의 기댓값에 대한 기울기를 효율적으로 추정할 수 있다.

7. 확률적 기울기 변분 베이즈 (Stochastic Gradient Variational Bayes, SGVB): 변분 하한의 기울기를 몬테카를로 추정을 통해 얻고, 확률적 경사 하강법과 같은 최적화 알고리즘을 사용하여 변분 파라미터를 학습하는 방법.

8. 자기 부호화기 (Autoencoder): 입력을 저차원 표현으로 인코딩하고, 이 표현으로부터 다시 원래 입력을 디코딩하는 신경망 모델. AEVB는 변분 추론을 자기 부호화기에 적용한 형태이다.

9. KL 발산 (Kullback-Leibler Divergence): 두 확률 분포 간의 차이를 측정하는 비대칭적인 척도. 변분 추론에서는 근사 사후 분포와 실제 사후 분포 간의 KL 발산을 최소화하는 것을 목표로 한다.

10. 주변 우도 (Marginal Likelihood): 모델 파라미터 θ를 고정했을 때 관측 데이터 X가 발생할 확률 pθ(X). AEVB의 목표는 이 주변 우도를 높이는 것이다.

## 13. **Neural Architecture Search with Reinforcement Learning (2016, Zoph & Le)** 

신경망 구조 탐색 (NAS) 연구 가이드

### 13.1 퀴즈

1. 신경망 구조 탐색 (Neural Architecture Search, NAS)의 핵심 아이디어는 무엇이며, 어떤 유형의 모델을 사용하여 이를 구현합니까?
   - NAS는 신경망 아키텍처의 설계 과정을 자동화하기 위해 순환 신경망(RNN) 기반의 컨트롤러를 사용하여 다양한 아키텍처를 생성하고 강화 학습을 통해 성능이 높은 아키텍처를 학습하는 방법입니다. 컨트롤러 RNN은 가변 길이 문자열로 신경망 구조를 설명하는 하이퍼파라미터를 예측합니다.
2.NAS에서 컨트롤러 RNN은 생성된 신경망 아키텍처의 성능을 어떻게 학습하고 개선해 나갑니까?
   - 컨트롤러 RNN이 생성한 "자식 네트워크"는 실제 데이터로 학습되고 검증 세트에서의 정확도가 보상 신호로 사용됩니다. 이 보상 신호를 사용하여 정책 경도 방법을 통해 컨트롤러 RNN의 파라미터를 업데이트하여, 높은 정확도를 얻는 아키텍처를 생성할 확률을 높입니다.
3. NAS에서 보상 신호로 사용되는 것은 무엇이며, 왜 이러한 보상 신호에 정책 경도 방법을 적용해야 합니까?
   - 보상 신호는 자식 네트워크가 검증 세트에서 달성한 정확도입니다. 이 정확도는 미분 불가능한(non-differentiable) 측정 기준이기 때문에, 컨트롤러 RNN의 파라미터를 최적화하기 위해 정책 경도와 같은 강화 학습 방법을 사용해야 합니다.
4. NAS 훈련 과정의 안정성과 효율성을 높이기 위해 사용되는 두 가지 주요 기술은 무엇이며, 각각 어떤 역할을 합니까?
   - 첫째, 기준 함수(baseline function)를 사용하여 정책 경도 추정치의 분산을 줄입니다. 이는 이전 아키텍처 정확도의 지수 이동 평균을 사용합니다. 둘째, 분산 훈련 및 비동기 파라미터 업데이트를 통해 여러 자식 네트워크를 병렬로 훈련시켜 컨트롤러의 학습 속도를 가속화합니다.
5. NAS가 더 복잡한 신경망 구조(예: 스킵 연결)를 탐색할 수 있도록 어떻게 확장되었습니까?
   - 스킵 연결 또는 분기 레이어를 예측할 수 있도록 앵커 포인트(anchor point)와 집합 선택(set-selection) 유형의 어텐션 메커니즘을 도입했습니다. 각 레이어에서 이전 레이어와의 연결 여부를 결정하는 시그모이드 함수를 사용하여 스킵 연결을 확률적으로 생성합니다.
6. NAS를 사용하여 순환 신경망(RNN) 셀 아키텍처를 생성하는 방법의 핵심 아이디어는 무엇입니까?
   - RNN 셀의 계산 과정을 입력(xt, ht-1)을 받아 최종 출력(ht)을 생성하는 트리 구조로 일반화합니다. 컨트롤러 RNN은 이 트리의 각 노드에 대한 결합 방법(덧셈, 요소별 곱셈 등)과 활성화 함수(tanh, sigmoid 등)를 순차적으로 예측합니다.
7. CIFAR-10 이미지 분류 작업에서 NAS가 발견한 주요 혁신적인 아키텍처 특징은 무엇입니까?
   - NAS는 스킵 연결을 많이 포함하고, 초기 레이어에서 더 큰 필터를 선호하는 얕지만 성능이 좋은 컨볼루션 신경망 아키텍처를 발견했습니다. 또한, 스트라이드와 풀링 레이어를 예측할 수 있도록 확장하여 더 깊고 성능이 뛰어난 아키텍처를 설계할 수 있었습니다.
8. Penn Treebank (PTB) 언어 모델링 작업에서 NAS가 발견한 새로운 순환 셀의 주요 성과는 무엇입니까?
   - NAS가 발견한 새로운 순환 셀은 널리 사용되는 LSTM 셀을 포함한 이전 최고 성능의 모델들을 능가하는 테스트 세트 퍼플렉서티를 달성했습니다. 또한, 이 셀은 더 빠르면서도 더 나은 성능을 보였습니다.
9. NAS가 발견한 순환 셀이 다른 작업으로 일반화될 수 있는지 어떻게 검증했습니까? 그 결과는 어떠했습니까?
   - 새로운 순환 셀을 PTB 문자 수준 언어 모델링 작업에 적용하여 일반화 가능성을 검증했습니다. 그 결과, 새로운 셀은 LSTM 셀보다 더 나은 성능을 보였으며, 더 큰 모델에서는 새로운 최고 성능(state-of-the-art)을 달성했습니다. 또한, GNMT 번역 프레임워크에 적용하여 BLEU 점수 향상을 확인했습니다.
10. NAS 연구의 주요 의의와 앞으로의 연구 방향에 대해 간략하게 설명하십시오.
   - NAS 연구는 신경망 아키텍처 설계를 자동화하는 새로운 가능성을 제시하며, 전문가의 지식 없이도 혁신적인 고성능 아키텍처를 발견할 수 있음을 보여줍니다. 향후 연구 방향으로는 탐색 공간 확장, 탐색 효율성 향상, 다양한 작업 및 데이터셋에 대한 일반화 능력 향상 등이 있습니다.

### 13.2 답안 해설
1. 신경망 구조 탐색 (Neural Architecture Search, NAS)의 핵심 아이디어는 신경망 아키텍처 설계 과정을 자동화하는 것이며, 이를 위해 순환 신경망(RNN) 기반의 컨트롤러를 사용합니다. 컨트롤러 RNN은 신경망 구조를 정의하는 하이퍼파라미터들을 순차적으로 예측합니다.

2. 컨트롤러 RNN은 생성된 신경망(자식 네트워크)을 학습시키고, 검증 세트에서의 정확도를 보상 신호로 활용합니다. 이 보상 신호를 사용하여 강화 학습의 정책 경도 방법을 통해 컨트롤러 RNN의 파라미터를 업데이트하여, 더 높은 정확도를 갖는 아키텍처를 생성할 가능성을 높입니다.

3. NAS에서 보상 신호는 자식 네트워크가 검증 세트에서 달성한 정확도입니다. 이 정확도는 미분 불가능하기 때문에, 컨트롤러 RNN이 더 나은 아키텍처를 생성하도록 학습시키기 위해 정책 경도와 같은 강화 학습 방법이 필요합니다.

4. NAS 훈련 과정의 안정성을 높이기 위해 기준 함수(baseline function)를 사용하여 정책 경도 추정치의 분산을 줄입니다. 효율성을 높이기 위해 분산 훈련 및 비동기 파라미터 업데이트를 통해 여러 자식 네트워크를 병렬로 훈련시켜 컨트롤러의 학습 속도를 가속화합니다.

5. NAS는 스킵 연결을 포함한 더 복잡한 구조를 탐색하기 위해 앵커 포인트와 집합 선택 어텐션을 사용합니다. 컨트롤러는 각 레이어에서 이전 레이어들을 입력으로 사용할지 여부를 확률적으로 결정하여 다양한 연결 패턴을 생성할 수 있습니다.

6. NAS를 사용하여 RNN 셀 아키텍처를 생성하는 핵심 아이디어는 RNN 셀의 계산 과정을 트리 구조로 표현하고, 컨트롤러 RNN이 이 트리의 각 노드에 적용할 연산(결합 방법 및 활성화 함수)을 예측하는 것입니다.

7. CIFAR-10에서 NAS는 스킵 연결을 많이 사용하고 초기 레이어에 큰 필터를 적용하는 아키텍처를 발견했습니다. 또한, 스트라이드 예측을 통해 더 깊은 네트워크를 탐색하고, 풀링 레이어를 통합하여 높은 성능을 달성했습니다.

8. PTB에서 NAS가 발견한 새로운 순환 셀은 이전 최고 성능의 LSTM 기반 모델보다 낮은 퍼플렉서티를 달성하며 언어 모델링 성능을 향상시켰습니다. 특히, 이전 최고 성능 모델보다 연산 속도가 빠르다는 장점도 있습니다.

9. NAS가 발견한 순환 셀의 일반화 가능성을 검증하기 위해 PTB 문자 수준 언어 모델링 작업에 적용한 결과, LSTM 셀보다 우수한 성능을 보였습니다. 더 나아가 GNMT 번역 모델에 적용했을 때도 성능 향상을 보여주었습니다.

10. NAS 연구는 신경망 설계를 자동화하고 새로운 고성능 아키텍처를 발견할 수 있는 가능성을 열었으며, 이는 딥러닝 연구의 중요한 진전입니다. 향후 연구는 탐색 공간, 효율성, 다양한 작업으로의 일반화 등을 개선하는 방향으로 진행될 수 있습니다.

### 13.3 논술형 문제

1. 신경망 구조 탐색 (NAS)이 전통적인 신경망 아키텍처 설계 방식과 비교하여 갖는 주요 장점과 단점은 무엇이며, NAS가 딥러닝 분야에 미치는 잠재적인 영향에 대해 논하십시오.

2. 본 논문에서 제시된 컨트롤러 RNN과 정책 경도 기반의 NAS 방법 외에, 신경망 아키텍처를 탐색하기 위한 다른 접근 방식(예: 진화 알고리즘, Bayesian Optimization)을 설명하고, 각 방법의 특징과 한계를 비교 분석하십시오.

3. NAS의 탐색 공간 설계는 최종적으로 발견되는 아키텍처의 성능에 큰 영향을 미칩니다. 컨볼루션 신경망과 순환 신경망을 위한 탐색 공간이 어떻게 정의되었는지 설명하고, 더 효과적인 탐색 공간을 설계하기 위한 고려 사항에 대해 논하십시오.

4. 본 논문에서는 NAS를 이미지 분류와 언어 모델링이라는 두 가지 서로 다른 작업에 적용하여 그 효과를 입증했습니다. NAS가 다른 딥러닝 응용 분야(예: 객체 탐지, 이미지 분할, 생성 모델)에 적용될 때 예상되는 어려움과 이를 극복하기 위한 전략에 대해 논하십시오.

5. NAS를 통해 발견된 신경망 아키텍처들은 종종 인간이 직관적으로 설계하는 방식과 다른 특징을 보입니다. 이러한 발견이 딥러닝 모델의 작동 방식에 대한 우리의 이해에 어떤 시사점을 주며, 앞으로 더 설명 가능하고 해석 가능한 신경망 아키텍처를 설계하기 위해 NAS 연구가 어떻게 활용될 수 있을지 논하십시오.

### 13.4 주요 용어 해설

1. 신경망 구조 탐색 (Neural Architecture Search, NAS): 데이터 기반의 성능 메트릭을 최적화하여 신경망 아키텍처를 자동으로 설계하는 기술.

2. 컨트롤러 (Controller): NAS에서 신경망 아키텍처의 하이퍼파라미터를 생성하는 데 사용되는 모델 (본 논문에서는 순환 신경망, RNN).

3. 자식 네트워크 (Child Network): 컨트롤러 RNN에 의해 생성된 특정 신경망 아키텍처. 실제 데이터로 학습되고 검증 세트에서 성능을 평가받음.

4. 정책 경도 (Policy Gradient): 강화 학습의 일종으로, 보상 신호에 기반하여 정책(여기서는 컨트롤러 RNN의 파라미터)을 직접적으로 최적화하는 방법.

5. 보상 신호 (Reward Signal): NAS에서 자식 네트워크의 성능을 나타내는 지표 (본 논문에서는 검증 세트 정확도 또는 퍼플렉서티의 역수).

6. 기준 함수 (Baseline Function): 정책 경도 방법의 분산을 줄이기 위해 사용되는 함수. 현재 행동에 의존하지 않는 값으로, 보상 값에서 빼주어 변동성을 감소시킴.

7. 스킵 연결 (Skip Connection): 신경망의 한 레이어의 출력을 몇 개의 중간 레이어를 건너뛰어 이후의 레이어에 직접 연결하는 방식. 깊은 신경망의 학습을 용이하게 함.

8. 어텐션 메커니즘 (Attention Mechanism): 입력 시퀀스의 각 부분에 대한 중요도를 가중치로 표현하여, 특정 부분에 집중하도록 하는 메커니즘. 본 논문에서는 스킵 연결을 결정하는 데 사용됨.

9. 퍼플렉서티 (Perplexity): 언어 모델의 성능을 평가하는 지표. 단어 시퀀스를 얼마나 잘 예측하는지를 나타내며, 값이 낮을수록 성능이 좋음.

10. 전이 학습 (Transfer Learning): 한 작업에서 학습한 지식을 다른 관련 작업에 적용하여 학습 효율성을 높이거나 성능을 개선하는 기법. 본 논문에서는 발견된 순환 셀을 다른 작업에 적용하는 것을 의미함.

11. GNMT (Google's Neural Machine Translation system): Google에서 개발한 신경망 기반 기계 번역 시스템. 본 논문에서 발견된 순환 셀의 성능을 검증하기 위해 사용됨.

12. BLEU (Bilingual Evaluation Understudy): 기계 번역 결과의 품질을 평가하는 데 사용되는 알고리즘.

13. 하이퍼파라미터 최적화 (Hyperparameter Optimization): 머신러닝 모델의 성능을 최적화하기 위해 학습률, 레이어 수 등과 같은 하이퍼파라미터의 최적 조합을 찾는 과정. NAS는 아키텍처 자체를 탐색한다는 점에서 차이가 있음.

## 14. **Diffusion Models Beat GANs on Image Synthesis (2021, Dhariwal & Nichol)**  

확산 모델 연구 가이드

### 14.1 퀴즈

1. 확산 모델의 핵심 아이디어는 무엇이며, 샘플링 과정은 어떻게 진행됩니까? (2-3 문장)

2. GAN과 비교했을 때 확산 모델의 주요 장점과 단점은 무엇입니까? (2-3 문장)

3. 이 논문에서 저자들이 확산 모델의 이미지 합성 품질을 향상시키기 위해 제안한 주요 건축학적 개선 사항은 무엇입니까? (2-3 문장)

4. "분류기 안내(classifier guidance)"는 무엇이며, 확산 모델의 샘플링 과정에 어떻게 통합되어 이미지 품질을 향상시킵니까? (2-3 문장)

5. 분류기 안내에서 "기울기 스케일(gradient scale)"을 조정하는 것은 생성된 이미지의 다양성과 충실도에 어떤 영향을 미칩니까? (2-3 문장)

6. FID는 이미지 생성 모델의 성능을 평가하는 데 어떻게 사용됩니까? 이 논문에서 제시된 주요 FID 결과는 무엇입니까? (2-3 문장)

7. 이 논문에서 unconditional 이미지 생성과 conditional 이미지 생성 모두에 대해 state-of-the-art 성능을 달성했다고 주장하는 방법은 무엇입니까? (2-3 문장)

8. "업샘플링 확산 모델(upsampling diffusion models)"은 무엇이며, 이미지 생성 품질을 어떻게 향상시킬 수 있습니까? 이 논문에서는 분류기 안내와 어떻게 결합됩니까? (2-3 문장)

9. 이 논문에서 언급된 확산 모델의 주요 한계점은 무엇이며, 앞으로 어떤 연구 방향이 제시되고 있습니까? (2-3 문장)

10. 이 연구가 사회에 미칠 수 있는 잠재적인 긍정적 및 부정적 영향에 대해 간략하게 설명하십시오. (2-3 문장)

### 14.2 퀴즈 정답

1. 확산 모델은 점진적인 노이즈 제거 과정을 역전시켜 데이터 분포에서 샘플링하는likelihood-based 모델입니다. 샘플링은 무작위 노이즈로 시작하여 학습된 역전파 과정을 통해 점차적으로 덜 시끄러운 샘플을 생성하여 최종 이미지를 얻습니다.

2. 확산 모델은 GAN보다 분포 범위를 더 잘 포착하고 학습이 더 안정적이지만, 샘플링 과정에서 여러 번의 순방향 패스가 필요하기 때문에 샘플링 속도가 느립니다. GAN은 높은 충실도의 샘플을 생성하는 데 강점을 가지지만, 다양성이 부족하고 학습이 불안정할 수 있습니다.

3. 주요 건축학적 개선 사항으로는 모델 깊이와 너비 조정, 어텐션 헤드 수 증가, 다양한 해상도에서의 어텐션 사용, BigGAN 잔차 블록 사용, 잔차 연결 재조정 등이 있습니다. 이러한 변경은 모델 크기를 상대적으로 유지하면서 FID를 크게 향상시켰습니다.

4. 분류기 안내는 사전 학습된 분류기의 기울기를 사용하여 확산 모델의 샘플링 과정을 특정 클래스 레이블 방향으로 유도하는 방법입니다. 각 샘플링 단계에서 분류기의 기울기를 추가하여 생성된 샘플이 원하는 클래스와 더 일치하도록 합니다.

5. 기울기 스케일을 높이면 분류기의 예측에 더 집중하게 되어 생성된 이미지의 충실도는 높아지지만 다양성은 감소합니다. 반대로 스케일을 낮추면 다양성은 증가하지만 충실도는 낮아지는 경향이 있습니다. 따라서 적절한 스케일 조정을 통해 다양성과 충실도 사이의 균형을 조절할 수 있습니다.

6. FID (Fréchet Inception Distance)는 생성된 이미지의 통계적 특징과 실제 이미지의 통계적 특징 간의 거리를 측정하여 이미지 생성 모델의 품질을 평가하는 지표입니다. 이 논문에서 저자들은 개선된 아키텍처와 분류기 안내를 통해 ImageNet 데이터셋에서 이전의 state-of-the-art 모델보다 낮은 FID 점수를 달성했습니다.

7. 개선된 UNet 아키텍처를 통해 unconditional 이미지 생성에서 state-of-the-art 성능을 달성했으며, 분류기 안내 기술을 통해 conditional 이미지 생성에서 높은 FID 점수를 기록하며 state-of-the-art 성능을 입증했습니다.

8. 업샘플링 확산 모델은 저해상도 이미지 생성 모델과 이를 기반으로 고해상도 이미지를 생성하는 업샘플링 모델을 결합한 것입니다. 이는 더 높은 해상도의 이미지 생성 품질을 향상시킬 수 있으며, 이 논문에서는 분류기 안내와 결합하여 최고 수준의 성능을 달성했습니다.

9. 주요 한계점은 GAN에 비해 샘플링 속도가 느리고 명시적인 잠재 표현을 학습하지 않는다는 것입니다. 앞으로의 연구 방향으로는 샘플링 속도 향상, 의미 있는 잠재 표현 학습, 텍스트-이미지 생성 등 다양한 조건부 생성으로의 확장 등이 제시되었습니다.

10. 긍정적인 영향으로는 계산 비용 효율적인 고품질 생성 모델 개발을 통해 다양한 분야에 기여할 수 있다는 점이 있습니다. 부정적인 영향으로는 가짜 뉴스, 허위 선전 이미지 생성 등 악의적인 사용 가능성과 예술, 디자인, 사진 등 분야의 일자리 감소 가능성이 있습니다.

### 14.3 에세이 형식 질문

1. 이 논문에서 제시된 확산 모델의 혁신적인 측면과 이것이 이미지 생성 분야에 미치는 영향에 대해 논의하십시오. 이전의 생성 모델들과 비교하여 확산 모델의 장점과 잠재력을 분석하십시오.

2. 분류기 안내 기술은 확산 모델의 성능 향상에 어떻게 기여합니까? 이 기술의 작동 원리를 자세히 설명하고, 이미지 품질과 다양성 사이의 트레이드오프를 중심으로 논의하십시오.

3. 저자들이 제시한 다양한 건축학적 개선 사항들이 확산 모델의 성능에 미치는 개별적인 영향과 그 결합된 효과를 분석하십시오. 어떤 변경 사항이 가장 중요한 성능 향상을 가져왔으며, 그 이유는 무엇이라고 생각하십니까?

4. 이 논문에서 state-of-the-art 이미지 생성 모델로 제시된 확산 모델이 여전히 가지고 있는 한계점들을 상세히 논하고, 이러한 한계점을 극복하기 위한 향후 연구 방향을 제안하십시오.

5. 생성 모델 기술의 발전이 사회에 미칠 수 있는 윤리적 및 사회적 영향에 대해 심층적으로 논의하십시오. 이 논문에서 언급된 잠재적 위험과 긍정적 활용 사례를 바탕으로 자신의 견해를 제시하십시오.

### 14.4 용어집

1. Diffusion Model (확산 모델): 점진적으로 노이즈를 제거하는 과정을 역전시켜 데이터 분포에서 샘플을 생성하는likelihood-based 생성 모델의 한 종류.

2. GAN (Generative Adversarial Network, 생성적 적대 신경망): 생성자와 판별자라는 두 개의 신경망이 경쟁하며 학습하여 실제 데이터와 유사한 데이터를 생성하는 생성 모델.

3. Image Synthesis (이미지 합성): 컴퓨터를 사용하여 새로운 이미지를 생성하는 과정.

4. Unconditional Image Synthesis (비조건부 이미지 합성): 특정 조건 없이 데이터 분포에서 임의의 이미지를 생성하는 것.

5. Conditional Image Synthesis (조건부 이미지 합성): 특정 조건 (예: 클래스 레이블)을 기반으로 이미지를 생성하는 것.

6. FID (Fréchet Inception Distance): 생성된 이미지의 품질과 다양성을 평가하는 지표로, 생성된 이미지의 통계적 특징과 실제 이미지의 통계적 특징 간의 거리를 측정함.

7. Inception Score (IS): 생성된 이미지의 품질과 다양성을 평가하는 또 다른 지표로, 생성된 이미지가 얼마나 실제적이고 다양한 클래스를 포함하는지를 측정함.

8. Precision and Recall (정밀도 및 재현율): 생성 모델 평가 지표로, 생성된 샘플이 실제 데이터 분포 내에 있는지 (정밀도) 그리고 실제 데이터 분포의 모든 모드를 얼마나 잘 커버하는지 (재현율)를 측정함.

9. Classifier Guidance (분류기 안내): 사전 학습된 분류기의 기울기를 사용하여 확산 모델의 샘플링 과정을 특정 방향으로 유도하여 생성된 샘플의 품질을 향상시키는 기술.

10. Sampling Steps (샘플링 단계): 확산 모델에서 노이즈로부터 최종 이미지를 생성하기 위해 반복되는 역전파 과정의 횟수.

11. UNet: 이미지 분할 분야에서 널리 사용되는 U자 형태의 신경망 아키텍처로, 확산 모델의 기본 아키텍처로 자주 사용됨.

12. Residual Block (잔차 블록): 신경망의 깊은 레이어를 학습시키는 데 사용되는 구성 요소로, 이전 레이어의 출력을 현재 레이어의 출력에 더해 학습을 안정화하고 정보 손실을 줄임.

13. Attention Layer (어텐션 계층): 입력 데이터의 중요한 부분에 집중하고 다른 부분은 덜 중요하게 취급하여 모델이 더 관련성 높은 정보를 학습하도록 돕는 신경망 계층.

14. BigGAN: 대규모 이미지 생성을 위해 개발된 GAN 모델로, 안정적인 학습과 높은 품질의 이미지 생성을 목표로 함.

15. Adaptive Group Normalization (AdaGN, 적응형 그룹 정규화): 그룹 정규화 후 시간 단계 및 클래스 임베딩을 통합하여 각 잔차 블록에 조건부 정보를 주입하는 레이어.

16. DDIM (Denoising Diffusion Implicit Models, 비노이즈 확산 암시적 모델): 확산 모델의 샘플링 속도를 향상시키기 위해 제안된 비마르코프 노이즈 추가 프로세스 및 샘플링 방법.

17. Upsampling Diffusion Model (업샘플링 확산 모델): 저해상도 확산 모델을 통해 생성된 이미지를 입력으로 받아 고해상도 이미지를 생성하는 확산 모델.

18. Latent Representation (잠재 표현): 데이터의 압축된 형태의 표현으로, 생성 모델이 데이터를 생성하는 데 사용됨.






