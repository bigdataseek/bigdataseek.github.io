---
title: 26차시 2:AI Research Papers(Basic)
layout: single
classes: wide
categories:
  - AI Research Papers
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---


## 9. **Playing Atari with Deep Reinforcement Learning (2013, Mnih et al.)**

심층 강화 학습 스터디 가이드

### 9.1 퀴즈
1. 이 논문에서 제시하는 심층 학습 모델은 어떤 종류의 신경망이며, 어떤 강화 학습 알고리즘의 변형을 사용하는가? 
   - 이 모델은 컨볼루션 신경망(Convolutional Neural Network, CNN)이며, Q-러닝 알고리즘의 변형을 사용하여 학습됩니다. CNN의 입력은 원시 픽셀이며, 출력은 미래 보상을 예측하는 가치 함수입니다.

2. 기존의 강화 학습 접근 방식과 비교하여 이 논문의 방법이 갖는 주요 차이점은 무엇인가? 
   - 기존의 강화 학습은 주로 수동으로 설계한 특징과 선형 가치 함수 또는 정책 표현에 의존했습니다. 반면, 이 논문은 원시 감각 입력으로부터 직접 특징을 추출하는 심층 학습 모델을 사용하여 특징 엔지니어링의 필요성을 줄였습니다.

3. 심층 학습 관점에서 강화 학습이 제시하는 주요 어려움 세 가지는 무엇인가? 
   - 첫째, 강화 학습은 종종 희소하고 잡음이 많으며 지연된 스칼라 보상 신호로부터 학습해야 합니다. 둘째, 강화 학습 데이터는 일반적으로 순차적이고 상관관계가 높습니다. 셋째, 알고리즘이 새로운 행동을 학습함에 따라 데이터 분포가 변하므로 심층 학습 방법이 가정하는 고정된 분포와 상반됩니다.

4. 이 논문에서 제안하는 심층 Q-러닝(Deep Q-learning) 알고리즘이 이러한 어려움을 해결하기 위해 사용하는 주요 기술 두 가지는 무엇인가? 
   - 첫째, 경험 재생(experience replay) 메커니즘을 사용하여 이전의 전환(transition)을 무작위로 샘플링하여 훈련 분포를 평활화하고 데이터의 상관관계를 줄입니다. 둘째, 대상 Q 값을 주기적으로 업데이트하여 학습의 안정성을 높입니다.

5. 경험 재생(experience replay) 메커니즘은 강화 학습에서 어떤 이점을 제공하는가? 
   - 경험 재생은 각 경험 단계를 여러 번의 가중치 업데이트에 재사용하여 데이터 효율성을 높입니다. 또한, 연속적인 샘플 간의 높은 상관관계를 깨뜨려 업데이트의 분산을 줄이고, 학습 과정에서 발생할 수 있는 원치 않는 피드백 루프를 방지하여 학습을 안정화합니다.

6. Q-함수를 매개변화하는 두 가지 주요 방법과 이 논문에서 채택한 방법의 장점은 무엇인가? 
   - 한 가지 방법은 히스토리와 행동 쌍을 입력으로 사용하여 Q-값을 예측하는 것이고, 다른 방법은 상태 표현을 입력으로 사용하고 각 가능한 행동에 대한 Q-값을 개별 출력으로 예측하는 것입니다. 이 논문에서는 후자를 채택하여 단일 순방향 패스만으로 주어진 상태의 모든 가능한 행동에 대한 Q-값을 계산할 수 있다는 장점이 있습니다.

7. Atari 2600 게임 환경에서 에이전트의 입력으로 사용되는 데이터는 어떻게 전처리되는가? 
   - 원시 RGB 프레임은 먼저 회색조로 변환되고 110x84 이미지로 다운샘플링됩니다. 최종 입력 표현은 게임 영역을 대략적으로 캡처하는 84x84 영역을 잘라내고, 마지막 4개의 프레임을 쌓아서 84x84x4 이미지를 만듭니다.

8. 이 논문의 실험에서 모든 Atari 게임에 걸쳐 일관되게 유지된 요소 세 가지는 무엇인가? 
   - 신경망 아키텍처, 학습 알고리즘, 하이퍼파라미터 설정은 모든 일곱 가지 Atari 게임에 걸쳐 동일하게 유지되었습니다. 이는 제안된 방법의 견고성을 보여줍니다.

9. 훈련 중 에이전트의 성능을 평가하기 위해 사용된 두 가지 주요 지표는 무엇이며, 각각의 특징은 무엇인가? 
   - 첫 번째 지표는 에피소드당 평균 총 보상으로, 실제 게임 성능을 나타내지만 정책 가중치의 작은 변화에도 크게 변동하는 경향이 있습니다. 두 번째 지표는 고정된 상태 집합에 대한 평균 최대 예측 Q-값으로, 학습 진행 상황을 더 안정적으로 보여주지만 직접적인 게임 성능 측정은 아닙니다.

10. 이 연구 결과는 몇몇 Atari 2600 게임에서 인간 전문가의 성능을 능가했지만, 그렇지 못한 게임도 있다. 그 이유는 무엇이라고 저자들은 추측하는가? 
   - Q*bert, Seaquest, Space Invaders와 같이 인간 성능에 미치지 못하는 게임은 네트워크가 장기간에 걸쳐 전략을 찾아야 하기 때문에 더 어렵다고 저자들은 추측합니다.

### 9.2 에세이 형식 질문
1. 이 논문에서 제시된 심층 강화 학습 접근 방식이 전통적인 강화 학습 방법론에 비해 갖는 혁신적인 측면과 잠재적인 영향에 대해 논하시오.

2. 경험 재생(experience replay)이 심층 Q-러닝의 안정성과 성능 향상에 기여하는 구체적인 방식과 그 한계점에 대해 분석하시오.

3. 이 논문에서 사용된 컨볼루션 신경망 아키텍처가 Atari 게임 플레이라는 강화 학습 문제에 적합한 이유를 설명하고, 다른 신경망 아키텍처와의 비교를 통해 논하시오.

4. 이 연구 결과가 보인 성공에도 불구하고, 여전히 인간 수준의 성능에 도달하지 못한 Atari 게임들을 분석하고, 향후 연구 방향에 대한 아이디어를 제시하시오.

5. 이 논문의 연구 결과가 인공 지능 및 관련 분야에 미치는 더 넓은 의미와 윤리적 고려 사항에 대해 논하시오.

### 9.3 용어 해설

1. 강화 학습 (Reinforcement Learning, RL): 에이전트가 환경과 상호작용하면서 보상을 최대화하는 방법을 학습하는 머신 러닝의 한 분야.

2. 심층 학습 (Deep Learning): 여러 계층으로 구성된 인공 신경망을 사용하여 복잡한 데이터의 패턴을 학습하는 머신 러닝의 한 분야.

3. 컨볼루션 신경망 (Convolutional Neural Network, CNN): 이미지 처리, 비디오 분석 등 시각적 데이터를 처리하는 데 특히 효과적인 심층 신경망의 한 종류. 컨볼루션 계층, 풀링 계층, 완전 연결 계층 등으로 구성됨.

4. Q-러닝 (Q-learning): 행동-가치 함수 (Q-함수)를 학습하는 모델-프리 강화 학습 알고리즘. 주어진 상태에서 특정 행동을 취했을 때 얻을 수 있는 미래 보상의 기댓값을 추정함.

5. 가치 함수 (Value Function): 특정 상태 또는 상태-행동 쌍이 얼마나 "좋은"지 나타내는 함수. 미래에 얻을 것으로 예상되는 보상의 양을 예측함.

6. 정책 (Policy): 에이전트가 주어진 상태에서 어떤 행동을 취할지 결정하는 규칙 또는 전략.

7. 경험 재생 (Experience Replay): 에이전트의 경험 (상태, 행동, 보상, 다음 상태)을 메모리에 저장하고, 학습 시에 이 메모리에서 무작위로 샘플링하여 학습하는 기법. 데이터 효율성을 높이고 샘플 간의 상관관계를 줄이는 데 도움을 줌.

8. 행동-가치 함수 (Action-Value Function) 또는 Q-함수: 특정 상태에서 특정 행동을 취한 후 최적의 정책을 따랐을 때 얻을 수 있는 미래 할인된 보상의 기댓값.

9. 벨만 방정식 (Bellman Equation): 최적 가치 함수가 만족하는 재귀적인 방정식. 현재 상태의 가치는 다음 상태의 최적 가치와 즉각적인 보상으로 표현될 수 있음을 나타냄.

10. ε-탐욕 정책 (Epsilon-Greedy Policy): 탐험과 활용 사이의 균형을 맞추기 위해 사용되는 정책. ε의 확률로 무작위 행동을 선택하고 (1-ε)의 확률로 현재 최적이라고 믿는 행동을 선택함.

11. 보상 신호 (Reward Signal): 에이전트가 환경과 상호작용한 결과로 받는 스칼라 값. 에이전트의 행동이 얼마나 바람직했는지 나타냄.

12. 상태 공간 (State Space): 에이전트가 처할 수 있는 모든 가능한 상태의 집합.

13. 행동 공간 (Action Space): 에이전트가 특정 상태에서 취할 수 있는 모든 가능한 행동의 집합.

14. 모델-프리 (Model-free): 환경의 모델 (상태 전환 확률 및 보상 함수)을 명시적으로 학습하지 않고 직접 경험으로부터 최적 정책 또는 가치 함수를 학습하는 강화 학습 방법.

15. 오프-폴리시 (Off-policy): 행동을 생성하는 정책과 평가 또는 학습하는 정책이 다른 강화 학습 방식. Q-러닝은 대표적인 오프-폴리시 알고리즘임.

16. 확률적 경사 하강법 (Stochastic Gradient Descent, SGD): 머신 러닝 모델의 파라미터를 최적화하기 위해 사용되는 반복적인 알고리즘. 전체 데이터셋 대신 무작위로 선택된 일부 데이터 (미니배치)에 대한 손실 함수의 기울기를 사용하여 파라미터를 업데이트함.

17. RMSProp: 학습률을 개별 파라미터에 적응적으로 조정하는 경사 하강법의 한 종류.

18. 미니배치 (Minibatch): 전체 데이터셋의 작은 부분 집합. 확률적 경사 하강법에서 각 업데이트 단계에 사용됨.

## 10. **Mastering the game of Go with deep neural networks and tree search (2016, Silver et al.)** 

딥러닝 기반 바둑 게임 마스터하기

### 10.1 핵심 개념

•완전 정보 게임: 모든 플레이어가 게임의 모든 시점에서 완벽한 정보를 알고 있는 게임.
•**최적 가치 함수 (v(s)):** 모든 보드 상태(s)에서 완벽한 플레이를 가정했을 때 게임의 결과를 결정하는 함수.
•탐색 트리: 가능한 모든 움직임의 순서를 나타내는 트리 구조.
•넓이 (b): 각 포지션에서 가능한 합법적인 움직임의 수.
•깊이 (d): 게임의 평균 길이 (총 움직임 수).
•위치 평가 (Position Evaluation): 탐색 트리의 특정 상태(s)에서 탐색을 중단하고, 그 아래 하위 트리를 예측된 게임 결과 (근사 가치 함수 v(s))로 대체하여 탐색 깊이를 줄이는 방법.
•정책 (Policy) (p(a|s)): 특정 보드 상태(s)에서 가능한 각 움직임(a)에 대한 확률 분포.
•몬테카를로 롤아웃 (Monte Carlo Rollouts): 정책(p)에 따라 양쪽 플레이어가 무작위로 행동을 선택하여 게임의 끝까지 진행하는 시뮬레이션. 여러 번의 롤아웃 결과를 평균하여 특정 포지션의 가치를 평가하는 방법.
•몬테카를로 트리 탐색 (MCTS): 몬테카를로 롤아웃을 사용하여 탐색 트리의 각 상태 가치를 추정하는 알고리즘. 시뮬레이션 횟수가 증가함에 따라 트리가 확장되고 가치 추정의 정확도가 향상됨.
•심층 컨볼루션 신경망 (Deep Convolutional Neural Networks): 여러 계층의 뉴런으로 구성된 신경망으로, 이미지와 같은 시각적 데이터를 처리하는 데 뛰어난 성능을 보임. 바둑판의 현재 상태를 입력으로 받아 특징을 추출하는 데 사용됨.
•정책 네트워크 (Policy Network) (pσ, pρ): 바둑 보드 상태를 입력으로 받아 가능한 다음 수에 대한 확률 분포를 출력하는 심층 신경망. 지도 학습 (SL 정책) 또는 강화 학습 (RL 정책)을 통해 학습됨.
•가치 네트워크 (Value Network) (vθ): 바둑 보드 상태를 입력으로 받아 현재 플레이어의 승리 확률을 예측하는 단일 값을 출력하는 심층 신경망. 자기 대국 데이터를 사용하여 회귀 학습을 통해 학습됨.
•감독 학습 (Supervised Learning - SL): 인간 전문가의 기보 데이터를 사용하여 정책 네트워크가 전문가의 수를 예측하도록 학습시키는 방법.
•강화 학습 (Reinforcement Learning - RL): 에이전트가 스스로 게임을 플레이하면서 승리라는 보상을 최대화하도록 정책 네트워크를 학습시키는 방법.
•자기 대국 (Self-Play): 학습 중인 에이전트가 자신 또는 이전 버전의 에이전트와 게임을 플레이하는 방식.
•정책 경사 (Policy Gradient): 강화 학습 알고리즘의 하나로, 정책의 매개변수를 직접 업데이트하여 기대되는 보상을 최대화하는 방향으로 학습하는 방법.
•회귀 (Regression): 가치 네트워크를 학습시키는 데 사용되는 머신러닝 방법으로, 입력 상태와 실제 게임 결과 사이의 평균 제곱 오차를 최소화하는 방향으로 네트워크의 가중치를 조정함.
•MCTS에서의 탐색 단계:
   ◦선택 (Selection): 트리 루트에서 시작하여 액션 가치(Q)와 탐색 보너스(u(P))를 기반으로 자식 노드를 재귀적으로 선택하여 리프 노드에 도달하는 단계.
   ◦확장 (Expansion): 리프 노드가 특정 방문 횟수 임계값을 넘으면, 정책 네트워크(pσ)를 사용하여 가능한 합법적인 수에 대한 사전 확률(P)을 계산하고 새로운 노드를 트리에 추가하는 단계.
   ◦평가 (Evaluation): 확장된 리프 노드의 가치를 평가하는 단계. AlphaGo에서는 가치 네트워크(vθ)를 사용한 평가와 빠른 롤아웃 정책(pπ)을 사용한 몬테카를로 롤아웃 결과를 혼합하여 사용함.
   ◦역전파 (Backup): 시뮬레이션 결과를 사용하여 탐색 경로에 있는 각 에지의 액션 가치(Q)와 방문 횟수(N)를 업데이트하는 단계.
•행동 가치 (Q(s, a)): 특정 상태(s)에서 특정 행동(a)을 취했을 때 기대되는 미래 보상.
•방문 횟수 (N(s, a)): 탐색 중에 특정 상태(s)에서 특정 행동(a)을 취한 횟수.
•사전 확률 (P(s, a)): 정책 네트워크(pσ)가 특정 상태(s)에서 특정 행동(a)을 선택할 초기 확률.
•가상 손실 (Virtual Loss): 병렬 탐색 시 여러 스레드가 동일한 노드를 동시에 탐색하는 것을 방지하기 위해 일시적으로 노드의 가치를 낮추는 기법.
•가장 많이 방문한 수 (Most Visited Move): MCTS 탐색이 완료된 후 루트 노드에서 가장 많이 방문한 행동을 최종 수로 선택하는 전략.


### 10.2 퀴즈 (2-3 문장 답안)

1. 완전 정보 게임과 불완전 정보 게임의 주요 차이점은 무엇이며, 바둑은 어떤 유형의 게임에 속하나요?

2. 탐색 트리의 넓이(b)와 깊이(d)가 클 때 완전 탐색이 비현실적인 이유는 무엇이며, 바둑 게임의 넓이와 깊이는 어느 정도인가요?

3. 정책 네트워크와 가치 네트워크는 바둑 AI에서 각각 어떤 역할을 수행하며, 이 두 네트워크는 어떻게 학습되나요?

4. 몬테카를로 트리 탐색(MCTS)의 기본적인 작동 원리는 무엇이며, 탐색 과정의 주요 네 단계를 간략하게 설명해주세요.

5. AlphaGo가 인간 전문가의 기보를 사용하여 초기 정책 네트워크를 학습시키는 이유는 무엇이며, 이 단계의 학습 방법을 무엇이라고 부르나요?

6. 강화 학습 단계에서 AlphaGo는 자기 대국을 통해 정책 네트워크를 개선합니다. 이 과정의 목표와 주요 이점은 무엇인가요?

7. 가치 네트워크를 학습시킬 때 전체 게임 데이터를 사용하는 대신 자기 대국에서 추출한 고유한 포지션을 사용하는 이유는 무엇인가요?

8. AlphaGo의 MCTS 알고리즘에서 정책 네트워크와 가치 네트워크의 평가는 어떻게 결합되어 최종 수 선택에 영향을 미치나요?

9. AlphaGo가 이전의 최고 바둑 프로그램들을 압도하고 인간 프로 기사를 이길 수 있었던 주요 요인들은 무엇이라고 생각하나요?

10. AlphaGo 연구의 의의는 바둑 분야를 넘어 인공지능의 다른 도전 과제에 대해 어떤 시사점을 던져주나요?

### 10.3 퀴즈 정답

1. 완전 정보 게임에서는 모든 플레이어가 게임의 모든 시점에서 완벽한 정보를 알 수 있는 반면, 불완전 정보 게임에서는 일부 정보가 가려져 있습니다. 바둑은 모든 돌의 위치가 공개되어 있고 운 요소가 없으므로 완전 정보 게임에 속합니다.

2. 탐색 트리의 넓이와 깊이가 클수록 탐색해야 할 가능한 움직임의 수가 기하급수적으로 증가하여 현실적인 시간 안에 모든 경우를 탐색하는 것이 불가능해집니다. 바둑은 넓이가 약 250, 깊이가 약 150으로 매우 큰 탐색 공간을 가집니다.

3. 정책 네트워크는 주어진 바둑판 상태에서 유망한 다음 수를 예측하는 역할을 하며, 가치 네트워크는 주어진 바둑판 상태에서 현재 플레이어의 승리 확률을 예측하는 역할을 합니다. 정책 네트워크는 지도 학습과 강화 학습을 통해, 가치 네트워크는 자기 대국 데이터를 이용한 회귀 학습을 통해 학습됩니다.

4. MCTS는 몬테카를로 롤아웃을 반복적으로 수행하여 탐색 트리의 각 노드 가치를 통계적으로 추정하는 알고리즘입니다. 주요 네 단계는 선택, 확장, 평가, 역전파입니다.

5. 인간 전문가의 기보는 바둑의 기본적인 전략과 유망한 수의 경향을 담고 있으므로, 초기 정책 네트워크가 좋은 성능을 빠르게 학습하도록 돕습니다. 이 단계의 학습 방법을 감독 학습이라고 부릅니다.

6. 강화 학습 단계의 목표는 정책 네트워크가 단순히 인간의 수를 흉내 내는 것이 아니라 게임에서 승리하는 것을 최적화하도록 개선하는 것입니다. 자기 대국을 통해 다양한 게임 상황을 경험하고 승패를 통해 스스로 학습할 수 있다는 이점이 있습니다.

7. 전체 게임 데이터 내의 연속적인 포지션들은 매우 높은 상관관계를 가지므로, 이를 사용하여 가치 네트워크를 학습시키면 새로운 포지션에 대한 일반화 능력이 떨어지는 과적합 문제가 발생하기 쉽습니다. 자기 대국에서 추출한 고유한 포지션들은 데이터의 다양성을 높여 과적합을 방지하고 가치 네트워크의 일반화 능력을 향상시킵니다.

8. AlphaGo의 MCTS는 탐색 과정에서 정책 네트워크가 제공하는 사전 확률을 활용하여 탐색의 방향을 설정하고, 리프 노드에 도달했을 때 가치 네트워크가 예측하는 가치와 롤아웃 시뮬레이션 결과를 혼합하여 해당 노드의 가치를 평가합니다. 이 평가는 역전파를 통해 트리의 액션 가치를 업데이트하고 최종적으로 가장 유망한 수를 선택하는 데 기여합니다.

9. 심층 신경망을 이용한 강력한 정책 네트워크와 가치 네트워크, 그리고 이들을 효과적으로 결합한 몬테카를로 트리 탐색 알고리즘 덕분에 AlphaGo는 넓고 깊은 탐색 공간과 복잡한 가치 판단이라는 바둑의 어려움을 극복하고 뛰어난 성능을 보일 수 있었습니다.

10. AlphaGo의 성공은 심층 학습과 트리 검색의 결합이 바둑과 같이 오랫동안 인공지능의 난제로 여겨졌던 영역에서 인간 수준의 성능을 달성할 수 있음을 보여주었으며, 이는 다른 복잡한 의사 결정 문제에 대한 인공지능 접근 방식에 대한 희망을 제시합니다.

### 10.4 논술형 질문
1. AlphaGo가 바둑에서 인간 프로 기사를 넘어선 것은 인공지능 역사에서 어떤 의미를 가지며, 이 성과가 미래의 인공지능 연구에 미칠 영향에 대해 논하시오.

2. AlphaGo의 핵심 요소 기술인 심층 신경망 (정책 네트워크와 가치 네트워크)과 몬테카를로 트리 탐색 (MCTS)은 각각 어떤 강점과 약점을 가지며, 이 두 가지 접근 방식을 결합함으로써 얻는 시너지 효과는 무엇인지 분석하시오.

3. AlphaGo의 학습 과정은 감독 학습, 강화 학습, 자기 대국 등 여러 단계를 거칩니다. 각 학습 단계의 목표와 중요성을 설명하고, 이 순서로 학습하는 것이 효과적인 이유에 대해 논하시오.

4. AlphaGo가 바둑의 복잡성을 해결하기 위해 사용한 방법 (예: 깊이와 넓이 축소, 위치 평가, 정책 기반 행동 샘플링)을 분석하고, 이러한 방법들이 다른 어려운 탐색 문제 해결에 어떻게 적용될 수 있을지 논하시오.

5. AlphaGo의 성공 이후, 바둑 인공지능 분야는 어떻게 발전해 왔으며, AlphaGo의 방법론이 다른 게임 인공지능 또는 일반 인공지능 분야에 미친 구체적인 영향을 설명하시오.

### 10.5 용어 해설

1. 게임 이론 (Game Theory): 전략적 상호작용을 수학적으로 분석하는 학문 분야.

2. 마르코프 결정 과정 (Markov Decision Process - MDP): 상태, 행동, 보상, 상태 전이 확률로 정의되는 수학적 프레임워크로, 강화 학습 문제의 모델링에 사용됨.

3. 알파-베타 가지치기 (Alpha-Beta Pruning): 탐색 트리의 모든 노드를 탐색하지 않고 최적의 수를 찾는 데 불필요한 부분을 제거하는 탐색 알고리즘의 최적화 기법.

4. 신경망 (Neural Network): 인간 뇌의 구조를 모방한 정보 처리 시스템으로, 여러 개의 연결된 노드(뉴런)로 구성되어 복잡한 패턴을 학습할 수 있음.

5. 컨볼루션 신경망 (Convolutional Neural Network - CNN): 이미지 처리, 음성 인식 등 공간적 계층 구조를 가지는 데이터 처리에 특화된 신경망의 한 종류.

6. 활성화 함수 (Activation Function): 신경망의 각 뉴런에서 입력 신호에 대한 출력 신호를 결정하는 비선형 함수.

7. 소프트맥스 함수 (Softmax Function): 벡터의 값들을 확률 분포로 변환하는 함수로, 정책 네트워크의 최종 출력 계층에서 사용됨.

8. 가중치 (Weights): 신경망 내의 연결 강도를 나타내는 파라미터로, 학습 과정에서 조정됨.

9. 편향 (Bias): 신경망 뉴런의 활성화 임계값을 조절하는 파라미터.

10. 손실 함수 (Loss Function): 예측 값과 실제 값의 차이를 측정하는 함수로, 학습의 목표는 이 손실 함수의 값을 최소화하는 것임.

11. 경사 하강법 (Gradient Descent): 손실 함수의 경사를 따라 파라미터를 반복적으로 업데이트하여 손실 함수를 최소화하는 최적화 알고리즘.

12. 학습률 (Learning Rate): 경사 하강법에서 파라미터를 업데이트하는 단계의 크기를 결정하는 하이퍼파라미터.

13. 과적합 (Overfitting): 학습 데이터에만 지나치게 잘 맞고 새로운 데이터에 대한 예측 성능이 떨어지는 현상.

14. 규제화 (Regularization): 과적합을 방지하기 위해 모델의 복잡성을 줄이는 기법.

15. 하이퍼파라미터 (Hyperparameter): 학습 과정 이전에 사용자가 직접 설정해야 하는 파라미터 (예: 학습률, 신경망 계층 수 등).

15. ELO 레이팅 시스템 (Elo Rating System): 체스나 바둑과 같은 경쟁 게임에서 플레이어의 상대적인 실력을 평가하는 시스템.

16. 코미 (Komi): 바둑에서 백돌을 쥔 플레이어에게 주어지는 추가 점수로, 선후수 불균형을 보정하는 역할을 함.

17. 핸디캡 (Handicap): 실력 차이가 큰 플레이어 간의 대국에서 약한 플레이어가 유리하도록 미리 돌을 놓거나 기타 이점을 주는 것.


## 11. **Generative Adversarial Networks (2014, Goodfellow et al.)** 
## 12. **Auto-Encoding Variational Bayes (2013, Kingma & Welling)** 
## 13. **Neural Architecture Search with Reinforcement Learning (2016, Zoph & Le)** 
## 14. **Diffusion Models Beat GANs on Image Synthesis (2021, Dhariwal & Nichol)**  








