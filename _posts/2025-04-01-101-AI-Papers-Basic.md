---
title: 26차시 1:AI Research Papers(Basic)
layout: single
classes: wide
categories:
  - AI Research Papers
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 1. 처음 논문(Research Papers)을 읽는 사람을 위해: 효과적인 논문 읽기 방법
논문을 처음 읽는 분들을 위한 방법을 알려드리겠습니다. 학술 논문은 처음에는 어렵게 느껴질 수 있지만, 다음 접근법을 통해 더 수월하게 읽을 수 있습니다:

1. **구조적 접근하기**
   - 제목, 초록(Abstract), 결론부터 읽어 전체 내용을 파악하세요
   - 그 다음 서론(Introduction)과 결론(Conclusion)을 읽으면 주요 내용을 이해할 수 있습니다
   - 마지막으로 본문 내용을 읽으면 상세 내용을 이해하기 쉬워집니다

2. **시각적 자료 활용하기**
   - 논문의 그림, 표, 다이어그램을 먼저 살펴보세요
   - 시각 자료는 복잡한 개념을 직관적으로 이해하는데 도움이 됩니다

3. **용어 사전 만들기**
   - 모르는 용어가 나올 때마다 따로 정리하고 찾아보세요
   - 시간이 지나면 자연스럽게 해당 분야의 용어집이 완성됩니다

4. **논문 요약본 활용하기**
   - 많은 논문들이 블로그나 유튜브에서 요약 설명되어 있습니다
   - Papers With Code, Two Minute Papers 같은 사이트/채널을 활용해보세요

5. **스터디 그룹 참여하기**
   - 같은 분야 관심 있는 사람들과 함께 논문을 읽고 토론하면 이해도가 높아집니다

6. **논문 읽기 도구 활용하기**
   - Connected Papers: 관련 논문들을 시각적으로 보여줍니다
     - [Attention is all you need 논문 관련 시각화 그래프](
   https://www.connectedpapers.com/main/204e3073870fae3d05bcbc2f6a8e263d9b72e776/graph?utm_source=share_popup&utm_medium=copy_link&utm_campaign=share_graph) 

   - Semantic Scholar: AI 기반으로 논문 검색과 이해를 도와줍니다
      - [Attention is all you need 논문 관련 내용](https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776#related-papers)

   - Elicit.org: 논문의 주요 내용을 요약해주는 AI 도구입니다
      - [Attention is all you need 논문 관련 요약](https://elicit.com/notebook/73d15714-6b1f-4dd5-979d-a06dd2b158dd)

7. **실습과 병행하기**
   - 특히 AI 논문의 경우, 코드 구현을 함께 보면 이해가 쉬워집니다
   - Papers With Code 사이트에서는 많은 AI 논문의 구현 코드를 제공합니다
      - [논문 내용을 코드로 구현한 곳](https://paperswithcode.com/paper/attention-is-all-you-need)

무엇보다 처음부터 모든 내용을 100% 이해하려고 하기보다는 핵심 아이디어를 파악하는 데 집중하고, 반복해서 읽다 보면 점차 이해도가 높아질 것입니다. 

## 2. 읽을 만한 논문
### 2.1 딥러닝의 기초 및 역사적 논문
- **"Backpropagation" (1986, Rumelhart et al.)**  
  - 신경망 학습의 핵심 알고리즘인 역전파(backpropagation)를 소개한 논문.
- **"ImageNet Classification with Deep Convolutional Neural Networks" (2012, Krizhevsky et al.)**  
  - AlexNet을 제안하며 현대 딥러닝 시대를 연 논문.

### 2.2 CNN (Convolutional Neural Networks)
- **"Very Deep Convolutional Networks for Large-Scale Image Recognition" (2014, Simonyan & Zisserman)**  
  - VGGNet을 소개하며 깊은 네트워크가 성능을 향상시킬 수 있음을 보인 논문.
- **"Deep Residual Learning for Image Recognition" (2015, He et al.)**  
  - ResNet을 제안하며 딥러닝 모델이 깊어질 때 발생하는 문제를 해결한 논문.

### 2.3 NLP (자연어 처리)
- **"Attention Is All You Need" (2017, Vaswani et al.)**  
  - Transformer 구조를 제안하며 BERT, GPT 등 최신 NLP 모델의 기반을 마련한 논문.
- **"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (2018, Devlin et al.)**  
  - BERT 모델을 소개한 논문으로, NLP에서 사전 훈련(pre-training) 기법을 혁신함.

### 2.4 강화학습 (Reinforcement Learning)
- **"Playing Atari with Deep Reinforcement Learning" (2013, Mnih et al.)**  
  - DQN(Deep Q-Network)을 소개하며 딥러닝을 활용한 강화학습의 기초를 확립한 논문.
- **"Mastering the game of Go with deep neural networks and tree search" (2016, Silver et al.)**  
  - AlphaGo 논문, 딥러닝과 강화학습을 활용한 바둑 AI의 발전을 보여줌.

### 2.5 생성 모델 (Generative Models)
- **"Generative Adversarial Networks" (2014, Goodfellow et al.)**  
  - GAN(생성적 적대 신경망)을 소개한 논문.
- **"Auto-Encoding Variational Bayes" (2013, Kingma & Welling)**  
  - VAE(Variational Autoencoder)를 제안한 논문.

### 2.6 기타 중요 논문
- **"Neural Architecture Search with Reinforcement Learning" (2016, Zoph & Le)**  
  - NAS(Neural Architecture Search)를 제안한 논문, 자동으로 최적의 신경망 구조를 찾는 방법을 다룸.
- **"Diffusion Models Beat GANs on Image Synthesis" (2021, Dhariwal & Nichol)**  
   - 확산 모델(Diffusion Models)이 GAN보다 뛰어난 이미지 생성을 할 수 있음을 보인 논문.
- **Deep Learning" (LeCun, Bengio, Hinton, 2015)**
   - 딥러닝의 기본 개념을 포괄적으로 설명한 리뷰 논문입니다.

## 3. **Backpropagation (1986, Rumelhart et al.)**  

- [Backpropagation](https://www.nature.com/articles/323533a0)

### 3.1 퀴즈 (단답형)
1.역전파(back-propagation) 학습 절차의 주요 목표는 무엇이며, 이 절차는 신경망의 어떤 부분을 조정하여 목표를 달성합니까?
   > 역전파 학습 절차의 주요 목표는 신경망의 실제 출력 벡터와 원하는 출력 벡터 사이의 차이를 최소화하는 것입니다. 이를 위해 네트워크 연결의 가중치를 반복적으로 조정합니다.

2.은닉층(hidden layers)이 있는 신경망이 입력층과 출력층만 있는 신경망보다 더 강력한 이유는 무엇입니까?
   > 은닉층은 입력 또는 출력의 일부가 아니지만 작업 도메인의 중요한 특징을 나타내는 내부 표현을 학습할 수 있게 합니다. 이러한 새로운 특징을 생성하는 능력은 단순한 연결 방식으로는 포착할 수 없는 작업의 규칙성을 모델링할 수 있게 합니다.

3.경사 하강법(gradient descent)에서 학습률(learning rate, $$\epsilon$$)과 모멘텀(momentum, $$\alpha$$)은 가중치 업데이트에 어떤 영향을 미칩니까?
   > 학습률($$\epsilon$$)은 오차 기울기에 따라 가중치를 얼마나 크게 조정할지를 결정합니다. 모멘텀($$\alpha$$)은 이전 가중치 변화의 영향을 유지시켜 학습 속도를 높이고 지역 최소값에 갇히는 것을 방지하는 데 도움을 줍니다.

4.그림 1에서 제시된 신경망은 어떤 특정 작업을 학습했으며, 은닉 유닛의 가중치에서 어떤 특징이 나타납니까?
   > 그림 1의 신경망은 입력 벡터의 거울 대칭성을 감지하는 작업을 학습했습니다. 은닉 유닛의 가중치는 입력 벡터의 중간을 기준으로 대칭적인 위치의 가중치가 동일한 크기와 반대 부호를 갖는 특징을 보입니다.

5.가족 관계 네트워크(family tree network)는 어떤 종류의 정보를 학습했으며, 은닉층의 활성화 패턴은 무엇을 나타냅니까?
   > 가족 관계 네트워크는 (사람 1, 관계, 사람 2) 형태의 삼중 정보들을 학습했습니다. 은닉층의 활성화 패턴은 사람과 관계에 대한 분산된 표현을 인코딩하며, 네트워크가 추론하고 일반화할 수 있도록 합니다.

6.재귀 신경망(recurrent network)과 계층형 신경망(layered network) 사이의 유사점은 무엇이며, 재귀 신경망 학습 시 어떤 추가적인 고려 사항이 발생합니까?
   > 재귀 신경망의 각 시간 단계는 등가의 계층형 신경망의 한 계층에 해당합니다. 재귀 신경망을 학습할 때는 순방향 패스의 중간 계층 유닛의 출력 기록을 저장하고, 대응하는 가중치 세트 간의 기울기를 평균화하는 추가적인 고려 사항이 발생합니다.

7.시각 피질에서 단안 박탈(monocular deprivation)의 주요 효과는 무엇이며, 이는 행동적으로 어떻게 나타납니까?
   > 단안 박탈의 주요 효과는 박탈되지 않은 눈의 시각 자극에만 대부분의 피질 세포가 반응하게 되는 시각 피질의 우세 눈 편향(ocular dominance shift)입니다. 행동적으로는 박탈된 눈이 거의 쓸모없게 됩니다.

8.역전 차폐(reverse occlusion)는 단안 박탈의 효과를 어떻게 변화시킬 수 있으며, 그 효과의 한계는 무엇입니까?
   > 역전 차폐는 초기에 박탈된 눈을 사용하도록 강제하여 시각 회복을 촉진할 수 있습니다. 그러나 역전 차폐 자체만으로는 정상적인 피질 눈 우세 분포를 회복시키지 못하고, 한쪽 눈의 시각 회복만을 촉진하는 경향이 있습니다.

9.연구자들이 어린 고양이에게 짧은 기간의 역전 차폐 후 양안 시각(binocular vision)을 경험하게 했을 때 놀라운 결과는 무엇이었습니까?
   > 짧은 기간의 역전 차폐 후 양안 시각을 경험한 고양이들은 심각한 양안 약시(bilateral amblyopia)를 보였으며, 양쪽 눈의 시력이 정상 수준의 약 1/3에 불과했습니다. 놀랍게도 피질 눈 우세는 정상 고양이와 유사하게 나타났습니다.

10.이 두 논문에서 제시된 연구들은 학습과 발달이라는 넓은 주제에 대해 어떤 공통적인 통찰력을 제공합니까?
   > 두 논문 모두 외부 경험 또는 학습 알고리즘이 신경계의 구조와 기능적 속성을 형성하는 데 중요한 역할을 한다는 것을 보여줍니다. 역전파는 특정 작업을 수행하기 위한 내부 표현을 학습할 수 있음을 보여주는 반면, 시각 박탈 연구는 초기 경험이 시각 피질 발달과 시력에 결정적인 영향을 미친다는 것을 보여줍니다.

### 3.2 에세이 형식 질문 

1. 역전파 알고리즘의 작동 방식과, 이 알고리즘이 어떻게 신경망이 복잡한 패턴 인식 작업을 수행할 수 있도록 하는 유용한 내부 표현을 학습하는지 자세히 설명하십시오.
> 역전파 알고리즘은 신경망의 출력층에서 계산된 오차를 네트워크의 이전 층으로 거슬러 올라가며 각 가중치에 대한 오차의 기여도를 계산하고, 이 정보를 바탕으로 가중치를 조정하여 전체 오차를 최소화하는 방식으로 작동합니다. 이러한 반복적인 가중치 업데이트 과정을 통해 신경망은 입력 데이터의 복잡한 특징을 계층적으로 포착하는 유용한 내부 표현을 학습하게 되며, 이는 높은 수준의 패턴 인식 성능을 가능하게 합니다.

2. 그림 1과 3에 제시된 사례 연구를 바탕으로, 신경망이 특정 작업을 해결하기 위해 적절한 은닉층 표현을 개발하는 능력에 대해 논의하십시오. 이러한 내부 표현의 특징은 무엇이며, 어떻게 네트워크의 성능에 기여합니까?
> 그림 1과 3의 사례 연구에서 신경망은 특정 작업을 수행하는 데 필요한 추상적이고 계층적인 특징을 포착하는 적절한 은닉층 표현을 학습하는 능력을 보여줍니다. 이러한 내부 표현은 입력 데이터의 중요한 패턴, 관계, 불변성을 인코딩하며, 이는 네트워크가 복잡한 매핑을 효과적으로 모델링하고 높은 예측 정확도를 달성하는 데 결정적인 역할을 합니다. 결국, 잘 학습된 은닉층 표현은 네트워크가 보지 못한 새로운 데이터에 대해서도 일반화 성능을 향상시키는 데 기여합니다.

3. 단안 박탈 실험은 시각 시스템 발달의 중요한 시기(critical period)의 개념을 어떻게 강조합니까? 역전 차폐가 효과가 있을 수도 있고 한계가 있을 수도 있는 이유는 무엇이며, 이 연구는 양안 시각 발달에 대해 어떤 점을 시사합니까?
> 단안 박탈 실험은 특정 시기에 한쪽 눈의 시각 입력을 차단함으로써 시각 피질 발달의 가소성이 가장 높은 중요한 시기가 존재함을 명확히 보여줍니다. 이 시기 동안 정상적인 양안 시각 경험이 부족하면 시각 피질의 연결이 비정상적으로 발달하여 영구적인 시력 저하를 유발할 수 있습니다. 역전 차폐는 영향을 받은 눈을 다시 열어주고 정상적인 눈을 가리는 치료법으로, 중요한 시기가 지나기 전에는 어느 정도 효과를 볼 수 있지만, 이미 비정상적인 연결이 고착화된 후에는 효과가 제한적일 수 있습니다. 이 연구는 양안 시각 발달에 있어 초기 경험의 중요성을 강조하며, 정상적인 시력 발달을 위해서는 양쪽 눈의 균형 잡힌 시각 입력이 필수적임을 시사합니다.

4. "1986-rumelhart-2.pdf" 논문에서 제시된 신경망 학습 절차가 생물학적 학습의 정확한 모델이 아니라고 주장하는 이유는 무엇입니까? 그럼에도 불구하고 이 연구는 뇌 기능에 대한 이해에 어떤 중요한 기여를 합니까?
> "1986-rumelhart-2.pdf" 논문에서 제시된 역전파(backpropagation) 알고리즘은 생물학적 학습 메커니즘과는 몇 가지 중요한 차이점을 보입니다. 예를 들어, 역전파는 전역적인 오류 신호의 즉각적인 전달을 필요로 하지만, 생물학적 뉴런은 국소적인 정보만을 처리하고 시간 지연을 겪습니다. 그럼에도 불구하고 이 연구는 인공 신경망의 학습 능력을 획기적으로 발전시켜 패턴 인식, 자연어 처리 등 다양한 분야에서 뇌 기능의 특정 측면, 특히 연합 학습과 정보 처리의 병렬 분산 방식에 대한 중요한 통찰력을 제공했습니다.

5. 두 논문에 제시된 연구 결과를 종합하여, 초기 경험(시각적 또는 학습 데이터)이 신경계의 장기적인 기능적 결과에 미치는 영향에 대해 논의하십시오. 신경망 학습과 시각 발달 연구 사이의 잠재적인 연결 고리는 무엇입니까?
> 두 논문의 연구 결과를 종합하면, 초기 경험은 신경계 발달의 결정적인 요소로서 시각적 입력이나 학습 데이터의 특성이 신경 회로의 형성과 장기적인 기능적 결과에 심오한 영향을 미치는 것으로 나타납니다. 신경망 학습에서 초기 가중치 설정이나 학습 데이터의 편향이 최종 모델의 성능과 일반화 능력에 영향을 미치는 것처럼, 시각 발달 연구에서도 초기 시각 경험의 부족이나 특정 자극에 대한 노출 부족이 시각 처리 능력의 결함으로 이어질 수 있다는 점이 유사한 연결 고리를 시사합니다. 따라서 초기 경험은 신경망 학습과 시각 발달 모두에서 최적의 기능 발달을 위한 중요한 토대임을 알 수 있습니다.

### 3.3 용어집

1. 역전파 (Back-propagation): 
   - 신경망의 출력층에서 계산된 오차를 네트워크의 이전 계층으로 다시 전파하여 각 연결의 가중치를 조정하는 지도 학습 알고리즘입니다.

2. 은닉층 (Hidden Layer): 
   - 입력층과 출력층 사이에 있는 신경망 계층으로, 입력 데이터를 변환하여 출력층이 원하는 결과를 생성하는 데 도움이 되는 중간 표현을 학습합니다.

3. 경사 하강법 (Gradient Descent): 
   - 함수의 최솟값을 찾기 위해 함수의 기울기(gradient)의 반대 방향으로 반복적으로 이동하는 최적화 알고리즘입니다. 신경망 학습에서는 오차 함수를 최소화하기 위해 가중치를 업데이트하는 데 사용됩니다.

4. 학습률 (Learning Rate): 
   - 경사 하강법에서 각 반복마다 가중치를 조정하는 단계의 크기를 결정하는 하이퍼파라미터입니다.

5. 모멘텀 (Momentum): 
   - 경사 하강법의 변형으로, 이전 가중치 업데이트의 일부를 유지하여 학습 속도를 높이고 지역 최소값에 갇히는 것을 방지하는 데 사용됩니다.

6. 단안 박탈 (Monocular Deprivation):
   -  초기 발달 기간 동안 한쪽 눈의 시각적 입력을 막는 실험 절차입니다.

7. 역전 차폐 (Reverse Occlusion): 
   - 단안 박탈 후, 초기에는 열려 있던 눈을 가리고 이전에 가려졌던 눈을 열어 시각 입력을 제공하는 절차입니다.

8. 양안 약시 (Bilateral Amblyopia): 
   - 양쪽 눈의 시력이 모두 비정상적으로 낮은 상태로, 일반적으로 초기 시각 발달의 이상으로 인해 발생합니다.

9. 피질 눈 우세 (Cortical Ocular Dominance): 
   - 시각 피질 뉴런이 한쪽 눈의 자극에 더 강하게 반응하는 경향입니다. 단안 박탈과 같은 경험에 의해 영향을 받을 수 있습니다.

10. 민감한 시기 (Sensitive Period) 또는 결정적 시기 (Critical Period): 
   - 특정 경험에 의해 신경계 발달이 특히 큰 영향을 받는 제한된 기간입니다.


## 4. **ImageNet Classification with Deep Convolutional Neural Networks**
[ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)

심층 합성곱 신경망을 이용한 ImageNet 분류 연구 가이드

### 4.1 퀴즈

1. 이 논문에서 제시하는 심층 합성곱 신경망(CNN) 아키텍처의 주요 특징은 무엇이며, 이전의 최첨단 기술과 비교했을 때 어떤 성능 향상을 보였습니까? 
> 이 논문의 심층 CNN 아키텍처는 5개의 컨볼루션 계층, 맥스 풀링 계층, 3개의 완전 연결 계층으로 구성되며, 최종 출력은 1000-way 소프트맥스입니다. 이 모델은 ILSVRC-2010 테스트 데이터에서 이전 최고 기록보다 훨씬 낮은 top-1 및 top-5 오류율을 달성했습니다.

2. ImageNet 데이터셋의 규모와 특징을 설명하고, ILSVRC 대회의 중요성은 무엇입니까? 또한, 이 연구에서 사용된 ImageNet의 특정 하위 집합에 대해 간략히 설명하십시오. 
> ImageNet은 약 1500만 개의 레이블된 고해상도 이미지를 포함하는 대규모 데이터셋이며, ILSVRC는 ImageNet의 하위 집합을 사용하여 객체 인식 분야의 발전을 촉진하는 연례 대회입니다. 이 연구에서는 ILSVRC-2010 및 ILSVRC-2012의 하위 집합, 특히 약 120만 개의 훈련 이미지와 1000개의 클래스를 사용했습니다.

3. ReLU 비선형성(Rectified Linear Units)의 장점은 무엇이며, 전통적인 활성화 함수와 비교했을 때 학습 속도에 어떤 영향을 미칩니까? 그림 1의 결과를 바탕으로 설명하십시오. 
> ReLU 비선형성은 $$f(x) = \max(0, x)$$로 정의되는 비포화 활성화 함수이며, 시그모이드나 tanh와 같은 전통적인 포화 함수보다 기울기 소실 문제가 적어 학습 속도가 훨씬 빠릅니다. 그림 1은 ReLU를 사용한 CNN이 tanh를 사용한 CNN보다 훨씬 빠르게 특정 훈련 오류율에 도달함을 보여줍니다.

4. 이 논문에서 다중 GPU를 활용한 훈련 방식의 주요 아이디어는 무엇이며, 이것이 모델의 성능과 훈련 시간에 어떤 영향을 미쳤습니까? 
> 다중 GPU 훈련 방식은 네트워크의 커널(또는 뉴런)을 두 개의 GPU에 분산시키고, 특정 계층에서만 GPU 간 통신을 허용하는 방식입니다. 이를 통해 단일 GPU 메모리 제약을 극복하고 더 큰 모델을 훈련할 수 있었으며, 약간의 성능 향상과 함께 훈련 시간도 단축되었습니다.

5. 국소 응답 정규화(Local Response Normalization)는 CNN에서 어떤 역할을 하며, 이 연구에서는 이를 어떻게 구현하고 어떤 효과를 얻었습니까? 
> 국소 응답 정규화는 인접한 커널 맵 간의 활성화를 정규화하여 큰 활성화 값에 대한 경쟁을 유도하는 방식으로, 실제 뉴런의 측면 억제와 유사한 효과를 냅니다. 이 연구에서는 ReLU 적용 후 특정 컨볼루션 계층에 이를 적용하여 모델의 일반화 성능을 향상시키고 오류율을 감소시키는 효과를 확인했습니다.

6. 오버래핑 풀링(Overlapping Pooling)은 전통적인 풀링 방식과 어떻게 다르며, 이 연구에서 사용했을 때 어떤 이점을 보였습니까? 
> 오버래핑 풀링은 풀링 연산 시 인접한 풀링 단위의 수용 영역이 겹치도록 설정하는 방식으로, 전통적인 비오버래핑 풀링($$s=z$$)과 달리 $$s < z$$로 설정합니다. 이 연구에서는 오버래핑 풀링($$s=2, z=3$$)을 사용하여 비오버래핑 풀링($$s=2, z=2$$)에 비해 오류율을 약간 감소시키고 과적합을 줄이는 효과를 관찰했습니다.

7. 이 논문에서 제안하는 CNN 아키텍처의 전체적인 구조를 간략하게 설명하고, 각 계층의 주요 특징(예: 컨볼루션 계층의 커널 크기, 완전 연결 계층의 뉴런 수)을 언급하십시오. 
> 이 논문의 CNN 아키텍처는 5개의 컨볼루션 계층(첫 번째 계층은 11x11x3 커널 96개, 두 번째는 5x5x48 커널 256개, 세 번째, 네 번째는 3x3x256 및 3x3x192 커널 384개, 다섯 번째는 3x3x192 커널 256개)과 3개의 완전 연결 계층(각각 4096개의 뉴런), 그리고 1000개의 클래스를 위한 소프트맥스 출력 계층으로 구성됩니다.

8. 이 연구에서 과적합(Overfitting)을 줄이기 위해 사용된 두 가지 주요 기술은 무엇이며, 각 기술이 어떻게 과적합을 방지하는 데 도움이 되는지 간략히 설명하십시오. 
> 과적합을 줄이기 위해 사용된 두 가지 주요 기술은 데이터 증강(이미지 변환 및 RGB 채널 강도 변경)과 드롭아웃입니다. 데이터 증강은 훈련 데이터셋의 효과적인 크기를 늘려 모델이 다양한 변이에 더 강건해지도록 하며, 드롭아웃은 훈련 중 무작위로 뉴런을 비활성화하여 뉴런 간의 복잡한 공동 적응을 방지하고 더 robust한 특징을 학습하도록 합니다.

9. 모델 훈련에 사용된 최적화 알고리즘(stochastic gradient descent)의 주요 파라미터(예: 배치 크기, 모멘텀, 가중치 감쇠)를 나열하고, 가중치 초기화 방식은 어떠했습니까? 
> 모델 훈련에는 배치 크기 128, 모멘텀 0.9, 가중치 감쇠 0.0005의 stochastic gradient descent가 사용되었습니다. 가중치는 평균이 0이고 표준 편차가 0.01인 가우시안 분포에서 초기화되었으며, 두 번째, 네 번째, 다섯 번째 컨볼루션 계층과 완전 연결 은닉 계층의 편향은 상수 1로, 나머지 계층의 편향은 0으로 초기화되었습니다.

10. 논문에서 제시된 정성적 평가(Qualitative Evaluations) 결과 중 두 가지를 설명하고, 이를 통해 네트워크가 어떤 특징을 학습했는지, 그리고 예측 결과에 대한 어떤 통찰력을 얻을 수 있는지 설명하십시오. 
> 그림 3은 첫 번째 컨볼루션 계층에서 학습된 96개의 커널을 보여주며, 네트워크가 다양한 주파수 및 방향 선택적 특징뿐만 아니라 색상별 블롭도 학습했음을 나타냅니다. 그림 4의 왼쪽 패널은 테스트 이미지에 대한 상위 5개 예측 레이블을 보여주며, 네트워크가 중심에서 벗어난 객체도 인식할 수 있고, 예측이 대부분 합리적임을 보여줍니다.

### 4.2 에세이 형식 질문

1. AlexNet의 아키텍처적 혁신(ReLU, 다중 GPU 훈련, 국소 응답 정규화, 오버래핑 풀링)이 당시의 컴퓨터 비전 분야에 미친 영향과 중요성을 논하십시오. 각 기술이 성능 향상에 기여한 바를 구체적인 실험 결과와 함께 분석하십시오.
> AlexNet은 ReLU 활성화 함수를 도입하여 기존의 tanh나 sigmoid 함수에 비해 학습 속도를 획기적으로 향상시켰으며, 다중 GPU 훈련을 통해 당시로서는 매우 깊은 네트워크를 효율적으로 학습할 수 있게 했습니다. 국소 응답 정규화(LRN)는 인접한 뉴런들의 활성화를 정규화하여 일반화 성능을 소폭 향상시켰고, 오버래핑 풀링은 풀링 과정에서 정보 손실을 줄여 약간의 성능 향상을 가져왔습니다. 이러한 혁신적인 아키텍처적 특징들은 AlexNet이 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012에서 압도적인 성능으로 우승하며 컴퓨터 비전 분야에 딥러닝의 가능성을 제시하는 중요한 계기가 되었습니다.

2. ImageNet 데이터셋이 대규모 심층 학습 모델의 훈련과 발전에 미친 근본적인 영향에 대해 논하고, AlexNet 연구가 ImageNet을 활용하여 객체 인식 분야에서 획기적인 성과를 달성한 사례를 상세히 설명하십시오.
> ImageNet 데이터셋은 수백만 개의 레이블링된 이미지를 제공하여 대규모 심층 학습 모델의 훈련을 가능하게 했으며, 이는 컴퓨터 비전 분야의 비약적인 발전을 이끌었습니다. 특히 AlexNet 연구는 ImageNet 데이터셋을 활용하여 깊은 컨볼루션 신경망(CNN) 구조를 성공적으로 훈련시켜 2012년 ImageNet Large Scale Visual Recognition Challenge(ILSVRC)에서 압도적인 성능으로 우승하며 객체 인식 분야에서 획기적인 성과를 달성했습니다. 이는 딥러닝 기반 컴퓨터 비전 연구의 새로운 시대를 열었으며, 이후 다양한 혁신적인 모델 개발의 토대가 되었습니다.

3. AlexNet에서 과적합을 방지하기 위해 사용된 데이터 증강과 드롭아웃 기술의 원리를 설명하고, 각 방법이 모델의 일반화 성능을 어떻게 향상시키는지 분석하십시오. 이러한 정규화 기법이 현대 심층 학습 모델 훈련에 여전히 중요한 이유를 갖는다.
> AlexNet에서 데이터 증강은 학습 데이터셋의 크기를 인위적으로 늘려 모델이 다양한 변형에도 강건하도록 돕고, 드롭아웃은 학습 과정에서 무작위로 일부 뉴런을 비활성화하여 특정 뉴런에 과도하게 의존하는 현상을 막습니다. 이러한 기법들은 모델이 학습 데이터에만 특화되는 과적합을 방지하고, 보지 못한 새로운 데이터에 대해서도 높은 성능을 유지하는 일반화 능력을 향상시킵니다. 과적합은 여전히 심층 학습 모델의 성능을 저해하는 주요 문제이므로, 이러한 정규화 기법들은 현대 심층 학습 모델 훈련에서 필수적인 요소로 남아있습니다.

4. AlexNet 연구에서 깊이(depth)가 모델 성능에 미치는 중요성이 어떻게 강조되었는지 설명하고, 초기 컨볼루션 신경망 모델과 비교하여 AlexNet의 깊이가 가지는 의미를 논하십시오. 더 깊은 네트워크를 훈련하는 데 따르는 어려움과 이를 해결하기 위한 후속 연구들을 간략히 언급하십시오.
> AlexNet 연구는 깊이가 모델 성능 향상에 결정적인 역할을 한다는 것을 명확히 보여주었습니다. 초기 컨볼루션 신경망 모델들이 비교적 얕은 구조를 가졌던 반면, AlexNet은 8개의 레이어를 쌓아 더 복잡한 특징들을 학습할 수 있게 했습니다. 그러나 깊어진 네트워크는 기울기 소실/폭주 문제를 야기하여 학습을 어렵게 만들었고, 이를 해결하기 위해 ReLU 활성화 함수, 드롭아웃, 배치 정규화, 잔차 연결(ResNet) 등 다양한 후속 연구들이 진행되었습니다.

5. AlexNet 연구의 결과가 컴퓨터 비전뿐만 아니라 인공지능 전반에 걸쳐 미친 광범위한 영향에 대해 논하십시오. 이 연구가 심층 학습의 부흥과 발전에 어떻게 기여했으며, 이후의 연구 및 응용 분야에 어떤 영감을 주었는지 구체적인 예시와 함께 설명하십시오.
> AlexNet 연구는 컴퓨터 비전 분야에서 딥러닝의 부흥을 이끌었을 뿐만 아니라, 인공지능 전반에 걸쳐 심층 학습의 잠재력을 입증하며 광범위한 영향을 미쳤습니다. 이 연구는 ReLU 활성화 함수, 드롭아웃, GPU 병렬 처리 등 혁신적인 기술을 도입하여 이미지 인식 성능을 획기적으로 향상시켰고, 이는 VGG, GoogLeNet, ResNet 등 후속 깊은 신경망 모델들의 개발에 영감을 주었습니다. 또한, AlexNet의 성공은 이미지 분류를 넘어 객체 탐지, 자연어 처리, 음성 인식 등 다양한 인공지능 응용 분야에서 심층 학습 연구를 활발하게 촉진하는 계기가 되었습니다.

### 4.3 주요 용어 해설

1. 합성곱 신경망 (Convolutional Neural Network, CNN): 
   - 이미지, 비디오 등 격자 구조 데이터를 처리하는 데 특화된 심층 학습 모델의 한 종류. 컨볼루션 연산을 통해 공간적 특징을 추출하고, 풀링, 활성화 함수 등을 거쳐 최종 출력을 생성한다.

2. ImageNet: 
   - 수백만 개의 레이블된 고해상도 이미지를 포함하는 대규모 이미지 데이터셋. 약 22,000개의 카테고리를 포함하며, 객체 인식 및 이미지 분류 연구의 벤치마크로 활용된다.

3. ILSVRC (ImageNet Large-Scale Visual Recognition Challenge): 
   - ImageNet 데이터셋의 하위 집합을 사용하여 매년 개최되는 객체 인식 대회. 다양한 시각 인식 과제를 포함하며, 최첨단 모델들의 성능을 평가하는 데 중요한 역할을 한다.

4. ReLU (Rectified Linear Unit): 
   - $$f(x) = \max(0, x)$$로 정의되는 비선형 활성화 함수. 기울기 소실 문제를 완화하고 학습 속도를 향상시키는 장점을 가져 심층 신경망에서 널리 사용된다.

5. 포화 비선형성 (Saturating Nonlinearity): 
   - 입력값이 매우 크거나 작아질수록 출력값의 기울기가 0에 가까워지는 활성화 함수 (예: 시그모이드, tanh). 심층 신경망에서 기울기 소실 문제를 야기할 수 있다.

6. GPU (Graphics Processing Unit): 
   - 원래 그래픽 처리를 위해 설계되었지만, 병렬 연산 능력이 뛰어나 심층 학습 모델의 훈련을 가속화하는 데 널리 활용된다.

7. 국소 응답 정규화 (Local Response Normalization): 
   - 인접한 뉴런들의 활성화를 정규화하여 활성화 값의 크기를 조정하는 기법. 네트워크의 일반화 성능을 향상시키는 데 도움을 줄 수 있다.

8. 오버래핑 풀링 (Overlapping Pooling): 
   - 풀링 연산 시 인접한 풀링 단위의 수용 영역이 겹치도록 설정하는 풀링 방식. 특징 맵의 공간적 해상도를 줄이면서도 약간의 이동 불변성을 확보하고 과적합을 줄이는 효과를 가질 수 있다.

9. 완전 연결 계층 (Fully-Connected Layer): 
   - 이전 계층의 모든 뉴런과 연결된 계층. 최종 분류 또는 회귀 작업을 수행하는 데 주로 사용된다.

10. 소프트맥스 (Softmax): 
   - 다중 클래스 분류 문제에서 각 클래스에 대한 확률 분포를 출력하는 함수. 출력 값의 합은 1이다.

11. 과적합 (Overfitting): 
   - 모델이 훈련 데이터에는 지나치게 잘 맞지만, 새로운 데이터에는 제대로 일반화하지 못하는 현상.

12. 데이터 증강 (Data Augmentation): 
   - 훈련 데이터의 양을 늘리고 다양성을 확보하기 위해 원본 데이터에 다양한 변환(예: 회전, 이동, 반전, 크기 조정, 색상 변경)을 적용하는 기법. 과적합을 방지하는 데 효과적이다.

13. 드롭아웃 (Dropout): 
   - 훈련 과정에서 무작위로 일부 뉴런의 출력을 0으로 설정하는 정규화 기법. 뉴런 간의 공동 적응을 줄이고 모델의 일반화 성능을 향상시킨다.

14. 확률적 경사 하강법 (Stochastic Gradient Descent, SGD): 
   - 신경망 모델의 가중치를 훈련 데이터의 각 미니배치에 대해 계산된 손실 함수의 기울기를 따라 업데이트하는 최적화 알고리즘.

15. 모멘텀 (Momentum): 
   - SGD의 한 변형으로, 이전 업데이트 방향을 고려하여 현재 업데이트 속도를 조절하는 방식. 학습의 안정성과 수렴 속도를 향상시키는 데 도움을 준다.

16. 가중치 감쇠 (Weight Decay): 
   - 모델의 가중치 크기가 너무 커지지 않도록 손실 함수에 가중치의 L2 노름(또는 L1 노름)을 추가하여 규제하는 기법. 과적합을 방지하는 데 사용된다.

17. 오류율 (Error Rate): 
   - 모델의 예측이 실제 레이블과 다른 비율. top-1 오류율은 가장 높은 확률로 예측된 클래스가 실제 레이블과 다른 경우의 비율이며, top-5 오류율은 가장 높은 확률로 예측된 상위 5개 클래스 중 실제 레이블이 포함되지 않은 경우의 비율이다.

18. 일반화 (Generalization): 
   - 훈련된 모델이 훈련 데이터가 아닌 새로운 데이터에 대해 얼마나 잘 작동하는지를 나타내는 능력.

## 5. **Very Deep Convolutional Networks for Large-Scale Image Recognition** 
초고심층 컨볼루션 네트워크 연구

### 5.1 퀴즈 (각 질문 당 2-3 문장)
1. 본 논문에서 가장 중요한 기여는 무엇이며, 어떤 구조를 사용하여 이를 입증했습니까?
> 본 논문의 가장 중요한 기여는 컨볼루션 네트워크의 깊이가 대규모 이미지 인식 정확도에 미치는 영향을 철저히 평가한 것입니다. 이를 위해 모든 계층에서 매우 작은 (3x3) 컨볼루션 필터를 사용하는 구조를 사용하여 네트워크 깊이를 16-19개 가중치 계층까지 늘림으로써 상당한 성능 향상을 보여주었습니다.

2. 저자들이 제시한 매우 깊은 컨볼루션 네트워크의 깊이는 어느 정도이며, 이는 이전 연구들과 비교하여 어떤 차이점을 갖습니까?
> 본 논문에서 조사된 매우 깊은 컨볼루션 네트워크의 깊이는 11개에서 19개의 가중치 계층이며, 이는 이전 연구들에서 일반적으로 사용되던 네트워크보다 훨씬 깊습니다. 이러한 깊이를 통해 더 복잡한 특징을 학습하고 이미지 인식 성능을 향상시킬 수 있었습니다.

3. 본 논문에서 사용된 컨볼루션 필터의 크기는 얼마이며, 작은 필터를 사용하는 것이 큰 필터를 사용하는 것에 비해 갖는 이점은 무엇입니까?
> 본 논문에서는 모든 컨볼루션 계층에서 매우 작은 3x3 크기의 필터를 사용했습니다. 작은 필터를 여러 개 쌓는 것은 하나의 큰 필터를 사용하는 것보다 더 많은 비선형 활성화 함수를 적용하여 결정 함수를 더 구별력 있게 만들고, 파라미터 수를 줄여 과적합을 방지하는 이점이 있습니다.

4. 저자들은 네트워크 훈련 시 어떤 데이터 증강 기법들을 사용했으며, 그 이유는 무엇입니까?
> 저자들은 훈련 데이터셋을 늘리기 위해 랜덤 수평 뒤집기(random horizontal flipping)와 랜덤 RGB 색상 이동(random RGB colour shift)을 사용했습니다. 또한, 다양한 물체 크기를 고려하기 위해 훈련 이미지의 크기를 다양하게 조절하는 멀티 스케일 훈련(multi-scale training) 방식을 사용했습니다.

5. 단일 스케일 훈련과 멀티 스케일 훈련의 차이점은 무엇이며, 실험 결과 멀티 스케일 훈련이 갖는 이점은 무엇이었습니까?
> 단일 스케일 훈련은 훈련 이미지의 크기를 고정된 값으로 사용하는 반면, 멀티 스케일 훈련은 각 훈련 이미지마다 특정 범위 내에서 랜덤하게 크기를 조절합니다. 실험 결과, 멀티 스케일 훈련은 다양한 크기의 물체 특징을 학습하는 데 도움을 주어 단일 스케일 훈련보다 더 나은 성능을 보였습니다.

6. 테스트 시, 이미지 분류를 위해 완전 연결 계층을 어떻게 컨볼루션 계층으로 변환하여 사용했습니까? 이 방법의 장점은 무엇입니까?
> 테스트 시, 완전 연결 계층을 컨볼루션 계층으로 변환하여 전체 이미지에 대해 조밀하게 특징을 추출했습니다. 첫 번째 FC 계층은 7x7 컨볼루션 계층으로, 마지막 두 FC 계층은 1x1 컨볼루션 계층으로 변환되었습니다. 이 방법을 통해 이미지의 다양한 위치에서 특징을 효율적으로 추출하고 계산량을 줄일 수 있었습니다.

7. 본 논문에서 제시된 다양한 네트워크 구조(A부터 E까지)는 어떤 주요 차이점을 가지며, 깊이가 성능에 미치는 영향은 어떠했습니까?
> 제시된 네트워크 구조 A부터 E까지는 컨볼루션 계층의 깊이가 주요 차이점입니다. A는 11개, E는 19개의 가중치 계층을 가집니다. 실험 결과, 네트워크의 깊이가 증가함에 따라 이미지 분류 오류율이 감소하는 것을 확인했으며, 이는 깊이가 깊을수록 더 복잡한 특징을 학습하여 성능 향상에 기여함을 보여줍니다.

8. LRN(Local Response Normalization)은 본 연구에서 어떤 결과를 보였으며, 더 깊은 네트워크 구조에서 LRN을 사용하지 않은 이유는 무엇입니까?
> 실험 결과, 초기 네트워크(A-LRN)에서 LRN을 사용한 것이 사용하지 않은 것(A)에 비해 성능 향상을 보이지 않았습니다. 따라서 메모리 소비와 계산 시간을 늘리는 LRN을 더 깊은 네트워크 구조(B-E)에서는 사용하지 않았습니다.

9. 저자들은 ILSVRC 2014 대회에서 분류와 위치 파악 트랙에서 각각 어떤 성과를 거두었으며, 제안된 네트워크의 성능은 당시 최고 수준과 비교하여 어떠했습니까?
> 저자들은 ILSVRC 2014 대회에서 "VGG" 팀으로 참가하여 위치 파악 트랙에서 25.3% 오류율로 1위를, 분류 트랙에서 7.3% 테스트 오류율로 2위를 차지했습니다. 이는 당시 최고 수준의 성능과 견줄 만하거나 능가하는 결과였습니다.

10. 사전 학습된 매우 깊은 컨볼루션 네트워크의 특징을 다른 이미지 인식 데이터셋에 적용했을 때 어떤 결과를 얻었으며, 이는 무엇을 의미합니까?
> 사전 학습된 매우 깊은 컨볼루션 네트워크의 특징을 VOC와 Caltech 데이터셋과 같은 다른 이미지 인식 데이터셋에 적용했을 때, 기존의 얕은 모델이나 수동 설계 특징보다 훨씬 뛰어난 성능을 보였습니다. 이는 ImageNet과 같은 대규모 데이터셋에서 학습된 깊은 시각적 표현이 다른 다양한 이미지 인식 문제에도 잘 일반화될 수 있음을 시사합니다.

### 5.2 에세이 형식 질문

1. 본 논문에서 제시된 매우 깊은 컨볼루션 네트워크가 이전의 얕은 네트워크에 비해 이미지 인식 성능을 크게 향상시킨 주요 요인들은 무엇이라고 생각하십니까? (예: 깊이, 작은 필터 크기, 활성화 함수 등) 각 요인이 성능 향상에 기여하는 방식에 대해 구체적인 논거를 제시하십시오.
> 매우 깊은 컨볼루션 네트워크(VGGNet 등)가 얕은 네트워크 대비 이미지 인식 성능을 크게 향상시킨 주요 요인은 깊이 증가와 작은 필터 크기 사용입니다. 깊이를 늘림으로써 네트워크는 이미지의 복잡한 특징들을 계층적으로 학습하고 추상화할 수 있으며, 작은 필터 크기를 반복적으로 사용함으로써 넓은 수용장을 확보하면서도 파라미터 수를 줄여 과적합을 방지하고 더 세밀한 특징 추출이 가능해졌습니다. 또한, ReLU와 같은 비선형 활성화 함수를 중간층에 적용하여 네트워크의 표현력을 풍부하게 만들고 기울기 소실 문제를 완화한 것도 성능 향상에 기여했습니다.

2. 본 논문에서는 3x3 컨볼루션 필터를 반복적으로 사용하는 것이 효과적임을 강조합니다. 단일의 큰 컨볼루션 필터 대신 여러 개의 작은 필터를 쌓아서 얻을 수 있는 이론적 및 실제적인 이점들을 심층적으로 논의하고, 본 연구의 실험 결과를 바탕으로 주장을 뒷받침하십시오.
> 본 논문에서는 단일의 큰 컨볼루션 필터 대신 여러 개의 3x3 컨볼루션 필터를 쌓는 것이 동일한 수용 영역을 가지면서도 파라미터 수를 줄여 계산 효율성을 높이고, 더 많은 비선형 활성화 함수를 적용하여 모델의 표현력을 풍부하게 만들 수 있다는 이론적 이점을 강조합니다. 실험 결과, 깊어진 네트워크 구조는 이미지의 복잡한 특징을 더 잘 학습하여 성능 향상을 보였으며, 이는 작은 필터의 반복적인 사용이 효과적인 방법임을 실증적으로 뒷받침합니다.

3. 이미지 인식 분야에서 컨볼루션 네트워크의 깊이가 갖는 중요성은 무엇이며, 네트워크를 무한정 깊게 만드는 것이 항상 바람직한지에 대한 당신의 생각을 논하십시오. 깊이에 따른 잠재적인 문제점(예: 기울기 소실/폭발, 과적합, 계산 복잡성 증가)과 이를 해결하기 위한 방법들을 함께 고려하십시오.
> 이미지 인식 분야에서 컨볼루션 네트워크의 깊이는 모델이 이미지의 복잡한 특징들을 계층적으로 학습하고 추상화하는 데 핵심적인 역할을 합니다. 깊은 네트워크는 낮은 수준의 에지나 코너부터 높은 수준의 객체 부품이나 전체 객체까지 점진적으로 파악하여 인식 성능을 향상시킬 수 있습니다. 그러나 네트워크를 무한정 깊게 만드는 것은 기울기 소실/폭발, 과적합, 계산 복잡성 증가와 같은 문제점을 야기할 수 있으므로 항상 바람직한 것은 아니며, 잔차 연결(Residual Connection), 배치 정규화(Batch Normalization) 등의 방법을 통해 이러한 문제를 완화할 수 있습니다.

4. 본 논문에서 사용된 훈련 및 테스트 전략(예: 데이터 증강, 멀티 스케일 처리, 앙상블)이 최종 성능 향상에 어떻게 기여했는지 분석하고, 각 전략의 장단점과 실제 적용 시 고려해야 할 사항들을 논하십시오. 또한, 제시된 전략 외에 성능 향상을 위해 고려할 수 있는 다른 방법들을 제안하십시오.
> 본 논문에서 활용된 데이터 증강은 모델의 일반화 능력을 향상시키고, 멀티 스케일 처리는 다양한 크기의 객체에 대한 인식 성능을 높이며, 앙상블 기법은 여러 모델의 예측을 결합하여 안정적이고 정확한 결과를 도출하는 데 기여했습니다. 각 전략은 성능 향상에 효과적이지만, 데이터 증강은 과적합 위험을, 멀티 스케일 처리는 계산 비용 증가를, 앙상블은 모델 복잡성 증가를 고려해야 합니다. 추가적인 성능 향상 방법으로는 더 많은 양질의 데이터 확보, 최신 모델 구조 적용, 최적화된 학습 스케줄링 등이 있습니다.

5. 본 논문의 연구 결과가 이미지 인식 분야에 미친 영향과 의의를 종합적으로 평가하고, 제시된 매우 깊은 컨볼루션 네트워크 구조가 향후 컴퓨터 비전 연구 및 응용 분야에 어떤 방식으로 활용될 수 있을지 전망해보십시오.
> 본 연구는 매우 깊은 컨볼루션 네트워크(VGGNet) 구조를 제시하여 이미지 인식 분야에서 깊이의 중요성을 입증하고, 당시 최고 성능을 달성하며 후속 연구의 중요한 토대를 마련했습니다. 이러한 깊은 네트워크 구조는 특징 추출 능력을 향상시켜 향후 컴퓨터 비전 연구에서 더욱 복잡하고 미묘한 시각적 패턴을 이해하는 모델 개발에 활용될 수 있으며, 객체 탐지, 이미지 분할, 비디오 분석 등 다양한 응용 분야에서 성능 향상을 이끌어낼 잠재력을 지닙니다.

### 5.3 용어집
1. 컨볼루션 신경망 (Convolutional Neural Network, CNN): 
   - 이미지, 비디오 등 격자 형태의 데이터를 처리하는 데 특화된 심층 학습 모델의 한 종류. 컨볼루션 연산을 통해 이미지의 지역적인 특징을 추출하고, 풀링(pooling) 연산을 통해 특징 맵의 차원을 축소하며, 완전 연결 계층을 통해 최종 출력을 생성합니다.

2. 깊이 (Depth): 
   - 신경망의 층(layer) 수를 의미합니다. 본 논문에서는 컨볼루션 계층과 완전 연결 계층의 수를 합하여 네트워크의 깊이를 측정하며, 깊이가 깊어질수록 더 복잡한 특징을 학습할 수 있다고 주장합니다.

3. 컨볼루션 필터 (Convolution Filter): 
   - 컨볼루션 연산에 사용되는 작은 크기의 가중치 행렬. 입력 데이터의 지역적인 특징을 감지하는 역할을 합니다. 본 논문에서는 작은 3x3 크기의 필터를 주로 사용했습니다.

4. 수용 영역 (Receptive Field): 
   - 특정 뉴런의 출력이 영향을 받는 입력 데이터의 영역 크기. 깊은 컨볼루션 네트워크에서 여러 개의 작은 필터를 쌓으면 더 넓은 수용 영역을 효과적으로 확보할 수 있습니다.

5. 풀링 (Pooling): 
   - 컨볼루션 계층 이후에 적용되는 연산으로, 특징 맵의 공간적 크기를 줄이고 연산량을 감소시키는 역할을 합니다. 최대 풀링(max-pooling)은 특정 영역에서 가장 큰 값을 선택하는 방식입니다.

6. 완전 연결 계층 (Fully-Connected Layer, FC): 
   - 신경망의 마지막 부분에 위치하며, 이전 계층의 모든 뉴런과 연결되어 최종 분류 또는 회귀 출력을 생성합니다.

7. ReLU (Rectified Linear Unit): 
   - 비선형 활성화 함수의 일종으로, 입력이 0보다 크면 입력을 그대로 출력하고, 0 이하이면 0을 출력합니다. 깊은 신경망 학습 시 기울기 소실 문제를 완화하는 데 도움을 줍니다.

8. LRN (Local Response Normalization): 
   - 인접한 특징 맵들의 활성화를 정규화하는 기법으로, AlexNet에서 사용되었으나 본 연구에서는 성능 향상에 기여하지 않는 것으로 나타났습니다.

9. 데이터 증강 (Data Augmentation): 
   - 훈련 데이터의 다양성을 늘려 모델의 일반화 성능을 향상시키기 위해 원본 데이터를 변형하는 기법. 본 논문에서는 이미지 뒤집기, 색상 변환, 크기 조절 등의 방법을 사용했습니다.

10. 미니 배치 경사 하강법 (Mini-batch Gradient Descent): 
   - 전체 훈련 데이터를 작은 묶음(미니 배치)으로 나누어 각 배치에 대해 경사를 계산하고 모델의 가중치를 업데이트하는 최적화 알고리즘.

11. 역전파 (Back-propagation): 
   - 신경망을 훈련시키는 데 사용되는 핵심 알고리즘으로, 출력 계층에서 계산된 오류를 네트워크의 이전 계층으로 역방향으로 전파하여 각 가중치에 대한 기울기를 계산하고 가중치를 업데이트합니다.

12. 가중치 감쇠 (Weight Decay): 
   - 모델의 가중치가 너무 커지는 것을 방지하여 과적합을 줄이는 정규화 기법. L2 정규화가 일반적으로 사용됩니다.

13. 드롭아웃 (Dropout): 
   - 훈련 과정에서 신경망의 일부 뉴런을 무작위로 비활성화하여 모델의 과적합을 방지하는 정규화 기법.

14. 이미지넷 (ImageNet): 
   - 대규모 이미지 데이터셋 및 이미지 인식 대회 (ILSVRC)의 이름. 1000개의 클래스와 약 120만 장의 훈련 이미지를 포함합니다.

15. ILSVRC (ImageNet Large-Scale Visual Recognition Challenge): 
   - 이미지넷 데이터셋을 기반으로 이미지 분류, 객체 위치 파악 등의 성능을 겨루는 대회.

16. Top-1 오류율 (Top-1 Error Rate): 
   - 모델이 예측한 가장 높은 확률의 클래스가 실제 정답 클래스와 다른 경우의 비율.

17. Top-5 오류율 (Top-5 Error Rate): 
   - 모델이 예측한 확률이 가장 높은 5개의 클래스 안에 실제 정답 클래스가 없는 경우의 비율. ILSVRC의 주요 평가 지표입니다.

18. 정규화 (Regularization): 
   - 모델이 훈련 데이터에 너무 과적합되는 것을 방지하고 일반화 성능을 향상시키기 위한 다양한 기법 (예: 가중치 감쇠, 드롭아웃).

19. 미세 조정 (Fine-tuning): 
   - 대규모 데이터셋으로 사전 학습된 모델의 가중치를 새로운 작업의 데이터셋에 맞게 다시 학습시키는 과정. 본 논문에서는 ImageNet으로 사전 학습된 모델을 다른 이미지 인식 데이터셋에 적용할 때 사용했습니다.

20. SVM (Support Vector Machine): 
   - 분류 및 회귀 분석을 위해 사용되는 지도 학습 모델. 본 논문에서는 사전 학습된 CNN 특징을 사용하여 SVM 분류기를 훈련했습니다.

21. 평균 정밀도 (mean Average Precision, mAP): 
   - 객체 검출 또는 다중 레이블 이미지 분류 작업에서 모델의 성능을 평가하는 지표. 각 클래스별로 Average Precision을 계산하고 이를 평균합니다.


## 6. **Deep Residual Learning for Image Recognition (2015, He et al.)**  
심층 잔차 학습 연구 가이드

### 6.1 퀴즈
1. 저자들이 제안하는 "잔차 학습 프레임워크"의 핵심 아이디어는 무엇이며, 기존의 심층 신경망 학습 방식과 어떻게 다른가요?
> 저자들은 네트워크가 직접적인 매핑 함수를 학습하는 대신, 입력에 대한 잔차 함수를 학습하도록 계층을 재구성하는 잔차 학습 프레임워크를 제안합니다. 이는 층 입력을 기준으로 학습되지 않은 함수를 학습하는 기존 방식과 대조됩니다.

2. 심층 신경망에서 깊이가 증가함에 따라 발생하는 "퇴화 문제(degradation problem)"는 무엇이며, 이 문제가 과적합(overfitting)과 어떻게 다른가요?
> 퇴화 문제는 네트워크 깊이가 증가함에 따라 정확도가 포화되고 급격히 저하되는 현상으로, 이는 더 깊은 모델이 얕은 모델보다 더 높은 학습 오류를 보이는 특징이 있습니다. 이는 학습 오류가 감소하는 과적합과는 다른 문제입니다.

3. 잔차 블록(residual block)의 구조와 작동 방식을 설명하고, "숏컷 연결(shortcut connection)"이 이 구조에서 어떤 역할을 하는지 구체적으로 기술하세요.
> 잔차 블록은 몇 개의 쌓인 계층과 입력이 출력에 더해지는 "숏컷 연결"로 구성됩니다. 숏컷 연결은 하나 이상의 계층을 건너뛰어 항등 매핑을 수행하며, 이를 통해 네트워크는 잔차 함수를 학습하는 데 집중할 수 있습니다.

4. 논문에서 제시된 두 가지 종류의 숏컷 연결은 무엇이며, 각각 언제 사용되고 어떤 장단점이 있나요?
> 두 가지 종류의 숏컷 연결은 항등 매핑 숏컷과 투영 숏컷입니다. 항등 매핑 숏컷은 입력과 출력의 차원이 동일할 때 사용되며, 추가적인 파라미터나 계산 복잡성을 도입하지 않습니다. 투영 숏컷은 입력과 출력의 차원이 다를 때 차원을 맞추기 위해 사용되며, 1x1 컨볼루션으로 구현될 수 있습니다.

5. ImageNet 데이터셋 실험에서 평범한(plain) 네트워크와 잔차 네트워크의 학습 곡선(training/validation error)은 깊이가 증가함에 따라 어떤 차이를 보였나요? 이러한 차이가 의미하는 바는 무엇인가요?
> ImageNet 실험에서 깊이가 증가함에 따라 평범한 네트워크는 학습 오류와 검증 오류가 모두 증가하는 퇴화 문제를 보였지만, 잔차 네트워크는 깊이가 증가해도 학습 오류가 감소하고 더 높은 정확도를 달성했습니다. 이는 잔차 학습이 깊은 네트워크의 최적화 어려움을 해결하는 데 효과적임을 시사합니다.

6. "병목 구조(bottleneck architecture)"는 무엇이며, ResNet-50, ResNet-101, ResNet-152와 같은 더 깊은 네트워크에서 이 구조를 사용하는 이유는 무엇인가요?
> 병목 구조는 각 잔차 함수 F를 2개의 계층 대신 1x1, 3x3, 1x1 컨볼루션으로 구성된 3개의 계층 스택으로 설계한 것입니다. 1x1 계층은 차원을 줄이고 늘리는 역할을 하며, 3x3 계층은 더 작은 입출력 차원으로 병목을 형성합니다. 이는 더 깊은 네트워크의 계산 효율성을 높이기 위해 사용됩니다.

7. CIFAR-10 데이터셋 실험에서 매우 깊은 잔차 네트워크(예: 110-layer, 1202-layer)는 어떤 성능을 보였으며, ImageNet 실험 결과와 비교했을 때 어떤 유사점과 차이점이 있나요?
> CIFAR-10 실험에서 매우 깊은 잔차 네트워크는 최적화 어려움 없이 학습 오류를 매우 낮게 달성했지만, 지나치게 깊은 모델(예: 1202-layer)은 과적합으로 인해 테스트 오류가 오히려 증가하는 경향을 보였습니다. 이는 깊이가 증가함에 따라 최적화는 가능하지만, 데이터셋의 크기에 맞는 적절한 깊이를 찾는 것이 중요함을 시사합니다.

8. 논문에서 층 응답(layer response)의 표준 편차 분석을 통해 발견한 주요 결과는 무엇이며, 이는 저자들의 잔차 학습에 대한 가설을 어떻게 뒷받침하나요?
> 층 응답의 표준 편차 분석 결과, 잔차 네트워크는 평범한 네트워크보다 전반적으로 더 작은 응답 값을 가졌으며, 더 깊은 잔차 네트워크일수록 응답 값의 크기가 더 작아지는 경향을 보였습니다. 이는 잔차 함수가 비잔차 함수보다 0에 더 가깝다는 저자들의 가설을 뒷받침하며, 항등 매핑이 합리적인 사전 조건화(preconditioning)를 제공함을 시사합니다.

9. PASCAL VOC 및 MS COCO 객체 감지 작업에서 VGG-16 대신 ResNet-101을 사용했을 때 얻은 성능 향상은 무엇을 의미하며, 잔차 학습의 어떤 장점을 시사하나요?
> PASCAL VOC 및 MS COCO 객체 감지 작업에서 VGG-16 대신 ResNet-101을 사용했을 때 상당한 성능 향상이 있었습니다. 이는 잔차 학습을 통해 학습된 더 깊고 효과적인 특징 표현이 다양한 시각 인식 작업에서 일반화 성능을 향상시킬 수 있음을 보여줍니다.

10. ILSVRC & COCO 2015 대회에서 잔차 네트워크 기반 모델이 달성한 주요 성과는 무엇이며, 이는 심층 잔차 학습의 실질적인 영향력을 어떻게 보여주나요?
> ILSVRC & COCO 2015 대회에서 잔차 네트워크 기반 모델은 ImageNet 분류, 객체 감지, локализация, COCO 객체 감지 및 분할 작업에서 1위를 차지했습니다. 이는 심층 잔차 학습이 실제 컴퓨터 비전 문제에서 매우 강력하고 효과적인 접근 방식임을 입증합니다.

### 6.2 에세이 형식 질문
1. 심층 학습의 발전 과정에서 네트워크 깊이의 중요성은 어떻게 인식되어 왔으며, 저자들이 "퇴화 문제"를 발견하고 이를 해결하기 위해 잔차 학습 프레임워크를 제안하게 된 배경을 상세히 논의하시오.
> 심층 학습 발전 초기에는 네트워크 깊이가 깊어질수록 모델의 표현력이 증가하여 성능 향상에 기여할 것이라는 믿음이 있었습니다. 하지만 깊이가 일정 수준 이상으로 깊어지면 오히려 학습이 어려워지고 성능이 저하되는 "퇴화 문제(degradation problem)"가 발견되었습니다. 저자들은 이러한 퇴화 문제가 과적합(overfitting) 때문이 아니라, 깊은 네트워크에서 최적화 자체가 어려워지기 때문에 발생한다는 것을 밝히고, 이를 해결하기 위해 입력과 출력 간의 "잔차(residual)"를 학습하는 잔차 학습 프레임워크를 제안했습니다.

2. 잔차 학습 프레임워크의 핵심 요소인 "잔차 블록"과 "숏컷 연결"은 심층 신경망의 학습 과정을 어떻게 용이하게 하며, 이는 기존의 심층 신경망 아키텍처와 비교했을 때 어떤 근본적인 차이를 가져오는가?
> 잔차 학습 프레임워크의 핵심 요소인 잔차 블록은 입력값을 레이어를 건너뛰어 출력에 직접 더하는 "숏컷 연결"을 통해 기울기 소실 문제를 완화하고, 네트워크가 항등 함수 또는 그 주변의 잔차 함수를 학습하도록 유도하여 최적화 과정을 용이하게 합니다. 이는 기존의 심층 신경망 아키텍처가 깊어질수록 학습이 어려워지는 것과 달리, 잔차 학습을 통해 훨씬 깊은 네트워크도 효과적으로 학습할 수 있게 하는 근본적인 차이를 가져옵니다.

3. 논문에서 제시된 ImageNet 및 CIFAR-10 데이터셋에 대한 다양한 실험 결과들을 종합적으로 분석하고, 잔차 네트워크가 평범한 네트워크에 비해 갖는 주요 장점들을 구체적인 성능 지표 및 학습 곡선 변화와 함께 설명하시오.
> 논문에서 ImageNet 및 CIFAR-10 데이터셋에 대한 실험 결과들을 종합적으로 분석하면, 잔차 네트워크(ResNet)는 평범한 네트워크에 비해 훨씬 깊은 레이어에서도 성능 저하 없이 효과적으로 학습되는 것을 확인할 수 있습니다. 구체적으로, ResNet은 vanishing gradient 문제를 해결하고 최적화 과정을 용이하게 하여 평범한 네트워크보다 높은 정확도를 달성하며, 학습 곡선에서도 더 빠르고 안정적인 수렴 양상을 보입니다. 이는 잔차 연결(residual connection)이 네트워크가 항등 함수(identity function)를 쉽게 학습하도록 도와주어 깊은 네트워크에서도 정보 손실을 최소화하기 때문입니다.

4. 심층 잔차 학습의 원리가 ImageNet 분류뿐만 아니라 객체 감지, локализация, 분할과 같은 다른 컴퓨터 비전 task에서도 성공적인 결과를 가져온 이유는 무엇이라고 생각하는가? 잔차 학습의 일반화 능력에 대한 당신의 견해를 뒷받침할 수 있는 추가적인 아이디어나 사례를 제시하시오.
> 심층 잔차 학습이 ImageNet 분류 외 다른 컴퓨터 비전 task에서도 성공적인 이유는 네트워크 깊이가 깊어질수록 발생하는 기울기 소실/폭주 문제를 효과적으로 해결하여 매우 깊은 네트워크 학습을 가능하게 하기 때문입니다. 잔차 연결(Residual Connection)은 이전 레이어의 정보를 건너뛰어 이후 레이어로 직접 전달함으로써, 네트워크가 항등 함수(identity function)를 쉽게 학습하도록 돕고, 이는 네트워크가 특정 task에 과적합되는 것을 방지하고 다양한 시각적 특징을 효과적으로 학습하는 일반화 능력을 향상시킵니다. 예를 들어, 객체 감지에서 잔차 학습은 이미지 내 객체의 위치와 종류를 정확하게 예측하는 데 기여하며, 분할에서는 픽셀 단위의 정확한 분류 성능을 높이는 데 활용될 수 있습니다.

5. 논문에서 제기된 매우 깊은 네트워크(예: 1202-layer ResNet)의 학습 및 일반화 성능에 대한 논의를 바탕으로, 심층 학습 모델의 깊이를 무한정 늘리는 것이 항상 바람직한지 여부에 대한 자신의 생각을 논리적으로 전개하고, 최적의 네트워크 깊이를 결정하는 데 고려해야 할 요인들을 설명하시오.
> 매우 깊은 네트워크가 이론적으로 더 복잡한 함수를 학습할 수 있지만, 실제로는 기울기 소실/폭발 문제, 과적합, 그리고 증가하는 계산 비용 때문에 깊이를 무한정 늘리는 것이 항상 바람직한 것은 아닙니다. 최적의 네트워크 깊이를 결정하기 위해서는 데이터의 복잡성, 사용 가능한 컴퓨팅 자원, 그리고 모델의 일반화 성능을 종합적으로 고려해야 하며, 깊이가 증가함에 따른 성능 향상 폭이 줄어들거나 오히려 감소하는 지점을 파악하는 것이 중요합니다.

### 6.3 용어 해설
1. 잔차 학습 (Residual Learning): 
   - 심층 신경망의 각 계층이 목표 함수 자체를 학습하는 대신, 입력에 대한 잔차 함수(목표 함수 - 입력)를 학습하도록 재구성하는 학습 프레임워크.

2. 퇴화 문제 (Degradation Problem): 
   - 심층 신경망에서 네트워크 깊이가 증가함에 따라 정확도가 포화되고 오히려 감소하는 현상으로, 더 깊은 모델이 얕은 모델보다 더 높은 학습 오류를 보이는 특징이 있음.

3. 잔차 블록 (Residual Block): 
   - 잔차 학습의 기본 building block으로, 몇 개의 쌓인 계층과 입력이 출력에 더해지는 "숏컷 연결"로 구성됨.

4. 숏컷 연결 (Shortcut Connection): 
   - 신경망의 하나 이상의 계층을 건너뛰어 이전 계층의 출력을 현재 계층의 출력에 직접 더하는 연결. 잔차 블록에서 항등 매핑 또는 투영 매핑을 수행하는 데 사용됨.

5. 항등 매핑 (Identity Mapping): 
   - 입력과 출력이 동일한 매핑. 잔차 블록에서 숏컷 연결이 입력값을 그대로 다음 계층에 전달하는 역할을 함.

6. 투영 숏컷 (Projection Shortcut): 
   - 입력과 출력의 차원이 다를 때 숏컷 연결에서 차원을 맞추기 위해 사용되는 선형 변환 (예: 1x1 컨볼루션).

7. 평범한 네트워크 (Plain Network): 
   - 숏컷 연결 없이 계층을 순차적으로 쌓아 올린 기본적인 심층 신경망 아키텍처.

8. 병목 구조 (Bottleneck Architecture): 
   - 잔차 블록 내에서 계산 효율성을 높이기 위해 중간 계층의 차원을 줄였다가 다시 늘리는 구조. 주로 매우 깊은 ResNet 아키텍처에서 사용됨.

9. ImageNet: 
   - 대규모 이미지 인식 데이터셋. 1000개의 클래스와 약 120만 개의 학습 이미지, 5만 개의 검증 이미지, 10만 개의 테스트 이미지로 구성됨.

10. CIFAR-10: 
   - 10개의 클래스와 각 클래스당 6000개의 32x32 컬러 이미지로 구성된 이미지 인식 데이터셋 (5만 개의 학습 이미지, 1만 개의 테스트 이미지).

11. ILSVRC (ImageNet Large Scale Visual Recognition Challenge): 
   - ImageNet 데이터셋을 기반으로 매년 개최되는 대규모 시각 인식 대회.

12. COCO (Microsoft Common Objects in Context): 
   - 객체 감지, 분할, 캡션 등을 위한 대규모 데이터셋.

13. PASCAL VOC (Visual Object Classes): 
   - 객체 감지 및 분할을 위한 데이터셋 및 챌린지.


## 7. **Attention Is All You Need (2017, Vaswani et al.)**  
[Attention is all you need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) 

### **7.1 퀴즈**

1. 본 논문에서 제안하는 새로운 네트워크 구조의 이름은 무엇이며, 기존의 순환 신경망(RNN) 또는 합성곱 신경망(CNN) 기반 모델과 비교했을 때 가장 큰 차이점은 무엇입니까? 
> 제안하는 네트워크 구조는 Transformer이며, 기존 모델과 달리 순환 신경망(RNN)이나 합성곱 신경망(CNN)을 전혀 사용하지 않고 오직 attention 메커니즘만을 기반으로 합니다. 이는 병렬 처리를 크게 향상시키고 학습 시간을 단축시키는 주요 이점입니다.

2. Transformer 모델의 인코더와 디코더는 각각 어떤 주요 구성 요소들로 이루어져 있으며, 각 구성 요소는 어떤 역할을 수행합니까? 
> 인코더는 N개의 동일한 레이어 스택으로 구성되며, 각 레이어는 Multi-Head Self-Attention과 Position-wise Fully Connected Feed-Forward Network라는 두 개의 서브 레이어를 가집니다. 디코더 역시 N개의 동일한 레이어 스택으로 구성되지만, 인코더의 출력에 대한 Multi-Head Attention 서브 레이어가 추가됩니다.

3. 본 논문에서 제시하는 Scaled Dot-Product Attention의 핵심 연산 과정은 무엇이며, 기존의 Dot-Product Attention과 비교했을 때 어떤 점이 추가되었고 그 이유는 무엇입니까? 
> Scaled Dot-Product Attention은 query와 key의 dot product를 계산한 후 $$\sqrt{d_k}$$로 스케일링하고 softmax 함수를 적용하여 value에 대한 가중치를 얻어 출력합니다. 기존의 Dot-Product Attention에 스케일링을 추가한 이유는 $$d_k$$가 커질수록 dot product 값이 커져 softmax 함수의 기울기가 매우 작아지는 것을 방지하기 위함입니다.

4. Multi-Head Attention 메커니즘은 Scaled Dot-Product Attention을 어떻게 활용하며, 이 메커니즘을 사용하는 주된 이점은 무엇입니까? 
> Multi-Head Attention은 query, key, value를 서로 다른 선형 투영(linear projection)을 통해 h번 투영한 후, 각 투영된 버전에 대해 병렬로 Scaled Dot-Product Attention을 수행합니다. 이를 통해 모델은 서로 다른 표현 하위 공간의 정보를 다양한 위치에서 공동으로 주목할 수 있게 됩니다.

5. Transformer 모델의 인코더와 디코더에서 사용되는 세 가지 다른 종류의 Multi-Head Attention은 각각 어떤 입력으로부터 query, key, value를 얻어오며, 그 목적은 무엇입니까? 
> "encoder-decoder attention"에서는 디코더의 이전 레이어 출력이 query가 되고, 인코더의 출력이 key와 value가 됩니다. "encoder self-attention"에서는 인코더의 이전 레이어 출력이 query, key, value 모두가 됩니다. "decoder self-attention"에서도 마찬가지로 디코더의 이전 레이어 출력이 query, key, value이지만, 미래 위치에 대한 attention을 막는 masking이 적용됩니다.

6. Position-wise Feed-Forward Network는 Transformer 모델의 각 레이어에서 어떤 역할을 수행하며, 그 구조적인 특징은 무엇입니까? 
> Position-wise Feed-Forward Network는 인코더와 디코더의 각 레이어에서 attention 서브 레이어 이후에 적용되는 fully connected 네트워크입니다. 이는 각 위치별로 독립적으로 적용되는 두 개의 선형 변환과 그 사이의 ReLU 활성화 함수로 구성됩니다.

7. Transformer 모델은 순환(recurrence)이나 컨볼루션(convolution)을 사용하지 않기 때문에 시퀀스의 순서 정보를 활용하기 위해 어떤 방법을 사용합니까? 그 방법의 핵심 아이디어는 무엇입니까? 
> Transformer 모델은 Positional Encoding이라는 기법을 사용하여 시퀀스의 순서 정보를 모델에 주입합니다. 이는 시퀀스 내 각 토큰의 절대적 또는 상대적 위치에 대한 정보를 담고 있는 벡터를 입력 임베딩에 더하는 방식으로 구현됩니다.

8. 본 논문에서 저자들이 Self-Attention 메커니즘을 기존의 RNN이나 CNN 기반 레이어 대신 사용한 세 가지 주요 동기는 무엇입니까? 
> 첫째, 레이어당 계산 복잡도가 RNN보다 낮고, 특히 문장 표현의 차원(d)이 시퀀스 길이(n)보다 클 때 더 효율적입니다. 둘째, RNN과 달리 시퀀스 길이에 의존적인 순차적 연산이 없어 병렬 처리가 용이합니다. 셋째, 네트워크 내의 장거리 의존성 사이의 경로 길이가 상수 시간으로 짧아 장거리 의존성을 학습하기 더 쉽습니다.

9. 본 논문에서 제시된 실험 결과 중, 영어-독일어 번역(WMT 2014 English-to-German)과 영어-프랑스어 번역(WMT 2014 English-to-French) 작업에서 Transformer 모델이 달성한 주요 성과는 무엇이며, 학습 비용 측면에서 어떤 장점을 보였습니까? 
> 영어-독일어 번역에서 Transformer (big) 모델은 이전 최고 성능 모델(앙상블 포함)보다 2 BLEU 이상 높은 28.4 BLEU를 달성했으며, 영어-프랑스어 번역에서는 이전 최고 단일 모델보다 높은 41.0 BLEU를 기록했습니다. 또한, 이러한 성능 향상을 훨씬 적은 학습 비용으로 달성했습니다.

10. 학습 과정에서 사용된 Adam optimizer의 파라미터 값과 learning rate schedule의 특징, 그리고 적용된 세 가지 regularization 기법은 각각 무엇이며, 모델 성능 향상에 어떻게 기여합니까? 
> Adam optimizer는 $$\beta_1 = 0.9$$, $$\beta_2 = 0.98$$, $$\epsilon = 10^{-9}$$ 값을 사용했습니다. Learning rate는 처음 warmup_steps 동안 선형적으로 증가하다가 이후 step 수의 역제곱근에 비례하여 감소하는 schedule을 따릅니다. Regularization으로는 Residual Dropout, Embedding 및 Positional Encoding 합에 대한 Dropout, Label Smoothing (εls = 0.1)이 사용되어 모델의 과적합을 방지하고 일반화 성능을 향상시킵니다.

### **7.2 논술형 질문** 

1. Transformer 모델이 기존의 순환 신경망(RNN) 기반 sequence transduction 모델을 능가하는 주요 요인은 무엇이라고 생각하십니까? 병렬 처리 능력, 장거리 의존성 학습 능력, 모델 해석력 등의 측면에서 논의해 보십시오.
> Transformer 모델이 기존 RNN 기반 모델을 능가하는 주요 요인은 병렬 처리 능력과 장거리 의존성 학습 능력의 탁월함 때문입니다. RNN은 순차적인 정보 처리에 의존하여 긴 문맥에서 정보 손실이 발생하는 반면, Transformer는 Self-Attention 메커니즘을 통해 문장 내 모든 단어 간의 관계를 병렬적으로 파악하여 장거리 의존성을 효과적으로 학습합니다. 또한, Transformer 모델은 각 어텐션 헤드가 문장의 특정 측면에 집중하는 경향을 보여 모델 해석력 향상에도 기여합니다.

2. Attention 메커니즘은 Transformer 모델의 핵심 구성 요소입니다. Scaled Dot-Product Attention과 Multi-Head Attention이 Transformer의 성능에 어떻게 기여하는지 구체적인 작동 방식을 중심으로 설명하고, 각각의 중요성을 논하십시오.
> Scaled Dot-Product Attention은 Transformer 모델에서 Query, Key, Value 벡터 간의 유사도를 계산하여 각 단어의 중요도를 결정하는 핵심 메커니즘입니다. Query와 Key 벡터의 내적 값을 Key 벡터 차원의 제곱근으로 스케일링한 후 Softmax 함수를 적용하여 Attention 가중치를 얻고, 이를 Value 벡터에 곱하여 최종 출력을 생성합니다. Multi-Head Attention은 이러한 Scaled Dot-Product Attention을 병렬로 여러 번 수행하여 다양한 관점에서 입력 시퀀스 내의 관계를 포착하고, 최종적으로 이 결과를 연결(Concatenate)하여 모델의 표현력을 극대화합니다. 각각의 Attention Head는 서로 다른 특징에 집중할 수 있게 되어 모델이 더욱 풍부한 문맥 정보를 학습하는 데 중요한 역할을 합니다.

3. Transformer 모델에서 Positional Encoding의 역할과 중요성에 대해 설명하고, 본 논문에서 사용된 sinusoidal positional encoding 방식의 장점과 한계점에 대해 논하십시오. 다른 positional encoding 방식이 존재한다면 간략히 소개하고 비교해 보십시오.
> Transformer 모델에서 Positional Encoding은 순차적인 정보를 처리하지 않는 Self-Attention 메커니즘의 한계를 극복하고 입력 토큰의 위치 정보를 모델에 제공하는 중요한 역할을 합니다. 본 논문에서 사용된 sinusoidal positional encoding 방식은 명시적인 위치 정보를 제공하며, 긴 시퀀스에도 효과적으로 일반화될 수 있다는 장점이 있지만, 토큰 간의 상대적인 위치 관계를 직접적으로 모델링하지 못한다는 한계점을 가집니다. 다른 positional encoding 방식으로는 학습 가능한 Positional Embedding, 상대적 위치 정보를 활용하는 Relative Positional Encoding 등이 있으며, 각 방식은 고유한 장단점을 지닙니다.

4. 본 논문에서는 Transformer 모델이 기계 번역 task에서 뛰어난 성능을 보이는 것을 입증했습니다. Transformer의 아키텍처적 특징들이 기계 번역이라는 task의 특성과 어떻게 부합하며, 다른 자연어 처리 task (예: 텍스트 요약, 질의 응답)에 Transformer 모델을 적용했을 때 예상되는 장점과 어려움은 무엇일지 논하십시오.
> Transformer 모델의 self-attention 메커니즘은 문장 내 단어 간의 장거리 의존성을 효과적으로 포착하여 문맥 파악이 중요한 기계 번역 task에서 뛰어난 성능을 발휘합니다. 이러한 능력은 텍스트 요약 task에서도 핵심 정보 추출 및 응집성 있는 요약문 생성에 유리하며, 질의 응답 task에서는 질문과 문서 간의 관련성 높은 정보를 정확히 찾아내는 데 도움이 될 것으로 예상됩니다. 하지만 긴 텍스트를 처리해야 하는 task에서는 self-attention의 계산 복잡도가 증가하는 어려움이 있으며, 특정 task의 특성에 맞는 모델 구조 최적화 및 학습 전략이 필요할 수 있습니다.

5. Transformer 모델 이후, attention 메커니즘을 기반으로 하는 다양한 모델들이 등장했습니다 (예: BERT, GPT). 본 논문의 Transformer 모델이 이러한 후속 연구들에 어떤 영향을 미쳤는지 분석하고, Transformer 모델의 한계를 극복하거나 확장하기 위해 어떤 아이디어들이 제시되었는지 논하십시오.
> Transformer 모델은 attention 메커니즘을 도입하여 순차적인 정보 처리에 의존하던 기존 순환 신경망(RNN) 기반 모델들의 한계를 극복하고, 병렬 처리를 가능하게 하여 자연어 처리 분야에 혁신적인 발전을 가져왔습니다. BERT는 Transformer의 인코더 구조를 활용하여 양방향 문맥 이해를 강화했으며, GPT는 디코더 구조를 기반으로 텍스트 생성 능력을 극대화하는 등 다양한 후속 연구들이 Transformer의 기본 구조를 확장하거나 특정 작업에 맞게 변형하는 방식으로 진행되었습니다. 또한, Transformer 모델의 긴 문맥 처리 능력 제한, 계산 비용 문제 등을 해결하기 위해 희소 어텐션(sparse attention), 모델 압축, 효율적인 학습 방법 등 다양한 아이디어들이 제시되고 연구되고 있습니다.

### **7.3 용어 해설**
1. Sequence Transduction: 
   - 하나의 시퀀스를 입력으로 받아 다른 시퀀스를 출력하는 task (예: 기계 번역, 텍스트 요약).

2. Recurrent Neural Network (RNN): 
   - 순환적인 연결을 통해 시퀀스 데이터를 처리하는 신경망 구조. 이전 시점의 출력이 현재 시점의 입력에 영향을 미치는 방식으로 작동하여 시퀀스의 순서 정보를 학습할 수 있다.

3. Convolutional Neural Network (CNN): 
   - Convolution 연산을 사용하여 공간적 특징을 추출하는 신경망 구조. 주로 이미지 처리 task에 사용되지만, 텍스트와 같은 시퀀스 데이터 처리에도 활용될 수 있다.

4. Attention Mechanism: 
   - 모델이 입력 시퀀스의 특정 부분에 집중할 수 있도록 하는 메커니즘. query, key, value라는 세 가지 요소를 사용하여 입력의 중요도를 가중치로 나타낸다.

5. Self-Attention (Intra-Attention): 
   - 하나의 시퀀스 내에서 각 위치가 다른 모든 위치에 대해 attention을 수행하여 시퀀스 내의 관계를 모델링하는 메커니즘.

6. Encoder-Decoder Architecture: 
   - sequence transduction 모델의 기본적인 구조. 인코더는 입력 시퀀스를 고정된 크기의 벡터 표현으로 압축하고, 디코더는 이 벡터 표현을 기반으로 출력 시퀀스를 생성한다.

7. Multi-Head Attention: 
   - 여러 개의 독립적인 attention 함수를 병렬로 수행한 후 그 결과를 연결(concatenate)하는 attention 메커니즘. 모델이 다양한 측면에서 정보에 주목할 수 있도록 한다.

8. Scaled Dot-Product Attention: 
   - attention 가중치를 계산하기 위해 query와 key의 dot product를 사용하고, 그 결과를 key 벡터 차원의 제곱근으로 스케일링하는 attention 함수.

9. Residual Connection: 
   - 신경망의 이전 레이어의 출력을 현재 레이어의 출력에 더하는 방식. 기울기 소실 문제를 완화하고 깊은 네트워크의 학습을 돕는다.

10. Layer Normalization: 
   - 각 레이어의 활성화 값을 정규화하는 기법. 학습 속도를 높이고 모델의 안정성을 향상시킨다.

11. Position-wise Feed-Forward Network: 
   - 시퀀스의 각 위치에 대해 독립적으로 적용되는 fully connected 신경망. 일반적으로 두 개의 선형 변환과 그 사이의 비선형 활성화 함수(ReLU)로 구성된다.

12. Positional Encoding: 
   - 시퀀스 내 토큰의 위치 정보를 모델에 제공하기 위한 벡터 표현. Transformer 모델은 순환 또는 컨볼루션 연산을 사용하지 않기 때문에 명시적으로 위치 정보를 주입해야 한다.

13. BLEU (Bilingual Evaluation Understudy): 
   - 기계 번역 시스템의 성능을 평가하는 지표 중 하나. 모델이 생성한 번역과 사람의 번역을 비교하여 유사성을 측정한다.

14. Word-piece: 
   - 단어 분할(tokenization) 기법 중 하나. 자주 등장하는 단어는 그대로 유지하고, 희귀하거나 처음 보는 단어는 더 작은 의미 단위(subword)로 분할한다.

15. Byte-Pair Encoding (BPE): 
   - 데이터 압축 알고리즘에서 유래한 subword 분할 기법. 가장 자주 등장하는 연속적인 바이트 쌍을 하나의 새로운 토큰으로 병합하는 과정을 반복하여 어휘 집합을 구축한다.

16. Dropout: 
   - 학습 과정에서 신경망의 일부 뉴런을 무작위로 비활성화시키는 regularization 기법. 모델의 과적합을 방지하고 일반화 성능을 향상시킨다.

17. Label Smoothing: 
   - 학습 데이터의 정답 레이블을 약간의 오류 가능성을 포함하도록 부드럽게 만드는 regularization 기법. 모델이 특정 예측에 지나치게 확신하는 것을 방지한다.

18. Adam Optimizer: 
   - 경사하강법 기반의 최적화 알고리즘 중 하나. 각 파라미터에 대해 적응적으로 학습률을 조정하며, 모멘텀과 RMSProp의 장점을 결합하였다.

19. Warmup Steps: 
   - 학습 초기 단계에서 학습률을 점진적으로 증가시키는 스케줄링 기법. 모델의 초기 불안정성을 완화하는 데 도움을 줄 수 있다.

## 8. **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018, Devlin et al.)** 

BERT 연구 가이드

### 8.1 퀴즈
1. BERT 모델의 핵심 아이디어는 무엇이며, 기존 언어 모델과 어떻게 다른가?  
   > BERT는 딥 양방향 트랜스포머를 사전 학습하여 언어 이해 능력을 향상시키는 새로운 언어 표현 모델이다. 기존 모델과 달리 BERT는 모든 계층에서 좌우 문맥을 공동으로 조건화하여 깊은 양방향 표현을 학습하며, 이를 통해 다양한 NLP task에 task-specific한 구조 변경 없이 fine-tuning만으로 최고 성능을 달성할 수 있다.

2. BERT 사전 학습의 두 가지 주요 task는 무엇이며, 각각의 목적은 무엇인가?  
   > BERT의 사전 학습 task는 Masked Language Model (MLM)과 Next Sentence Prediction (NSP)이다. MLM은 입력 토큰의 일부를 가리고 주변 문맥을 이용하여 가려진 토큰을 예측하는 방식으로 양방향 문맥 이해 능력을 학습시키고, NSP는 주어진 두 문장이 이어지는 문장인지 아닌지를 예측하는 방식으로 문장 간 관계 이해 능력을 학습시킨다.

3. BERT 모델 아키텍처의 핵심 구성 요소는 무엇이며, 이 구성 요소의 주요 특징은 무엇인가?  
   > BERT 모델 아키텍처는 여러 개의 트랜스포머 인코더 계층으로 구성된 깊은 양방향 트랜스포머이다. 트랜스포머는 self-attention 메커니즘을 사용하여 입력 시퀀스 내의 각 토큰이 다른 모든 토큰과의 관계를 파악하도록 하며, 이를 통해 장거리 의존성을 효과적으로 모델링할 수 있다. BERT는 특히 양방향 self-attention을 사용하여 좌우 문맥 정보를 모두 활용한다.

4. BERT를 다양한 downstream task에 적용하는 두 가지 주요 전략은 무엇이며, 각각의 장단점은 무엇인가?  
   > BERT를 downstream task에 적용하는 전략은 feature-based approach와 fine-tuning approach이다. Feature-based approach는 사전 학습된 BERT 모델에서 고정된 feature를 추출하여 task-specific한 모델의 입력으로 사용하는 방식이며, 연산 비용이 저렴하다는 장점이 있지만 BERT 모델의 모든 능력을 활용하지 못할 수 있다. Fine-tuning approach는 사전 학습된 BERT 모델의 모든 파라미터를 downstream task의 데이터로 업데이트하는 방식이며, 높은 성능을 달성할 수 있지만 task-specific한 데이터가 충분해야 효과적이다.

5. BERT 논문에서 state-of-the-art 결과를 달성한 주요 NLP task는 무엇이며, 성능 향상 정도는 어떠한가?  
   > BERT는 GLUE, MultiNLI, SQuAD v1.1, SQuAD v2.0 등 11개의 NLP task에서 새로운 최고 성능을 달성했다. 특히 GLUE benchmark에서 이전 최고 기록 대비 7.7% 향상된 80.5%의 점수를 기록했으며, MultiNLI 정확도는 4.6%, SQuAD v1.1 F1 점수는 1.5%, SQuAD v2.0 F1 점수는 5.1% 향상시키는 등 상당한 성능 향상을 보였다.

6. BERT의 bidirectional pre-training이 unidirectional language model에 비해 갖는 주요 이점은 무엇인가?  
   > BERT의 양방향 사전 학습은 각 토큰의 표현을 학습할 때 좌우 모든 문맥 정보를 활용할 수 있게 한다. 이는 단방향 언어 모델이 이전 또는 이후 토큰 정보만을 사용할 수 있는 것에 비해 문맥 이해 능력을 훨씬 풍부하게 만들어 sentence-level task뿐만 아니라 token-level task에서도 뛰어난 성능을 발휘할 수 있도록 한다. 특히 질의 응답과 같이 양방향 문맥 정보가 중요한 task에서 큰 이점을 보인다.

7. BERT 논문에서 수행된 ablation study 중 Next Sentence Prediction (NSP) task 제거가 성능에 미치는 영향은 무엇인가?  
   > BERT 논문에서 NSP task를 제거한 ablation study 결과, QNLI, MNLI, SQuAD 1.1 task에서 상당한 성능 하락이 관찰되었다. 이는 문장 간의 관계를 이해하는 능력을 사전 학습하는 것이 질의 응답 및 자연어 추론과 같은 downstream task의 성능 향상에 중요함을 시사한다.

8. BERT 모델 크기(layers, hidden size, attention heads)가 downstream task 성능에 미치는 영향은 무엇인가?  
   > BERT 논문에서 모델 크기에 따른 성능 변화를 분석한 결과, 모델 크기가 클수록(layers 수, hidden size, attention heads 증가) GLUE benchmark의 다양한 task에서 지속적으로 성능이 향상되는 것을 확인했다. 이는 충분한 양의 데이터로 사전 학습된 대규모 모델이 downstream task의 데이터 양이 적더라도 더 강력하고 표현력이 풍부한 representation을 제공할 수 있음을 보여준다.

9. BERT의 feature-based approach를 CoNLL-2003 Named Entity Recognition (NER) task에 적용한 결과는 fine-tuning approach와 비교하여 어떠한가?  
   > BERT의 feature-based approach를 NER task에 적용한 결과, 사전 학습된 트랜스포머의 마지막 4개 계층의 토큰 representation을 연결하여 BiLSTM의 입력으로 사용했을 때 fine-tuning approach와 불과 0.3 F1 점수 차이로 매우 경쟁력 있는 성능을 보였다. 이는 BERT가 fine-tuning 방식뿐만 아니라 feature extraction 방식으로도 효과적임을 입증한다.

10. BERT 사전 학습 시 Masked Language Model (MLM) objective에서 masking 전략이 성능에 미치는 영향은 무엇인가?  
   > BERT 논문에서는 MLM 사전 학습 시 [MASK] 토큰으로만 masking하거나, 랜덤 토큰으로 대체하거나, 원래 토큰을 그대로 유지하는 혼합 전략을 사용한다. Ablation study 결과, fine-tuning은 다양한 masking 전략에 비교적 robust했지만, feature-based approach에서는 [MASK] 토큰만 사용하는 전략이 문제가 되었으며, 제안된 혼합 전략이 가장 좋은 성능을 보였다. 이는 사전 학습과 fine-tuning 간의 불일치를 줄이는 것이 중요함을 시사한다.

### 8.2 에세이 형식 질문
1. BERT가 자연어 처리 분야에 미친 영향에 대해 논하고, BERT 이전의 주요 연구들과 비교하여 BERT의 혁신적인 측면을 상세히 설명하시오.
> BERT는 양방향 트랜스포머 인코더를 기반으로 사전 학습된 언어 모델로, NLP 분야에 큰 영향을 미쳤습니다. BERT 이전에는 단방향 언어 모델이나 단어 임베딩 기반 연구가 주를 이루었으나, BERT는 마스크 언어 모델링(MLM)과 다음 문장 예측(NSP) 등의 혁신적인 사전 학습 방식을 통해 문맥을 깊이 있게 이해하는 능력을 획기적으로 향상시켰습니다. 이러한 혁신 덕분에 BERT는 다양한 NLP task에서 최고 성능을 달성하며 이후 연구의 중요한 기반이 되었습니다.

2. BERT 사전 학습 방법(MLM, NSP)의 각 목적과 효과를 분석하고, 이 두 가지 task가 BERT 모델의 일반적인 언어 이해 능력 향상에 어떻게 기여하는지 구체적인 예시와 함께 논하시오.
>BERT의 MLM(Masked Language Model)은 문장 내 일부 단어를 가리고(masking), 주변 단어들의 문맥을 이용하여 가려진 단어를 예측하는 방식으로 모델이 양방향 문맥 이해 능력을 학습하도록 돕습니다. 예를 들어 "나는 [MASK] 먹었다"라는 문장에서 "[MASK]" 부분을 예측하는 과정을 통해 모델은 문장 전체의 의미를 파악하고 단어 간의 관계를 학습합니다.
>
>NSP(Next Sentence Prediction)는 두 개의 문장이 주어졌을 때, 두 번째 문장이 첫 번째 문장의 다음 문장인지 여부를 예측하는 task입니다. 예를 들어 "[CLS] 오늘 날씨가 좋다. [SEP] 그래서 산책을 나갔다. [SEP]"와 같은 입력에서 모델은 두 문장이 의미적으로 연결되어 있다는 것을 학습합니다. 이러한 MLM과 NSP task를 통해 BERT는 단어와 문장 수준에서의 풍부한 의미 정보를 포착하고, 일반적인 언어 이해 능력을 크게 향상시킵니다.

3. BERT를 다양한 downstream task에 fine-tuning하거나 feature-based 방식으로 적용할 때 고려해야 할 사항들을 논하고, 각 접근 방식의 장단점을 구체적인 NLP task를 예시로 들어 설명하시오.
> BERT를 다양한 downstream task에 적용할 때, fine-tuning 방식은 task별 데이터에 맞춰 모델 전체를 업데이트하여 높은 성능을 달성할 수 있지만, 많은 양의 labeled 데이터와 긴 학습 시간이 요구될 수 있습니다. 반면, feature-based 방식은 사전 학습된 BERT의 representation을 고정하고 추가적인 얕은 레이어를 학습시키므로 데이터가 적거나 학습 속도가 중요한 경우에 유리하지만, BERT가 학습한 풍부한 정보를 task에 맞게 최적화하는 데 한계가 있을 수 있습니다. 예를 들어, 감성 분석 task에서는 fine-tuning이 문맥에 따른 미묘한 감정을 더 잘 포착하여 높은 정확도를 보일 수 있지만, 텍스트 분류 task에서 데이터가 부족한 경우에는 feature-based 방식이 과적합을 방지하며 안정적인 성능을 제공할 수 있습니다.

4. BERT 모델의 크기(depth, width, attention heads)가 성능에 미치는 영향을 분석하고, 대규모 모델 학습의 장점과 잠재적인 문제점들을 논하시오. 또한, 모델 크기 외에 BERT 성능에 영향을 미치는 다른 요인들을 함께 고려하여 설명하시오.
> BERT 모델의 크기가 커질수록(depth, width, attention heads 증가) 더 복잡한 패턴을 학습하고 문맥을 깊이 이해하여 전반적인 성능 향상을 가져올 수 있습니다. 하지만 모델 크기 증가는 학습에 필요한 데이터 양과 컴퓨팅 자원을 크게 증가시키고, 과적합의 위험 또한 높입니다. 모델 크기 외에도 pre-training 데이터의 품질과 양, fine-tuning 데이터셋의 적절성, 학습 hyperparameter 설정 등이 BERT 성능에 중요한 영향을 미칩니다.

5. BERT 이후 등장한 다양한 Transformer 기반 언어 모델들의 발전 방향을 제시하고, BERT의 한계점을 극복하거나 새로운 기능을 추가한 모델들의 주요 특징과 응용 분야를 설명하시오.
> BERT 이후 등장한 Transformer 기반 언어 모델들은 모델 크기 확장, 학습 데이터 증대, 새로운 사전 학습 방식 도입 등 다양한 방향으로 발전했습니다. BERT의 단방향 학습 방식의 한계를 극복하고 양방향 컨텍스트 이해도를 높인 RoBERTa, XLNet 등이 등장했으며, GPT 계열은 텍스트 생성 능력을 극대화하여 챗봇, 콘텐츠 생성 등 새로운 응용 분야를 개척했습니다. 또한, Transformer 구조를 변형하여 효율성을 높이거나 특정 작업에 특화된 모델(예: 시각-언어 모델, 멀티모달 모델)들도 활발히 연구되고 있습니다.

### 8.3 용어 해설

1. BERT (Bidirectional Encoder Representations from Transformers): 
   - 구글에서 개발한 새로운 언어 표현 모델로, 트랜스포머 아키텍처를 기반으로 깊은 양방향 표현을 사전 학습하여 다양한 자연어 처리 task에서 최고 성능을 달성했다.

2. Pre-training (사전 학습): 
   - 레이블링되지 않은 대규모 텍스트 데이터를 사용하여 모델의 초기 가중치를 학습시키는 과정. 이를 통해 모델은 일반적인 언어적 특징과 패턴을 이해할 수 있게 된다.

3. Fine-tuning (미세 조정): 
   - 사전 학습된 모델의 가중치를 특정 downstream task의 레이블링된 데이터를 사용하여 업데이트하는 과정. 사전 학습된 지식을 활용하여 소량의 데이터로도 높은 성능을 낼 수 있다.

4. Bidirectional (양방향): 
   - 텍스트 시퀀스 내의 특정 토큰을 이해하기 위해 해당 토큰의 왼쪽과 오른쪽 문맥을 모두 고려하는 방식.

5. Transformer: 
   - self-attention 메커니즘을 핵심 구성 요소로 사용하는 신경망 아키텍처로, 병렬 처리가 가능하고 장거리 의존성을 효과적으로 모델링할 수 있다.

6. Self-attention: 
   - 시퀀스 내의 각 위치에서 다른 모든 위치와의 관계를 계산하여 각 위치의 표현을 업데이트하는 메커니즘.

7. Masked Language Model (MLM): 
   - BERT 사전 학습 task 중 하나로, 입력 토큰의 일부를 [MASK] 토큰으로 가리고 주변 문맥을 이용하여 원래 토큰을 예측하는 방식.

8. Next Sentence Prediction (NSP): 
   - BERT 사전 학습 task 중 하나로, 주어진 두 문장이 이어지는 문장인지 아닌지를 예측하는 이진 분류 task. 문장 간 관계 이해 능력을 학습시킨다.

9. Downstream task: 
   - 사전 학습된 모델을 평가하거나 실제 문제를 해결하기 위해 적용하는 특정 자연어 처리 task (예: 질의 응답, 텍스트 분류, 개체명 인식 등).

10. Feature-based approach (특징 기반 접근 방식): 
   - 사전 학습된 모델에서 추출한 고정된 특징 벡터를 downstream task를 위한 별도의 모델의 입력으로 사용하는 방식.

11. GLUE (General Language Understanding Evaluation): 
   - 다양한 자연어 이해 task의 성능을 평가하기 위한 벤치마크 데이터셋 모음.

12. SQuAD (Stanford Question Answering Dataset): 
   - 주어진 지문과 질문에 대해 지문 내에서 정답을 찾는 질의 응답 데이터셋.

13. WordPiece embedding: 
   - 단어를 더 작은 의미 단위(subword)로 분할하여 각 단위에 대한 임베딩을 학습하는 방식. 어휘 외(out-of-vocabulary) 문제에 강건하다.

14. Token embedding: 
   - 텍스트의 각 토큰(단어 또는 subword)을 벡터 공간에 매핑하는 표현.

15. Segment embedding: 
   - 두 개 이상의 문장으로 구성된 입력 시퀀스에서 각 토큰이 어떤 문장에 속하는지 나타내는 임베딩.

16. Position embedding: 
   - 시퀀스 내 각 토큰의 위치 정보를 나타내는 임베딩. 트랜스포머는 순환 신경망과 달리 위치 정보를 명시적으로 제공해야 한다.

17. State-of-the-art (SOTA): 
   - 특정 task에서 현재까지 가장 높은 성능을 보이는 모델 또는 방법.

18. Ablation study (제거 연구): 
   - 모델의 특정 구성 요소나 학습 방법을 제거하거나 변경했을 때 성능 변화를 분석하여 해당 요소의 중요성을 평가하는 실험.

