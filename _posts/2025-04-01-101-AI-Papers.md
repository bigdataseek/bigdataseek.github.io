---
title: 26차시 1:AI Research Papers
layout: single
classes: wide
categories:
  - AI Research Papers
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 1. 처음 논문(Research Papers)을 읽는 사람을 위해: 효과적인 논문 읽기 방법
논문을 처음 읽는 분들을 위한 방법을 알려드리겠습니다. 학술 논문은 처음에는 어렵게 느껴질 수 있지만, 다음 접근법을 통해 더 수월하게 읽을 수 있습니다:

1. **구조적 접근하기**
   - 제목, 초록(Abstract), 결론부터 읽어 전체 내용을 파악하세요
   - 그 다음 서론(Introduction)과 결론(Conclusion)을 읽으면 주요 내용을 이해할 수 있습니다
   - 마지막으로 본문 내용을 읽으면 상세 내용을 이해하기 쉬워집니다

2. **시각적 자료 활용하기**
   - 논문의 그림, 표, 다이어그램을 먼저 살펴보세요
   - 시각 자료는 복잡한 개념을 직관적으로 이해하는데 도움이 됩니다

3. **용어 사전 만들기**
   - 모르는 용어가 나올 때마다 따로 정리하고 찾아보세요
   - 시간이 지나면 자연스럽게 해당 분야의 용어집이 완성됩니다

4. **논문 요약본 활용하기**
   - 많은 논문들이 블로그나 유튜브에서 요약 설명되어 있습니다
   - Papers With Code, Two Minute Papers 같은 사이트/채널을 활용해보세요

5. **스터디 그룹 참여하기**
   - 같은 분야 관심 있는 사람들과 함께 논문을 읽고 토론하면 이해도가 높아집니다

6. **논문 읽기 도구 활용하기**
   - Connected Papers: 관련 논문들을 시각적으로 보여줍니다
     - [Attention is all you need 논문 관련 시각화 그래프](
   https://www.connectedpapers.com/main/204e3073870fae3d05bcbc2f6a8e263d9b72e776/graph?utm_source=share_popup&utm_medium=copy_link&utm_campaign=share_graph) 

   - Semantic Scholar: AI 기반으로 논문 검색과 이해를 도와줍니다
      - [Attention is all you need 논문 관련 내용](https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776#related-papers)

   - Elicit.org: 논문의 주요 내용을 요약해주는 AI 도구입니다
      - [Attention is all you need 논문 관련 요약](https://elicit.com/notebook/73d15714-6b1f-4dd5-979d-a06dd2b158dd)

7. **실습과 병행하기**
   - 특히 AI 논문의 경우, 코드 구현을 함께 보면 이해가 쉬워집니다
   - Papers With Code 사이트에서는 많은 AI 논문의 구현 코드를 제공합니다
      - [논문 내용을 코드로 구현한 곳](https://paperswithcode.com/paper/attention-is-all-you-need)

무엇보다 처음부터 모든 내용을 100% 이해하려고 하기보다는 핵심 아이디어를 파악하는 데 집중하고, 반복해서 읽다 보면 점차 이해도가 높아질 것입니다. 

## 2. 읽을 만한 논문
### 2.1 딥러닝의 기초 및 역사적 논문
- **"Backpropagation" (1986, Rumelhart et al.)**  
  - 신경망 학습의 핵심 알고리즘인 역전파(backpropagation)를 소개한 논문.
- **"ImageNet Classification with Deep Convolutional Neural Networks" (2012, Krizhevsky et al.)**  
  - AlexNet을 제안하며 현대 딥러닝 시대를 연 논문.

### 2.2 CNN (Convolutional Neural Networks)
- **"Very Deep Convolutional Networks for Large-Scale Image Recognition" (2014, Simonyan & Zisserman)**  
  - VGGNet을 소개하며 깊은 네트워크가 성능을 향상시킬 수 있음을 보인 논문.
- **"Deep Residual Learning for Image Recognition" (2015, He et al.)**  
  - ResNet을 제안하며 딥러닝 모델이 깊어질 때 발생하는 문제를 해결한 논문.

### 2.3 NLP (자연어 처리)
- **"Attention Is All You Need" (2017, Vaswani et al.)**  
  - Transformer 구조를 제안하며 BERT, GPT 등 최신 NLP 모델의 기반을 마련한 논문.
- **"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (2018, Devlin et al.)**  
  - BERT 모델을 소개한 논문으로, NLP에서 사전 훈련(pre-training) 기법을 혁신함.

### 2.4 강화학습 (Reinforcement Learning)
- **"Playing Atari with Deep Reinforcement Learning" (2013, Mnih et al.)**  
  - DQN(Deep Q-Network)을 소개하며 딥러닝을 활용한 강화학습의 기초를 확립한 논문.
- **"Mastering the game of Go with deep neural networks and tree search" (2016, Silver et al.)**  
  - AlphaGo 논문, 딥러닝과 강화학습을 활용한 바둑 AI의 발전을 보여줌.

### 2.5 생성 모델 (Generative Models)
- **"Generative Adversarial Networks" (2014, Goodfellow et al.)**  
  - GAN(생성적 적대 신경망)을 소개한 논문.
- **"Auto-Encoding Variational Bayes" (2013, Kingma & Welling)**  
  - VAE(Variational Autoencoder)를 제안한 논문.

### 2.6 기타 중요 논문
- **"Neural Architecture Search with Reinforcement Learning" (2016, Zoph & Le)**  
  - NAS(Neural Architecture Search)를 제안한 논문, 자동으로 최적의 신경망 구조를 찾는 방법을 다룸.
- **"Diffusion Models Beat GANs on Image Synthesis" (2021, Dhariwal & Nichol)**  
   - 확산 모델(Diffusion Models)이 GAN보다 뛰어난 이미지 생성을 할 수 있음을 보인 논문.
- **Deep Learning" (LeCun, Bengio, Hinton, 2015)**
   - 딥러닝의 기본 개념을 포괄적으로 설명한 리뷰 논문입니다.

## 3. **Backpropagation (1986, Rumelhart et al.)**  

- [Backpropagation](https://www.nature.com/articles/323533a0)

### 3.1 퀴즈 (단답형)
1.역전파(back-propagation) 학습 절차의 주요 목표는 무엇이며, 이 절차는 신경망의 어떤 부분을 조정하여 목표를 달성합니까?
   - 역전파 학습 절차의 주요 목표는 신경망의 실제 출력 벡터와 원하는 출력 벡터 사이의 차이를 최소화하는 것입니다. 이를 위해 네트워크 연결의 가중치를 반복적으로 조정합니다.

2.은닉층(hidden layers)이 있는 신경망이 입력층과 출력층만 있는 신경망보다 더 강력한 이유는 무엇입니까?
   - 은닉층은 입력 또는 출력의 일부가 아니지만 작업 도메인의 중요한 특징을 나타내는 내부 표현을 학습할 수 있게 합니다. 이러한 새로운 특징을 생성하는 능력은 단순한 연결 방식으로는 포착할 수 없는 작업의 규칙성을 모델링할 수 있게 합니다.

3.경사 하강법(gradient descent)에서 학습률(learning rate, $$\epsilon$$)과 모멘텀(momentum, $$\alpha$$)은 가중치 업데이트에 어떤 영향을 미칩니까?
   - 학습률($$\epsilon$$)은 오차 기울기에 따라 가중치를 얼마나 크게 조정할지를 결정합니다. 모멘텀($$\alpha$$)은 이전 가중치 변화의 영향을 유지시켜 학습 속도를 높이고 지역 최소값에 갇히는 것을 방지하는 데 도움을 줍니다.

4.그림 1에서 제시된 신경망은 어떤 특정 작업을 학습했으며, 은닉 유닛의 가중치에서 어떤 특징이 나타납니까?
   - 그림 1의 신경망은 입력 벡터의 거울 대칭성을 감지하는 작업을 학습했습니다. 은닉 유닛의 가중치는 입력 벡터의 중간을 기준으로 대칭적인 위치의 가중치가 동일한 크기와 반대 부호를 갖는 특징을 보입니다.

5.가족 관계 네트워크(family tree network)는 어떤 종류의 정보를 학습했으며, 은닉층의 활성화 패턴은 무엇을 나타냅니까?
   - 가족 관계 네트워크는 (사람 1, 관계, 사람 2) 형태의 삼중 정보들을 학습했습니다. 은닉층의 활성화 패턴은 사람과 관계에 대한 분산된 표현을 인코딩하며, 네트워크가 추론하고 일반화할 수 있도록 합니다.

6.재귀 신경망(recurrent network)과 계층형 신경망(layered network) 사이의 유사점은 무엇이며, 재귀 신경망 학습 시 어떤 추가적인 고려 사항이 발생합니까?
   - 재귀 신경망의 각 시간 단계는 등가의 계층형 신경망의 한 계층에 해당합니다. 재귀 신경망을 학습할 때는 순방향 패스의 중간 계층 유닛의 출력 기록을 저장하고, 대응하는 가중치 세트 간의 기울기를 평균화하는 추가적인 고려 사항이 발생합니다.

7.시각 피질에서 단안 박탈(monocular deprivation)의 주요 효과는 무엇이며, 이는 행동적으로 어떻게 나타납니까?
   - 단안 박탈의 주요 효과는 박탈되지 않은 눈의 시각 자극에만 대부분의 피질 세포가 반응하게 되는 시각 피질의 우세 눈 편향(ocular dominance shift)입니다. 행동적으로는 박탈된 눈이 거의 쓸모없게 됩니다.

8.역전 차폐(reverse occlusion)는 단안 박탈의 효과를 어떻게 변화시킬 수 있으며, 그 효과의 한계는 무엇입니까?
   - 역전 차폐는 초기에 박탈된 눈을 사용하도록 강제하여 시각 회복을 촉진할 수 있습니다. 그러나 역전 차폐 자체만으로는 정상적인 피질 눈 우세 분포를 회복시키지 못하고, 한쪽 눈의 시각 회복만을 촉진하는 경향이 있습니다.

9.연구자들이 어린 고양이에게 짧은 기간의 역전 차폐 후 양안 시각(binocular vision)을 경험하게 했을 때 놀라운 결과는 무엇이었습니까?
   - 짧은 기간의 역전 차폐 후 양안 시각을 경험한 고양이들은 심각한 양안 약시(bilateral amblyopia)를 보였으며, 양쪽 눈의 시력이 정상 수준의 약 1/3에 불과했습니다. 놀랍게도 피질 눈 우세는 정상 고양이와 유사하게 나타났습니다.

10.이 두 논문에서 제시된 연구들은 학습과 발달이라는 넓은 주제에 대해 어떤 공통적인 통찰력을 제공합니까?
   - 두 논문 모두 외부 경험 또는 학습 알고리즘이 신경계의 구조와 기능적 속성을 형성하는 데 중요한 역할을 한다는 것을 보여줍니다. 역전파는 특정 작업을 수행하기 위한 내부 표현을 학습할 수 있음을 보여주는 반면, 시각 박탈 연구는 초기 경험이 시각 피질 발달과 시력에 결정적인 영향을 미친다는 것을 보여줍니다.

### 3.2 에세이 형식 질문 

1.역전파 알고리즘의 작동 방식과, 이 알고리즘이 어떻게 신경망이 복잡한 패턴 인식 작업을 수행할 수 있도록 하는 유용한 내부 표현을 학습하는지 자세히 설명하십시오.

2.그림 1과 3에 제시된 사례 연구를 바탕으로, 신경망이 특정 작업을 해결하기 위해 적절한 은닉층 표현을 개발하는 능력에 대해 논의하십시오. 이러한 내부 표현의 특징은 무엇이며, 어떻게 네트워크의 성능에 기여합니까?

3.단안 박탈 실험은 시각 시스템 발달의 중요한 시기(critical period)의 개념을 어떻게 강조합니까? 역전 차폐가 효과가 있을 수도 있고 한계가 있을 수도 있는 이유는 무엇이며, 이 연구는 양안 시각 발달에 대해 어떤 점을 시사합니까?

4."1986-rumelhart-2.pdf" 논문에서 제시된 신경망 학습 절차가 생물학적 학습의 정확한 모델이 아니라고 주장하는 이유는 무엇입니까? 그럼에도 불구하고 이 연구는 뇌 기능에 대한 이해에 어떤 중요한 기여를 합니까?

5.두 논문에 제시된 연구 결과를 종합하여, 초기 경험(시각적 또는 학습 데이터)이 신경계의 장기적인 기능적 결과에 미치는 영향에 대해 논의하십시오. 신경망 학습과 시각 발달 연구 사이의 잠재적인 연결 고리는 무엇입니까?

### 3.3 용어집

1. 역전파 (Back-propagation): 신경망의 출력층에서 계산된 오차를 네트워크의 이전 계층으로 다시 전파하여 각 연결의 가중치를 조정하는 지도 학습 알고리즘입니다.

2. 은닉층 (Hidden Layer): 입력층과 출력층 사이에 있는 신경망 계층으로, 입력 데이터를 변환하여 출력층이 원하는 결과를 생성하는 데 도움이 되는 중간 표현을 학습합니다.

3. 경사 하강법 (Gradient Descent): 함수의 최솟값을 찾기 위해 함수의 기울기(gradient)의 반대 방향으로 반복적으로 이동하는 최적화 알고리즘입니다. 신경망 학습에서는 오차 함수를 최소화하기 위해 가중치를 업데이트하는 데 사용됩니다.

4. 학습률 (Learning Rate): 경사 하강법에서 각 반복마다 가중치를 조정하는 단계의 크기를 결정하는 하이퍼파라미터입니다.

5. 모멘텀 (Momentum): 경사 하강법의 변형으로, 이전 가중치 업데이트의 일부를 유지하여 학습 속도를 높이고 지역 최소값에 갇히는 것을 방지하는 데 사용됩니다.

6. 단안 박탈 (Monocular Deprivation): 초기 발달 기간 동안 한쪽 눈의 시각적 입력을 막는 실험 절차입니다.

7. 역전 차폐 (Reverse Occlusion): 단안 박탈 후, 초기에는 열려 있던 눈을 가리고 이전에 가려졌던 눈을 열어 시각 입력을 제공하는 절차입니다.

8. 양안 약시 (Bilateral Amblyopia): 양쪽 눈의 시력이 모두 비정상적으로 낮은 상태로, 일반적으로 초기 시각 발달의 이상으로 인해 발생합니다.

9. 피질 눈 우세 (Cortical Ocular Dominance): 시각 피질 뉴런이 한쪽 눈의 자극에 더 강하게 반응하는 경향입니다. 단안 박탈과 같은 경험에 의해 영향을 받을 수 있습니다.

10. 민감한 시기 (Sensitive Period) 또는 결정적 시기 (Critical Period): 특정 경험에 의해 신경계 발달이 특히 큰 영향을 받는 제한된 기간입니다.


## 4. **ImageNet Classification with Deep Convolutional Neural Networks**
## 5. **Very Deep Convolutional Networks for Large-Scale Image Recognition** 
## 6. **Deep Residual Learning for Image Recognition (2015, He et al.)**  
## 7. **Attention Is All You Need (2017, Vaswani et al.)**  
[Attention is all you need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) 

### **7.1 퀴즈**

1. 본 논문에서 제안하는 새로운 네트워크 구조의 이름은 무엇이며, 기존의 순환 신경망(RNN) 또는 합성곱 신경망(CNN) 기반 모델과 비교했을 때 가장 큰 차이점은 무엇입니까? (2-3 문장)

2. Transformer 모델의 인코더와 디코더는 각각 어떤 주요 구성 요소들로 이루어져 있으며, 각 구성 요소는 어떤 역할을 수행합니까? (2-3 문장)

3. 본 논문에서 제시하는 Scaled Dot-Product Attention의 핵심 연산 과정은 무엇이며, 기존의 Dot-Product Attention과 비교했을 때 어떤 점이 추가되었고 그 이유는 무엇입니까? (2-3 문장)

4. Multi-Head Attention 메커니즘은 Scaled Dot-Product Attention을 어떻게 활용하며, 이 메커니즘을 사용하는 주된 이점은 무엇입니까? (2-3 문장)

5. Transformer 모델의 인코더와 디코더에서 사용되는 세 가지 다른 종류의 Multi-Head Attention은 각각 어떤 입력으로부터 query, key, value를 얻어오며, 그 목적은 무엇입니까? (2-3 문장)

6. Position-wise Feed-Forward Network는 Transformer 모델의 각 레이어에서 어떤 역할을 수행하며, 그 구조적인 특징은 무엇입니까? (2-3 문장)

7. Transformer 모델은 순환(recurrence)이나 컨볼루션(convolution)을 사용하지 않기 때문에 시퀀스의 순서 정보를 활용하기 위해 어떤 방법을 사용합니까? 그 방법의 핵심 아이디어는 무엇입니까? (2-3 문장)

8. 본 논문에서 저자들이 Self-Attention 메커니즘을 기존의 RNN이나 CNN 기반 레이어 대신 사용한 세 가지 주요 동기는 무엇입니까? (2-3 문장)

9. 본 논문에서 제시된 실험 결과 중, 영어-독일어 번역(WMT 2014 English-to-German)과 영어-프랑스어 번역(WMT 2014 English-to-French) 작업에서 Transformer 모델이 달성한 주요 성과는 무엇이며, 학습 비용 측면에서 어떤 장점을 보였습니까? (2-3 문장)

10. 학습 과정에서 사용된 Adam optimizer의 파라미터 값과 learning rate schedule의 특징, 그리고 적용된 세 가지 regularization 기법은 각각 무엇이며, 모델 성능 향상에 어떻게 기여합니까? (2-3 문장)


### **7.2 퀴즈 정답**
1. 제안하는 네트워크 구조는 Transformer이며, 기존 모델과 달리 순환 신경망(RNN)이나 합성곱 신경망(CNN)을 전혀 사용하지 않고 오직 attention 메커니즘만을 기반으로 합니다. 이는 병렬 처리를 크게 향상시키고 학습 시간을 단축시키는 주요 이점입니다.

2. 인코더는 N개의 동일한 레이어 스택으로 구성되며, 각 레이어는 Multi-Head Self-Attention과 Position-wise Fully Connected Feed-Forward Network라는 두 개의 서브 레이어를 가집니다. 디코더 역시 N개의 동일한 레이어 스택으로 구성되지만, 인코더의 출력에 대한 Multi-Head Attention 서브 레이어가 추가됩니다.

3. Scaled Dot-Product Attention은 query와 key의 dot product를 계산한 후 $$\sqrt{d_k}$$로 스케일링하고 softmax 함수를 적용하여 value에 대한 가중치를 얻어 출력합니다. 기존의 Dot-Product Attention에 스케일링을 추가한 이유는 $$d_k$$가 커질수록 dot product 값이 커져 softmax 함수의 기울기가 매우 작아지는 것을 방지하기 위함입니다.

4. Multi-Head Attention은 query, key, value를 서로 다른 선형 투영(linear projection)을 통해 h번 투영한 후, 각 투영된 버전에 대해 병렬로 Scaled Dot-Product Attention을 수행합니다. 이를 통해 모델은 서로 다른 표현 하위 공간의 정보를 다양한 위치에서 공동으로 주목할 수 있게 됩니다.

5. "encoder-decoder attention"에서는 디코더의 이전 레이어 출력이 query가 되고, 인코더의 출력이 key와 value가 됩니다. "encoder self-attention"에서는 인코더의 이전 레이어 출력이 query, key, value 모두가 됩니다. "decoder self-attention"에서도 마찬가지로 디코더의 이전 레이어 출력이 query, key, value이지만, 미래 위치에 대한 attention을 막는 masking이 적용됩니다.

6. Position-wise Feed-Forward Network는 인코더와 디코더의 각 레이어에서 attention 서브 레이어 이후에 적용되는 fully connected 네트워크입니다. 이는 각 위치별로 독립적으로 적용되는 두 개의 선형 변환과 그 사이의 ReLU 활성화 함수로 구성됩니다.

7. Transformer 모델은 Positional Encoding이라는 기법을 사용하여 시퀀스의 순서 정보를 모델에 주입합니다. 이는 시퀀스 내 각 토큰의 절대적 또는 상대적 위치에 대한 정보를 담고 있는 벡터를 입력 임베딩에 더하는 방식으로 구현됩니다.

8. 첫째, 레이어당 계산 복잡도가 RNN보다 낮고, 특히 문장 표현의 차원(d)이 시퀀스 길이(n)보다 클 때 더 효율적입니다. 둘째, RNN과 달리 시퀀스 길이에 의존적인 순차적 연산이 없어 병렬 처리가 용이합니다. 셋째, 네트워크 내의 장거리 의존성 사이의 경로 길이가 상수 시간으로 짧아 장거리 의존성을 학습하기 더 쉽습니다.

9. 영어-독일어 번역에서 Transformer (big) 모델은 이전 최고 성능 모델(앙상블 포함)보다 2 BLEU 이상 높은 28.4 BLEU를 달성했으며, 영어-프랑스어 번역에서는 이전 최고 단일 모델보다 높은 41.0 BLEU를 기록했습니다. 또한, 이러한 성능 향상을 훨씬 적은 학습 비용으로 달성했습니다.

10. Adam optimizer는 $$\beta_1 = 0.9$$, $$\beta_2 = 0.98$$, $$\epsilon = 10^{-9}$$ 값을 사용했습니다. Learning rate는 처음 warmup_steps 동안 선형적으로 증가하다가 이후 step 수의 역제곱근에 비례하여 감소하는 schedule을 따릅니다. Regularization으로는 Residual Dropout, Embedding 및 Positional Encoding 합에 대한 Dropout, Label Smoothing (εls = 0.1)이 사용되어 모델의 과적합을 방지하고 일반화 성능을 향상시킵니다.

### **7.3 논술형 질문** 

1. Transformer 모델이 기존의 순환 신경망(RNN) 기반 sequence transduction 모델을 능가하는 주요 요인은 무엇이라고 생각하십니까? 병렬 처리 능력, 장거리 의존성 학습 능력, 모델 해석력 등의 측면에서 논의해 보십시오.

2. Attention 메커니즘은 Transformer 모델의 핵심 구성 요소입니다. Scaled Dot-Product Attention과 Multi-Head Attention이 Transformer의 성능에 어떻게 기여하는지 구체적인 작동 방식을 중심으로 설명하고, 각각의 중요성을 논하십시오.

3. Transformer 모델에서 Positional Encoding의 역할과 중요성에 대해 설명하고, 본 논문에서 사용된 sinusoidal positional encoding 방식의 장점과 한계점에 대해 논하십시오. 다른 positional encoding 방식이 존재한다면 간략히 소개하고 비교해 보십시오.

4. 본 논문에서는 Transformer 모델이 기계 번역 task에서 뛰어난 성능을 보이는 것을 입증했습니다. Transformer의 아키텍처적 특징들이 기계 번역이라는 task의 특성과 어떻게 부합하며, 다른 자연어 처리 task (예: 텍스트 요약, 질의 응답)에 Transformer 모델을 적용했을 때 예상되는 장점과 어려움은 무엇일지 논하십시오.

5. Transformer 모델 이후, attention 메커니즘을 기반으로 하는 다양한 모델들이 등장했습니다 (예: BERT, GPT). 본 논문의 Transformer 모델이 이러한 후속 연구들에 어떤 영향을 미쳤는지 분석하고, Transformer 모델의 한계를 극복하거나 확장하기 위해 어떤 아이디어들이 제시되었는지 논하십시오.

### **7.4 용어 해설**
1. Sequence Transduction: 하나의 시퀀스를 입력으로 받아 다른 시퀀스를 출력하는 task (예: 기계 번역, 텍스트 요약).

2. Recurrent Neural Network (RNN): 순환적인 연결을 통해 시퀀스 데이터를 처리하는 신경망 구조. 이전 시점의 출력이 현재 시점의 입력에 영향을 미치는 방식으로 작동하여 시퀀스의 순서 정보를 학습할 수 있다.
3. Convolutional Neural Network (CNN): Convolution 연산을 사용하여 공간적 특징을 추출하는 신경망 구조. 주로 이미지 처리 task에 사용되지만, 텍스트와 같은 시퀀스 데이터 처리에도 활용될 수 있다.

4. Attention Mechanism: 모델이 입력 시퀀스의 특정 부분에 집중할 수 있도록 하는 메커니즘. query, key, value라는 세 가지 요소를 사용하여 입력의 중요도를 가중치로 나타낸다.

5. Self-Attention (Intra-Attention): 하나의 시퀀스 내에서 각 위치가 다른 모든 위치에 대해 attention을 수행하여 시퀀스 내의 관계를 모델링하는 메커니즘.

6. Encoder-Decoder Architecture: sequence transduction 모델의 기본적인 구조. 인코더는 입력 시퀀스를 고정된 크기의 벡터 표현으로 압축하고, 디코더는 이 벡터 표현을 기반으로 출력 시퀀스를 생성한다.

7. Multi-Head Attention: 여러 개의 독립적인 attention 함수를 병렬로 수행한 후 그 결과를 연결(concatenate)하는 attention 메커니즘. 모델이 다양한 측면에서 정보에 주목할 수 있도록 한다.

8. Scaled Dot-Product Attention: attention 가중치를 계산하기 위해 query와 key의 dot product를 사용하고, 그 결과를 key 벡터 차원의 제곱근으로 스케일링하는 attention 함수.
Residual Connection: 신경망의 이전 레이어의 출력을 현재 레이어의 출력에 더하는 방식. 기울기 소실 문제를 완화하고 깊은 네트워크의 학습을 돕는다.

9. Layer Normalization: 각 레이어의 활성화 값을 정규화하는 기법. 학습 속도를 높이고 모델의 안정성을 향상시킨다.

10. Position-wise Feed-Forward Network: 시퀀스의 각 위치에 대해 독립적으로 적용되는 fully connected 신경망. 일반적으로 두 개의 선형 변환과 그 사이의 비선형 활성화 함수(ReLU)로 구성된다.

11. Positional Encoding: 시퀀스 내 토큰의 위치 정보를 모델에 제공하기 위한 벡터 표현. Transformer 모델은 순환 또는 컨볼루션 연산을 사용하지 않기 때문에 명시적으로 위치 정보를 주입해야 한다.

12. BLEU (Bilingual Evaluation Understudy): 기계 번역 시스템의 성능을 평가하는 지표 중 하나. 모델이 생성한 번역과 사람의 번역을 비교하여 유사성을 측정한다.

13. Word-piece: 단어 분할(tokenization) 기법 중 하나. 자주 등장하는 단어는 그대로 유지하고, 희귀하거나 처음 보는 단어는 더 작은 의미 단위(subword)로 분할한다.

14. Byte-Pair Encoding (BPE): 데이터 압축 알고리즘에서 유래한 subword 분할 기법. 가장 자주 등장하는 연속적인 바이트 쌍을 하나의 새로운 토큰으로 병합하는 과정을 반복하여 어휘 집합을 구축한다.

15. Dropout: 학습 과정에서 신경망의 일부 뉴런을 무작위로 비활성화시키는 regularization 기법. 모델의 과적합을 방지하고 일반화 성능을 향상시킨다.

16. Label Smoothing: 학습 데이터의 정답 레이블을 약간의 오류 가능성을 포함하도록 부드럽게 만드는 regularization 기법. 모델이 특정 예측에 지나치게 확신하는 것을 방지한다.

17. Adam Optimizer: 경사하강법 기반의 최적화 알고리즘 중 하나. 각 파라미터에 대해 적응적으로 학습률을 조정하며, 모멘텀과 RMSProp의 장점을 결합하였다.

18. Warmup Steps: 학습 초기 단계에서 학습률을 점진적으로 증가시키는 스케줄링 기법. 모델의 초기 불안정성을 완화하는 데 도움을 줄 수 있다.

## 8. **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018, Devlin et al.)** 
## 9. **Playing Atari with Deep Reinforcement Learning (2013, Mnih et al.)**
## 10. **Mastering the game of Go with deep neural networks and tree search (2016, Silver et al.)** 
## 11. **Generative Adversarial Networks (2014, Goodfellow et al.)** 
## 12. **Auto-Encoding Variational Bayes (2013, Kingma & Welling)** 
## 13. **Neural Architecture Search with Reinforcement Learning (2016, Zoph & Le)** 
## 14. **Diffusion Models Beat GANs on Image Synthesis (2021, Dhariwal & Nichol)**  








