---
title: 1ì°¨ì‹œ 7(ë¹…ë°ì´í„° ë¶„ì„):ë°ì´í„° ì „ì²˜ë¦¬ ë° í´ë¦¬ë‹ 
layout: single
classes: wide
categories:
  - Pandas
toc: true # ì´ í¬ìŠ¤íŠ¸ì—ì„œ ëª©ì°¨ë¥¼ í™œì„±í™”
toc_sticky: true # ëª©ì°¨ë¥¼ ê³ ì •í• ì§€ ì—¬ë¶€ (ì„ íƒ ì‚¬í•­)
---


## 1. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Missing Value Handling)

### 1.1 ì´ë¡ ì  ë°°ê²½

- ê²°ì¸¡ì¹˜ëŠ” ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ë¶ˆì™„ì „í•œ ì •ë³´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ê²°ì¸¡ì¹˜ íŒ¨í„´ì— ë”°ë¼ ì²˜ë¦¬ ë°©ë²•ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤.

**ê²°ì¸¡ì¹˜ ìœ í˜•:**

| êµ¬ë¶„                                                      | ì •ì˜                                  | ì˜ˆì‹œ                       | ë¶„ì„ ì‹œ ë¬¸ì œì                   | ì²˜ë¦¬ ì „ëµ                                                               |
| ------------------------------------------------------- | ----------------------------------- | ------------------------ | ------------------------- | ------------------------------------------------------------------- |
| **MCAR**<br>(Missing Completely At Random)<br>ì™„ì „ ë¬´ì‘ìœ„ ê²°ì¸¡ | ê²°ì¸¡ ë°œìƒì´ ë°ì´í„°ì™€ ë¬´ê´€í•˜ê²Œ ì™„ì „íˆ ëœë¤             | ì„¤ë¬¸ì§€ ì‘ì„± ì¤‘ ì‰í¬ ë²ˆì§ìœ¼ë¡œ ì‘ë‹µì´ ì§€ì›Œì§ | ë°ì´í„° ì œê±°Â·ë‹¨ìˆœ ëŒ€ì²´í•´ë„ í¸í–¥ ì—†ìŒ      | - ë‹¨ìˆœ ì œê±°(Listwise Deletion)<br>- í‰ê· /ì¤‘ì•™ê°’ ëŒ€ì²´                           |
| **MAR**<br>(Missing At Random)<br>ë¬´ì‘ìœ„ ê²°ì¸¡                | ê²°ì¸¡ì´ ë‹¤ë¥¸ ë³€ìˆ˜ì™€ëŠ” ê´€ë ¨ ìˆì§€ë§Œ, í•´ë‹¹ ë³€ìˆ˜ ê°’ ìì²´ì™€ëŠ” ë¬´ê´€ | ê³ ì—°ë ¹ìê°€ ì†Œë“ ì§ˆë¬¸ì„ ìƒëµ          | ê´€ë ¨ ë³€ìˆ˜ ê³ ë ¤ ì—†ì´ ë‹¨ìˆœ ì œê±° ì‹œ í¸í–¥ ë°œìƒ | - ë‹¤ì¤‘ ëŒ€ì²´(Multiple Imputation)<br>- íšŒê·€ ëŒ€ì²´                             |
| **MNAR**<br>(Missing Not At Random)<br>ë¹„ë¬´ì‘ìœ„ ê²°ì¸¡          | ê²°ì¸¡ ë°œìƒì´ í•´ë‹¹ ë³€ìˆ˜ ê°’ ìì²´ì™€ ê´€ë ¨               | ê³ ì†Œë“ìê°€ ì†Œë“ì„ ìˆ¨ê¹€             | ê²°ì¸¡ ìì²´ê°€ ì¤‘ìš”í•œ ì •ë³´, ë‹¨ìˆœ ëŒ€ì²´ ë¶ˆê°€   | - "ê²°ì¸¡ ì—¬ë¶€" ë³€ìˆ˜ ì¶”ê°€<br>- íŠ¹ìˆ˜í•œ í†µê³„ ê¸°ë²•(Selection model ë“±)<br>- ë°ì´í„° ìˆ˜ì§‘ ì„¤ê³„ ë³´ì™„ |

*   **ì£¼ìš” ì²˜ë¦¬ ë°©ë²•:**
    1. **ì™„ì „ ì‚­ì œ (Listwise Deletion)**
    - ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì „ì²´ë¥¼ ì œê±°
    - ì¥ì : ê°„ë‹¨í•˜ê³  í¸í–¥ ì—†ìŒ (MCARì¸ ê²½ìš°)
    - ë‹¨ì : ë°ì´í„° ì†ì‹¤, í‘œë³¸ í¬ê¸° ê°ì†Œ

    2. **ë¶€ë¶„ ì‚­ì œ (Pairwise Deletion)**
    - ë¶„ì„ì— í•„ìš”í•œ ë³€ìˆ˜ì—ì„œë§Œ ê²°ì¸¡ì¹˜ ì œê±°
    - ë¶„ì„ë§ˆë‹¤ ë‹¤ë¥¸ í‘œë³¸ í¬ê¸° ì‚¬ìš©

    3. **í‰ê· /ì¤‘ìœ„ìˆ˜/ìµœë¹ˆê°’ ëŒ€ì²´**
    - ì—°ì†í˜•: í‰ê·  ë˜ëŠ” ì¤‘ìœ„ìˆ˜
    - ë²”ì£¼í˜•: ìµœë¹ˆê°’
    - ë‹¨ì : ë¶„ì‚° ê³¼ì†Œì¶”ì •

    4. **ë³´ê°„ë²• (Interpolation)**
    - ì„ í˜•ë³´ê°„: ì‹œê³„ì—´ ë°ì´í„°ì˜ ì¶”ì„¸ í™œìš©
    - ë‹¤í•­ì‹ ë³´ê°„: ë³µì¡í•œ íŒ¨í„´ ëª¨ë¸ë§

    5. **ë‹¤ì¤‘ ëŒ€ì²´ (Multiple Imputation)**
    - ì†Œë“ì´ ê²°ì¸¡ì¼ ë•Œ, ë‚˜ì´Â·ì§ì—…Â·í•™ë ¥ ë“±ì„ í™œìš©í•´ "ê°€ëŠ¥í•œ ì†Œë“ê°’"ì„ 5ê°€ì§€ ì •ë„ ë§Œë“¤ì–´ì„œ 5ê°œì˜ ë°ì´í„°ì…‹ ìƒì„±
    - ë¶ˆí™•ì‹¤ì„±ì„ ë°˜ì˜í•œ í†µê³„ì  ì¶”ë¡ 

### 1.2 ì‹¤ìŠµ ì˜ˆì œ: ì˜¨ë¼ì¸ ì‡¼í•‘ëª° ê³ ê° ë°ì´í„°

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import KNNImputer

# ìƒ˜í”Œ ë°ì´í„° ìƒì„±
np.random.seed(42)
data = {
    'customer_id': range(1, 1001),
    'age': np.random.normal(35, 12, 1000).astype(int),
    'income': np.random.normal(50000, 15000, 1000),
    'purchase_amount': np.random.normal(200, 100, 1000),
    'website_visits': np.random.poisson(5, 1000),
    'gender': np.random.choice(['M', 'F'], 1000),
    'region': np.random.choice(['Seoul', 'Busan', 'Daegu', 'Incheon'], 1000)
}

df = pd.DataFrame(data)

# ì¸ìœ„ì ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ìƒì„±
missing_indices_age = np.random.choice(df.index, 80, replace=False)
missing_indices_income = np.random.choice(df.index, 120, replace=False)
missing_indices_purchase = np.random.choice(df.index, 60, replace=False)

df.loc[missing_indices_age, 'age'] = np.nan
df.loc[missing_indices_income, 'income'] = np.nan
df.loc[missing_indices_purchase, 'purchase_amount'] = np.nan

# ê²°ì¸¡ì¹˜ í˜„í™© íŒŒì•…
print("ê²°ì¸¡ì¹˜ í˜„í™©:")
print(df.isnull().sum())
print(f"\nì „ì²´ ê²°ì¸¡ì¹˜ ë¹„ìœ¨: {df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100:.2f}%")

# ê²°ì¸¡ì¹˜ íŒ¨í„´ ì‹œê°í™”
import missingno as msno
msno.matrix(df)
plt.title('ê²°ì¸¡ì¹˜ íŒ¨í„´')
plt.show()

# ë°©ë²• 1: ì™„ì „ ì‚­ì œ
df_complete = df.dropna()
print(f"ì™„ì „ ì‚­ì œ í›„ ë°ì´í„° í¬ê¸°: {df_complete.shape[0]} (ì›ë³¸: {df.shape[0]})")

# ë°©ë²• 2: í‰ê· /ì¤‘ìœ„ìˆ˜ ëŒ€ì²´
df_mean_imputed = df.copy()
df_mean_imputed['age'].fillna(df['age'].median(), inplace=True)
df_mean_imputed['income'].fillna(df['income'].mean(), inplace=True)
df_mean_imputed['purchase_amount'].fillna(df['purchase_amount'].median(), inplace=True)

# ë°©ë²• 3: KNN ëŒ€ì²´
numeric_cols = ['age', 'income', 'purchase_amount', 'website_visits']
knn_imputer = KNNImputer(n_neighbors=5)
df_knn_imputed = df.copy()
df_knn_imputed[numeric_cols] = knn_imputer.fit_transform(df[numeric_cols])
```

## 2. ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬ (Outlier Detection and Treatment)

### 2.1 ì´ë¡ ì  ë°°ê²½

-   ì´ìƒì¹˜ëŠ” ë°ì´í„°ì˜ ì¼ë°˜ì ì¸ íŒ¨í„´ì—ì„œ í¬ê²Œ ë²—ì–´ë‚œ ê°’ìœ¼ë¡œ, ë°ì´í„° ì…ë ¥ ì˜¤ë¥˜, ì¸¡ì • ì˜¤ë¥˜, ì‹¤ì œ ê·¹ê°’ì¼ ìˆ˜ ìˆë‹¤.

-   **ì´ìƒì¹˜ íƒì§€ ë°©ë²•:**
    1. **IQR (Interquartile Range) ë°©ë²•**
    - Q1 - 1.5Ã—IQR ë¯¸ë§Œ ë˜ëŠ” Q3 + 1.5Ã—IQR ì´ˆê³¼ì¸ ê°’
    - ì¥ì : ë¶„í¬ì— ë¬´ê´€í•˜ê²Œ ì ìš© ê°€ëŠ¥
    - ë‹¨ì : ê²½ì§ì ì¸ ê¸°ì¤€

    2. **Z-Score ë°©ë²•**
    - |Z-score| > 2 ë˜ëŠ” 3ì¸ ê°’
    - ê°€ì •: ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„
    - ê³µì‹: Z = (X - Î¼) / Ïƒ

    3. **Modified Z-Score**
    - ì¤‘ìœ„ìˆ˜ì™€ MAD(Median Absolute Deviation) ì‚¬ìš©
    - ì´ìƒì¹˜ì— ë” ê°•ê±´í•¨

### 2.2 ì‹¤ìŠµ ì˜ˆì œ: ì „ììƒê±°ë˜ ê±°ë˜ ë°ì´í„°

```python
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# ì´ìƒì¹˜ê°€ í¬í•¨ëœ ê±°ë˜ ë°ì´í„° ìƒì„±
np.random.seed(42)
normal_transactions = np.random.normal(100, 30, 950)
outlier_transactions = np.random.uniform(500, 1000, 50)  # ì´ìƒì¹˜
transaction_amounts = np.concatenate([normal_transactions, outlier_transactions])

df_transactions = pd.DataFrame({
    'transaction_id': range(1, 1001),
    'amount': transaction_amounts,
    'customer_type': np.random.choice(['Bronze', 'Silver', 'Gold'], 1000)
})

# 1. IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€
def detect_outliers_iqr(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return (data < lower_bound) | (data > upper_bound)

outliers_iqr = detect_outliers_iqr(df_transactions['amount'])
print(f"IQR ë°©ë²•ìœ¼ë¡œ íƒì§€ëœ ì´ìƒì¹˜ ê°œìˆ˜: {outliers_iqr.sum()}")

# 2. Z-Score ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€
z_scores = np.abs(stats.zscore(df_transactions['amount']))
outliers_zscore = z_scores > 3
print(f"Z-Score ë°©ë²•ìœ¼ë¡œ íƒì§€ëœ ì´ìƒì¹˜ ê°œìˆ˜: {outliers_zscore.sum()}")

# ì´ìƒì¹˜ ì‹œê°í™”
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# ë°•ìŠ¤í”Œë¡¯
axes[0,0].boxplot(df_transactions['amount'])
axes[0,0].set_title('Box Plot - ì´ìƒì¹˜ íƒì§€')
axes[0,0].set_ylabel('ê±°ë˜ê¸ˆì•¡')

# íˆìŠ¤í† ê·¸ë¨
axes[0,1].hist(df_transactions['amount'], bins=50, alpha=0.7)
axes[0,1].set_title('íˆìŠ¤í† ê·¸ë¨')
axes[0,1].set_xlabel('ê±°ë˜ê¸ˆì•¡')

# ì‚°ì ë„ (ì¸ë±ìŠ¤ vs ê¸ˆì•¡)
axes[1,0].scatter(df_transactions.index, df_transactions['amount'], alpha=0.5)
axes[1,0].scatter(df_transactions.index[outliers_iqr], 
                 df_transactions['amount'][outliers_iqr], color='red', label='IQR ì´ìƒì¹˜')
axes[1,0].set_title('IQR ì´ìƒì¹˜ í‘œì‹œ')
axes[1,0].legend()

# Z-score ì‚°ì ë„
axes[1,1].scatter(df_transactions.index, df_transactions['amount'], alpha=0.5)
axes[1,1].scatter(df_transactions.index[outliers_zscore], 
                 df_transactions['amount'][outliers_zscore], color='orange', label='Z-score ì´ìƒì¹˜')
axes[1,1].set_title('Z-Score ì´ìƒì¹˜ í‘œì‹œ')
axes[1,1].legend()

plt.tight_layout()
plt.show()

# ì´ìƒì¹˜ ì²˜ë¦¬ ë°©ë²•ë“¤
# ë°©ë²• 1: ì œê±°
df_no_outliers = df_transactions[~outliers_iqr]

# ë°©ë²• 2: ë³€í™˜ (ë¡œê·¸ ë³€í™˜)
df_log_transformed = df_transactions.copy()
df_log_transformed['log_amount'] = np.log1p(df_transactions['amount'])

# ë°©ë²• 3: ìº¡í•‘ (ìƒí•œ/í•˜í•œ ì„¤ì •)
df_capped = df_transactions.copy()
Q1 = df_transactions['amount'].quantile(0.25)
Q3 = df_transactions['amount'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

df_capped['amount'] = df_capped['amount'].clip(lower=lower_bound, upper=upper_bound)

print(f"ì›ë³¸ ë°ì´í„° í†µê³„:")
print(df_transactions['amount'].describe())
print(f"\nì´ìƒì¹˜ ì œê±° í›„ í†µê³„:")
print(df_no_outliers['amount'].describe())
print(f"\nìº¡í•‘ í›„ í†µê³„:")
print(df_capped['amount'].describe())
```

## 3. ë°ì´í„° íƒ€ì… ë³€í™˜ ë° ì •ê·œí™” (Data Type Conversion and Normalization)

### 3.1 ì´ë¡ ì  ë°°ê²½

-   **ë°ì´í„° íƒ€ì… ë³€í™˜:**
    - ë¶„ì„ ëª©ì ì— ë§ëŠ” ì ì ˆí•œ ë°ì´í„° íƒ€ì… ì„¤ì •
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
    - ì—°ì‚° ì„±ëŠ¥ ê°œì„ 

- **ì •ê·œí™” ê¸°ë²•:**
    1. **Min-Max ì •ê·œí™”**
    - ê³µì‹: (x - min) / (max - min)
    - ë²”ìœ„: $\[0, 1\]$
    - ì´ìƒì¹˜ì— ë¯¼ê°í•¨

    2. **Z-Score í‘œì¤€í™”**
    - ê³µì‹: (x - Î¼) / Ïƒ
    - í‰ê· : 0, í‘œì¤€í¸ì°¨: 1
    - ì •ê·œë¶„í¬ ê°€ì •

### 3.2 ì‹¤ìŠµ ì˜ˆì œ: ë‹¤ì–‘í•œ íƒ€ì…ì˜ ê³ ê° ë°ì´í„°

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# ë‹¤ì–‘í•œ íƒ€ì…ì˜ ë°ì´í„° ìƒì„±
np.random.seed(42)
customer_data = {
    'customer_id': range(1, 1001),
    'registration_date': pd.date_range('2020-01-01', periods=1000, freq='D')[:1000],
    'age_group': np.random.choice(['18-25', '26-35', '36-45', '46-55', '55+'], 1000),
    'income': np.random.normal(60000, 20000, 1000),
    'credit_score': np.random.normal(700, 100, 1000).astype(int),
    'num_purchases': np.random.poisson(10, 1000),
    'satisfaction': np.random.choice(['ë§¤ìš°ë¶ˆë§Œ', 'ë¶ˆë§Œ', 'ë³´í†µ', 'ë§Œì¡±', 'ë§¤ìš°ë§Œì¡±'], 1000),
    'is_premium': np.random.choice([True, False], 1000),
    'spending_category': np.random.choice(['Low', 'Medium', 'High'], 1000)
}

df_customers = pd.DataFrame(customer_data)

print("ì›ë³¸ ë°ì´í„° íƒ€ì…:")
print(df_customers.dtypes)
print("\në°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
print(df_customers.head())

# 1. ë°ì´í„° íƒ€ì… ë³€í™˜
df_converted = df_customers.copy()

# ë‚ ì§œ íƒ€ì… ë³€í™˜ ë° íŒŒìƒ ë³€ìˆ˜ ìƒì„±
df_converted['registration_date'] = pd.to_datetime(df_converted['registration_date'])
df_converted['days_since_registration'] = (pd.Timestamp.now() - df_converted['registration_date']).dt.days
df_converted['registration_year'] = df_converted['registration_date'].dt.year
df_converted['registration_month'] = df_converted['registration_date'].dt.month

# ë²”ì£¼í˜• ë°ì´í„° ìˆœì„œ ì„¤ì •
age_order = ['18-25', '26-35', '36-45', '46-55', '55+']
satisfaction_order = ['ë§¤ìš°ë¶ˆë§Œ', 'ë¶ˆë§Œ', 'ë³´í†µ', 'ë§Œì¡±', 'ë§¤ìš°ë§Œì¡±']
spending_order = ['Low', 'Medium', 'High']

df_converted['age_group'] = pd.Categorical(df_converted['age_group'], categories=age_order, ordered=True)
df_converted['satisfaction'] = pd.Categorical(df_converted['satisfaction'], categories=satisfaction_order, ordered=True)
df_converted['spending_category'] = pd.Categorical(df_converted['spending_category'], categories=spending_order, ordered=True)

# ë¶ˆë¦° íƒ€ì… ë³€í™˜
df_converted['is_premium'] = df_converted['is_premium'].astype('bool')

print("\në³€í™˜ í›„ ë°ì´í„° íƒ€ì…:")
print(df_converted.dtypes)

# 2. ë²”ì£¼í˜• ë°ì´í„° ì¸ì½”ë”©
# One-Hot Encoding
df_encoded = pd.get_dummies(df_converted, columns=['age_group', 'spending_category'], prefix=['age', 'spending'])

# Label Encoding (ìˆœì„œê°€ ìˆëŠ” ë²”ì£¼í˜•)
le = LabelEncoder()
df_encoded['satisfaction_encoded'] = le.fit_transform(df_converted['satisfaction'])

# 3. ìˆ˜ì¹˜í˜• ë°ì´í„° ì •ê·œí™”
numeric_columns = ['income', 'credit_score', 'num_purchases', 'days_since_registration']

# Min-Max ì •ê·œí™”
scaler_minmax = MinMaxScaler()
df_minmax = df_encoded.copy()
df_minmax[numeric_columns] = scaler_minmax.fit_transform(df_encoded[numeric_columns])

# í‘œì¤€í™” (Z-Score)
scaler_standard = StandardScaler()
df_standard = df_encoded.copy()
df_standard[numeric_columns] = scaler_standard.fit_transform(df_encoded[numeric_columns])


# ì •ê·œí™” ê²°ê³¼ ë¹„êµ ì‹œê°í™”
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# ì›ë³¸ ë°ì´í„°
axes[0,0].hist(df_encoded['income'], bins=50, alpha=0.7)
axes[0,0].set_title('ì›ë³¸ Income ë¶„í¬')

# Min-Max ì •ê·œí™”
axes[0,1].hist(df_minmax['income'], bins=50, alpha=0.7, color='orange')
axes[0,1].set_title('Min-Max ì •ê·œí™” í›„ Income ë¶„í¬')

# í‘œì¤€í™”
axes[1,0].hist(df_standard['income'], bins=50, alpha=0.7, color='green')
axes[1,0].set_title('í‘œì¤€í™” í›„ Income ë¶„í¬')

plt.tight_layout()
plt.show()

# í†µê³„ ìš”ì•½
print("\nì •ê·œí™” ë°©ë²•ë³„ í†µê³„ ìš”ì•½ (Income):")
print("ì›ë³¸:", df_encoded['income'].describe().round(2))
print("Min-Max:", df_minmax['income'].describe().round(2))
print("í‘œì¤€í™”:", df_standard['income'].describe().round(2))
```

## 4. ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬ (Duplicate Data Handling)

### 4.1 ì´ë¡ ì  ë°°ê²½
-   ì¤‘ë³µ ë°ì´í„°ëŠ” ë™ì¼í•œ ê´€ì¸¡ì¹˜ê°€ ì—¬ëŸ¬ ë²ˆ ê¸°ë¡ëœ ê²½ìš°ë¥¼ ì˜ë¯¸í•˜ë©°, ë¶„ì„ ê²°ê³¼ë¥¼ ì™œê³¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

-   **ì¤‘ë³µ ìœ í˜•:**
    1. **ì™„ì „ ì¤‘ë³µ**: ëª¨ë“  ì»¬ëŸ¼ì´ ë™ì¼
    2. **ë¶€ë¶„ ì¤‘ë³µ**: ì¼ë¶€ í•µì‹¬ ì»¬ëŸ¼ì´ ë™ì¼
    3. **ìœ ì‚¬ ì¤‘ë³µ**: ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆì§€ë§Œ ì‹¤ì§ˆì ìœ¼ë¡œ ê°™ì€ ë°ì´í„°

-   **ì²˜ë¦¬ ì „ëµ:**
    - ì™„ì „ ì¤‘ë³µ: ì œê±°
    - ë¶€ë¶„ ì¤‘ë³µ: ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ì— ë”°ë¼ ì²˜ë¦¬
    - ìœ ì‚¬ ì¤‘ë³µ: ë°ì´í„° ë§¤ì¹­ ë° í†µí•©

### 4.2 ì‹¤ìŠµ ì˜ˆì œ: ê³ ê° ì£¼ë¬¸ ë°ì´í„°

```python
import pandas as pd
import numpy as np

# 1. ì˜ˆì‹œ ë°ì´í„° ë§Œë“¤ê¸°
# ì´ 1000ê°œì˜ ê°€ìƒ ì£¼ë¬¸ ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
# ì¼ë¶€ëŸ¬ 100ê°œì˜ ì™„ì „ ì¤‘ë³µ ë°ì´í„°ë¥¼ í¬í•¨ì‹œì¼°ìŠµë‹ˆë‹¤.
orders_data = {
    'order_id': [f'ORD{i:03d}' for i in range(1000)],
    'customer_id': np.random.randint(1, 301, 1000),
    'product_name': np.random.choice(['ë…¸íŠ¸ë¶', 'ìŠ¤ë§ˆíŠ¸í°', 'íƒœë¸”ë¦¿'], 1000),
    'quantity': np.random.randint(1, 5, 1000),
}
df = pd.DataFrame(orders_data)

# ì™„ì „ ì¤‘ë³µ ë°ì´í„° ì¶”ê°€
duplicate_rows = df.sample(n=100, random_state=42)
df = pd.concat([df, duplicate_rows], ignore_index=True)

print(f"ì „ì²´ ë°ì´í„° ê±´ìˆ˜: {len(df)} ê±´")
print("---")

# 2. ì™„ì „ ì¤‘ë³µ ë°ì´í„° ì°¾ê¸° ë° ì œê±°í•˜ê¸°
# `duplicated()` í•¨ìˆ˜ëŠ” ì¤‘ë³µëœ í–‰ì„ True, ì•„ë‹Œ í–‰ì„ Falseë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
# `sum()`ì„ ì‚¬ìš©í•˜ë©´ True ê°’ì˜ ê°œìˆ˜, ì¦‰ ì¤‘ë³µëœ í–‰ì˜ ìˆ˜ë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
print("ğŸ” ì™„ì „ ì¤‘ë³µ ë°ì´í„° ì°¾ê¸°")
complete_duplicates = df.duplicated()
print(f"âœ”ï¸ ë°œê²¬ëœ ì™„ì „ ì¤‘ë³µ ê±´ìˆ˜: {complete_duplicates.sum()} ê±´")
print("---")

# `drop_duplicates()` í•¨ìˆ˜ë¡œ ì¤‘ë³µ ë°ì´í„°ë¥¼ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# `keep='first'`ëŠ” ì¤‘ë³µëœ í–‰ ì¤‘ ì²« ë²ˆì§¸ í–‰ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ë¥¼ ì œê±°í•©ë‹ˆë‹¤. (ê¸°ë³¸ê°’)
# `keep='last'`ëŠ” ë§ˆì§€ë§‰ í–‰ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
# `keep=False`ëŠ” ì¤‘ë³µëœ ëª¨ë“  í–‰ì„ ì œê±°í•©ë‹ˆë‹¤.
print("ğŸ—‘ï¸ ì™„ì „ ì¤‘ë³µ ë°ì´í„° ì œê±°í•˜ê¸°")
df_no_duplicates = df.drop_duplicates()
print(f"âœ”ï¸ ì¤‘ë³µ ì œê±° í›„ ë°ì´í„° ê±´ìˆ˜: {len(df_no_duplicates)} ê±´")
print("---")

# 3. íŠ¹ì • ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì°¾ê¸°
# `subset` ì˜µì…˜ì— ì¤‘ë³µì„ í™•ì¸í•  ì»¬ëŸ¼ ì´ë¦„ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤.
# ì—¬ê¸°ì„œëŠ” 'customer_id'ì™€ 'product_name'ì´ ê°™ìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
print("ğŸ” íŠ¹ì • ì»¬ëŸ¼(ê³ ê°+ìƒí’ˆ) ê¸°ì¤€ ì¤‘ë³µ ë°ì´í„° ì°¾ê¸°")
subset_duplicates = df.duplicated(subset=['customer_id', 'product_name'])
print(f"âœ”ï¸ ë°œê²¬ëœ ë¶€ë¶„ ì¤‘ë³µ ê±´ìˆ˜: {subset_duplicates.sum()} ê±´")
print("---")

# 4. ì¤‘ë³µ ë°ì´í„°ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì²˜ë¦¬ ì „ëµ
# ë°ì´í„°ì˜ ì„±ê²©ì— ë”°ë¼ ì¤‘ë³µì„ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ì˜ˆë¥¼ ë“¤ì–´, í•œ ê³ ê°ì´ ê°™ì€ ìƒí’ˆì„ ì—¬ëŸ¬ ë²ˆ êµ¬ë§¤í–ˆì„ ë•Œ,
# ì´ êµ¬ë§¤ ë‚´ì—­ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê³  ì‹¶ë‹¤ë©´ `groupby()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

print("ğŸ“Š ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬ ì „ëµ (ì˜ˆì‹œ)")
# `groupby()`ë¡œ 'customer_id'ì™€ 'product_name'ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¶ìŠµë‹ˆë‹¤.
# `agg()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¶ì¸ ë°ì´í„°ì˜ ìˆ˜ëŸ‰ì„ ëª¨ë‘ ë”í•©ë‹ˆë‹¤.
df_aggregated = df.groupby(['customer_id', 'product_name']).agg(
    total_quantity=('quantity', 'sum')
).reset_index()

print("âœ”ï¸ í†µí•©ëœ ë°ì´í„° ê±´ìˆ˜:", len(df_aggregated), "ê±´")
print("\n[í†µí•©ëœ ë°ì´í„° ì˜ˆì‹œ]")
print(df_aggregated.head())
```
