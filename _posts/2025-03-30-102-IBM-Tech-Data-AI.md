---
title: 23차시 1:IBM TECH(Data and AI)
layout: single
classes: wide
categories:
  - Data and AI
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 1. 데이터 마이닝
- 출처: [What is Data Mining?](https://www.youtube.com/watch?v=7rs0i-9nOjo&list=PLOspHqNVtKABpPMnwkx27tEO2KOBvHLu6&index=2)


### **1.1 데이터 마이닝 정의** 
- 대규모 데이터 세트에서 가치 있는 정보를 추출하는 과정으로, 빅데이터 시대에 중요성이 더욱 부각
- 이는 단순한 데이터 수집을 넘어 숨겨진 패턴과 의미 있는 관계를 발견하는 복잡한 분석 과정입니다.

### **1.2 유용성**

* 미래 추세 예측 
    - 과거 데이터 패턴을 기반으로 시장 변화, 소비자 행동, 경제 동향 등을 예측할 수 있습니다.
* 데이터 간의 관계 파악 
    - 겉으로 보이지 않는 변수들 간의 상관관계를 발견하여 새로운 비즈니스 인사이트를 얻을 수 있습니다.
* 정보에 입각한 의사 결정 지원 
    - 직관이나 경험에만 의존하지 않고 실제 데이터에 기반한 객관적인 의사결정이 가능해집니다.

### **1.3 데이터 마이닝 프로세스 4단계**

1. **목표 설정:** 데이터 과학자와 이해 관계자가 협력하여 비즈니스 문제 정의
   - 해결하고자 하는 구체적인 비즈니스 문제나 기회를 명확히 합니다.
   - 성공 기준과 평가 지표를 사전에 설정합니다.

2. **데이터 준비:**
   - 문제 해결에 필요한 데이터 식별 - 내부 데이터베이스, 외부 소스, 공개 데이터셋 등 다양한 출처에서 관련 데이터를 수집합니다.
   - 데이터 정제 (중복, 결측값, 이상치 제거) - 데이터 품질은 결과의 신뢰성에 직결되므로 철저한 전처리 과정이 필요합니다. 표준화, 정규화 등의 기법을 적용합니다.

3. **데이터 마이닝 알고리즘 적용:**
   - 데이터 관계 탐색 - 다양한 통계적, 수학적 알고리즘을 통해 데이터 속 패턴을 찾아냅니다.
   - 딥러닝 기술 활용 - 복잡한 데이터셋에서는 신경망 기반의 고급 기법을 적용하여 심층적인 패턴을 발견합니다.

4. **결과 평가:** 유효하고, 새롭고, 유용하고, 이해하기 쉬운 결과 해석
   - 도출된 결과가 실제 비즈니스 문제 해결에 기여하는지 검증합니다.
   - 모델의 정확도, 신뢰성, 일반화 가능성을 다양한 지표로 평가합니다.

### **1.5 주요 데이터 마이닝 기술**

* **연관 분석 (Association):** 
    - 데이터 세트 내 변수 간의 관계 발견 (예: 딸기 구매 시 크림 구매)
        - 장바구니 분석, 교차판매, 추천 시스템 개발에 활용됩니다.
        - Apriori, FP-Growth와 같은 알고리즘이 주로 사용됩니다.

* **분류 (Classification):** 
    - 여러 속성을 사용하여 객체 또는 고객 유형 분류 (예: 자동차 종류 분류)
        - 이메일 스팸 필터링, 질병 진단, 신용 위험 평가 등에 활용됩니다.
        - 로지스틱 회귀, 서포트 벡터 머신, 랜덤 포레스트 등의 알고리즘이 사용됩니다.

* **군집화 (Clustering):** 
    - 데이터 조각들을 그룹화하여 구조 형성 (예: 유사한 고객끼리 그룹화)
        - 고객 세분화, 이상 탐지, 이미지 분할 등에 활용됩니다.
        - K-means, 계층적 군집화, DBSCAN 등이 대표적인 알고리즘입니다.

* **예측 (Prediction):** 
    - 과거 데이터를 기반으로 미래 사건 예측 (의사 결정 트리, KNN 알고리즘 등 활용)
        - 수요 예측, 가격 변동 예측, 장비 고장 예측 등 다양한 분야에 적용됩니다.
        - 시계열 분석, 회귀 모델, 앙상블 기법 등이 활용됩니다.

### **1.6 핵심**

* 데이터 마이닝 기술은 만능 해결책이 아니며, 데이터, 비즈니스 질문, 목표에 따라 효과가 달라짐 
    - 적절한 기법 선택과 상황에 맞는 접근 방식이 중요합니다.
* 성공적인 데이터 마이닝은 비즈니스 이해 관계자와 데이터 과학자의 협업을 통해 이루어짐 
    - 도메인 지식과 기술적 전문성의 결합이 필수적입니다.
* 데이터 마이닝을 통해 얻은 통찰력은 비즈니스 혁신을 가져올 수 있음 
    - 데이터에 기반한 의사결정은 경쟁 우위를 확보하고 새로운 비즈니스 기회를 창출합니다.

## 2. 데이터 관측 가능성(Data Observability)
- 출처: [What is Data Observability?](https://www.youtube.com/watch?v=jfg9wBJBtKk&list=PLOspHqNVtKABpPMnwkx27tEO2KOBvHLu6&index=3)


### 2.1 데이터 관측 가능성의 등장 배경  

*   **소프트웨어 엔지니어링의 진화**: CI/CD(지속적 통합/배포), DevOps 문화, 인프라 코드화(Infra as Code) 등의 기술 발전으로 소프트웨어 개발 생명주기가 혁신적으로 변화  
*   **데이터 중심 기업으로의 전환**: 모든 산업에서 데이터가 핵심 자원으로 부상하며, 기업들은 단순한 소프트웨어 회사를 넘어 데이터 기반 의사결정 체계를 갖춘 조직으로 진화  
*   **역할의 변화**: 소프트웨어 엔지니어들이 데이터 엔지니어링 분야로 진출하며, 기존의 모니터링 및 관측 기술(예: APM)을 데이터 인프라에 적용하려는 시도 증가  
*   **분업화의 심화**:  
    *   소프트웨어 엔지니어 → 클라우드 네이티브 애플리케이션, 마이크로서비스 아키텍처에 집중  
    *   데이터 엔지니어 → 데이터 파이프라인 구축 및 유지보수에 집중  

### 2.2 데이터 파이프라인의 중요성 및 문제점  

*   **핵심 인프라 역할**: 데이터 소스(DB, API, 로그 등)부터 분석/머신러닝 시스템까지 데이터를 안정적으로 전달하는 중추적 기능 수행  
*   **높은 의존도**: 대부분의 조직에서 데이터 흐름의 80~90%를 담당하며, 장애 시 전사적 업무 차질 발생  
*   **모니터링 격차**:  
    *   소프트웨어 분야: New Relic, Datadog 등의 APM 도구로 성능/장애 실시간 추적 가능  
    *   데이터 분야: 파이프라인 내 데이터 품질, 지연, 오류 등을 종합적으로 관측할 수 있는 도구 미흡  

### 2.3 데이터 관측 가능성의 정의 및 필요성  

*   **정의**: 데이터 파이프라인 전 구간(수집→변환→적재)에서 발생할 수 있는 이상 신호(장애, 지연, 품질 저하 등)를 실시간으로 탐지, 진단, 대응할 수 있는 능력  
*   **필요성**:  
    *   사후 대응형 모니터링의 한계 극복 (예: ETL 작업 실패 후 수동 확인)  
    *   데이터 품질 이슈(중복, NULL 값 증가 등)나 파이프라인 장애를 사전에 예방

### 2.4 기차 비유를 통한 데이터 관측 가능성 설명  

*   **기차 = 데이터 파이프라인**: 데이터를 운반하는 복잡한 시스템으로, 각 차량(처리 단계)이 안정적으로 연결되어야 함  
*   **첫 번째 질문: 기차가 움직이는가?**  
    *   **파이프라인 가동 상태**: 크론(Cron) 작업 실행 여부, Airflow DAG 실패 감지, 배치 처리 중단 등의 모니터링  
    *   **중요성**: 파이프라인 정지 시 데이터 유통이 끊겨 분석/보고서 시스템에 공백 발생  
*   **두 번째 질문: 기차는 얼마나 빨리 가는가?**  
    *   **처리 속도**: 예상 시간 대비 지연(예: 매일 6AM 완료되는 작업이 9AM까지 지연) 감지  
    *   **SLA 준수**: 데이터 팀과 비즈니스 팀 간 계약된 서비스 수준(예: "주문 데이터는 오전 8시 전에 반드시 제공") 충족 여부 확인  
*   **세 번째 질문: 기차 화물은 무엇인가?**  
    *   **데이터 품질**:  
        *   스키마 변경(예: 컬럼 추가/삭제) 감지  
        *   레코드 수 이상 감소(예: 일일 평균 100만 건 데이터가 10만 건으로 급감)  
        *   값의 분포 변화(예: 결제 금액 필드에 음수 값 출현)  
    *   **영향**: 품질 이슈가 ML 모델 성능 하락 또는 잘못된 의사결정으로 이어질 수 있음  
*   **네 번째 질문: 기차는 어디로 가는가?**  
    *   **데이터 리니지(Data Lineage)**:  
        *   데이터의 출처, 이동 경로, 변환 과정, 최종 소비자까지의 전체 흐름 추적  
        *   문제 발생 시 영향받는 다운스트림 시스템(예: 재고 관리 대시보드, 추천 엔진) 신속히 식별  
    *   **협업 효율화**: 관련 팀(예: 분석팀, 마케팅팀)에 사전 경고 가능  

### 2.5 데이터 관측 가능성의 핵심  

*   **자동화된 사고 관리**:  
    *   단순 알림을 넘어 근본 원인(RCA) 추적 및 해결 절차 자동화(예: 장애 시 관련 로그/메트릭 자동 수집)  
*   **다운스트림 보호**:  
    *   문제가 최종 사용자(예: CFO의 재무 보고서)에게 전파되기 전에 차단  
*   **비즈니스 연속성**:  
    *   데이터 장애로 인한 수익 손실(예: 실시간 광고 입찰 시스템 중단) 또는 고객 신뢰 하락 방지  

### 2.6 데이터 관측 가능성의 목표  

*   **조기 감지**:  
    *   근본 원인(소스 시스템 오류, 코드 버그 등)에서 즉시 문제 포착  
    *   예: 소스 DB의 커넥션 풀 고갈 → ETL 작업 실패로 확대되기 전에 경고  
*   **신속한 해결**:  
    *   영향 범위 시각화(예: "이 테이블 오류로 5개의 대시보드와 2개의 ML 모델 영향받음")를 통한 우선순위 설정  
*   **신뢰성 확보**:  
    *   데이터 SLA(예: "99.9% 가용성") 달성을 통해 내부/외부 스테이크홀더의 신뢰 구축  

### 2.7 결론  

*   **데이터 인프라의 핵심 요소**: 현대 데이터 스택(Modern Data Stack)에서 관측 가능성은 필수 요건  
*   **복잡성 해결**: 분산 환경(클라우드, 온프레미스 혼합)에서도 데이터 흐름을 투명하게 관리할 도구 필요  
*   **비유의 효과**: 기차 프레임워크를 통해 기술적 개념을 비기술적 이해관계자에게도 명확히 전달  
*   **액션 촉구**: 채널 구독을 통해 데이터 엔지니어링 최신 트렌드 지속적으로 학습 유도

## 3. 합성 데이터(Synthetic Data)
- 출처: [What is Synthetic Data? No, It's Not "Fake" Data
](https://www.youtube.com/watch?v=HIusawrGBN4&list=PLOspHqNVtKABpPMnwkx27tEO2KOBvHLu6&index=4)

### **3.1. 합성 데이터 정의**

*   **실제 데이터가 아닌 컴퓨터로 생성된 데이터.**  
    - 실제 데이터와 달리 물리적 수집 과정 없이 인공적으로 만들어진 데이터입니다. 이는 다양한 알고리즘 및 모델을 통해 구현됩니다.
    
*   **기존 데이터 세트, 알고리즘, 모델 등을 기반으로 실제 데이터의 속성 및 특성을 복제.**  
    - 합성 데이터는 실제 데이터의 패턴, 분포, 상관관계 등을 모방하도록 설계됩니다. 이를 통해 실제 데이터를 대체하거나 보완할 수 있습니다. 
    - 예를 들어, 의료 데이터의 경우 환자의 민감 정보를 제거하고 질병의 패턴만 복제하여 새로운 데이터 세트를 만들 수 있습니다.
    
*   **단순한 데이터 합성부터 딥러닝 모델까지 다양한 기술을 포괄하는 광범위한 개념.**  
    - 합성 데이터 생성은 단순한 랜덤 데이터 생성에서부터 GAN(Generative Adversarial Networks) 등 고도화된 딥러닝 기술을 활용한 복잡한 데이터 생성까지 폭넓게 적용될 수 있습니다. 이는 합성 데이터의 유연성과 확장성을 보여줍니다.

### **3.2 사용 목적 및 장점**

*   **데이터 확보의 어려움 해소:**  
    - 실제 데이터를 얻기 어렵거나 민감한 개인 정보/기밀 정보인 경우 활용. (금융 기록, 의료 기록 등)  
      - 특히 금융이나 의료 분야에서는 개인정보 보호법(GDPR, HIPAA 등)으로 인해 데이터 접근이 제한적입니다. 
      - 합성 데이터는 이러한 제약을 해결하면서도 데이터의 통계적 특성을 유지할 수 있습니다.

*   **저렴하고 쉬운 생산:**  
    - 실제 데이터 수집에는 시간과 비용이 많이 들지만, 합성 데이터는 프로그래밍 방식으로 빠르고 저렴하게 생성할 수 있습니다. 또한, 특정 요구사항에 맞춘 데이터를 쉽게 조정할 수 있습니다.
    
*   **정확한 레이블링:**  
    - 필요한 대로 정확하게 정의된 데이터를 얻을 수 있음. (실제 데이터는 불완전하거나 레이블링이 안 되어 있는 경우가 많음)  
      - 실제 데이터는 종종 노이즈가 포함되거나 레이블이 누락되어 있어 전처리 과정이 필요합니다. 
      - 반면 합성 데이터는 원하는 형태로 정확하게 레이블링된 상태로 생성됩니다.

*   **AI 및 머신러닝 학습 데이터 활용:**
    - **대량의 잘 레이블링된 합성 데이터로 모델 학습 가능.**  
      - AI 모델은 대규모의 레이블링된 데이터를 필요로 합니다. 합성 데이터는 이러한 데이터를 무한정 생성할 수 있어 모델 학습에 매우 유리합니다.
    - **실제 데이터 기반 모델보다 더 공정하고 정확하며 신뢰성 높은 AI 모델 구축 가능.**  
      - 실제 데이터는 편향(bias)이 포함될 가능성이 높습니다. 합성 데이터는 이러한 편향을 줄이고 균형 잡힌 데이터 세트를 제공할 수 있습니다.
    - **2025년까지 AI 학습에 필요한 실제 데이터 양이 70% 감소할 것으로 예상.**  
      - 합성 데이터 기술의 발전으로 인해 실제 데이터에 대한 의존도가 크게 줄어들 전망입니다. 이는 비용 절감과 효율성 증대에 기여할 것입니다.

*   **다양한 활용:**
    - **사기 탐지 알고리즘의 보안 취약점 테스트.**  
      - 합성 데이터를 사용하면 사기 시나리오를 가상으로 생성하여 알고리즘의 성능을 검증
    - **자율 주행 차량의 가상 시나리오 테스트 (실존하지 않는 도로 레이아웃).**  
      - 자율 주행 시스템은 다양한 환경에서 안전성을 검증해야 합니다. 합성 데이터는 현실 세계에서 발생하기 어려운 극한 상황을 시뮬레이션하여 테스트할 수 있게 해줍니다.
    - **모델의 편향성 최소화.**  
      - 특정 그룹에 치우친 데이터를 보완하거나 다양성을 확보할 수 있습니다.

### **3.3 도전 과제**

*   **현실 세계 요인 반영의 한계:**  
    - 실제 세계의 다양한 변수와 예측 불가능한 상황을 정확하게 반영하기 어려움.  
      - 예를 들어, 합성 데이터는 특정 조건 하에서만 유효할 수 있으며, 현실 세계의 복잡성과 예측 불가능성을 완벽히 재현하기는 어렵습니다.

*   **예시:** 10년 전 프리미어 리그 우승팀 예측 모델에서 레스터 시티 우승을 예측하기 어려웠던 점. (레스터 시티는 2015년 우승, 시즌 시작 전 우승 확률 5000:1)  
     - 레스터 시티의 우승은 예외적인 사건으로, 데이터 모델이 이를 예측하려면 매우 드문 사건들을 충분히 학습해야 했습니다. 그러나 합성 데이터는 이러한 예외적 사건을 미리 고려하기 어려울 수 있습니다.

### **3.4 생성 방법**

*   **기본 원리:** 필요한 데이터 유형 정의, 데이터 소스 식별, 사양에 따른 데이터 생성.  
    - 합성 데이터 생성은 먼저 목표를 설정하고, 어떤 데이터가 필요한지 명확히 정의하는 것으로 시작됩니다. 이후 적합한 알고리즘을 선택하고 데이터를 생성합니다.

*   **간단한 방법:** 기존 데이터 세트 조작 (노이즈 추가, 데이터 변환 등).  
    - 간단한 합성 데이터는 기존 데이터를 변형하여 생성할 수 있습니다. 예를 들어, 이미지 데이터에 노이즈를 추가하거나 숫자 데이터를 약간 변경하는 방식입니다.

*   **고급 기술:**
    - **GAN(Generative Adversarial Networks) 활용:** 기존 데이터 학습을 통해 새로운 데이터 생성.  
      - GAN은 생성기(generator)와 판별기(discriminator) 두 개의 네트워크로 구성되며, 생성기가 실제 데이터와 유사한 데이터를 생성하도록 학습합니다.
    - **합성 데이터 생성기 활용:** 수학 및 통계적 방법을 사용하여 특정 분포를 따르는 데이터 생성.  
      - 예를 들어, 정규 분포나 포아송 분포를 따르는 데이터를 생성할 수 있습니다. 이는 특정 통계적 특성을 가진 데이터를 필요로 할 때 유용합니다.

## 4. 데이터 리니지의 중요성
- 출처: [What is Data Lineage?](https://www.youtube.com/watch?v=Jar5Rr_7TOU&list=PLOspHqNVtKABpPMnwkx27tEO2KOBvHLu6&index=6)

### **4.1 데이터 리니지의 중요성 재확인**

* **데이터 기반 의사결정의 핵심 요소:** 
    - 현대 비즈니스 환경에서 데이터는 의사결정의 핵심 요소입니다. 
    - 하지만 데이터의 출처와 흐름을 제대로 파악하지 못하면 잘못된 의사결정으로 이어져 고객 이탈, 수익 감소, 평판 하락 등의 심각한 문제를 야기할 수 있습니다.
* **신뢰성 확보 및 오류 최소화:** 
    - 데이터 리니지는 데이터의 이력을 추적하고 출처를 명확히 함으로써 데이터의 정확성과 일관성을 검증하고 품질을 향상시킵니다. 
    - 이는 데이터에 대한 신뢰도를 높여 잘못된 데이터로 인한 오류를 최소화하고 데이터 기반 의사결정의 성공률을 높이는 데 기여합니다.

### **4.2 데이터 리니지의 개념 및 역할**

* **데이터의 여정 추적:** 
    - 데이터 리니지는 데이터가 생성되고 변환되는 모든 과정을 추적하고 기록하는 것을 의미합니다. 
    - 이를 통해 데이터의 출처, 변환 과정, 영향 등을 파악하고 데이터의 정확성과 일관성을 검증할 수 있습니다.
* **규제 준수 및 투명성 확보:** 
    - 데이터 리니지는 데이터 변환 과정을 명확히 보여줌으로써 규제 준수를 용이하게 하고 데이터 사용의 투명성을 확보합니다. 
    - 이는 기업의 신뢰성을 높이고 법적 위험을 줄이는 데 중요한 역할을 합니다.
* **데이터 소비자에게 신뢰할 수 있는 데이터 제공:** 
    - 데이터 과학자, 분석가, 감사자 등 데이터 소비자는 데이터 리니지 도구를 통해 데이터의 출처와 변환 과정을 확인하고 신뢰할 수 있는 데이터를 활용할 수 있습니다.
* **AI 모델 및 학습 데이터의 신뢰성 확보:** 
    - 데이터 리니지는 AI 모델 및 학습 데이터의 신뢰성을 확보하는 데 필수적인 기반을 제공합니다. 
    - 이는 AI 기술의 발전과 활용을 가속화하고 AI 기반 의사결정의 신뢰도를 높이는 데 기여합니다.

### **4.3 데이터 리니지 도구의 활용**

* **데이터 생명주기 전반에 걸친 자동 기록:** 데이터 리니지 도구는 데이터 생명주기 전반에 걸쳐 출처 정보, 변환 과정, 영향 분석 등을 자동으로 기록하고 관리합니다.
* **데이터 이력 추적 및 시각화:** 데이터 리니지 도구는 데이터의 이력을 추적하고 시각화하여 데이터의 흐름과 변화를 쉽게 파악할 수 있도록 돕습니다.
* **정확성 및 일관성 검증:** 데이터 리니지 도구는 데이터의 정확성과 일관성을 검증하여 데이터 품질을 향상시키고 오류를 최소화합니다.
* **규제 준수 지원:** 데이터 리니지 도구는 데이터 변환 과정을 명확히 보여줌으로써 규제 준수를 용이하게 하고 데이터 사용의 투명성을 확보합니다.

### **4.4 데이터 리니지의 중요성 강조**

* **데이터 신뢰도 확보 및 프로젝트 성공률 향상:** 데이터 리니지 솔루션을 통해 데이터에 대한 신뢰도를 확보하고 잘못된 데이터로 인해 프로젝트를 망치는 것을 방지할 수 있습니다.
* **IBM 데이터 리니지 솔루션:** IBM은 데이터 리니지 솔루션을 제공하여 기업들이 데이터의 신뢰성을 확보하고 데이터 기반 의사결정을 성공적으로 수행할 수 있도록 지원합니다.


## 5. 데이터 계약 (Data Contract)
- 출처: [What is a Data Contract?](https://www.youtube.com/watch?v=-n3OD-ml_k0&list=PLOspHqNVtKABpPMnwkx27tEO2KOBvHLu6&index=13)

### **5.1 문제 상황**

* 아들이 데이터 엔지니어로 일하며, 데이터 이슈 발생 시 급하게 연락하는 경우가 있음. 이는 데이터 품질이나 접근성 문제로 인해 발생하는 긴급 상황을 반영합니다.
* 데이터를 사용하는 팀에서 필요한 데이터가 제대로 제공되지 않아 불만이 발생하는 경우가 많습니다. 이러한 불만은 주로 상위 시스템의 문제로 인해 발생하며, 데이터의 신뢰성과 일관성이 결여된 상황

### **5.2 데이터 계약이란?**

* 데이터 계약은 데이터 생산자와 소비자 간의 공식적인 합의(계약)를 의미합니다. 이 계약은 데이터의 품질, 접근성, 사용 조건 등을 명확히 정의하여 양측의 기대치를 조율합니다.
* 목표:
    * 더 나은 문서화: 데이터의 메타데이터와 사용 규칙을 명확히 하여, 데이터 사용자가 필요한 정보를 쉽게 찾을 수 있도록 합니다.
    * 데이터 품질 향상: 데이터의 정확성, 일관성, 완전성을 보장하기 위한 규칙을 설정하여 데이터 품질을 높입니다.
    * 서비스 수준 계약(SLA) 개선: 데이터 제공자가 소비자에게 제공할 서비스의 품질과 가용성을 명확히 하여, 기대치를 관리합니다.
    * 궁극적으로 AI 비용 절감: 데이터 품질이 향상되면 모델 재학습의 필요성이 줄어들고, "쓰레기 입력 - 쓰레기 출력" 문제를 해결하여 AI 시스템의 효율성을 높입니다.

### **5.3 Open Data Contract Standard**

* Open Data Contract Standard는 Linux Foundation에서 지원하는 표준으로, 데이터 계약의 일관성과 신뢰성을 높이기 위해 개발되었습니다.
* 구성 요소 (8가지 섹션):
    1. **Demographics:** 계약의 이름, 버전, 상세 정보 등을 포함하여 계약의 기본 정보를 제공합니다.
    2. **Dataset & Schema:** 데이터의 내용 및 구조를 정의하여 데이터 소비자가 이해할 수 있도록 합니다.
    3. **Data Quality Rules:** 데이터 품질을 보장하기 위한 규칙을 명시하여 데이터의 신뢰성을 높입니다.
    4. **Pricing:** 데이터 공유에 대한 규칙을 설정하여 조직 내외부에서의 데이터 거래를 관리합니다.
    5. **Stakeholders:** 계약의 생성 및 유지 관리에 참여하는 사람들을 명시하여 책임을 분명히 합니다.
    6. **Security Access:** 데이터에 대한 보안 접근 규칙을 정의하여 데이터 보호를 강화합니다.
    7. **Service Level Agreements (SLA):** 서비스 수준 계약을 통해 데이터 제공자가 소비자에게 제공할 서비스의 품질을 명확히 합니다.
    8. **Custom Property:** 추후 참조 및 확장을 위한 사용자 정의 속성을 추가하여 계약의 유연성을 높입니다.

### **5.4 데이터 계약 적용 효과**

* 데이터 품질 규칙 및 SLA 개선을 통해 데이터 관련 문제를 예방할 수 있습니다. 이는 데이터의 신뢰성을 높이고, 데이터 소비자의 만족도를 향상시킵니다.
* 아들의 사례: 데이터 계약을 통해 고객에게 기대하는 데이터를 제공하고 문제 발생을 줄일 수 있으며, 이는 데이터 엔지니어링 팀의 효율성을 높이는 데 기여합니다. 데이터 계약은 데이터 생산자와 소비자 간의 신뢰를 구축하고, 데이터 사용의 일관성을 보장하는 중요한 도구입니다.


