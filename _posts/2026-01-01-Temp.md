### 빅데이터 분석 부트캠프 강의 계획안

아래는 제공된 자료를 기반으로 중복된 내용을 제거하고, 전체 커리큘럼을 논리적으로 연결되도록 재구성한 강의 계획안입니다. 이론:실습 비중을 3:7로 유지하며, 각 모듈 끝에 미니 프로젝트를 추가하여 연결성을 강화했습니다. 총 학습 기간은 약 37주(9개월)로, 초반 기초 모듈부터 후반 프로젝트 중심으로 점진적으로 심화됩니다. 각 모듈은 다음과 같은 구조로 정리했습니다:
- **설명**: 모듈의 목적과 핵심 포인트.
- **주요 내용**: 키워드 목록으로 압축.
- **실습 예제**: 중복 제거 후 대표적인 예시만 선별 (미니 프로젝트 포함).

| 모듈 번호 | 모듈 이름 | 학습 기간 | 설명 | 주요 내용 | 실습 예제 |
|----------|-----------|-----------|------|-----------|-----------|
| 1 | Python 기초 & 개발환경 (Git, Jupyter, Colab) | 2주 | 기초 문법과 개발 환경에 초점을 맞춰 데이터 분석 필수 요소를 익히며, 실습 중심으로 환경 설정을 숙달. | - Python 문법 (변수, 조건문, 반복문, 함수, 클래스)<br>- Git/GitHub 기본 사용법<br>- Jupyter Notebook/Lab, Google Colab 활용<br>- 가상환경 관리 (conda, venv) | - 간단한 계산기 프로그램 작성 (변수, 함수, 루프 활용).<br>- 로또 번호 생성기 (리스트, 랜덤 모듈, 반복문).<br>- 영화 평점 관리 시스템 (딕셔너리, 리스트).<br>- GitHub 리포지토리 생성 후 Jupyter 노트북 업로드 및 Colab 연동 (e.g., "Hello World" 스크립트 버전 관리).<br>- 간단한 텍스트 데이터 분석 (영화 리뷰 단어 카운트).<br>**미니 프로젝트**: 개인 GitHub 포트폴리오 저장소 구축 및 일일 커밋 (Jupyter로 주식 데이터 수집 → 기본 연산). |
| 2 | 데이터 수집 (웹스크래핑, OpenAPI, SQL 기초) | 3주 | 데이터 분석의 출발점으로 웹/API/SQL을 통해 데이터 수집 방법을 학습. | - 웹스크래핑 (requests, BeautifulSoup, Selenium)<br>- Open API 활용 (REST API, JSON 처리)<br>- SQL 기초 (SELECT, WHERE, GROUP BY, JOIN) | - 웹스크래핑: 네이버 뉴스 헤드라인 수집 → CSV 저장.<br>- OpenAPI: 공공데이터 포털 (e.g., 대기질 데이터) → JSON 파싱.<br>- SQL: SQLite로 가상의 "온라인 쇼핑몰 DB" 생성 → 쿼리 작성.<br>- BeautifulSoup로 위키피디아 페이지 스크래핑 (e.g., 영화 목록 추출).<br>- OpenWeather API로 날씨 데이터 수집 및 JSON 파싱.<br>**미니 프로젝트**: 네이버 뉴스 기사 크롤링 → 워드클라우드 생성; 기상청 OpenAPI → 기온 데이터 시각화; SQLite 학생 성적 DB 구축 후 SQL 쿼리. |
| 3 | Numpy & Pandas | 3주 | 데이터 분석 핵심 도구로 배열과 데이터프레임 조작을 중점 학습. | - Numpy 배열 연산, 브로드캐스팅, 인덱싱<br>- Pandas DataFrame/Series 조작<br>- 그룹화, 피벗, 병합 연산<br>- 시간 데이터 처리 | - Numpy: 이미지 데이터 (e.g., MNIST) 배열 처리 → 필터링.<br>- Pandas: Kaggle Titanic 데이터셋 로드, 필터링, 그룹화 (e.g., 생존율 계산).<br>- 주식 가격 CSV 불러오기 → 이동평균 계산.<br>- 올림픽 메달 집계 시스템 (groupby, pivot_table).<br>**미니 프로젝트**: Kaggle Titanic 데이터셋 → 요약 통계; COVID-19 확진자 데이터 지역별 집계; 이커머스 주문 데이터 분석 (merge, join). |
| 4 | 데이터 전처리 & 클리닝 | 2주 | 실제 데이터의 불완전성을 처리하는 기술 학습. | - 결측치 처리 (삭제, 대체, 보간)<br>- 이상치 탐지 및 처리 (IQR, Z-score)<br>- 데이터 타입 변환 및 정규화<br>- 중복 데이터 처리 | - 결측치: 의료 데이터 (e.g., 당뇨병 데이터셋) 대체 전략 비교.<br>- 이상치: IQR로 신용카드 사기 거래 탐지.<br>**미니 프로젝트**: Kaggle House Prices 데이터셋 → 결측치 imputation (mean/median), 이상치 제거 (IQR), 데이터 타입 변환; 전/후 비교 보고서 작성; 부동산 실거래가 데이터 정제. |
| 5 | 데이터 요약 & 시각화 (Matplotlib, Seaborn) | 2주 | 데이터 특징을 시각적으로 요약하고 전달하는 능력 배양. | - Matplotlib 기본 차트 (선, 막대, 산점도, 히스토그램)<br>- Seaborn 고급 시각화 (heatmap, pairplot, violin)<br>- 시각화 디자인 원칙 | - Matplotlib: 주식 가격 추이 선/막대 그래프.<br>- Seaborn: Iris 데이터셋 히트맵, 산점도 생성.<br>- 타이타닉 데이터: 성별/등급별 생존율 시각화.<br>**미니 프로젝트**: 서울시 교통량 데이터 시각화; 넷플릭스 콘텐츠 분석 대시보드; Airbnb 데이터 지역별 가격 분포 박스플롯. |
| 6 | 통계 기초 + 심화 | 3주 | 데이터 분석의 이론적 기반 마련, 가설 검정과 회귀 중심. | - 기술통계 (평균, 분산, 분포)<br>- 확률분포와 중심극한정리<br>- 가설검정 (t-test, chi-square, ANOVA)<br>- 상관분석, 회귀분석 기초<br>- 시계열 분석 입문 | - 가설검정: A/B 테스트 (e.g., 광고 버전 전환율 비교).<br>- 회귀분석: Boston Housing 데이터셋 가격 예측.<br>- SciPy로 t-검정; Statsmodels로 선형 회귀.<br>**미니 프로젝트**: A/B 테스트 모의실험 (웹페이지 클릭률); 소비자 지출 데이터 회귀분석; 코로나 일별 확진자 시계열 예측 (기초 ARIMA); 주식 가격 예측 모델 (시계열 분해). |
| 7 | EDA 프로젝트 (실제 데이터셋) | 2주 | 배운 내용을 종합하여 자율 EDA 수행. | - EDA 방법론<br>- 실제 공개데이터셋 탐색과 인사이트 도출 | - 자유 주제 EDA (e.g., 코로나19 데이터, 기상청 데이터).<br>**미니 프로젝트**: 서울시 공공자전거 이용 패턴 분석; 기업 재무제표 트렌드 분석; 영화 개봉 월별 매출 패턴 (TMDB 데이터); Kaggle Netflix 데이터셋 EDA (클리닝, 시각화, 인사이트 보고서). |
| 8 | 머신러닝 기초 (분류/회귀 + 성능평가) | 3주 | 머신러닝 기본 알고리즘과 평가 지표 학습. | - Scikit-learn 활용, train/test split, 교차검증<br>- 지도학습 (분류: 로지스틱회귀, 결정트리, 랜덤포레스트)<br>- 지도학습 (회귀: 선형회귀, 다항회귀, 릿지/라쏘)<br>- 비지도학습 (클러스터링: K-means, 계층적)<br>- 모델 평가 (정확도, 정밀도, 재현율, F1-score, AUC)<br>- 하이퍼파라미터 튜닝 | - 분류: Iris 데이터셋 KNN/Logistic Regression, Accuracy/Confusion Matrix.<br>- 회귀: California Housing 데이터셋 Random Forest, MSE/R2 스코어.<br>**미니 프로젝트**: 타이타닉 생존자 분류; 보스턴 주택가격 회귀; 고객 이탈 예측; 신용카드 사기 탐지; 자전거 대여 수요 예측 (시계열 반영). |
| 9 | AutoML & LLM 활용 데이터 분석 | 1주 | 자동화 도구와 LLM을 활용한 효율적 분석 소개. | - AutoML (AutoViz, PyCaret)<br>- LLM 활용 데이터 분석 (ChatGPT, Claude API)<br>- 클라우드 플랫폼 기초 (AWS S3, BigQuery)<br>- ETL 파이프라인 구축 | - PyCaret: Titanic 데이터셋 AutoML (모델 비교).<br>- LLM: OpenAI API로 데이터 요약/쿼리 기반 분석 (e.g., 텍스트 감성 분석).<br>**미니 프로젝트**: PyCaret으로 Bank Marketing 고객 이탈 예측 자동화; ChatGPT API로 자동 리포트 생성; Google Cloud 대용량 데이터 처리. |
| 10 | 데이터 엔지니어링 기초 (SQL 심화, ETL, 클라우드 DW 맛보기) | 2주 | 데이터 관리와 파이프라인 기본 학습. | - SQL 고급 (JOIN, 윈도우 함수)<br>- ETL 개념<br>- Google BigQuery or AWS Redshift 맛보기 | - SQL: 10만 행 데이터 윈도우 함수 (e.g., 매출 누적 합계).<br>- ETL: AWS S3 → Pandas → PostgreSQL 파이프라인.<br>**미니 프로젝트**: Python으로 ETL 파이프라인 (CSV → DB 적재); BigQuery 공개 데이터셋 쿼리; PostgreSQL 조인/서브쿼리 (E-commerce 데이터셋). |
| 11 | 데이터 시각화 & 전달 (Tableau, Dash/Streamlit, 스토리텔링) | 2주 | 분석 결과를 시각화하고 전달하는 기술. | - 인터랙티브 대시보드<br>- 데이터 스토리텔링 기법 | - Tableau: COVID-19 데이터 대시보드 (지도/차트 결합).<br>- Streamlit: 사용자 입력 기반 Pandas 분석 대시보드.<br>**미니 프로젝트**: Tableau 판매 데이터 대시보드 (인터랙티브 필터); Streamlit 머신러닝 예측 웹앱; EDA 결과 슬라이드 제작 및 발표 연습. |
| 12 | 산업별 개인 프로젝트 | 4주 | 개인 전문성 강화, 심화 프로젝트. | - 프로젝트 유형 선택 (금융, 마케팅, 헬스케어, 이커머스, 부동산 등) | - 금융: 주식 투자 전략 백테스팅.<br>- 마케팅: 소셜미디어 브랜드 감성 분석.<br>- 헬스케어: 웨어러블 데이터 건강 모니터링.<br>**미니 프로젝트**: 2가지 결과물 작성 (e.g., EDA+시각화, ML 엔드투엔드). GitHub 포트폴리오 업로드. |
| 13 | 산업별 팀 프로젝트 (프로젝트 발표 & 피드백) | 8주 | 협업 능력 개발, 취업 포트폴리오용 종합 프로젝트. | - 팀 구성 (4-5명, 역할 분담: PM, DA, DE, Viz)<br>- 주제 선정 → 데이터 수집 → 분석 → 모델링 → 대시보드 → 발표<br>- 실제 기업 데이터 또는 공모전 활용 | - 금융: 주식 데이터 위험 분석.<br>- 마케팅: 고객 이탈 예측.<br>- 헬스케어: 환자 질병 예측.<br>**미니 프로젝트**: Kaggle 스타일 (e.g., 사기 탐지 모델링); Tableau/Power BI 대시보드; Streamlit 웹앱; GitHub 업로드 및 mock interview. 평가 기준: 문제 정의(20%), 데이터 품질(25%), 분석 적절성(25%), 시각화/스토리(20%), 협업(10%). |
| 14 | 최종 결과물 | - (별도 기간 없음, 전체 과정 통합) | 포트폴리오 품질 관리와 취업 지원 강조. | - 개인 프로젝트 2가지, 팀 프로젝트 2가지 (EDA+시각화, ML 엔드투엔드)<br>- GitHub 레포지토리 구조화<br>- Notion/Github Pages 웹사이트 구축 | - 포트폴리오: /portfolio/ (Personal/Project1-2, Team/Project1_EDA, Project2_ML, daily_practice).<br>- 강조 포인트: "데이터 수집 → 분석 → 시각화 → 배포" 전 과정 경험.