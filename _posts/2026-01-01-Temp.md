# PCA(주성분 분석) 알고리즘 쉬운 해석

## PCA란 무엇인가?
**PCA(Principal Component Analysis, 주성분 분석)**는 고차원 데이터를 저차원으로 축소하는 대표적인 차원 축소 기법입니다. 쉽게 말해, 복잡한 데이터에서 가장 중요한 정보만 뽑아내는 방법입니다.

## 왜 PCA를 사용할까?

### 실생활 예시로 이해하기
학생 100명의 성적표가 있다고 가정해봅시다. 각 학생마다 10개 과목 점수가 있습니다.
- 원본 데이터: 100명 × 10과목 = 1000개의 숫자
- PCA 적용 후: 100명 × 2개 주요 요인 = 200개의 숫자

이때 2개 주요 요인은 "문과 성향"과 "이과 성향" 같은 개념일 수 있습니다.

## 코드 단계별 해석

### 1. 클래스 초기화
```python
def __init__(self, n_components):
    self.n_components = n_components  # 축소할 차원 수
    self.components = None           # 주성분들 (나중에 계산됨)
    self.mean = None                # 데이터 평균 (나중에 계산됨)
```

**의미**: PCA 객체를 만들고, 몇 개의 주성분을 사용할지 정합니다.

### 2. 학습 단계 (fit 메서드)

#### 2-1. 평균 중심화 (Mean Centering)
```python
self.mean = np.mean(X, axis=0)  # 각 특성의 평균 계산
X = X - self.mean               # 모든 데이터에서 평균을 빼기
```

**왜 하는가?**: 데이터의 중심을 원점(0,0)으로 옮깁니다.
- **예시**: 키와 몸무게 데이터가 있다면, 평균 키 170cm, 평균 몸무게 70kg를 각각 0으로 만듭니다.

#### 2-2. 공분산 행렬 계산
```python
cov = np.cov(X.T)  # 공분산 행렬 계산
```

**공분산이란?**: 두 변수가 함께 변하는 정도
- 양수: 한 변수가 커지면 다른 변수도 커짐 (키가 클수록 몸무게도 무거움)
- 음수: 한 변수가 커지면 다른 변수는 작아짐
- 0에 가까움: 두 변수는 관계없음

#### 2-3. 고유벡터와 고유값 계산
```python
eigenvectors, eigenvalues = np.linalg.eig(cov)
```

**고유벡터(Eigenvector)**: 데이터가 가장 많이 퍼진 방향
**고유값(Eigenvalue)**: 그 방향으로 얼마나 많이 퍼졌는지

**직관적 이해**:
- 럭비공을 생각해보세요
- 가장 긴 축 = 1번째 주성분 (가장 큰 고유값)
- 두 번째로 긴 축 = 2번째 주성분
- 가장 짧은 축 = 마지막 주성분

#### 2-4. 주성분 정렬 및 선택
```python
idxs = np.argsort(eigenvalues)[::-1]  # 고유값 큰 순서로 정렬
eigenvalues = eigenvalues[idxs]
eigenvectors = eigenvectors[idxs]
self.components = eigenvectors[:self.n_components]  # 상위 n개만 선택
```

**의미**: 가장 중요한 방향들만 골라냅니다.

### 3. 변환 단계 (transform 메서드)
```python
def transform(self, X):
    X = X - self.mean                    # 평균 중심화
    return np.dot(X, self.components.T)  # 주성분으로 투영
```

**변환 과정**:
1. 새로운 데이터도 같은 평균으로 중심화
2. 주성분 방향으로 데이터를 투영 (그림자 만들기)

## 실제 예시 분석

코드에서 사용한 아이리스 데이터셋:
- **원본**: 150개 꽃 × 4개 특성 (꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비)
- **PCA 후**: 150개 꽃 × 2개 주성분

### 결과 해석
```python
print("Shape of X:", X.shape)              # (150, 4)
print("Shape of transformed X:", X_projected.shape)  # (150, 2)
```

4차원 데이터가 2차원으로 축소되었습니다!

## PCA의 장점과 한계

### 장점
- **차원 축소**: 저장 공간 절약, 계산 속도 향상
- **시각화 가능**: 고차원 데이터를 2D/3D로 볼 수 있음
- **노이즈 제거**: 중요하지 않은 정보 제거

### 한계
- **해석의 어려움**: 주성분이 실제로 무엇을 의미하는지 알기 어려움
- **선형 관계만**: 비선형 관계는 잡아내지 못함
- **정보 손실**: 차원을 줄이면서 일부 정보는 사라짐

## 언제 PCA를 사용하나?

1. **고차원 데이터 시각화**: 100차원 데이터를 2D 그래프로 보고 싶을 때
2. **전처리**: 다른 머신러닝 알고리즘 전에 차원 축소
3. **저장 공간 절약**: 큰 데이터를 압축해서 저장
4. **노이즈 제거**: 데이터의 주요 패턴만 추출

## 핵심 기억할 점

1. **PCA = 데이터에서 가장 중요한 방향 찾기**
2. **주성분 = 데이터가 가장 많이 퍼진 방향**
3. **차원 축소 = 중요한 정보만 남기고 나머지 버리기**
4. **손실 압축 = 100% 복원은 불가능하지만 주요 정보는 보존**

이렇게 PCA는 복잡한 수학 공식 뒤에 숨어있지만, 실제로는 "데이터에서 가장 중요한 패턴을 찾아 간단하게 표현하는" 직관적인 아이디어입니다!