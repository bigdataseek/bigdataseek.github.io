---
title: 42차시 1:NeuralNine
layout: single
classes: wide
categories:
  - NeuralNine
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 1. Wikipedia 기반 RAG (Retrieval Augmented Generation) 시스템 구축
- 출처: [Wikipedia RAG System in Python - Beginner Tutorial with LlamaIndex](https://www.youtube.com/watch?v=M9GtHb32F8w)

### 1.1 개요

  * **목표:** 방대한 정보를 담고 있는 Wikipedia를 기반으로 사용자의 질문에 정확하고 관련성 높은 답변을 제공하는 RAG (Retrieval Augmented Generation) 시스템을 구축합니다. 이 시스템은 특정 주제에 대한 심층적인 이해를 돕고, 정보 검색 시간을 단축하며, 다양한 관점을 제공할 수 있습니다.
  * **기술 스택:** 시스템 개발에는 다음과 같은 강력한 기술들이 활용됩니다.
      * **Python:** 높은 생산성과 다양한 라이브러리 지원으로 데이터 처리, 모델 통합, 웹 애플리케이션 개발 등 전반적인 시스템 구축에 사용됩니다.
      * **Llama Index:** RAG 파이프라인의 핵심 엔진 역할을 수행하며, 데이터 로딩, 인덱싱, 검색, 답변 생성 등 복잡한 과정을 추상화하여 개발자가 비즈니스 로직에 집중할 수 있도록 지원합니다.
      * **Streamlit:** 개발된 RAG 시스템을 사용자가 웹 브라우저를 통해 직관적으로 상호작용할 수 있는 사용자 친화적인 인터페이스를 쉽고 빠르게 구축할 수 있도록 돕습니다.
  * **핵심 컨셉:** 이 시스템은 다음의 핵심 컨셉들을 기반으로 작동합니다.
      * **RAG (Retrieval Augmented Generation):** 
        *   질문에 대한 답변을 생성할 때, 먼저 Wikipedia에서 질문과 관련된 정보를 효율적으로 검색(Retrieval)하고, 검색된 정보를 기반으로 답변을 생성(Generation)하는 방식입니다. 이는 LLM(Large Language Model)이 가진 지식의 한계를 보완하고, 최신 정보나 특정 도메인에 특화된 답변을 제공할 수 있도록 합니다.
      * **Llama Index:** 
        *   다양한 데이터 소스에 대한 연결, 텍스트 분할, 벡터 임베딩 생성, 검색 엔진 구축 등 복잡한 RAG 파이프라인 구축 과정을 단순화하고 자동화합니다. 이를 통해 개발자는 몇 줄의 코드로 강력한 검색 및 답변 기능을 구현할 수 있습니다.
      * **Streamlit:** 
        *   Python 스크립트만으로 데이터 시각화, 모델 데모, 간단한 웹 애플리케이션 등을 빠르게 개발하고 배포할 수 있도록 지원하는 라이브러리입니다. 이를 통해 개발자는 RAG 시스템의 기능을 사용자가 쉽게 경험하고 활용할 수 있는 인터페이스를 구축할 수 있습니다.

### 1.2 개발 과정

1.  **필요 패키지 설치:** 시스템 구축에 필요한 핵심 라이브러리들을 `pip` 명령어를 사용하여 설치합니다.
      * `streamlit`: 
        *   웹 인터페이스 구축을 위한 필수 라이브러리입니다.
      * `llama-index`: 
        *   RAG 시스템의 핵심 기능을 제공합니다.
      * `python-dotenv` (선택 사항이지만 권장): 
        *   중요한 API 키와 같은 환경 변수를 코드 외부의 `.env` 파일에서 안전하게 관리할 수 있도록 돕습니다.
      * `llama-index-embeddings-openai`: 
        *   OpenAI의 강력한 임베딩 모델을 Llama Index에서 쉽게 사용할 수 있도록 지원합니다. 텍스트 데이터를 의미론적 벡터 공간으로 변환하여 검색의 정확도를 높입니다.
      * `llama-index-llms-openai`: 
        *   OpenAI의 다양한 LLM (GPT-4, gpt-3.5-turbo 등)을 Llama Index와 통합하여 답변 생성 기능을 구현합니다.
      * `llama-index-readers-wikipedia`: 
        *   Wikipedia API를 통해 특정 문서의 내용을 Llama Index로 쉽게 불러올 수 있도록 지원합니다.
2.  **OpenAI API 키 설정:** OpenAI의 LLM과 임베딩 모델을 사용하기 위해서는 API 키가 필요합니다.
      * [OpenAI 개발자 플랫폼](https://platform.openai.com/)에서 계정을 생성하고 API 키를 발급받습니다.
      * 발급받은 API 키를 코드에 직접 문자열 형태로 입력할 수도 있지만, 보안을 위해 `.env` 파일을 생성하여 API 키를 저장하고 `python-dotenv` 라이브러리를 사용하여 로드하는 방식을 권장합니다. `.env` 파일은 Git과 같은 버전 관리 시스템에서 제외하여 API 키가 외부에 노출되지 않도록 주의해야 합니다.
3.  **Wikipedia 문서 선택:** RAG 시스템의 지식 기반으로 활용할 특정 Wikipedia 문서의 제목 리스트를 준비합니다. 
    *   예를 들어, '인공지능', '머신러닝', '자연어 처리' 등 관심 있는 분야의 문서를 선택할 수 있습니다.
4.  **코드 구현 (main.py):** RAG 시스템의 핵심 로직과 Streamlit 웹 인터페이스를 구현하는 Python 스크립트(`main.py`)를 작성합니다.
      * 필요한 라이브러리들을 import 합니다: 
        *   `os`, `streamlit`, `load_dotenv` (선택 사항), `OpenAI`, `OpenAIEmbedding`, `WikipediaReader`, `VectorStoreIndex`, `StorageContext`, `load_index_from_storage`.
      * `.env` 파일이 존재한다면 `load_dotenv()` 함수를 호출하여 환경 변수를 로드합니다.
      * Vector Index를 저장할 디렉토리 경로를 정의합니다. 이는 생성된 Index를 재사용하여 시스템의 효율성을 높이는 데 사용됩니다.
      * **`get_index()` 함수:** 이 함수는 Wikipedia 문서를 기반으로 Vector Store Index를 생성하거나, 이미 생성된 Index가 있다면 로컬 저장소에서 불러오는 역할을 합니다.
          * 정의된 Index 저장 디렉토리가 존재하는지 확인합니다.
          * 디렉토리가 존재하면 `load_index_from_storage` 함수를 사용하여 저장된 Index를 불러옵니다. 이를 통해 매번 새로운 질문마다 Wikipedia 문서를 다시 로드하고 인덱싱하는 비효율성을 방지합니다.
          * 디렉토리가 존재하지 않으면, `WikipediaReader`를 사용하여 지정된 Wikipedia 문서들을 로드합니다.
          * `OpenAIEmbedding` 모델을 사용하여 로드된 각 문서의 내용을 벡터 임베딩으로 변환합니다. 임베딩은 텍스트의 의미를 수치화한 표현으로, 의미적으로 유사한 텍스트는 벡터 공간에서 가까운 거리에 위치하게 됩니다.
          * `VectorStoreIndex.from_documents()` 함수를 사용하여 문서와 해당 임베딩을 기반으로 Vector Store Index를 생성합니다. 이 Index는 질문과 관련된 문서를 효율적으로 검색하는 데 사용됩니다.
          * 생성된 Index를 정의된 디렉토리에 저장하여 이후 재사용할 수 있도록 합니다.
          * 생성 또는 로드된 Index 객체를 반환합니다.
      * **`get_query_engine()` 함수:** 
          *   이 함수는 생성된 Vector Store Index를 기반으로 질문에 답변할 수 있는 Query Engine을 생성.
          * `OpenAI` LLM 모델 (예: `gpt-3.5-turbo`, `gpt-4`)을 지정하고, 답변의 창의성을 조절하는 `temperature` 파라미터를 설정합니다 (0에 가까울수록 결정적인 답변을 생성).
          * Index 객체의 `as_query_engine()` 메서드를 호출하여 Query Engine을 생성합니다. 이때, 사용할 LLM 모델과 검색할 관련 문서의 개수(`similarity_top_k`) 등을 설정할 수 있습니다. Llama Index는 내부적으로 질문과 Index 내의 문서 임베딩 간의 유사도를 계산하여 가장 관련 있는 문서를 찾고, 이를 LLM에 전달하여 답변을 생성하는 과정을 자동화합니다.
          * 생성된 Query Engine 객체를 반환합니다.
      * **`main()` 함수:** 
          * 이 함수는 Streamlit 웹 인터페이스를 구축하고 사용자 입력을 처리하며 답변을 화면에 표시하는 역할을 합니다.
          * `streamlit.title()` 함수를 사용하여 웹 애플리케이션의 제목을 표시합니다.
          * `streamlit.text_input()` 함수를 사용하여 사용자로부터 질문을 입력받을 수 있는 텍스트 입력 상자를 만듭니다.
          * `streamlit.button()` 함수를 사용하여 질문 제출 버튼을 생성합니다.
          * 버튼이 클릭되면, `get_query_engine()` 함수를 호출하여 Query Engine을 얻고, 사용자가 입력한 질문에 대해 `query()` 메서드를 사용하여 답변을 생성합니다.
          * 생성된 답변을 `streamlit.write()` 함수를 사용하여 화면에 표시합니다.
          * 답변과 함께, 답변 생성의 근거가 된 검색된 Wikipedia 문서의 내용 (context)을 함께 표시하여 답변의 신뢰도를 높이고 사용자의 이해를 돕습니다.
5.  **실행:** 개발된 Streamlit 웹 애플리케이션을 실행합니다.
      * 터미널을 열고 `main.py` 파일이 있는 디렉토리로 이동합니다.
      * `streamlit run main.py` 명령어를 실행합니다.
      * 명령어가 성공적으로 실행되면, 웹 브라우저가 자동으로 열리면서 개발된 RAG 시스템의 인터페이스를 확인할 수 있습니다.
      * 웹 인터페이스의 질문 입력 창에 질문을 입력하고 제출 버튼을 클릭하여 답변을 확인합니다.

### 1.3 핵심 코드 설명

  * **Wikipedia 문서 로드:** 
    *   지정된 Wikipedia 페이지 제목 리스트를 기반으로 해당 문서의 내용을 불러옵니다. `auto_suggest=False` 옵션은 정확한 페이지 제목만 로드하도록 설정합니다.

    ```python
    reader = WikipediaReader()
    documents = reader.load_data(pages=pages, auto_suggest=False)
    ```

  * **Vector Store Index 생성:** 
    *   로드된 Wikipedia 문서들을 OpenAI 임베딩 모델을 사용하여 벡터화하고, 이를 기반으로 Vector Store Index를 구축합니다. 이 Index는 의미 기반 검색을 가능하게 합니다.

    ```python
    embedding_model = OpenAIEmbedding(model_name="text-embedding-3-small")
    index = VectorStoreIndex.from_documents(documents, embedding_model=embedding_model)
    ```

  * **Query Engine 생성:** 
    *   생성된 Vector Store Index와 OpenAI LLM을 연결하여 질문에 답변할 수 있는 Query Engine을 만듭니다. `similarity_top_k=3`은 답변 생성 시 가장 관련 있는 상위 3개의 문서를 참고하도록 설정합니다.

    ```python
    llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
    query_engine = index.as_query_engine(llm=llm, similarity_top_k=3)
    ```

  * **질문 및 답변:** 
    *   사용자가 입력한 질문을 Query Engine에 전달하고, 생성된 답변을 `response` 변수에 저장합니다.

    ```python
    response = query_engine.query(question)
    ```

### 1.4 장점

  * **간편한 RAG 시스템 구축:** 
    *   Llama Index는 복잡한 RAG 파이프라인을 추상화하여 몇 줄의 코드로 강력한 기능을 구현할 수 있도록 지원합니다. 데이터 로딩부터 검색, 답변 생성까지의 과정을 효율적으로 관리할 수 있습니다.
  * **쉬운 웹 인터페이스 개발:** 
    *   Streamlit은 Python 스크립트만으로 대화형 웹 애플리케이션을 빠르게 구축할 수 있도록 해줍니다. 이를 통해 개발자는 RAG 시스템의 기능을 사용자가 쉽게 접근하고 사용할 수 있는 인터페이스를 직관적으로 만들 수 있습니다.
  * **풍부한 지식 기반 활용:** 
    *   세계 최대의 온라인 백과사전인 Wikipedia를 지식 베이스로 활용하여 다양한 분야의 질문에 대해 폭넓고 깊이 있는 답변을 제공할 수 있습니다.
  * **최신 정보 접근 용이:** 
    *   Wikipedia는 지속적으로 업데이트되므로, 시스템은 비교적 최신 정보를 기반으로 답변을 생성할 수 있습니다.

### 1.5 개선 방향

  * **임베딩 모델 성능 향상:** 
    *   더 크고 성능이 우수한 임베딩 모델 (예: `text-embedding-ada-002`, Cohere Embed)을 사용하여 검색의 정확도와 답변의 관련성을 높일 수 있습니다.
  * **다양한 데이터 소스 통합:** 
    *   Wikipedia 외에도 다양한 형태의 데이터 소스 (예: 내부 문서, 웹사이트, 데이터베이스)를 통합하여 답변의 범위를 넓히고 특정 요구사항에 맞는 정보를 제공할 수 있도록 확장할 수 있습니다. 
    *   Llama Index는 다양한 데이터 로더를 지원합니다.
  * **사용자 인터페이스 개선:** 
    *   Streamlit의 다양한 기능을 활용하여 더욱 직관적이고 사용하기 쉬운 웹 인터페이스를 구축할 수 있습니다. 예를 들어, 검색 결과 미리보기, 답변의 출처 표시, 사용자 피드백 반영 기능 등을 추가할 수 있습니다.
  * **답변 정확도 및 관련성 향상:** 
    *   프롬프트 엔지니어링 기법을 적용하여 LLM이 더욱 정확하고 맥락에 맞는 답변을 생성하도록 유도할 수 있습니다. 또한, 특정 도메인에 대한 파인튜닝을 통해 답변의 전문성을 높일 수 있습니다.
  * **검색 알고리즘 최적화:** 
    *   Llama Index가 제공하는 다양한 검색 알고리즘 및 파라미터 조정을 통해 검색 성능을 최적화하고, 사용자의 질문 의도에 더욱 부합하는 정보를 찾을 수 있도록 개선할 수 있습니다.
  * **캐싱 및 성능 최적화:** 
    *   대규모 데이터에 대한 인덱싱 및 검색 성능을 최적화하기 위해 캐싱 전략을 적용하고, 필요에 따라 더 강력한 컴퓨팅 자원을 활용할 수 있습니다.


    