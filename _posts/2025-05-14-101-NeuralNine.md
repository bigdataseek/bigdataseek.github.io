---
title: 42차시 1:NeuralNine
layout: single
classes: wide
categories:
  - NeuralNine
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 1. Wikipedia 기반 RAG (Retrieval Augmented Generation) 시스템 구축
- 출처: [Wikipedia RAG System in Python - Beginner Tutorial with LlamaIndex](https://www.youtube.com/watch?v=M9GtHb32F8w)

### 1.1 개요

  * **목표:** 방대한 정보를 담고 있는 Wikipedia를 기반으로 사용자의 질문에 정확하고 관련성 높은 답변을 제공하는 RAG (Retrieval Augmented Generation) 시스템을 구축합니다. 이 시스템은 특정 주제에 대한 심층적인 이해를 돕고, 정보 검색 시간을 단축하며, 다양한 관점을 제공할 수 있습니다.
  * **기술 스택:** 시스템 개발에는 다음과 같은 강력한 기술들이 활용됩니다.
      * **Python:** 높은 생산성과 다양한 라이브러리 지원으로 데이터 처리, 모델 통합, 웹 애플리케이션 개발 등 전반적인 시스템 구축에 사용됩니다.
      * **Llama Index:** RAG 파이프라인의 핵심 엔진 역할을 수행하며, 데이터 로딩, 인덱싱, 검색, 답변 생성 등 복잡한 과정을 추상화하여 개발자가 비즈니스 로직에 집중할 수 있도록 지원합니다.
      * **Streamlit:** 개발된 RAG 시스템을 사용자가 웹 브라우저를 통해 직관적으로 상호작용할 수 있는 사용자 친화적인 인터페이스를 쉽고 빠르게 구축할 수 있도록 돕습니다.
  * **핵심 컨셉:** 이 시스템은 다음의 핵심 컨셉들을 기반으로 작동합니다.
      * **RAG (Retrieval Augmented Generation):** 
        *   질문에 대한 답변을 생성할 때, 먼저 Wikipedia에서 질문과 관련된 정보를 효율적으로 검색(Retrieval)하고, 검색된 정보를 기반으로 답변을 생성(Generation)하는 방식입니다. 이는 LLM(Large Language Model)이 가진 지식의 한계를 보완하고, 최신 정보나 특정 도메인에 특화된 답변을 제공할 수 있도록 합니다.
      * **Llama Index:** 
        *   다양한 데이터 소스에 대한 연결, 텍스트 분할, 벡터 임베딩 생성, 검색 엔진 구축 등 복잡한 RAG 파이프라인 구축 과정을 단순화하고 자동화합니다. 이를 통해 개발자는 몇 줄의 코드로 강력한 검색 및 답변 기능을 구현할 수 있습니다.
      * **Streamlit:** 
        *   Python 스크립트만으로 데이터 시각화, 모델 데모, 간단한 웹 애플리케이션 등을 빠르게 개발하고 배포할 수 있도록 지원하는 라이브러리입니다. 이를 통해 개발자는 RAG 시스템의 기능을 사용자가 쉽게 경험하고 활용할 수 있는 인터페이스를 구축할 수 있습니다.

### 1.2 개발 과정

1.  **필요 패키지 설치:** 시스템 구축에 필요한 핵심 라이브러리들을 `pip` 명령어를 사용하여 설치합니다.
      * `streamlit`: 
        *   웹 인터페이스 구축을 위한 필수 라이브러리입니다.
      * `llama-index`: 
        *   RAG 시스템의 핵심 기능을 제공합니다.
      * `python-dotenv` (선택 사항이지만 권장): 
        *   중요한 API 키와 같은 환경 변수를 코드 외부의 `.env` 파일에서 안전하게 관리할 수 있도록 돕습니다.
      * `llama-index-embeddings-openai`: 
        *   OpenAI의 강력한 임베딩 모델을 Llama Index에서 쉽게 사용할 수 있도록 지원합니다. 텍스트 데이터를 의미론적 벡터 공간으로 변환하여 검색의 정확도를 높입니다.
      * `llama-index-llms-openai`: 
        *   OpenAI의 다양한 LLM (GPT-4, gpt-3.5-turbo 등)을 Llama Index와 통합하여 답변 생성 기능을 구현합니다.
      * `llama-index-readers-wikipedia`: 
        *   Wikipedia API를 통해 특정 문서의 내용을 Llama Index로 쉽게 불러올 수 있도록 지원합니다.
2.  **OpenAI API 키 설정:** OpenAI의 LLM과 임베딩 모델을 사용하기 위해서는 API 키가 필요합니다.
      * [OpenAI 개발자 플랫폼](https://platform.openai.com/)에서 계정을 생성하고 API 키를 발급받습니다.
      * 발급받은 API 키를 코드에 직접 문자열 형태로 입력할 수도 있지만, 보안을 위해 `.env` 파일을 생성하여 API 키를 저장하고 `python-dotenv` 라이브러리를 사용하여 로드하는 방식을 권장합니다. `.env` 파일은 Git과 같은 버전 관리 시스템에서 제외하여 API 키가 외부에 노출되지 않도록 주의해야 합니다.
3.  **Wikipedia 문서 선택:** RAG 시스템의 지식 기반으로 활용할 특정 Wikipedia 문서의 제목 리스트를 준비합니다. 
    *   예를 들어, '인공지능', '머신러닝', '자연어 처리' 등 관심 있는 분야의 문서를 선택할 수 있습니다.
4.  **코드 구현 (main.py):** RAG 시스템의 핵심 로직과 Streamlit 웹 인터페이스를 구현하는 Python 스크립트(`main.py`)를 작성합니다.
      * 필요한 라이브러리들을 import 합니다: 
        *   `os`, `streamlit`, `load_dotenv` (선택 사항), `OpenAI`, `OpenAIEmbedding`, `WikipediaReader`, `VectorStoreIndex`, `StorageContext`, `load_index_from_storage`.
      * `.env` 파일이 존재한다면 `load_dotenv()` 함수를 호출하여 환경 변수를 로드합니다.
      * Vector Index를 저장할 디렉토리 경로를 정의합니다. 이는 생성된 Index를 재사용하여 시스템의 효율성을 높이는 데 사용됩니다.
      * **`get_index()` 함수:** 이 함수는 Wikipedia 문서를 기반으로 Vector Store Index를 생성하거나, 이미 생성된 Index가 있다면 로컬 저장소에서 불러오는 역할을 합니다.
          * 정의된 Index 저장 디렉토리가 존재하는지 확인합니다.
          * 디렉토리가 존재하면 `load_index_from_storage` 함수를 사용하여 저장된 Index를 불러옵니다. 이를 통해 매번 새로운 질문마다 Wikipedia 문서를 다시 로드하고 인덱싱하는 비효율성을 방지합니다.
          * 디렉토리가 존재하지 않으면, `WikipediaReader`를 사용하여 지정된 Wikipedia 문서들을 로드합니다.
          * `OpenAIEmbedding` 모델을 사용하여 로드된 각 문서의 내용을 벡터 임베딩으로 변환합니다. 임베딩은 텍스트의 의미를 수치화한 표현으로, 의미적으로 유사한 텍스트는 벡터 공간에서 가까운 거리에 위치하게 됩니다.
          * `VectorStoreIndex.from_documents()` 함수를 사용하여 문서와 해당 임베딩을 기반으로 Vector Store Index를 생성합니다. 이 Index는 질문과 관련된 문서를 효율적으로 검색하는 데 사용됩니다.
          * 생성된 Index를 정의된 디렉토리에 저장하여 이후 재사용할 수 있도록 합니다.
          * 생성 또는 로드된 Index 객체를 반환합니다.
      * **`get_query_engine()` 함수:** 
          *   이 함수는 생성된 Vector Store Index를 기반으로 질문에 답변할 수 있는 Query Engine을 생성.
          * `OpenAI` LLM 모델 (예: `gpt-3.5-turbo`, `gpt-4`)을 지정하고, 답변의 창의성을 조절하는 `temperature` 파라미터를 설정합니다 (0에 가까울수록 결정적인 답변을 생성).
          * Index 객체의 `as_query_engine()` 메서드를 호출하여 Query Engine을 생성합니다. 이때, 사용할 LLM 모델과 검색할 관련 문서의 개수(`similarity_top_k`) 등을 설정할 수 있습니다. Llama Index는 내부적으로 질문과 Index 내의 문서 임베딩 간의 유사도를 계산하여 가장 관련 있는 문서를 찾고, 이를 LLM에 전달하여 답변을 생성하는 과정을 자동화합니다.
          * 생성된 Query Engine 객체를 반환합니다.
      * **`main()` 함수:** 
          * 이 함수는 Streamlit 웹 인터페이스를 구축하고 사용자 입력을 처리하며 답변을 화면에 표시하는 역할을 합니다.
          * `streamlit.title()` 함수를 사용하여 웹 애플리케이션의 제목을 표시합니다.
          * `streamlit.text_input()` 함수를 사용하여 사용자로부터 질문을 입력받을 수 있는 텍스트 입력 상자를 만듭니다.
          * `streamlit.button()` 함수를 사용하여 질문 제출 버튼을 생성합니다.
          * 버튼이 클릭되면, `get_query_engine()` 함수를 호출하여 Query Engine을 얻고, 사용자가 입력한 질문에 대해 `query()` 메서드를 사용하여 답변을 생성합니다.
          * 생성된 답변을 `streamlit.write()` 함수를 사용하여 화면에 표시합니다.
          * 답변과 함께, 답변 생성의 근거가 된 검색된 Wikipedia 문서의 내용 (context)을 함께 표시하여 답변의 신뢰도를 높이고 사용자의 이해를 돕습니다.
5.  **실행:** 개발된 Streamlit 웹 애플리케이션을 실행합니다.
      * 터미널을 열고 `main.py` 파일이 있는 디렉토리로 이동합니다.
      * `streamlit run main.py` 명령어를 실행합니다.
      * 명령어가 성공적으로 실행되면, 웹 브라우저가 자동으로 열리면서 개발된 RAG 시스템의 인터페이스를 확인할 수 있습니다.
      * 웹 인터페이스의 질문 입력 창에 질문을 입력하고 제출 버튼을 클릭하여 답변을 확인합니다.

### 1.3 핵심 코드 설명

  * **Wikipedia 문서 로드:** 
    *   지정된 Wikipedia 페이지 제목 리스트를 기반으로 해당 문서의 내용을 불러옵니다. `auto_suggest=False` 옵션은 정확한 페이지 제목만 로드하도록 설정합니다.

    ```python
    reader = WikipediaReader()
    documents = reader.load_data(pages=pages, auto_suggest=False)
    ```

  * **Vector Store Index 생성:** 
    *   로드된 Wikipedia 문서들을 OpenAI 임베딩 모델을 사용하여 벡터화하고, 이를 기반으로 Vector Store Index를 구축합니다. 이 Index는 의미 기반 검색을 가능하게 합니다.

    ```python
    embedding_model = OpenAIEmbedding(model_name="text-embedding-3-small")
    index = VectorStoreIndex.from_documents(documents, embedding_model=embedding_model)
    ```

  * **Query Engine 생성:** 
    *   생성된 Vector Store Index와 OpenAI LLM을 연결하여 질문에 답변할 수 있는 Query Engine을 만듭니다. `similarity_top_k=3`은 답변 생성 시 가장 관련 있는 상위 3개의 문서를 참고하도록 설정합니다.

    ```python
    llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
    query_engine = index.as_query_engine(llm=llm, similarity_top_k=3)
    ```

  * **질문 및 답변:** 
    *   사용자가 입력한 질문을 Query Engine에 전달하고, 생성된 답변을 `response` 변수에 저장합니다.

    ```python
    response = query_engine.query(question)
    ```

### 1.4 장점

  * **간편한 RAG 시스템 구축:** 
    *   Llama Index는 복잡한 RAG 파이프라인을 추상화하여 몇 줄의 코드로 강력한 기능을 구현할 수 있도록 지원합니다. 데이터 로딩부터 검색, 답변 생성까지의 과정을 효율적으로 관리할 수 있습니다.
  * **쉬운 웹 인터페이스 개발:** 
    *   Streamlit은 Python 스크립트만으로 대화형 웹 애플리케이션을 빠르게 구축할 수 있도록 해줍니다. 이를 통해 개발자는 RAG 시스템의 기능을 사용자가 쉽게 접근하고 사용할 수 있는 인터페이스를 직관적으로 만들 수 있습니다.
  * **풍부한 지식 기반 활용:** 
    *   세계 최대의 온라인 백과사전인 Wikipedia를 지식 베이스로 활용하여 다양한 분야의 질문에 대해 폭넓고 깊이 있는 답변을 제공할 수 있습니다.
  * **최신 정보 접근 용이:** 
    *   Wikipedia는 지속적으로 업데이트되므로, 시스템은 비교적 최신 정보를 기반으로 답변을 생성할 수 있습니다.

### 1.5 개선 방향

  * **임베딩 모델 성능 향상:** 
    *   더 크고 성능이 우수한 임베딩 모델 (예: `text-embedding-ada-002`, Cohere Embed)을 사용하여 검색의 정확도와 답변의 관련성을 높일 수 있습니다.
  * **다양한 데이터 소스 통합:** 
    *   Wikipedia 외에도 다양한 형태의 데이터 소스 (예: 내부 문서, 웹사이트, 데이터베이스)를 통합하여 답변의 범위를 넓히고 특정 요구사항에 맞는 정보를 제공할 수 있도록 확장할 수 있습니다. 
    *   Llama Index는 다양한 데이터 로더를 지원합니다.
  * **사용자 인터페이스 개선:** 
    *   Streamlit의 다양한 기능을 활용하여 더욱 직관적이고 사용하기 쉬운 웹 인터페이스를 구축할 수 있습니다. 예를 들어, 검색 결과 미리보기, 답변의 출처 표시, 사용자 피드백 반영 기능 등을 추가할 수 있습니다.
  * **답변 정확도 및 관련성 향상:** 
    *   프롬프트 엔지니어링 기법을 적용하여 LLM이 더욱 정확하고 맥락에 맞는 답변을 생성하도록 유도할 수 있습니다. 또한, 특정 도메인에 대한 파인튜닝을 통해 답변의 전문성을 높일 수 있습니다.
  * **검색 알고리즘 최적화:** 
    *   Llama Index가 제공하는 다양한 검색 알고리즘 및 파라미터 조정을 통해 검색 성능을 최적화하고, 사용자의 질문 의도에 더욱 부합하는 정보를 찾을 수 있도록 개선할 수 있습니다.
  * **캐싱 및 성능 최적화:** 
    *   대규모 데이터에 대한 인덱싱 및 검색 성능을 최적화하기 위해 캐싱 전략을 적용하고, 필요에 따라 더 강력한 컴퓨팅 자원을 활용할 수 있습니다.


## 2. Ragas를 사용한 RAG 시스템 평가 튜토리얼

- 출처: [Evaluate AI Agents in Python with Ragas](https://www.youtube.com/watch?v=dOKHuw52YTA)

### 2.1 **주요 내용**

*   **Ragas 패키지를 사용하여 컨텍스트 기반 LLM 응답의 품질을 평가하는 방법 학습**: 
    * Ragas는 RAG(Retrieval Augmented Generation) 시스템의 성능을 평가하기 위한 오픈소스 프레임워크로, LLM(대규모 언어 모델)과 검색된 컨텍스트 간의 상호작용을 분석합니다. 이를 통해 시스템이 얼마나 정확하고 유용한 답변을 제공하는지 평가할 수 있습니다.
*   **RAG 시스템에서 검색된 컨텍스트와 답변의 품질을 평가**: 
    * RAG는 검색(retrieval)과 생성(generation)을 결합한 시스템으로, 외부 문서에서 관련 정보를 검색한 뒤 이를 기반으로 LLM이 답변을 생성합니다. Ragas는 이 과정에서 검색된 문서의 적합성과 생성된 답변의 품질을 측정합니다.
*   **LLM을 활용하여 답변의 정확성, 관련성, 충실도, 컨텍스트 정밀도 및 재현율을 측정**: 
    * Ragas는 다양한 메트릭을 제공하여 답변의 품질을 다각도로 평가합니다. 예를 들어, 답변이 사실에 기반하는지, 질문과 관련 있는지, 검색된 정보를 얼마나 잘 반영하는지를 확인할 수 있습니다.

### 2.2 **설치**

*   **터미널에서 `pip install jupyterlab` 실행 후 Jupyter Lab 실행**: 
    *   Jupyter Lab은 Python 코드를 대화형으로 실행할 수 있는 웹 기반 환경입니다. 설치 후 `jupyter lab` 명령어로 실행하여 코드 작성 및 테스트를 시작합니다.
*   **필요한 패키지 설치**: 
    *   Ragas와 RAG 시스템을 구현하기 위해 다음 패키지를 설치합니다.
    *   **`pip install faiss-cpu` 또는 `pip install faiss-gpu`**:
        * Faiss는 Facebook에서 개발한 효율적인 벡터 검색 라이브러리로, 텍스트를 벡터로 변환한 뒤 유사한 문서를 빠르게 검색하는 데 사용됩니다. GPU를 사용할 수 있다면 `faiss-gpu`를 설치하여 성능을 향상시킬 수 있습니다.
    *   **`pip install openai`**: 
        *   OpenAI API를 호출하여 LLM(예: GPT-4)과 임베딩 모델(예: text-embedding-3-small)사용.
    *   **`pip install datasets`**: 
        *   Hugging Face의 데이터셋 라이브러리로, 평가에 사용할 데이터셋을 쉽게 로드하고 처리.
    *   **`pip install ragas`**: 
        *   Ragas 프레임워크를 설치하여 평가 메트릭을 계산합니다.
    *   **`pip install "langchain[llms]`**: 
        *   LangChain은 LLM과 외부 데이터를 연결하는 데 프레임워크로, RAG 시스템을 쉽게 구현.
    *   **`pip install langchain-community`**: 
        *   LangChain의 커뮤니티 확장 패키지로, 추가적인 도구와 통합 기능을 제공합니다.
*   **`requirements.txt` 파일이 제공될 경우 `pip install -r requirements.txt` 실행**: 
    *   여러 패키지를 한 번에 설치하려면, 필요한 패키지 목록이 담긴 `requirements.txt` 파일을 사용하면 편리합니다. 
    *   예: `pip install -r requirements.txt`는 파일에 명시된 모든 패키지를 자동으로 설치합니다.

### 2.3 **RAG 시스템 설정 (선택 사항)**

*   **문서 (지식 기반) 준비**: 
    *   RAG 시스템은 외부 문서(지식 기반)에서 정보를 검색하여 답변을 생성합니다.
    *   **예**: 문서로 "파리는 프랑스의 수도이다", "제인 오스틴은 오만과 편견으로 유명한 영국 소설가이다" 등을 준비합니다. 이는 시스템이 질문에 답변할 때 참조할 정보입니다. 실제로는 PDF, 웹 페이지, 데이터베이스 등 다양한 소스에서 문서를 수집할 수 있습니다.
*   **Embedding 모델 및 LLM 설정**: 
    *   텍스트를 벡터로 변환하는 임베딩 모델(예: OpenAI의 `text-embedding-3-small`)과 답변을 생성하는 LLM(예: `GPT-4`)을 선택합니다. 임베딩 모델은 텍스트를 고차원 벡터로 변환하여 유사성 검색을 가능하게 합니다.
*   **텍스트 임베딩 함수 정의**: 
    *   문서와 쿼리를 벡터로 변환하는 함수를 작성합니다. 
    *   예: OpenAI의 임베딩 API를 호출하여 텍스트를 1536차원 벡터로 변환.
*   **벡터 데이터베이스 (Faiss) 생성 및 문서 추가**:
    *    Faiss를 사용해 변환된 벡터를 저장하고, 쿼리와 유사한 문서를 검색할 수 있는 데이터베이스를 구축. 
    *   예: 문서 벡터를 Faiss 인덱스에 추가하여 빠른 검색을 지원.
*   **사용자 쿼리 기반으로 관련 컨텍스트 검색 함수 정의**: 
    *   사용자가 입력한 질문(예: "파리의 수도는 어디인가?")을 벡터로 변환한 뒤 Faiss에서 유사한 문서를 검색합니다.
*   **검색된 컨텍스트를 기반으로 답변 생성 함수 정의**: 
    *   검색된 문서를 LLM에 제공하여 질문에 맞는 답변을 생성합니다. 
    *   예: "파리는 프랑스의 수도이다"라는 문서를 기반으로 "파리는 프랑스의 수도입니다"라는 답변 생성.

### 2.4 **Ragas를 사용한 평가**

1.  **평가 데이터 세트 생성**:
    *   평가를 위해 질문, 검색된 컨텍스트, LLM이 생성한 답변, 정답(ground truth/reference)을 포함한 데이터셋을 준비합니다. 
    *   예: 질문: "제인 오스틴은 누구인가?", 컨텍스트: "제인 오스틴은 오만과 편견으로 유명한 영국 소설가이다", LLM 답변: "제인 오스틴은 영국 소설가로, 오만과 편견을 썼습니다", 정답: "제인 오스틴은 19세기 영국 소설가로, 오만과 편견의 저자이다".
2.  **Ragas 메트릭 가져오기**:
    *   Ragas에서 제공하는 메트릭(`answer_correctness`, `answer_relevancy`, `faithfulness`, `context_precision`, `context_recall`)을 가져와 평가에 사용합니다. 각 메트릭은 특정 측면의 품질을 평가합니다.
3.  **`evaluate` 함수를 사용하여 데이터 세트 및 메트릭으로 평가 실행**:
    * Ragas의 `evaluate` 함수에 데이터셋과 메트릭을 입력하여 평가를 수행합니다. 
    * 예: `result = evaluate(dataset, metrics=[answer_correctness, faithfulness])`를 실행하면 각 질문에 대한 메트릭 점수가 반환됩니다.

### 2.5 **각 메트릭 설명**

*   **Answer Correctness**: 
    *   답변의 사실적 정확도를 평가합니다. 
    *   예: "파리는 프랑스의 수도입니다"라는 답변이 정답과 얼마나 일치하는지 확인합니다.
*   **Answer Relevancy**: 
    *   답변이 질문과 얼마나 관련 있는지를 측정합니다. 
    *   예: 질문이 "파리의 수도는?"인데 답변이 "파리는 아름다운 도시입니다"라면 관련성이 낮습니다.
*   **Faithfulness**: 
    *   답변이 검색된 컨텍스트에 얼마나 충실한지를 평가합니다. 
    *   예: 컨텍스트에 없는 정보를 답변에 포함하면 충실도가 낮아집니다.
*   **Context Precision**: 
    *   검색된 컨텍스트가 질문과 관련 있는 정보를 얼마나 정확히 포함하는지를 측정합니다. 
    *   예: 질문과 무관한 문서가 포함되면 정밀도가 낮습니다.
*   **Context Recall**: 
    *   검색된 컨텍스트가 질문에 필요한 모든 정보를 포함하는지를 평가합니다. 
    *   예: "파리의 수도"에 대한 질문에서 "파리는 프랑스의 수도이다"가 빠지면 재현율이 낮습니다.

### 2.6 **중요 사항**

*   **일부 메트릭은 LLM을 사용하여 계산해야 하므로 완벽한 수학적 공식이 아닐 수 있음**: 
    *   Ragas는 LLM(예: GPT-4)을 사용해 메트릭을 계산하므로, 결과는 LLM의 판단에 따라 약간의 주관성이 있을 수 있습니다.
*   **메트릭 값은 서로 반드시 연관되지 않을 수 있음**: 
    *   예를 들어, 잘못된 컨텍스트를 충실히 따르는 답변은 `faithfulness`는 높지만 `answer_correctness`는 낮을 수 있습니다.
*   **평가 결과 해석 시 주의**: 
    *   메트릭 값은 시스템 개선을 위한 참고 자료로 사용되며, 단일 메트릭에만 의존하기보다는 전체적인 성능을 고려해야 합니다.

### 2.7 **Ollama 통합**

*   **Ollama를 사용하여 로컬에서 LLM을 실행하고 평가에 활용 가능**: 
    *   Ollama는 로컬 환경에서 오픈소스 LLM(예: LLaMA)을 실행할 수 있는 도구입니다. 이를 통해 OpenAI API 없이도 평가를 수행할 수 있습니다.
*   **Langchain을 사용하여 Ollama 모델 로드 및 Ragas와 호환되도록 래핑**:
    * LangChain의 `ChatOllama` 클래스를 사용하여 Ollama 모델을 로드하고, Ragas의 평가 메트릭과 통합할 수 있습니다. 예: `llm = ChatOllama(model="llama3")`.

### 2.8 **OpenAI API 키 설정**

*   **`.env` 파일에 본인의 OpenAI API 키를 `OPENAI_API_KEY=YOUR_API_KEY` 형식으로 저장**: 
*   OpenAI API를 사용하려면 API 키를 설정해야 합니다. `.env` 파일을 프로젝트 디렉토리에 생성하고, `OPENAI_API_KEY=sk-xxx` 형식으로 키를 입력합니다. Python에서는 `python-dotenv` 패키지를 사용해 이 파일을 로드할 수 있습니다.

### 2.9 **결론**

Ragas는 LLM 기반 RAG 시스템의 품질을 평가하는 강력한 도구로, 답변의 정확성, 관련성, 충실도와 검색된 컨텍스트의 정밀도, 재현율을 체계적으로 분석할 수 있습니다. 이를 통해 RAG 시스템의 성능을 객관적으로 평가하고 개선점을 도출할 수 있습니다. 초보자는 튜토리얼을 따라 설치와 설정을 단계적으로 진행하며, 실제 데이터로 평가를 실행해보면서 Ragas의 유용성을 체감할 수 있습니다.


## 3. AWS Bedrock & Python을 활용한 생성형 AI 애플리케이션 개발
- 출처: [Deploy Generative AI Models with Amazon Bedrock & Python](https://www.youtube.com/watch?v=wLNBsr_JKuc&t=229s)

### 3.1   **AWS Bedrock** 
자체 AI 모델 관리 부담 없이 foundation model을 활용하여 생성형 AI 애플리케이션을 쉽게 구축하고 확장할 수 있는 완전 관리형 서비스.

*    AWS Bedrock은 대규모 언어 모델(LLM)과 같은 foundation model을 API 형태로 제공하여, 사용자가 모델 훈련, 서버 관리, 인프라 설정 등의 복잡한 작업 없이 생성형 AI 애플리케이션을 빠르게 개발할 수 있도록 지원합니다. 예를 들어, 챗봇, 텍스트 생성, 이미지 생성 등 다양한 AI 애플리케이션을 간단히 구현할 수 있습니다.
*   **로드 밸런싱, 확장 등의 관리 필요 없이 사용량 기반 과금 모델 적용:** Bedrock은 서버리스 아키텍처를 기반으로 하여, 사용자가 인프라 관리(예: 서버 확장, 부하 분산)를 신경 쓸 필요 없이 요청한 만큼만 비용을 지불하는 구조입니다. 이는 소규모 스타트업부터 대기업까지 비용 효율적으로 AI를 도입할 수 있게 합니다.
*   **Llama, Claude, Mistral 등 다양한 모델 지원:** Meta AI의 Llama, Anthropic의 Claude, Mistral AI의 Mistral 등 최신 foundation model을 지원하며, 각 모델은 텍스트 생성, 요약, 번역 등 특정 작업에 최적화되어 있습니다. 사용자는 애플리케이션 요구사항에 따라 적합한 모델을 선택할 수 있습니다.

### 3.2   **사용 이유** 
AI 모델 직접 관리의 어려움 해소, On-Demand 방식의 편리한 사용, 합리적인 가격.

*    AI 모델을 직접 관리하려면 GPU 서버 설정, 모델 최적화, 지속적인 업데이트 등 상당한 자원과 전문 지식이 필요합니다. Bedrock은 이러한 복잡성을 제거하고, API 호출로 즉시 사용 가능한(On-Demand) 환경을 제공합니다. 또한, 사용량 기반 과금 방식은 초기 투자 비용을 줄이고, 소규모 프로젝트에서도 AI를 쉽게 도입할 수 있도록 합니다. 예를 들어, 테스트 단계에서는 소량의 API 호출로 저렴하게 시작할 수 있습니다.

### 3.3   **모델 선택** 
제공 모델 및 지원 지역 확인 필요 (특히, 유럽의 경우 GDPR 등 규제 준수 중요).

*    AWS Bedrock은 지역별로 지원되는 모델이 다를 수 있으므로, AWS Management Console 또는 공식 문서에서 사용 가능한 모델과 지역을 확인해야 합니다. 특히 유럽 지역에서는 GDPR(일반 데이터 보호 규정) 준수를 위해 데이터 처리 및 저장 위치를 신중히 고려해야 합니다. 예를 들어, Claude 모델은 특정 지역에서만 사용 가능하며, 데이터 주권 요구사항을 충족하는 지역을 선택해야 할 수 있습니다.

### 3.4   **Python 연동 방법**
1.  **AWS CLI 설치:** [설치 링크](https://aws.amazon.com/cli/) 참조.
    *    AWS CLI(Command Line Interface)는 AWS 서비스를 터미널에서 관리할 수 있는 도구입니다. Bedrock API 호출을 위해 Python에서 AWS SDK를 사용하기 전에, CLI를 설치하여 계정 인증 및 설정을 간편히 수행할 수 있습니다. 설치 후 `aws --version` 명령어로 제대로 설치되었는지 확인.
2.  **AWS 계정 설정:** 액세스 키 ID 및 액세스 키 필요 (보안 자격 증명에서 생성).
    *    AWS Management Console에서 IAM(Identity and Access Management) 메뉴로 이동해 액세스 키를 생성합니다. 이 키는 Python 코드에서 Bedrock API를 호출할 때 인증 정보로 사용됩니다. 보안상, 액세스 키는 안전한 곳에 저장하고 외부에 노출되지 않도록 주의해야 합니다.
3.  **터미널 설정:** `aws configure` 명령어 실행 후 액세스 키 ID, 액세스 키, 지역 이름, 출력 형식 입력.
    *    터미널에서 `aws configure`를 실행하면, AWS CLI에 자격 증명을 설정할 수 있는 대화형 인터페이스가 열립니다. 예를 들어, `us-east-1` 또는 `eu-west-1`과 같은 지역 이름을 입력하고, 출력 형식을 `json`으로 설정하면 Bedrock API 호출 결과가 JSON 형식으로 반환됩니다.
4.  **Python 패키지 설치:**
    *    Python 환경에서 Bedrock과 상호작용하려면 몇 가지 필수 및 선택 패키지를 설치해야 합니다.
    *   **`boto3`:** AWS SDK for Python으로, Bedrock API 호출에 필수입니다. `pip install boto3`로 설치합니다.
    *   **`pydantic`, `instructor`:** (선택 사항) 구조화된 데이터를 처리하고자 할 때 유용합니다. 예를 들어, `pydantic`은 데이터 유효성 검사와 직렬화를 지원하며, `instructor`는 모델 출력에서 원하는 데이터 구조를 추출하는 데 도움을 줍니다. 설치 명령어는 `pip install pydantic instructor`입니다.
5.  **Python 코드 작성:**
    *    Python 코드를 통해 Bedrock API를 호출하는 과정은 다음과 같습니다.
    *   **`boto3`를 사용하여 Bedrock 런타임 클라이언트 생성:** 예를 들어, `boto3.client('bedrock-runtime', region_name='us-east-1')`로 클라이언트를 초기화합니다.
    *   **모델에 맞는 API 요청 파라미터 설정:** 각 모델(Llama, Claude 등)은 고유한 파라미터 형식을 요구합니다. AWS Bedrock 문서에서 해당 모델의 API 요청 형식을 확인하세요. 예: Claude 모델은 `max_tokens`와 `temperature` 같은 파라미터를 설정할 수 있습니다.
    *   **`invoke_model` 메서드를 사용하여 모델 호출 및 결과 스트림 처리:** `invoke_model`은 요청한 모델에 프롬프트를 보내고 결과를 반환합니다. 예를 들어, 텍스트 생성 요청 후 JSON 응답을 파싱하여 원하는 데이터를 추출할 수 있습니다.
6.  **구조화된 출력 활용 (선택 사항):**
    *    구조화된 출력은 AI 모델의 출력을 특정 데이터 형식(예: JSON 객체)으로 정리하여 애플리케이션에서 쉽게 활용할 수 있도록 합니다.
    *   **`instructor` 패키지를 사용하여 Bedrock 런타임 클라이언트 초기화:** `instructor`는 Bedrock 클라이언트와 통합되어 모델 출력을 구조화된 형태로 변환합니다.
    *   **`pydantic`의 `BaseModel`을 상속받아 원하는 데이터 구조 정의:** 예를 들어, 챗봇 응답에서 사용자 이름과 메시지만 추출하도록 데이터 클래스를 정의할 수 있습니다.
    *   **`client.completions.create` 메서드에 모델 ID, 메시지, 응답 모델 정보 전달:** `instructor`를 사용하면 모델 ID(예: `anthropic.claude-v2`)와 프롬프트를 전달하여 원하는 출력 형식을 보장할 수 있습니다.

### 3.5 **요약**
*   AWS Bedrock은 AI 모델 관리에 대한 복잡성을 줄이고, Python을 통해 생성형 AI 애플리케이션을 효율적으로 개발할 수 있도록 돕는 강력한 서비스입니다. AWS CLI를 설정하고, `boto3`와 같은 Python 패키지를 설치한 후, 모델별 API 요청 파라미터를 적절히 설정하여 모델을 호출하면 됩니다. 추가로 `instructor`와 `pydantic` 패키지를 사용하면 모델 출력을 구조화하여 더 정교한 애플리케이션을 구현할 수 있습니다. 초보자는 간단한 텍스트 생성부터 시작해, 점차 복잡한 워크플로우로 확장해 나갈 수 있습니다.

**추가 학습 제안**
*   **SageMaker, Google Vertex AI 등 다른 플랫폼에 대한 정보:** AWS SageMaker는 모델 훈련과 배포에 중점을 둔 반면, Google Vertex AI는 Google Cloud 환경에서 유사한 생성형 AI 기능을 제공합니다. 두 플랫폼을 비교하여 프로젝트 요구사항에 맞는 최적의 선택을 고려하세요.
*   **AWS Bedrock 심화 기능 (커스텀 모델 등):** Bedrock은 커스텀 모델 파인튜닝 및 임베딩 생성과 같은 고급 기능을 제공합니다. 예를 들어, 특정 도메인에 특화된 데이터를 사용하여 모델을 미세 조정할 수 있습니다.

**참고:** UV 패키지 매니저는 pip의 대안으로, 더 빠른 의존성 관리와 프로젝트 환경 설정을 제공합니다. 하지만 초보자는 익숙한 `pip`를 사용해도 충분하며, `pip install boto3 pydantic instructor`로 필요한 패키지를 설치할 수 있습니다.

## 4. Alpha Evolve
- 출처: [Deep Dive Into Google's AlphaEvolve](https://www.youtube.com/watch?v=j-9S6wjr9eI&t=110s)

### **4.1 개요**

*   Google에서 개발한 Gemini 기반의 혁신적인 코딩 에이전트, **Alpha Evolve** 소개  
    *   Alpha Evolve는 인공지능이 코드를 자율적으로 작성하고 최적화하는 도구로, 기존의 코딩 방식과 차별화된 접근법을 제공합니다.  
    *   이 시스템은 Google의 최신 언어 모델인 Gemini를 기반으로 하며, 복잡한 컴퓨팅 문제를 해결하기 위해 진화 알고리즘과 대규모 언어 모델(LLM)을 결합한 하이브리드 접근 방식을 채택했습니다.  
*   Alpha Evolve는 자체적으로 코드를 개선하며, 새로운 행렬 곱셈 방식 발견, TPU(텐서 처리 장치) 회로 설계 최적화, Gemini 모델 학습 속도 향상 등의 성과를 달성했습니다.  
    *   예를 들어, 행렬 곱셈 최적화는 수학적 연산의 효율성을 높여 Google의 대규모 컴퓨팅 작업에서 상당한 성능 개선을 가져왔습니다.  
*   본 영상은 Alpha Evolve의 핵심 내용 및 동작 방식을 **Google의 백서**를 기반으로 심층 분석합니다.  
    *   백서는 Alpha Evolve의 알고리즘 구조, 성과, 한계 등을 상세히 다루며, 이 기술이 어떻게 AI의 자율적 코드 생성을 가능케 했는지 설명합니다.

### 4.2 **Alpha Evolve의 주요 특징**

*   **진화 알고리즘 기반의 코딩 에이전트** (유전자 알고리즘과 유사)  
    *   진화 알고리즘은 생물학적 진화에서 영감을 받아, 해결책을 생성하고 변형하며 최적의 솔루션을 도출하는 방식입니다.  
    *   해결책 생성, 변형, 결합을 통해 반복적으로 성능을 개선합니다. 예를 들어, 초기 코드를 생성한 후 이를 변형하거나 다른 코드와 결합하여 더 나은 성능을 가진 코드를 만듭니다.  
    *   **평가 함수 (Fitness function)**를 통해 코드의 성능을 정량적으로 측정하고, 이를 바탕으로 개선 방향을 결정합니다. 예를 들어, 행렬 곱셈의 경우 스칼라 곱셈 횟수를 최소화하는 방향으로 평가합니다.  
*   **최첨단 LLM의 성능을 크게 향상**  
    *   Gemini 2.0 flash 및 pro 모델을 사용하여 빠르고 효율적인 코드 생성 및 최적화를 수행합니다. (참고: Gemini 2.5는 본 시스템에서 사용되지 않음)  
    *   LLM 파이프라인을 자율적으로 관리하여, 코드 작성부터 평가, 최적화까지 전 과정을 자동화합니다. 이는 인간 개발자의 개입을 최소화합니다.  
*   **자동 평가 기능이 있는 문제에 적용 가능**  
    *   수학적으로 명확한 평가 기준이 있는 문제(예: 연산 속도, 메모리 사용량)에 적합합니다.  
    *   예를 들어, 수학 문제나 컴파일러 최적화처럼 정량적 지표로 성과를 측정할 수 있는 작업에 강점을 발휘합니다.  
*   **새로운 알고리즘 발견 및 기존 알고리즘 개선 가능**  
    *   1969년 Strassen 알고리즘 이후 56년 만에 행렬 곱셈 알고리즘을 개선하여, 특정 행렬 연산에서 연산 횟수를 줄이는 성과를 달성했습니다.  
    *   기존 알고리즘을 최적화하거나, 완전히 새로운 알고리즘을 창안할 수 있는 잠재력을 보유하고 있습니다. 예를 들어, 특정 문제에 대해 인간이 발견하지 못한 새로운 해법을 제안할 수 있습니다.

### 4.3 **Alpha Evolve 동작 방식**

1.  **문제 정의:**  
    *   사용자가 해결하고자 하는 문제의 평가 기준, 초기 솔루션(코드 또는 알고리즘), 배경 지식을 제공.  
    *   예: 행렬 곱셈 문제의 경우, 행렬 크기와 목표(스칼라 곱셈 횟수 최소화)를 정의합니다.  
2.  **프롬프트 생성:**  
    *   Alpha Evolve가 문제의 문맥, 과거 시도, 성공/실패 사례, 새로운 아이디어를 포함한 프롬프트를 생성.  
    *   이 프롬프트는 Gemini 모델이 이해하고 처리할 수 있는 형태로, 최적의 코드를 생성하도록 유도.  
3.  **LLM 앙상블:**  
    *   Gemini 2.0 flash 및 pro 모델이 협력하여 개선된 프로그램(코드 또는 알고리즘)을 제안합니다.  
    *   여러 모델이 다양한 관점에서 솔루션을 생성하므로, 단일 모델보다 더 창의적이고 효율적인 결과를 도출할 가능성이 높습니다.  
4.  **평가:**  
    *   결정론적 평가자(예: 연산 속도 측정기, 메모리 사용량 분석기)를 통해 생성된 프로그램의 성능을 평가.  
    *   이 과정은 완전히 자동화되어, 인간의 주관적 판단 없이 객관적인 성과를 측정합니다.  
5.  **프로그램 데이터베이스:**  
    *   평가 점수를 기반으로 프로그램을 데이터베이스에 저장하여 지식 베이스를 구축합니다.  
    *   이 데이터베이스는 이후 반복에서 성공 사례를 참조하거나 실패 사례를 피하는 데 사용됩니다.  
6.  **반복:**  
    *   프롬프트 샘플러가 데이터베이스의 성공/실패 사례를 활용하여 다음 프롬프트를 생성하고, 1~5 단계를 반복합니다.  
    *   이 과정은 최적의 솔루션이 발견될 때까지, 또는 정해진 반복 횟수에 도달할 때까지 계속됩니다.

### 4.4 **Alpha Evolve의 성과**

*   **행렬 곱셈:**  
    *   4x4 복소 행렬 곱셈에서 필요한 스칼라 곱셈 횟수를 49회에서 48회로 단축하며, 1969년 Strassen 알고리즘 이후 56년 만의 기록 경신을 달성했습니다.  
    *   이는 수학 및 컴퓨터 과학 분야에서 오랜 난제를 해결한 사례로, 대규모 데이터 처리와 인공지능 학습에 직접적인 영향을 미칩니다.  
*   **수학 문제:**  
    *   다양한 수학 문제에서 기존 최고 해법과 동등하거나 더 나은 결과를 도출했습니다.  
    *   예를 들어, 최적화 문제나 조합론 문제에서 새로운 접근법을 제안하여 계산 효율성을 높였습니다.  
*   **Google 컴퓨팅 생태계 최적화:**  
    *   컴퓨팅 작업 스케줄링 효율성을 평균 0.7% 향상시켜, Google 전체 컴퓨팅 자원을 절약했습니다. 이는 대규모 데이터센터 운영에서 상당한 비용 절감을 의미합니다.  
    *   Gemini 모델 학습에 사용되는 커널 엔지니어링 최적화로, 커널 속도를 평균 23% 향상시키고 전체 학습 시간을 약 1% 단축했습니다.  
*   **회로 설계:**  
    *   하드웨어 기술 언어인 Verilog를 사용하여 TPU 회로 설계를 최적화했습니다. 이는 Google의 AI 하드웨어 성능을 강화하는 데 기여했습니다.  
*   **컴파일러 최적화:**  
    *   Flash Attention 컴파일러의 생성 코드를 직접 최적화하여 32%의 속도 향상을 달성했습니다. 이는 대규모 언어 모델의 학습 및 추론 속도를 크게 개선했습니다.

### 4.5 **Alpha Evolve의 한계**

*   **자동 평가가 불가능한 작업에는 적용 불가**  
    *   예를 들어, 사용자 경험(UX) 설계나 창의적 글쓰기처럼 주관적 판단이 필요한 작업에는 부적합.  
    *   명확한 정량적 평가 기준(예: 속도, 메모리 사용량, 정확도)이 없는 경우, Alpha Evolve의 성능이 제한.

### **4.6 결론**

*   Alpha Evolve는 자동 평가가 가능한 문제에 대해 스스로 코드를 개선하고 새로운 알고리즘을 발견하는 **강력한 도구**입니다.  
    *   이는 컴퓨터 과학과 인공지능 분야에서 혁신적인 가능성을 열어줍니다.  
*   AI가 스스로 개선하는 **SF적 상상**을 현실화하는 첫걸음으로, 자율적이고 창의적인 문제 해결 능력을 제시.  
*   현재 Alpha Evolve는 Google 내부 기술로, 일반 사용자는 직접 사용할 수 없습니다.  
    *   향후 공개 여부나 상용화 가능성에 대한 정보는 아직 발표되지 않았습니다.

