---
title: 16차시 1:IBM TECH(Understanding AI Models)
layout: single
classes: wide
categories:
  - IBM TECH
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---


## 1. Foundation 모델을 활용한 AI 모델 개발
- 출처: [Five Steps to Create a New AI Model](https://www.youtube.com/watch?v=jcgaNrC4ElU&list=PLOspHqNVtKAC-FUNMq8qjYVw6_semZHw0)

### 1.1 **기존 방식의 문제점:**

* **특정 분야의 AI 모델 개발 시 데이터 수집, 레이블링, 모델 개발, 훈련, 검증 등 모든 과정을 처음부터 다시 시작해야 함:** 이는 시간과 비용이 많이 소요되며, 개발 효율성을 저하시키는 주요 원인입니다. 특히, 데이터가 부족하거나 레이블링이 어려운 분야에서는 모델 개발 자체가 불가능한 경우도 많습니다.

### 1.2 **Foundation 모델이란?**

* **Base 모델을 집중적으로 개발하고, Fine-tuning을 통해 특정 분야에 특화된 모델로 빠르게 적용 가능한 모델:** Foundation 모델은 다양한 분야의 방대한 데이터를 학습하여 일반적인 능력을 갖춘 모델입니다. Fine-tuning을 통해 특정 분야의 데이터로 추가 학습하면 해당 분야에 특화된 모델을 빠르게 개발할 수 있습니다. 이는 개발 시간과 비용을 획기적으로 줄이고, 모델의 성능을 향상시키는 데 기여합니다.

### 1.3 **Foundation 모델 개발 5단계 워크플로우:**
1.  **데이터 준비:**
    * **다양한 도메인의 대규모 데이터(오픈 소스 및 독점 데이터) 수집 및 처리:** 데이터의 다양성은 모델의 일반적인 능력을 향상시키고, 편향성을 줄이는 데 중요합니다.
    * **데이터 범주화 (예: 언어, 프로그래밍 언어 등):** 데이터 범주화는 모델이 다양한 유형의 데이터를 학습하고 처리할 수 있도록 돕습니다.
    * **필터링 (혐오 발언, 욕설, 저작권 자료, 개인 정보, 중복 데이터 제거 등):** 필터링은 모델의 안정성과 윤리적 문제를 방지하고, 데이터 품질을 향상시키는 데 필수적입니다.
    * **결과물: 버전 관리 및 태깅이 가능한 Base Data Pile 생성:** 버전 관리 및 태깅은 데이터의 추적성과 재현성을 확보하고, 모델 개발 과정에서 발생할 수 있는 문제를 해결하는 데 도움이 됩니다.
2.  **모델 훈련:**
    * **Base Data Pile을 사용하여 Foundation 모델 훈련:** Foundation 모델 훈련은 막대한 컴퓨팅 자원과 시간이 소요되는 과정입니다. 분산 훈련 기술과 효율적인 알고리즘을 활용하여 훈련 시간을 단축하고 성능을 향상시키는 것이 중요합니다.
    * **적절한 Foundation 모델 선택 (생성 모델, 인코더 모델, 경량 모델, 고 파라미터 모델 등):** 모델의 종류와 크기는 개발 목표와 컴퓨팅 자원에 따라 결정됩니다.
    * **데이터 Pile을 토큰화하여 모델 훈련에 사용 (수개월, 수천 개의 GPU 소요 가능):** 토큰화는 텍스트 데이터를 모델이 이해할 수 있는 작은 단위로 분할하는 과정입니다.
3.  **검증:**
    * **모델 훈련 후 성능을 벤치마킹하여 모델 품질 평가:** 다양한 벤치마크를 사용하여 모델의 성능을 객관적으로 평가합니다.
    * **모델 카드 생성 (훈련된 모델 정보 및 벤치마크 점수 기록):** 모델 카드는 모델의 정보, 훈련 데이터, 성능, 윤리적 고려 사항 등을 기록한 문서입니다.
    * **주요 담당자: 데이터 과학자:** 데이터 과학자는 모델의 성능을 평가하고 개선 방향을 설정하는 데 중요한 역할을 합니다.
4.  **튜닝:**
    * **애플리케이션 개발자가 모델과 상호 작용하여 프롬프트 생성, 추가 데이터 Fine-tuning:** 프롬프트 엔지니어링은 모델에게 원하는 결과를 얻기 위한 최적의 질문 또는 명령을 설계하는 기술입니다. Fine-tuning은 특정 분야의 데이터를 사용하여 모델을 추가적으로 훈련하는 과정입니다.
    * **모델 성능 개선 (수 시간 또는 수 일 내에 완료 가능):** 튜닝은 모델의 성능을 향상시키고 특정 작업에 특화된 모델을 개발하는 데 사용됩니다.
    * **주요 담당자: 애플리케이션 개발자 (AI 전문가가 아니어도 가능):** 애플리케이션 개발자는 모델을 활용하여 다양한 애플리케이션을 개발하고, 모델의 성능을 개선하는 데 기여합니다.
5.  **배포:**
    * **Public Cloud 또는 네트워크 Edge에 모델 배포:** 모델 배포 환경은 애플리케이션의 요구 사항과 사용 환경에 따라 결정됩니다.
    * **모델 지속적인 개선:** 모델은 지속적인 개선을 통해 성능을 향상시키고 새로운 기능과 기술을 적용할 수 있습니다.

### 1.4 **IBM watsonx 플랫폼**
* **Foundation 모델 개발 5단계 워크플로우를 지원하는 플랫폼:** IBM watsonx 플랫폼은 Foundation 모델 개발의 모든 단계를 지원하는 통합 플랫폼입니다.
* **구성 요소:**
    * **watsonx.data:** 데이터 레이크 하우스, 1단계 데이터 리포지토리 연결: 다양한 형태의 데이터를 통합하고 관리하는 플랫폼입니다.
    * **watsonx.governance:** 1단계 데이터 카드 및 3단계 모델 카드 관리, AI 프로세스 및 라이프사이클 관리: 모델의 개발, 배포, 사용 전반에 걸쳐 투명성, 책임성, 윤리성을 확보하는 데 중요한 역할을 합니다.
    * **watsonx.ai:** 4단계에서 애플리케이션 개발자가 모델과 상호 작용할 수 있도록 지원: 애플리케이션 개발자가 모델을 활용하여 다양한 애플리케이션을 개발할 수 있도록 지원합니다.
* **Red Hat OpenShift 기반의 하이브리드 클라우드 플랫폼에서 구축:** Red Hat OpenShift는 확장성과 유연성을 제공하는 클라우드 플랫폼입니다.

### 1.5 **결론**
* Foundation 모델은 AI 모델 개발 방식을 혁신하며, 5단계 워크플로우를 통해 AI 및 AI 기반 애플리케이션을 보다 정교하게, 더 빠르게 개발할 수 있도록 지원한다. IBM watsonx 플랫폼은 Foundation 모델 개발의 복잡성을 줄이고 개발자들이 혁신에 집중할 수 있도록 돕습니다.

## 2. GPT와 LLM (Large Language Model)
- 출처: [How Large Language Models work?](https://www.youtube.com/watch?v=5sLYAQS9sWQ&list=PLOspHqNVtKAC-FUNMq8qjYVw6_semZHw0&index=2)

### **2.1 LLM 이란?**
*   **LLM (Large Language Model)** 은 대규모 언어 모델로, 인간과 유사한 텍스트를 생성할 수 있음.
*   **Foundation Model** 의 한 종류이며, 대량의 비 레이블 및 자율 학습 데이터를 기반으로 사전 훈련됨.
    *   데이터 패턴 학습을 통해 일반화 및 적응 가능한 결과물을 생성.
*   텍스트 및 코드와 같은 텍스트 유사 항목에 특화된 Foundation Model 의 응용 사례.
*   **학습 데이터:** 서적, 기사, 대화 등 방대한 텍스트 데이터 세트 (수십 GB ~ 수십 PB 규모)
*   **파라미터:** 모델이 학습하면서 독립적으로 변경할 수 있는 값. 파라미터 수가 많을수록 모델의 복잡성이 증가.
    *   GPT-3 는 45TB 데이터, 1,750억 개 ML 파라미터 사용

### **2.2 LLM 작동 방식**
*   **LLM = 데이터 + 아키텍처 + 훈련**
    *   **데이터:** 방대한 양의 텍스트 데이터
    *   **아키텍처:** 신경망 (GPT의 경우 Transformer)
        *   Transformer 아키텍처는 문장 또는 코드 줄과 같은 데이터 시퀀스를 처리 가능.
        *   문장 내 각 단어의 맥락을 다른 모든 단어와 관련지어 이해.
    *   **훈련:** 모델이 문장 내 다음 단어를 예측하도록 학습
        *   예측과 실제 결과 간의 차이를 줄이도록 내부 파라미터를 조정
        *   일관성 있는 문장을 생성할 수 있을 때까지 단어 예측을 점진적으로 개선
        *   **Fine Tuning:** 더 작고 구체적인 데이터 세트로 모델을 미세 조정하여 특정 작업에 대한 정확도를 향상

### **2.3 LLM 비즈니스 활용 분야**
*   **고객 서비스:** 지능형 챗봇을 구축하여 다양한 고객 문의 처리, 상담원의 업무 부담 경감
*   **콘텐츠 제작:** 기사, 이메일, 소셜 미디어 게시물, YouTube 비디오 스크립트 생성 지원
*   **소프트웨어 개발:** 코드 생성 및 검토 지원


## 3. 왜 이렇게 많은 Foundation Model이 있을까?

### **3.1 Foundation Model이란?**
* **대규모 데이터를 학습한 거대 신경망** 
    - 방대한 양의 데이터를 학습하여 다양한 패턴과 관계를 이해합니다. 이는 모델이 일반적인 능력을 갖추고 다양한 작업에 적용될 수 있도록 합니다.
* **다양한 어플리케이션의 기반(Foundation) 역할 수행** 
    - 다양한 애플리케이션의 기반이 되어 새로운 애플리케이션 개발의 효율성을 높입니다.
* **Transfer Learning**
    - Transfer Learning은 모델이 이미 학습한 지식을 새로운 작업에 적용하여 학습 효율성을 높이는 기술로, Foundation Model은 Transfer Learning을 통해 다양한 분야에 적용가능.
* **소량의 데이터로 새로운 task 학습 가능 (Fine-tuning)** 
    - Fine-tuning은 모델을 특정 작업에 맞게 추가 학습하는 과정. Foundation Model은 Fine-tuning을 통해 소량의 데이터로도 새로운 작업을 학습할 수 있습니다.

### **3.2 NASA 사례**

* **데이터:** 방대한 지구과학 데이터 보유 (위성 이미지)
    * 현재 약 70 페타바이트: NASA는 지구 관측 위성 데이터를 통해 방대한 양의 데이터를 축적.
    * 2030년 약 300 페타바이트 예상: 데이터 양은 지속적으로 증가하고 있으며, 이는 Foundation Model 개발에 필요한 충분한 데이터를 제공합니다.
* **활용:** 기후 변화 관련 통찰력 제공
    - NASA는 Foundation Model을 활용하여 기후 변화 관련 데이터를 분석하고, 이를 통해 기후 변화에 대한 이해를 높이고 예측 모델을 개발합니다.
* **IBM NASA Geospatial model:** 지구 관측을 위한 AI Foundation Model
    - Huggingface에서 오픈 소스로 다운로드 가능
    - IBM과 NASA는 협력하여 지구 관측을 위한 Foundation Model을 개발하고, 이를 오픈 소스로 공개하여 다양한 연구 및 개발에 활용될 수 있도록 합니다.

### **3.3 Foundation Model의 핵심: Transformer**

* **텍스트, 오디오, 위성 이미지 등 원시 데이터를 압축하여 데이터의 기본 구조 파악**     
    - Transformer는 다양한 형태의 데이터를 처리하고 데이터의 핵심 구조를 파악하는 데 뛰어난 성능.
* **추가 Label 데이터 및 튜닝을 통해 다양한 task에 활용 가능** 
    - Transformer 기반 Foundation Model은 추가적인 레이블 데이터와 튜닝을 통해 다양한 작업에 적용될 수 있습니다

### **3.4 Foundation Model의 장점**

* **수동 작업 감소** 
    - 위성 이미지 분석 시 인간 전문가의 Feature Annotation에 소요되는 시간 절약
    - 이미지 분석 작업을 자동화하여 인간 전문가의 작업 시간을 절약하고 효율성을 제고.
* **구조 추출** 
    - 원시 이미지의 구조를 추출하여 적은 Label 데이터로도 학습 가능
    - Foundation Model은 이미지의 핵심 구조를 추출하여 적은 양의 레이블 데이터로도 학습이 가능.
* **응용** 
    - 홍수, 산불 예측 모델 개발 (과거 데이터 기반 미래 위험 지역 예측) Foundation Model은 과거 데이터를 기반으로 미래 위험 지역을 예측하는 모델 개발에 활용될 수 있습니다.

### **3.5 Foundation Model의 확장성**

* **Fine-tuning:** 모델을 기반으로 다양한 task 수행 가능
    * 예시: 홍수/산불 예측 모델 → 삼림 벌채 추적, 작물 수확량 예측, 온실 가스 감지 및 모니터링
* **다양한 데이터:** 코드 생성, 특정 산업 관련 등 다양한 데이터로 학습 및 튜닝된 모델 존재
    - 다양한 데이터로 학습되고 튜닝된 Foundation Model은 특정 분야에 특화된 모델 개발을 가능.
* **향후 전망** 
    - 적절한 Foundation Model을 선택하고 Fine-tuning하면 다양한 방식으로 활용 가능 
    하여 많은 Foundation Model이 존재하는 이유

### 3.6 **요약** 
* Foundation Model은 방대한 데이터를 기반으로 다양한 작업에 응용될 수 있는 확장성 덕분에 그 수가 계속 증가하고 있습니다. NASA의 사례처럼 특정 목적에 맞춰 개발된 Foundation Model을 활용하고 Fine-tuning을 통해 새로운 분야에 적용하는 것이 가능합니다. 이는 Foundation Model이 다양한 분야에서 활용될 수 있는 잠재력을 보여줍니다.


## 4. CNN(Convolutional Neural Networks)에 대한 설명
- 출처:[What are Convolutional Neural Networks (CNNs)?](https://www.youtube.com/watch?v=QzY57FaENXg&list=PLOspHqNVtKAC-FUNMq8qjYVw6_semZHw0&index=4)

### **4.1 서론**
* **간단한 집 그림을 예시로, 컴퓨터가 사물 인식을 얼마나 어려워하는지 보여준다** 
    - 컴퓨터는 이미지를 픽셀의 집합으로 인식하며, 인간처럼 전체적인 맥락을 이해하는 데 어려움을 겪습니다. 따라서 간단한 집 그림도 컴퓨터에게는 복잡한 데이터일 수 있습니다.
* **CNN은 패턴 인식에 특화된 딥러닝 영역이다** 
    - CNN은 인간의 시각 인지 방식을 모방하여 이미지 내의 특징을 추출하고 학습하는 데 특화된 딥러닝 모델입니다.

### **4.2 CNN 작동 방식 (개념)**
* **인공 신경망 (Artificial Neural Network)** 
    - 여러 층으로 구성되어 각 층이 입력을 변환하여 다음 층으로 전달하는 표준 네트워크
    - 인공 신경망은 복잡한 함수를 근사하고 데이터에서 패턴을 학습하는 데 사용됩니다.
* **필터 (Filters)** 
    - CNN 내 특정 층에 존재하며, 패턴 인식을 수행하는 핵심 요소
    - 필터는 이미지의 특정 영역에서 특징을 추출하는 역할을 합니다.

### **4.3 집 그림 예시를 통한 CNN 설명**
* **이미지는 픽셀로 구성된다** 
    - 이미지는 수많은 작은 사각형인 픽셀로 구성되며, 각 픽셀은 색상 정보를 담고 있습니다.
* **특정 영역(예: 창문)을 확대하면 직선으로 이루어진 패턴을 발견할 수 있다** 
    - 이미지의 특정 영역을 확대하면 픽셀의 배열에서 직선, 곡선 등 다양한 패턴을 발견
* **CNN은 필터를 사용하여 다양한 형태의 창문도 동일한 객체로 인식할 수 있다** 
    - CNN은 필터를 사용하여 다양한 변형에도 불구하고 창문이라는 객체의 특징을 추출하고 인식.

### **4.4 필터 작동 방식**
* **필터는 일반적으로 3x3 크기의 블록이다** 
    - 필터는 이미지의 작은 영역을 탐색하며 특징을 추출합니다.
* **필터 내에 특정 패턴(예: 직각)을 정의한다** 
    - 필터는 특정 패턴을 감지하도록 설계되며, 예를 들어 직각, 수직선, 수평선 등을 감지할 수 있습니다.
* **필터를 이미지의 3x3 픽셀 블록에 적용하여 필터 패턴과의 유사도를 측정한다** 
    - 필터는 이미지의 작은 영역과 비교하여 유사도를 측정하고, 해당 영역에 특정 패턴이 존재하는지 확인.
* **이미지 전체를 슬라이딩하며 모든 3x3 블록에 대해 유사도 점수를 계산한다** 
    - 필터는 이미지를 전체적으로 탐색하며 각 영역에서 패턴의 존재 여부를 확인.
* **여러 개의 필터를 사용하여 다양한 패턴을 감지할 수 있다** 
    - CNN은 다양한 필터를 사용하여 이미지에서 다양한 특징을 추출하고 객체를 인식.

### **4.5 풀링 (Pooling)**
* **여러 필터에서 얻은 숫자 배열을 결합하여 이미지에 포함된 내용에 대한 더 나은 이해를 제공한다** 
    - 풀링은 필터에서 추출된 특징 맵의 크기를 줄이고 중요한 정보만 남겨 연산량을 줄이고 모델의 일반화 성능을 향상시키는 데 사용됩니다.

### **4.6 CNN 심층 레이어**
* **CNN의 레이어가 깊어질수록 필터는 더욱 추상적인 작업을 수행할 수 있다** 
    - CNN의 깊은 레이어는 이미지의 복잡한 특징을 학습하고 추상적인 객체를 인식할 수 있습니다.
* **초기 레이어: 기본 형태 인식** 
    - 초기 레이어는 이미지의 가장 기본적인 특징인 선, 모서리 등을 인식합니다.
* **중간 레이어: 창문, 문, 지붕 등 객체 인식** 
    - 중간 레이어는 기본 특징을 조합하여 창문, 문, 지붕과 같은 객체의 부분적인 특징을 인식합니다.
* **후반 레이어: 집, 아파트, 고층 건물 등 더욱 추상적인 객체 분류** 
    - 후반 레이어는 객체의 부분적인 특징을 조합하여 집, 아파트, 고층 건물과 같은 전체적인 객체를 인식하고 분류합니다.

### **4.7 CNN 활용 분야**
* **OCR (광학 문자 인식):** 손으로 쓴 문서 이해: CNN은 손으로 쓴 문서의 이미지를 분석하여 텍스트를 인식하는 데 사용됩니다.
* **시각적 인식:** 얼굴 인식, 시각적 검색: CNN은 얼굴 인식, 이미지 검색 등 다양한 시각적 인식 작업에 활용됩니다.
* **의료 영상:** 영상 스캔 분석: CNN은 의료 영상 분석을 통해 질병을 진단하고 치료 계획을 수립하는 데 도움을 줍니다.
* **간단한 그림 인식:** CNN은 간단한 그림을 인식하고 분류하는 데 사용될 수 있습니다.


## 5.GAN (Generative Adversarial Networks)
- 출처:[What are GANs (Generative Adversarial Networks)?](https://www.youtube.com/watch?v=TpMIssRdhco&list=PLOspHqNVtKAC-FUNMq8qjYVw6_semZHw0&index=5)

### **5.1 GAN이란?**
* **두 개의 AI 모델(생성자, 판별자)을 경쟁시켜 학습하는 머신러닝 알고리즘** 
    - GAN은 생성자와 판별자라는 두 개의 신경망을 경쟁시켜 실제 데이터와 유사한 가짜 데이터를 생성하는 알고리즘입니다. 
    - 이 경쟁적인 학습 방식은 GAN이 실제 데이터의 복잡한 분포를 학습하고 고품질의 가짜 데이터를 생성하는 데 효과적입니다.
* **비지도 학습 방식 (자체적으로 학습)** 
    - GAN은 레이블이 지정된 데이터 없이도 학습할 수 있습니다. 
    - 판별자는 실제 데이터와 생성자가 만든 가짜 데이터를 구별하면서 스스로 학습하고, 생성자는 판별자를 속이는 방향으로 학습합니다. 
    - 이러한 비지도 학습 방식은 GAN이 다양한 유형의 데이터에 적용될 수 있도록 합니다.

### **4.2 구성 요소**

* **생성자 (Generator)** 
    - 가짜 데이터(샘플) 생성
        - 생성자는 랜덤한 노이즈 벡터를 입력받아 실제 데이터와 유사한 가짜 데이터를 생성합니다. 생성자는 판별자를 속일 수 있을 정도로 실제와 가까운 데이터를 생성하는 것을 목표로 합니다.
* **판별자 (Discriminator)** 
    - 입력받은 샘플이 진짜인지 가짜인지 판별
        - 판별자는 입력받은 데이터가 실제 데이터인지 생성자가 만든 가짜 데이터인지 판별합니다. 판별자는 실제 데이터와 가짜 데이터를 정확하게 구별하는 것을 목표

### **4.3 작동 방식**
1.  **판별자 사전 학습** 
    - 실제 데이터(꽃 사진 등)를 학습하여 특징(색, 음영, 모양 등)을 파악하고, 가짜 데이터를 구별하는 능력 향상
    - 판별자는 실제 데이터를 학습하여 데이터의 특징을 파악하고 가짜 데이터를 구별하는 능력을 향상시킵니다. 
    - 이는 판별자가 생성자의 가짜 데이터를 정확하게 판별할 수 있도록 돕습니다.
2.  **생성자 가짜 데이터 생성** 
    - 랜덤한 입력 벡터를 기반으로 가짜 데이터 생성
    - 생성자는 랜덤한 노이즈 벡터를 입력받아 실제 데이터와 유사한 가짜 데이터를 생성합니다. 생성자는 판별자를 속일 수 있을 정도로 실제와 가까운 데이터를 생성하는 것을 목표로 합니다.
3.  **판별자의 진위 판별** 
    - 생성자가 만든 가짜 데이터와 실제 데이터를 판별
    - 판별자는 생성자가 만든 가짜 데이터와 실제 데이터를 입력받아 각 데이터가 진짜인지 가짜인지 판별.
4.  **결과에 따른 모델 업데이트 (Zero-Sum Game)**
    * **판별자 승리** 
        - 생성자 모델 업데이트 (더 정교한 가짜 데이터 생성)
        - 판별자가 가짜 데이터를 정확하게 판별하면 생성자는 판별자를 속일 수 있도록 가짜 데이터를 더 정교하게 생성하도록 업데이트됩니다.
    * **생성자 승리** 
        - 판별자 모델 업데이트 (진짜/가짜 구별 능력 향상)
        - 생성자가 판별자를 속여 가짜 데이터를 진짜로 판별하게 만들면 판별자는 진짜와 가짜를 구별하는 능력을 향상시키도록 업데이트.
5.  **반복 학습** 
    - 생성자가 판별자를 속일 정도로 정교한 가짜 데이터를 생성할 때까지 3~4단계 반복
    - 생성자와 판별자는 경쟁적인 학습을 통해 서로의 성능을 향상시키고, 생성자는 판별자를 속일 수 있을 정도로 정교한 가짜 데이터를 생성하게 됩니다.

### **5.4 GAN 활용 예시**
* **이미지 생성** 
    - 가짜 3D 모델, 얼굴, 고양이 등 생성
    - GAN은 실제와 매우 유사한 고품질의 이미지를 생성할 수 있습니다. 이는 엔터테인먼트, 디자인, 예술 등 다양한 분야에서 활용.
* **동영상 프레임 예측** 
    - 특정 프레임을 기반으로 다음 프레임 예측 (감시 시스템 활용 가능)
    - GAN은 동영상의 특정 프레임을 기반으로 다음 프레임을 예측하여 자연스러운 동영상을 생성할 수 있습니다. 이는 감시 시스템, 영상 편집 등 다양한 분야에서 활용됩니다.
* **이미지 해상도 개선** 
    - 저해상도 이미지를 고해상도로 변환
    - GAN은 저해상도 이미지를 고해상도로 변환하여 이미지의 품질을 향상시킬 수 있습니다. 이는 의료 영상 분석, 위성 이미지 분석 등 다양한 분야에서 활용됩니다.
* **암호화** 
    - GAN을 이용한 안전한 암호화 알고리즘 개발
    - GAN은 복잡한 데이터 분포를 학습하고 생성하는 능력을 활용하여 안전한 암호화 알고리즘 개발에 활용.

**5. 추가 정보**
* 이미지 관련 작업 시, 생성자와 판별자는 CNN(Convolutional Neural Networks, 합성곱 신경망)으로 구현되는 경우가 많음
    - CNN은 이미지 처리 분야에서 뛰어난 성능을 보이는 신경망으로, GAN의 생성자와 판별자를 CNN으로 구현하면 이미지 생성 및 판별 성능을 향상시킬 수 있습니다.
* GAN은 단순히 가짜 이미지 생성을 넘어 다양한 분야에 적용 가능
    - GAN은 이미지 생성뿐만 아니라 동영상 생성, 텍스트 생성, 음성 생성 등 다양한 분야에 적용될 수 있습니다.
    - 또한, GAN은 데이터 증강, 이상 탐지, 스타일 변환 등 다양한 작업에 활용될 수 있습니다.


## 6. 트랜스포머(Transformer)

- 출처:[What are Transformers (Machine Learning Model)?](https://www.youtube.com/watch?v=ZXiruGOCn9s&list=PLOspHqNVtKAC-FUNMq8qjYVw6_semZHw0&index=6)

### **6.1 소개**
*   트랜스포머는 단순한 변환기가 아니라, 한 시퀀스를 다른 시퀀스로 변환하는 모델이다.
    *   시퀀스란 단어, 이미지 패치, 음성 신호 등 순차적 데이터를 의미하며, 트랜스포머는 이러한 데이터 간의 복잡한 관계를 학습한다.
    *   기존 RNN, LSTM과 달리 순차적 처리 제약을 없애 혁신적인 성능 향상을 이끌었다.
*   GPT-3는 트랜스포머 모델의 한 종류로, 인간이 쓴 것 같은 텍스트를 생성할 수 있다.
    *   GPT-3는 1750억 개의 매개변수를 가진 초대규모 언어 모델(LLM)로, Few-shot Learning이 가능하다.
    *   즉, 소량의 예시만으로도 새로운 작업을 수행할 수 있다.
*   GPT-3는 농담, 시, 이메일 작성 등 다양한 작업이 가능하다. (예: "왜 바나나는 길을 건넜을까?" 농담)
    *   이러한 창의적 생성 능력은 방대한 양의 인터넷 텍스트 데이터 학습 결과이며, 때로는 편향적이거나 부정확한 출력을 생성할 수도 있다.

### **6.2 트랜스포머의 작동 방식 (언어 번역 예시)**
*   **인코더-디코더 구조**
    *   **인코더 (Encoder)** 
        - 입력 시퀀스(예: 영어 문장)를 처리한다.
        *   인코더는 여러 층(레이어)으로 구성되며, 각 층은 Self-Attention과 Feed-Forward Neural Network로 이루어져 있다.
        *   Self-Attention은 입력 시퀀스 내 모든 단어 간의 관계를 동시에 분석하여 문맥을 파악한다.
    *   **디코더 (Decoder)** 
        - 목표 출력 시퀀스(예: 프랑스어 번역)를 생성한다.
        *   디코더는 인코더의 출력과 이전에 생성된 단어를 참조하여 다음 단어를 예측한다.
        *   Masked Self-Attention을 사용해 미래 단어를 참조하지 못하도록 제한한다.
*   **시퀀스-투-시퀀스 학습 (Sequence-to-Sequence Learning)**
    *   트랜스포머는 문장의 단어와 같은 토큰 시퀀스를 입력받아 출력 시퀀스의 다음 단어를 예측한다.
        *   토큰화 과정에서 BPE(Byte Pair Encoding) 등 효율적인 알고리즘이 사용된다.
    *   인코더는 입력 시퀀스의 각 부분이 서로에게 얼마나 관련 있는지 정의하는 인코딩을 생성하고, 다음 인코더 레이어로 전달한다.
        *   Multi-Head Attention은 여러 관점에서 Attention을 계산해 더 풍부한 표현을 학습한다.
    *   디코더는 이러한 인코딩과 문맥을 사용하여 출력 시퀀스를 생성한다.
        *   Teacher Forcing 기법을 사용해 훈련 시 정답 시퀀스를 제공한다.
*   **세미-슈퍼바이즈드 학습 (Semi-Supervised Learning)**
    *   레이블이 없는 대규모 데이터셋으로 비지도 학습 방식으로 사전 훈련(pre-training)한다.
        *   예: 언어 모델링 작업(다음 단어 예측)으로 일반적인 언어 이해 능력을 획득한다.
    *   지도 학습을 통해 미세 조정(fine-tuning)하여 성능을 향상시킨다.
        *   특정 작업(감정 분석, 질문 응답 등)에 맞춰 모델의 마지막 층을 조정한다.

### **6.3 트랜스포머의 특징 및 장점**
*   **어텐션 메커니즘 (Attention Mechanism)**
    *   입력 시퀀스 내의 항목들에 대한 문맥을 제공한다.
        *   "bank"라는 단어가 "강둑"인지 "은행"인지 문맥에 따라 동적으로 가중치를 부여해 해석한다.
    *   문장 시작 단어부터 번역을 시작하는 것이 아니라, 각 단어의 의미를 파악하여 문맥을 이해한다.
        *   장거리 의존성 문제를 해결해 긴 문장에서도 정확한 번역이 가능하다.
*   **병렬 처리 (Parallel Processing)**
    *   RNN과 달리 데이터를 순차적으로 처리할 필요가 없다.
        *   모든 단어를 동시에 처리해 학습 및 추론 속도가 크게 향상된다.
    *   여러 시퀀스를 병렬로 실행하여 훈련 속도를 크게 향상시킨다.
        *   GPU/TPU와 같은 병렬 컴퓨팅 장치와 매우 효율적으로 호환된다.

### **6.4 트랜스포머의 다양한 응용 분야**
*   **문서 요약:** 긴 기사를 요약하여 핵심 내용을 추출한다.
    *   추출적 요약(Extractive)과 생성적 요약(Abstractive) 방식 모두에 적용된다.
*   **새로운 문서 생성:** 블로그 게시물과 같은 새로운 문서를 작성한다.
    *   최근에는 AI 저널리즘, 마케팅 콘텐츠 생성 등에 활용된다.
*   **체스 게임:** 체스 게임을 학습하고 수행한다.
    *   AlphaZero와 같은 모델에서 트랜스포머가 게임 전략 학습에 사용된다.
*   **이미지 처리:** 컨볼루션 신경망(CNN)과 경쟁할 수 있는 이미지 처리 능력을 보여준다.
    *   Vision Transformer(ViT)는 이미지를 패치로 분할해 트랜스포머에 입력한다.

### **6.5 결론**

*   트랜스포머는 강력한 딥러닝 모델이며, 어텐션 메커니즘의 병렬화 덕분에 지속적으로 발전하고 있다.
    *   최근에는 효율성을 개선한 Sparse Transformer, Longformer 등 다양한 변형 모델이 등장했다.
*   언어 이해 및 생성 능력은 계속 향상될 것이다.
    *   멀티모달 학습(텍스트+이미지+음성)과 결합해 더욱 일반적인 AI로 발전할 전망이다.


## 7. LSTM (Long Short-Term Memory)
- 출처:[What is LSTM (Long Short Term Memory)?](https://www.youtube.com/watch?v=b61DPVFX03I&list=PLOspHqNVtKAC-FUNMq8qjYVw6_semZHw0&index=7)

### **7.1 문제 상황**
- **단기 기억 상실**  
  - 신경망이 최근 정보만을 기억하면 전체 맥락을 파악하기 어렵습니다. 
  - 예를 들어, 살인 미스터리 게임에서 최근 단서만 기억한다면 사건의 전체 흐름이나 초기 단서를 놓쳐 범인을 제대로 추측하지 못할 수 있습니다. 이는 순차적인 데이터를 처리할 때 초기 정보가 점차 희석되는 문제를 의미합니다.
  
- **과도한 기억력**  
  - 반대로 모든 정보를 무작정 기억하면 중요한 정보를 선별적으로 추출하기 어려워집니다. 
  - 예를 들어, 결혼 서약에서 모든 세부 사항을 다 기억하려고 하면 핵심 약속(예: "서로 사랑하고 존중한다")이 묻혀버릴 수 있습니다. 따라서 필요 없는 정보는 잊고 중요한 정보만 유지하는 능력이 필요합니다.

### **7.2 LSTM 등장**

- LSTM은 신경망이 **필요한 정보는 기억하고, 불필요한 정보는 잊도록 설계된 모델**입니다. 이를 통해 긴 시간에 걸친 데이터(장기 의존성)를 효과적으로 학습할 수 있습니다. 
- RNN의 단점을 보완한 LSTM은 메모리 셀과 게이트라는 구조를 추가해 정보를 선택적으로 관리하며, 문맥을 유지하거나 업데이트하는 데 탁월한 성능을 발휘합니다.


### **7.3 LSTM 작동 방식 (예시: 문자열 예측)**
- **RNN (Recurrent Neural Network):**  
  RNN은 순차적인 데이터를 처리하는 신경망으로, 노드가 입력을 받아 계산한 후 출력을 생성합니다.  
  - **핵심:** 이전 단계의 출력이 다음 단계의 입력으로 사용되어 과거 정보를 기억합니다. 예를 들어, "I am" 다음에 "happy"가 올 확률을 예측할 때 "I am"을 기반으로 계산합니다.  
  - 하지만 RNN은 시간이 지남에 따라 초기 정보가 희석되는 **장기 의존성 문제**를 겪습니다. 긴 문장에서 앞부분의 의미를 잊어버리면 뒷부분을 제대로 이해하지 못하게 됩니다.

- **LSTM의 해결책**  
  LSTM은 RNN 노드에 **내부 상태(Internal State)**를 추가해 이 문제를 해결합니다.  
  - 내부 상태는 **셀(Cell)**로 구성되며, 셀은 세 개의 **게이트(Gate)**로 제어됩니다:  
    1. **Forget Gate (망각 게이트):** 어떤 정보를 잊을지 결정합니다. 시그모이드 함수를 통해 0(모두 잊음)에서 1(모두 기억) 사이의 값을 출력합니다. 예: "Martin"에 대한 정보가 더 이상 필요 없으면 0에 가까운 값으로 설정.  
    2. **Input Gate (입력 게이트):** 어떤 새로운 정보를 저장할지 결정합니다. 0(추가 안 함)에서 1(모두 추가) 사이의 값을 출력합니다. 예: "Jennifer"의 정보를 추가할 때 1에 가까운 값으로 설정.  
    3. **Output Gate (출력 게이트):** 어떤 정보를 출력할지 결정합니다. 0(출력 안 함)에서 1(모두 출력) 사이의 값을 출력합니다. 예: 현재 문맥에 맞는 예측을 위해 필요한 정보를 출력.

### **7.4 LSTM 작동 예시 ("Martin is buying apples." -> "Jennifer is...")**

1. **"Martin is buying apples."**  
   - LSTM은 이 문장을 처리하면서 "Martin"이 남성이고 "apples"가 복수라는 정보를 기억합니다. 
   - Forget Gate는 이전에 불필요한 정보를 잊고, 
   - Input Gate는 이 새로운 정보를 셀에 저장합니다. 
   - Output Gate는 이 정보를 기반으로 다음 예측에 사용할 데이터를 출력합니다.

2. **"Jennifer is..."**  
   - 다음 문장을 처리할 때, LSTM은 "Martin"에 대한 정보를 Forget Gate를 통해 잊고, - "Jennifer"가 여성이라는 새로운 정보를 Input Gate로 추가합니다. 이를 통해 문맥이 바뀌어도 적절히 업데이트된 정보를 유지하며 예측을 수행합니다.
   -  예를 들어, "Jennifer is..." 다음에 올 단어를 예측할 때 "Martin"의 정보에 얽매이지 않고 새로운 맥락에 맞춘 결과를 내놓습니다.

### **7.5 LSTM 활용 분야**
- **기계 번역**  
  - 문장의 앞부분과 뒷부분을 모두 고려해 자연스럽고 정확한 번역을 수행합니다. 
  - 예: "I like to eat"를 "나는 먹는 것을 좋아해"로 번역할 때 전체 문맥을 이해해야 합니다.

- **챗봇 (Q&A)**  
  - 대화의 맥락을 유지하며 적절한 응답을 생성합니다. 
  - 예: 사용자가 "어제 뭐했어?"라고 물으면 이전 대화 내용을 기억해 "어제는 책 읽었어" 같은 답변을 할 수 있습니다.

- **긴 시퀀스 데이터와 장기 의존성 데이터가 필요한 모든 분야:**  
  주식 시장 예측(과거 데이터를 바탕으로 미래 추세 예측), 음성 인식(긴 음성 데이터에서 단어 간 관계 파악), 텍스트 생성 등에서 활용됩니다.

### **7.6 결론**

- LSTM은 복잡한 맥락을 이해하고 장기적인 데이터를 처리하는 데 강력한 도구입니다. 
- 예를 들어, 살인 미스터리 게임에서 단서를 분석해 범인이 집사임을 예측할 수 있습니다. 이는 LSTM이 반복적인 패턴(집사가 범인이라는 클리셰)을 학습해 문맥에 맞는 결론을 도출할 수 있기 때문입니다. 단순히 최근 정보만 기억하거나 모든 것을 무작정 저장하는 대신, 필요한 정보만 선택적으로 기억하는 LSTM의 능력이 이를 가능하게 합니다.


## 8. 자연어 처리 (NLP)
- 출처:[What is NLP (Natural Language Processing)?](https://www.youtube.com/watch?v=fLvJ8VdHLA0&list=PLOspHqNVtKAC-FUNMq8qjYVw6_semZHw0&index=8)

### **8.1 NLP의 정의**
- **사람이 사용하는 언어를 컴퓨터가 이해하고 처리하도록 하는 기술**  
  - 자연어 처리(NLP, Natural Language Processing)는 인간이 일상에서 사용하는 언어를 컴퓨터가 분석하고, 이해하며, 필요에 따라 응답하거나 작업을 수행할 수 있게 만드는 기술입니다. 
  - 예를 들어, 우리가 "오늘 날씨가 어때?"라고 말하면, 컴퓨터가 이 문장을 해석하고 적절한 날씨 정보를 제공하는 것이 NLP의 목표입니다. 이는 인간과 기계 간의 원활한 소통을 가능하게 하는 인공지능(AI)의 핵심 분야 중 하나입니다.

### **8.2 NLP의 핵심**
- **비정형 텍스트 (Unstructured Text)** 
  - 사람이 사용하는 자연스러운 언어 (예: "쇼핑 리스트에 계란과 우유를 추가해줘.")  
  - 비정형 텍스트는 사람들이 일상적으로 사용하는 자유로운 형태의 언어를 의미합니다. 문법이나 형식에 엄격히 얽매이지 않고, 맥락과 감정까지 담고 있어 컴퓨터가 바로 처리하기 어렵습니다. 
  - 위 예시처럼, 이 문장은 자연스럽지만 컴퓨터가 "계란"과 "우유"를 리스트에 추가해야 한다는 의도를 알아채려면 분석이 필요합니다.

- **정형 데이터 (Structured Data)** 
  - 컴퓨터가 처리할 수 있도록 구조화된 형태 (예: 쇼핑 리스트 - 항목 (계란), 항목 (우유))  
  - 정형 데이터는 컴퓨터가 쉽게 읽고 처리할 수 있도록 정리된 데이터입니다. 
  - 예를 들어, 쇼핑 리스트를 데이터베이스 형태로 변환하면 "항목: 계란, 수량: 1", "항목: 우유, 수량: 1"처럼 명확한 구조를 갖게 됩니다. 이는 컴퓨터가 작업을 수행하거나 저장하기에 적합한 형태입니다.

- **NLP는 비정형 텍스트와 정형 데이터 간의 변환을 수행**  
  - NLP의 핵심 역할은 비정형 텍스트를 정형 데이터로 변환하거나, 반대로 정형 데이터를 인간이 이해하기 쉬운 비정형 텍스트로 바꾸는 것입니다. 
  - 예를 들어, 사용자가 "내일 회의 일정 알려줘"라고 말하면, NLP는 이를 분석해 일정 데이터를 찾아 "내일 오전 10시에 회의가 있습니다"라는 자연스러운 답변으로 변환합니다.

### **8.3 NLP 관련 용어**
- **NLU (Natural Language Understanding)** 
  - 비정형 텍스트 -> 정형 데이터 (이해)  
  - NLU는 컴퓨터가 인간의 언어를 분석하여 그 의미를 파악하고, 이를 구조화된 데이터로 변환하는 과정. 
  - 예를 들어, "저녁 7시에 레스토랑 예약해줘"라는 문장을 들으면, NLU는 시간(7시), 장소(레스토랑), 작업(예약)이라는 정보를 추출해 컴퓨터가 이해할 수 있는 명령으로 만듭니다.

- **NLG (Natural Language Generation)** 
  - 정형 데이터 -> 비정형 텍스트 (생성)  
  - NLG는 컴퓨터가 가진 데이터를 기반으로 인간이 이해할 수 있는 자연스러운 문장을 생성하는 기술입니다. 
  - 예를 들어, 날씨 데이터를 바탕으로 "오늘 서울 날씨는 맑고, 기온은 20도입니다"라는 문장을 만드는 것이 NLG의 역할입니다.

### **8.4 NLP 활용 사례**
- **기계 번역** 
  - 문장의 전체적인 맥락을 이해하여 정확한 번역 제공  
  - 기계 번역은 한 언어로 된 문장을 다른 언어로 바꾸는 기술로, 단순히 단어를 대체하는 것이 아니라 문맥과 의도를 파악해 자연스러운 결과를 제공합니다. 
  - 예를 들어, 구글 번역은 "I’m feeling blue"를 "나는 우울해"로 번역하며 감정까지 반영합니다.

- **가상 비서/챗봇** 
  - 사용자의 명령을 이해하고 적절한 작업 수행  
  - 시리, 알렉사 같은 가상 비서는 "불 켜줘" 같은 명령을 이해하고 실제로 조명을 켜는 작업을 수행하거나, "오늘 일정 알려줘"라는 질문에 맞는 답변을 제공합니다. 이는 NLP가 실시간으로 사용자와 상호작용하는 대표적인 사례입니다.

- **감성 분석** 
  - 텍스트에 담긴 감정 (긍정/부정, 진지함/비꼬는 말투) 분석  
  - 감성 분석은 소셜 미디어 게시물이나 제품 리뷰를 분석해 긍정적("정말 좋아요!")인지 부정적("별로예요")인지, 또는 비꼬는 뉘앙스("와, 진짜 대단하네요…")를 파악합니다. 기업은 이를 통해 고객 의견을 이해하고 대응 전략을 세웁니다.

- **스팸 메일 탐지** 
  - 메일 내용 분석을 통해 스팸 여부 판단  
  - NLP는 이메일 내용을 분석해 "당신은 복권에 당첨되었습니다!" 같은 의심스러운 패턴을 감지하고, 이를 스팸으로 분류해 사용자에게 불필요한 메시지를 걸러줍니다.

### **8.5 NLP 작동 방식 (도구 모음)**
- **토큰화 (Tokenization)** 
  - 문장을 단어 또는 구 (토큰) 단위로 분리  
  - 토큰화는 텍스트를 분석하기 위해 문장을 작은 단위로 나누는 첫 단계입니다. 
  - 예를 들어, "나는 책을 읽는다"라는 문장은 "나는", "책을", "읽는다"로 나뉘며, 컴퓨터가 각 부분을 개별적으로 분석할 수 있게 합니다.

- **어간 추출 (Stemming)** 
  - 단어의 어간을 추출 (예: running, runs, ran -> run)  
  - 어간 추출은 단어의 기본 형태를 찾아내는 과정으로, 시제나 어미를 제거합니다. 
  - "running", "runs", "ran"은 모두 "run"이라는 동작을 나타내므로, 이를 하나로 통합해 분석을 단순화

- **표제어 추출 (Lemmatization)** 
  - 단어의 사전적 의미를 파악하여 기본 형태 추출 (예: better -> good)  
  - 표제어 추출은 어간 추출보다 더 정교하게, 단어의 의미를 고려해 기본 형태를 찾습니다. 
  - "better"와 "best"는 "good"의 비교급과 최상급이므로, 모두 "good"으로 변환됩니다.

- **품사 태깅 (Part-of-speech Tagging)** 
  - 문맥 내에서 토큰의 품사 식별 (예: make (동사), make (명사))  
  - 품사 태깅은 단어가 문장에서 어떤 역할을 하는지(명사, 동사, 형용사 등)를 알아내는 과정입니다. 
  - "I make a cake"에서 "make"는 동사(만들다), "This is a good make"에서는 명사(제품)로 사용됩니다.

- **개체명 인식 (Named Entity Recognition)** 
  - 토큰과 관련된 개체 식별 (예: Arizona (미국 주), Ralph (사람 이름))  
  - 개체명 인식은 텍스트에서 사람, 장소, 조직 등 특정 개체를 찾아냅니다. 
  - "Ralph가 Arizona에 갔다"라는 문장에서 "Ralph"는 사람 이름, "Arizona"는 지명으로 인식됩니다.

## 9. NLP, NLU, NLG
- 출처: [NLP vs NLU vs NLG](https://www.youtube.com/watch?v=1I6bQ12VxV0&list=PLOspHqNVtKAC-FUNMq8qjYVw6_semZHw0&index=9)

### **9.1 NLP (Natural Language Processing, 자연어 처리)**

- **개념** 
  - 컴퓨터가 인간의 언어(글, 말)를 이해하고 활용할 수 있도록 하는 **가장 포괄적인 분야**입니다. 이는 인간과 컴퓨터 간의 소통을 가능하게 하는 핵심 기술로, 단순히 단어를 인식하는 것을 넘어 언어를 분석하고 처리하는 전반적인 과정을 포함합니다.
- **주요 작업** 
  - 언어 번역(예: 영어를 한국어로 번역), 챗봇 대화(예: 고객 문의에 자동 응답) 등 실생활에서 자주 접할 수 있는 작업들을 수행합니다.
- **세부 기술**
  - **개체명 인식 (Named Entity Recognition)**
    - 텍스트에서 이름(예: "김영수"), 장소(예: "서울"), 조직(예: "삼성") 등 특정 정보를 식별합니다. 
    - 예를 들어, "애플이 새로운 제품을 출시했다"에서 "애플"을 회사 이름으로 인식합니다.
  - **토큰화 (Tokenization)**
    - 문장을 단어 또는 구 단위로 분리하는 과정입니다. 
    - 예: "나는 책을 읽는다" → ["나는", "책을", "읽는다"].
  - **어간 추출 (Stemming)** 및 **표제어 추출 (Lemmatization)**
    - 단어의 기본 형태를 찾아내는 기술입니다. 
    - Stemming은 "running"을 "run"으로 줄이고, Lemmatization은 문맥을 고려해 "better"를 "good"으로 변환합니다.

### **9.2 NLU (Natural Language Understanding, 자연어 이해)**

- **개념**
  - 컴퓨터가 문장이나 텍스트의 의미를 **정확하게 파악**하도록 하는 NLP의 **하위 분야**입니다. 단순히 단어를 나열하는 수준을 넘어, 문맥과 의도를 이해하는 데 초점을 맞춘 기술입니다.
- **주요 역할**
  - 문장 구조(구문 분석)와 의미(의미 분석)를 분석하여 언어의 **미묘한 차이**를 이해합니다. 
  - 이는 인간이 사용하는 언어의 복잡성과 모호성을 컴퓨터가 해석할 수 있게 합니다.
- **예시**
  - "현재 파일 버전이 클라우드에 있다"라는 문장에서 '현재'가 단순히 '지금 이 순간'을 뜻하는지, 아니면 '최신 버전'을 의미하는지 문맥을 통해 파악합니다. 
  - 또 다른 예로, "배고프다"라는 표현이 실제 배고픔을 뜻하는지, 아니면 비유적으로 "지루하다"는 의미로 쓰였는지 구분할 수 있습니다.

### **9.3 NLG (Natural Language Generation, 자연어 생성)**

- **개념** 
  - 컴퓨터가 데이터를 기반으로 **자연스러운 인간의 언어 텍스트를 생성**하도록 하는 NLP의 **하위 분야**
  - 이는 사람이 직접 작성한 것처럼 보일 정도로 매끄럽고 자연스러운 텍스트를 만드는 데 목적
- **주요 과정**
  - 문법, 어휘 선택, 문장 구조, 의미의 일관성을 고려하여 적절한 응답이나 텍스트를 생성합니다. 단순히 단어를 조합하는 것이 아니라, 맥락에 맞는 이야기를 만들어냅니다.
- **세 단계**
  1. **텍스트 계획 (Text Planning)**
    - 전달하고자 하는 내용의 순서와 구조를 논리적으로 설계합니다. 
    - 예: 날씨 예보를 생성할 때 "온도 → 강수량 → 바람" 순으로 구성.
  2. **문장 계획 (Sentence Planning)**
    - 문장 부호, 단어 선택, 흐름을 조정해 자연스러운 문단으로 나눕니다. 
    - 예: "비가 오고 있다" 대신 "현재 비가 내리고 있으며, 우산을 챙기는 것이 좋겠다"로 확장.
  3. **실현 (Realization)**
    - 문법 규칙을 적용해 최종 문장을 완성합니다. 
    - 예: "run"을 과거형 "ran"으로 바꾸거나, 한국어에서는 "먹다"를 "먹었다"로 변환.
- **예시**
  - 날씨 데이터를 받아 "오늘 오후에는 기온이 25도까지 오르며 맑은 날씨가 예상됩니다"와 같은 문장을 생성하거나, 
  - 챗봇이 "주문이 완료되었습니다. 배송은 내일 시작됩니다"라는 응답을 만들어냅니다.

### **9.4 핵심**
- **NLP > NLU & NLG**
  - NLP는 자연어 처리 전반을 아우르는 큰 개념이며, NLU와 NLG는 그 안에서 각각 언어 이해와 생성에 특화된 하위 분야입니다.
- **NLU**
  - 컴퓨터가 인간의 언어를 읽고 해석하는 **독해 능력**을 향상시키는 데 집중합니다. 이는 언어의 깊은 의미를 파악하는 데 필수적입니다.
- **NLG**
  - 컴퓨터가 인간처럼 자연스러운 텍스트를 쓰는 **작문 능력**을 강화하는 데 주력합니다. 이는 데이터를 사람이 이해하기 쉬운 형태로 변환하는 데 유용합니다.

### **9.5 활용 분야**
- 의료 진단(예: 환자 증상 데이터를 분석해 진단문 작성), 
- 온라인 고객 서비스(예: 챗봇으로 실시간 문의 응대), 
- 동영상 제작(예: 자동 자막 생성이나 스크립트 작성) 등 
- 예를 들어, 병원에서는 환자의 설명을 NLU로 분석하고, NLG로 의사에게 보고서를 생성할 수 있으며, 
- 유튜브에서는 영상 내용을 기반으로 자막을 자동 생성할 수 있습니다.

## 10. 골프 치러 갈까? - 의사 결정 트리 vs. 랜덤 포레스트
- 출처: [What is Random Forest?](https://www.youtube.com/watch?v=gkXX4h3qYm4&list=PLOspHqNVtKAC-FUNMq8qjYVw6_semZHw0&index=10)


### 10.1 의사 결정 트리 (Decision Tree)
1.**개념**  
- 의사 결정 트리는 "골프를 칠까 말까?"와 같은 분류 문제를 해결하기 위한 간단하면서도 직관적인 방법. 
- 이 모델은 특정 조건을 기준으로 데이터를 분류하여 결정을 내리는 과정을 트리 형태로 나타냅니다. 
- 마치 우리가 질문을 던지고 그 답에 따라 선택을 좁혀가는 과정과 비슷합니다.

2.**결정 과정**  
- 의사 결정 트리는 단계별 질문을 통해 결정을 내립니다. 예를 들어, 다음과 같은 흐름을 따를 수 있습니다:  
  - **시간이 있는가?** (Yes/No)  
    - *No:* 골프 No (시간이 없으면 골프를 칠 수 없음)  
    - *Yes:* 다음 질문으로 넘어감  
  - **날씨가 좋은가?** (Yes/No)  
    - *Yes:* 골프 Yes (날씨가 좋으면 골프를 칠 수 있음)  
    - *No:* 다음 질문으로 넘어감  
  - **골프채가 있는가?** (Yes/No)  
    - *No:* 골프 No (골프채가 없으면 칠 수 없음)  
    - *Yes:* 골프 Yes (모든 조건이 충족되면 골프를 감)  

3.**장점**  
- 이해하기 쉽습니다. 
- 트리 구조는 사람이 직접 의사결정을 내리는 과정을 시각적으로 표현하므로, 모델의 작동 방식을 직관적으로 파악할 수 있습니다.

4.**단점**  
- **편향(Bias):** 데이터가 특정 변수에 치우쳐 있으면 모델의 예측이 왜곡될 수 있습니다.  
- **과적합(Overfitting):** 데이터가 복잡하거나 변수가 많을 경우, 트리가 지나치게 깊어져 학습 데이터에만 과도하게 맞춰질 수 있습니다. 이는 새로운 상황에 대한 일반화 능력을 떨어뜨립니다.


### 10.2 랜덤 포레스트 (Random Forest)

1.**개념**  
- 랜덤 포레스트는 의사 결정 트리의 단점을 보완한 머신러닝 모델로, 여러 개의 의사 결정 트리를 결합(앙상블)하여 예측 정확도를 높입니다. 
- 이는 마치 여러 전문가의 의견을 모아 더 나은 결정을 내리는 것과 같습니다. 
- "골프 치러 갈까?"라는 질문에 대해 더 신뢰할 수 있는 답을 얻기 위해 이 모델을 사용

2.**구축 과정**  
  - **데이터에서 랜덤 샘플 추출** 
    - 원본 데이터에서 무작위로 여러 개의 작은 데이터셋을 생성합니다.  
  - **각 샘플에 대해 의사 결정 트리 구축** 
    - 각 데이터셋을 기반으로 독립적인 의사 결정 트리를 만듭니다.  
  - **다양한 기준을 고려한 여러 트리 생성** 시
    - 간, 날씨, 골프채 유무, 심지어 코스 난이도 같은 다양한 특성을 고려하여 트리를 생성

3.**원리**  
- 개별 트리의 예측을 종합하여 최종 예측을 도출합니다. 
- 예를 들어, 다수결 투표나 평균을 통해 결정을 내립니다.  
- 일부 트리가 잘못된 예측을 하더라도, 다른 트리의 결과로 오류가 상쇄됩니다. 이는 모델의 강건성을 높이는 핵심 요소입니다.

4.**장점**  
- **과적합 감소** 
  - 여러 트리의 결과를 평균화함으로써, 특정 데이터에 지나치게 맞춰지는 현상을 방지합니다.  
- **편향 감소** 
  - 데이터 불균형이나 특정 변수의 과도한 영향력을 줄여 더 공정한 예측을 가능하게 합니다.  
- **예측 정확도 향상** 
  - 앙상블 학습의 특성상 개별 의사 결정 트리보다 더 높은 성능을 발휘합니다.

5.**파라미터 설정**  
- **노드 크기 (Node Size)** 
  - 각 노드에 포함될 데이터 포인트의 최소 개수를 설정합니다.  
- **트리 개수 (Number of Trees)** 
  - 트리가 너무 적으면 성능이 떨어지고, 너무 많으면 학습 시간과 메모리 사용량이 증가합니다. 보통 100~500개가 적당합니다.  
- **특성 개수 (Number of Features)** 
  - 각 트리에서 사용할 변수의 수를 결정합니다.  

6.**활용**  
- 모델 성능 평가를 위해 특정 조건(예: 골프 코스 종류, 시간대 등)에 따른 결과를 분석할 수 있습니다. 
- 이를 통해 어떤 변수가 결정에 큰 영향을 미치는지 파악할 수 있습니다.

7.**적용 분야**  
- **금융:** 고객의 채무 불이행 가능성을 예측하여 대출 승인 여부를 결정.  
- **의료:** 환자의 병력과 검사 결과를 바탕으로 치료 옵션에 따른 예후나 생존율을 예측.  
- **경제:** 정책 변화가 경제 지표에 미치는 영향을 분석.  
