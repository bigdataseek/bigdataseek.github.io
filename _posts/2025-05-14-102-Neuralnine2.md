---
title: 8차시 2:Neuralnine 2
layout: single
classes: wide
categories:
  - NeuralNine
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 8. 파이썬 다중 프로세싱

- 출처: [Python Multiprocessing Explained in 7 Minutes](https://www.youtube.com/watch?v=EI1gLCvdX_U&t=3s)

파이썬에서 **다중 프로세싱(Multiprocessing)**은 여러 작업을 동시에 처리하여 프로그램 속도를 확 끌어올리는 기술입니다. 마치 여러 명이 동시에 일해서 작업 시간을 단축하는 것과 비슷하다고 생각하면 됩니다.

### 8.1 다중 스레딩 vs. 다중 프로세싱: 뭐가 다를까?

파이썬에는 **다중 스레딩(Multi-threading)**이라는 개념도 있습니다. 하지만 파이썬은 **GIL(Global Interpreter Lock)**이라는 특성 때문에 다중 스레딩으로는 CPU를 많이 쓰는 작업에서 **진정한 동시 처리(병렬 처리)**가 불가능해요. 마치 한 명의 일꾼이 여러 작업을 번갈아 하는 것과 같아서, 실제로는 동시에 처리되는 게 아닙니다.

반면, **다중 프로세싱**은 여러 개의 독립적인 **프로세스(Process)**를 만들어서 각각의 프로세스가 동시에 작업을 처리하게 합니다. 이는 마치 여러 명의 일꾼이 각자 맡은 일을 동시에 처리하는 것과 같아서, **진정한 동시 처리(병렬 처리)**가 가능하고, 그만큼 작업 속도도 빨라지는 거죠.

### 8.2 다중 프로세싱을 위한 파이썬 도구들

파이썬의 `multiprocessing` 모듈에는 다중 프로세싱을 효율적으로 사용할 수 있도록 도와주는 여러 가지 도구(클래스)들이 있습니다. 몇 가지 주요 도구들을 알아볼까요?

* **`Pool` (풀)**:
    * **언제 쓸까요?** 여러 개의 작업에 똑같은 계산을 동시에 적용할 때 유용해요. 예를 들어, 12,000개의 숫자에 복잡한 계산을 해야 한다면, 이 `Pool`을 사용하면 혼자 계산할 때보다 훨씬 빠르게 끝낼 수 있어요. 마치 여러 명의 전문가가 각자 할당된 숫자에 대해 동시에 계산하는 것과 같아요.
    * **예시**: 12,000개의 숫자에 팩토리얼(factorial) 계산을 할 때, 혼자 하면 9.1초 걸리던 작업을 5개의 `Pool` 프로세스를 사용하면 2.9초 만에 끝낼 수 있어요. 엄청 빠르죠!

* **`Process` (프로세스)**:
    * **언제 쓸까요?** 아주 특정한 한 가지 작업을 별도의 프로세스로 실행하고 싶을 때 사용해요.
    * **예시**: 컴퓨터가 복잡한 계산을 하는 동안에도, 동시에 다른 프로세스가 5초마다 "잘 작동하고 있어요!"라는 메시지를 계속 출력하게 할 수 있어요. 계산과 메시지 출력이 동시에 이루어지는 거죠.

* **`Queue` (큐, 대기열)**:
    * **언제 쓸까요?** 여러 프로세스가 서로 정보를 주고받을 때 사용해요. 특히 정보가 들어간 순서대로 정확하게 처리되어야 할 때 아주 유용합니다.
    * **예시**: 한 프로세스가 물건을 생산해서 큐에 넣어두면, 다른 프로세스들이 큐에 있는 물건들을 하나씩 가져다가 소비해요. 아무리 여러 프로세스가 동시에 가져가도 큐에 들어간 순서대로 정확하게 처리됩니다.

* **`Pipe` (파이프)**:
    * **언제 쓸까요?** 두 개의 프로세스가 직접적으로 대화할 수 있는 통로를 만들 때 사용해요.
    * **예시**: 한 프로세스가 다른 프로세스에게 메시지를 보내면, 다른 프로세스가 그 메시지를 바로 받아서 처리할 수 있어요. 필요하다면 양쪽에서 서로 메시지를 주고받을 수도 있습니다.

* **`Lock` (잠금)**:
    * **언제 쓸까요?** 여러 프로세스가 동시에 한 가지 중요한 데이터(자원)를 건드려서 문제가 생기는 것을 막을 때 사용해요. 마치 화장실 문에 잠금장치가 있어서 한 번에 한 명만 들어갈 수 있게 하는 것과 같아요.
    * **예시**: 네 개의 프로세스가 하나의 공유된 숫자 값을 100,000번씩 늘리려고 할 때, `Lock`을 사용하지 않으면 값이 엉망이 될 수 있어요. 하지만 `Lock`을 사용하면 한 번에 하나의 프로세스만 값을 변경할 수 있도록 보장해서, 우리가 원하는 정확한 최종 값(400,000)을 얻을 수 있습니다.

* **`Semaphore` (세마포어)**:
    * **언제 쓸까요?** `Lock`과 비슷하지만, 동시에 여러 프로세스가 특정 자원에 접근할 수 있도록 허용할 때 사용해요. 하지만 그 허용 개수는 우리가 정할 수 있습니다.
    * **예시**: 6개의 프로세스가 있는데, 동시에 2개의 프로세스만 특정 작업을 하도록 제한하고 싶을 때 `Semaphore`를 사용합니다. 마치 6명의 일꾼이 있지만, 2개의 작업대만 있어서 동시에 2명만 작업할 수 있는 것과 비슷해요.


### 8.3 `Pool` (풀) 예제

`Pool`을 사용하여 여러 숫자의 제곱을 병렬로 계산하는 예제입니다.

```python
import multiprocessing
import os
import time

def calculate_square(number):
    """주어진 숫자의 제곱을 계산합니다."""
    # print(f"Process ID: {os.getpid()} - Calculating square of {number}")
    return number * number

if __name__ == "__main__":
    numbers = range(10)  # 0부터 9까지의 숫자

    print("--- Pool 예제 시작 ---")
    start_time = time.time()

    # 4개의 프로세스 풀을 생성
    with multiprocessing.Pool(processes=4) as pool:
        # map 함수를 사용하여 각 숫자에 calculate_square 함수 적용
        results = pool.map(calculate_square, numbers)

    print(f"결과: {results}")
    end_time = time.time()
    print(f"총 실행 시간: {end_time - start_time:.4f} 초")
    print("--- Pool 예제 종료 ---")

```

**설명:**

  * `calculate_square` 함수는 주어진 숫자의 제곱을 계산합니다.
  * `multiprocessing.Pool(processes=4)`는 4개의 프로세스를 사용하는 풀을 만듭니다.
  * `pool.map(calculate_square, numbers)`는 `numbers` 리스트의 각 항목에 `calculate_square` 함수를 병렬로 적용하고 결과를 수집합니다.


### 8.4 `Process` (프로세스) 예제

두 개의 독립적인 프로세스를 생성하여 각각 다른 작업을 수행하는 예제입니다.

```python
import multiprocessing
import time
import os

def task1():
    """첫 번째 프로세스에서 실행될 작업입니다."""
    print(f"Process ID: {os.getpid()} - Task 1 시작")
    for i in range(3):
        time.sleep(1)
        print(f"Process ID: {os.getpid()} - Task 1 진행 중... ({i+1}/3)")
    print(f"Process ID: {os.getpid()} - Task 1 완료")

def task2():
    """두 번째 프로세스에서 실행될 작업입니다."""
    print(f"Process ID: {os.getpid()} - Task 2 시작")
    for i in range(2):
        time.sleep(1.5)
        print(f"Process ID: {os.getpid()} - Task 2 진행 중... ({i+1}/2)")
    print(f"Process ID: {os.getpid()} - Task 2 완료")

if __name__ == "__main__":
    print("--- Process 예제 시작 ---")
    
    # Process 객체 생성
    p1 = multiprocessing.Process(target=task1)
    p2 = multiprocessing.Process(target=task2)

    # 프로세스 시작
    p1.start()
    p2.start()

    # 모든 프로세스가 완료될 때까지 기다림
    p1.join()
    p2.join()

    print("모든 프로세스 완료. 메인 프로세스 종료.")
    print("--- Process 예제 종료 ---")
```

**설명:**

  * `task1`과 `task2`는 각각 다른 메시지를 출력하며 일정 시간 대기하는 함수입니다.
  * `multiprocessing.Process(target=함수명)`을 사용하여 각 함수를 실행할 새 프로세스를 생성합니다.
  * `p.start()`로 프로세스를 시작합니다.
  * `p.join()`은 해당 프로세스가 끝날 때까지 메인 프로세스가 기다리도록 합니다.


### 8.5 `Queue` (큐, 대기열) 예제

`Queue`를 사용하여 생산자 프로세스와 소비자 프로세스 간에 데이터를 주고받는 예제입니다.

```python
import multiprocessing
import time
import os

def producer(queue):
    """항목을 큐에 추가하는 생산자 함수입니다."""
    print(f"Process ID: {os.getpid()} - 생산자 시작")
    for i in range(5):
        item = f"아이템 {i+1}"
        queue.put(item)
        print(f"Process ID: {os.getpid()} - {item} 생산 및 큐에 추가")
        time.sleep(0.5)
    queue.put(None) # 소비자가 작업을 마쳤음을 알리는 신호
    print(f"Process ID: {os.getpid()} - 생산자 완료")

def consumer(queue, name):
    """큐에서 항목을 가져와 처리하는 소비자 함수입니다."""
    print(f"Process ID: {os.getpid()} - 소비자 {name} 시작")
    while True:
        item = queue.get()
        if item is None:
            # 생산자가 보낸 종료 신호를 받으면 다시 큐에 넣고 종료
            queue.put(None)
            break
        print(f"Process ID: {os.getpid()} - 소비자 {name} - {item} 소비")
        time.sleep(1)
    print(f"Process ID: {os.getpid()} - 소비자 {name} 완료")

if __name__ == "__main__":
    print("--- Queue 예제 시작 ---")
    
    q = multiprocessing.Queue() # 큐 생성

    # 생산자 프로세스 생성
    p_producer = multiprocessing.Process(target=producer, args=(q,))
    # 소비자 프로세스 생성
    p_consumer1 = multiprocessing.Process(target=consumer, args=(q, "A"))
    p_consumer2 = multiprocessing.Process(target=consumer, args=(q, "B"))

    # 프로세스 시작
    p_producer.start()
    p_consumer1.start()
    p_consumer2.start()

    # 모든 프로세스가 완료될 때까지 기다림
    p_producer.join()
    p_consumer1.join()
    p_consumer2.join()

    print("모든 프로세스 완료. 메인 프로세스 종료.")
    print("--- Queue 예제 종료 ---")
```

**설명:**

  * `producer` 함수는 `queue.put()`을 사용하여 큐에 데이터를 넣습니다.
  * `consumer` 함수는 `queue.get()`을 사용하여 큐에서 데이터를 가져옵니다.
  * `queue.put(None)`은 생산자가 더 이상 생산할 것이 없음을 소비자에게 알리는 "종료 신호"로 사용됩니다.


### 8.6 `Pipe` (파이프) 예제

두 프로세스 간에 단방향 및 양방향 통신을 설정하는 예제입니다.

```python
import multiprocessing
import time
import os

def sender(conn, messages):
    """메시지를 파이프를 통해 보내는 함수입니다."""
    print(f"Process ID: {os.getpid()} - Sender 시작")
    for msg in messages:
        conn.send(msg)
        print(f"Process ID: {os.getpid()} - 메시지 보냄: {msg}")
        time.sleep(0.5)
    conn.close() # 통신 종료
    print(f"Process ID: {os.getpid()} - Sender 완료")

def receiver(conn):
    """파이프를 통해 메시지를 받는 함수입니다."""
    print(f"Process ID: {os.getpid()} - Receiver 시작")
    while True:
        try:
            msg = conn.recv()
            print(f"Process ID: {os.getpid()} - 메시지 받음: {msg}")
        except EOFError: # 파이프가 닫히면 발생하는 오류
            break
        time.sleep(0.7)
    print(f"Process ID: {os.getpid()} - Receiver 완료")

if __name__ == "__main__":
    print("--- Pipe 예제 시작 (단방향) ---")
    
    # 단방향 파이프 생성 (부모-자식 연결)
    parent_conn, child_conn = multiprocessing.Pipe(duplex=False) # duplex=False는 단방향을 의미

    messages_to_send = ["안녕", "파이프", "예제", "데이터"]
    
    p_sender = multiprocessing.Process(target=sender, args=(child_conn, messages_to_send))
    p_receiver = multiprocessing.Process(target=receiver, args=(parent_conn,))

    p_sender.start()
    p_receiver.start()

    p_sender.join()
    p_receiver.join()

    print("\n--- Pipe 예제 시작 (양방향) ---")
    
    # 양방향 파이프 생성
    conn1, conn2 = multiprocessing.Pipe(duplex=True) # duplex=True는 양방향을 의미

    def worker1(conn):
        conn.send("Worker1 입니다. 안녕!")
        print(f"Process ID: {os.getpid()} - Worker1 받음: {conn.recv()}")
        conn.close()

    def worker2(conn):
        print(f"Process ID: {os.getpid()} - Worker2 받음: {conn.recv()}")
        conn.send("Worker2 입니다. 잘 가!")
        conn.close()

    p_worker1 = multiprocessing.Process(target=worker1, args=(conn1,))
    p_worker2 = multiprocessing.Process(target=worker2, args=(conn2,))

    p_worker1.start()
    p_worker2.start()

    p_worker1.join()
    p_worker2.join()

    print("모든 프로세스 완료. 메인 프로세스 종료.")
    print("--- Pipe 예제 종료 ---")
```

**설명:**

  * `multiprocessing.Pipe()`를 호출하면 두 개의 연결 객체(`conn1`, `conn2`)가 반환됩니다. 이 두 객체는 파이프의 양쪽 끝을 나타냅니다.
  * `conn.send()`를 사용하여 데이터를 파이프를 통해 보냅니다.
  * `conn.recv()`를 사용하여 파이프에서 데이터를 받습니다.
  * `duplex=True`로 설정하면 양방향 통신이 가능하며, 기본값은 `False`로 단방향입니다.


### 8.7 `Lock` (잠금) 예제

`Lock`을 사용하여 여러 프로세스가 공유 자원(공유 숫자)을 안전하게 수정하는 예제입니다.

```python
import multiprocessing
import time
import os

def increment_shared_counter(counter, lock):
    """공유 카운터 값을 증가시키는 함수입니다."""
    print(f"Process ID: {os.getpid()} - 카운터 증가 시작")
    for _ in range(100000): # 10만 번 증가
        lock.acquire() # 락 획득 (다른 프로세스가 접근 못하게 잠금)
        try:
            counter.value += 1
        finally:
            lock.release() # 락 해제 (다른 프로세스가 접근 가능하게 풀어줌)
    print(f"Process ID: {os.getpid()} - 카운터 증가 완료")

if __name__ == "__main__":
    print("--- Lock 예제 시작 ---")
    
    # 공유할 숫자 변수 생성 (Value 객체는 여러 프로세스에서 공유 가능)
    shared_counter = multiprocessing.Value('i', 0) # 'i'는 정수형을 의미, 초기값 0
    lock = multiprocessing.Lock() # 락 객체 생성

    processes = []
    num_processes = 4

    for _ in range(num_processes):
        p = multiprocessing.Process(target=increment_shared_counter, args=(shared_counter, lock))
        processes.append(p)
        p.start()

    for p in processes:
        p.join()

    print(f"최종 카운터 값: {shared_counter.value}") # 예상 값: 400000
    print("--- Lock 예제 종료 ---")
```

**설명:**

  * `multiprocessing.Value('i', 0)`는 여러 프로세스에서 공유할 수 있는 정수형 변수를 생성합니다.
  * `multiprocessing.Lock()`으로 락 객체를 생성합니다.
  * `lock.acquire()`는 락을 획득하여 다른 프로세스가 해당 코드 블록에 접근하지 못하도록 합니다.
  * `lock.release()`는 락을 해제하여 다른 프로세스가 접근할 수 있도록 합니다. `try...finally` 블록을 사용하여 락이 항상 해제되도록 보장하는 것이 중요합니다.


### 8.8 `Semaphore` (세마포어) 예제

`Semaphore`를 사용하여 동시에 특정 작업에 접근할 수 있는 프로세스의 수를 제한하는 예제입니다.

```python
import multiprocessing
import time
import os

def worker_with_semaphore(semaphore, worker_id):
    """세마포어를 사용하여 작업하는 함수입니다."""
    print(f"Process ID: {os.getpid()} - Worker {worker_id} 대기 중...")
    semaphore.acquire() # 세마포어 획득 (접근 허용 대기)
    try:
        print(f"Process ID: {os.getpid()} - Worker {worker_id} 작업 시작! (동시 실행 가능 수: 2)")
        time.sleep(2) # 작업 수행 시간
        print(f"Process ID: {os.getpid()} - Worker {worker_id} 작업 완료!")
    finally:
        semaphore.release() # 세마포어 해제 (다른 프로세스가 접근 가능하도록)

if __name__ == "__main__":
    print("--- Semaphore 예제 시작 ---")
    
    # 동시에 2개의 프로세스만 접근을 허용하는 세마포어 생성
    semaphore = multiprocessing.Semaphore(2) 

    processes = []
    num_workers = 6 # 총 6개의 워커 프로세스 생성

    for i in range(num_workers):
        p = multiprocessing.Process(target=worker_with_semaphore, args=(semaphore, i + 1))
        processes.append(p)
        p.start()

    for p in processes:
        p.join()

    print("모든 워커 프로세스 완료. 메인 프로세스 종료.")
    print("--- Semaphore 예제 종료 ---")
```

**설명:**

  * `multiprocessing.Semaphore(2)`는 동시에 2개의 프로세스만 `acquire()`를 성공적으로 호출할 수 있도록 제한합니다.
  * `semaphore.acquire()`는 세마포어를 획득하여 허용된 개수 내에서 작업을 시작합니다.
  * `semaphore.release()`는 세마포어를 해제하여 다른 프로세스가 접근할 수 있도록 합니다.


## 9. 파이썬으로 가우시안 나이브 베이즈 분류기 만들기 
- 출처: [Gaussian Naive Bayes From Scratch in Python (Mathematical)](https://www.youtube.com/watch?v=AqW3aPSPIhc)

직접 파이썬(Python)으로 **가우시안 나이브 베이즈(Gaussian Naive Bayes)**라는 인공지능 분류기를 만드는 방법을 알려줘요. 복잡한 머신러닝 도구(SciKit-learn 같은)를 사용하지 않고, 오직 **순수한 파이썬과 넘파이(NumPy)**만으로 핵심 수학 공식을 직접 구현하는 데 초점을 맞춰요.

### 9.1 가우시안 나이브 베이즈, 이게 무슨 뜻일까요?

* **가우시안(Gaussian):** 
    *   이 분류기는 데이터를 '**정규 분포(normal distribution)**' 또는 '**가우시안 분포(Gaussian distribution)**'라는 특별한 모양의 종(鐘)처럼 생긴 곡선으로 가정하고 사용해요. 통계에서 많이 쓰는 분포 모양이에요.
* **나이브(Naive):** 
    *   '순진하다'는 뜻인데, 여기서는 데이터의 여러 **특징(features)** (예: 키, 몸무게)들이 서로에게 아무 영향을 주지 않고 '**완전히 독립적**'이라고 가정한다는 의미예요. 실제로는 그렇지 않을 때가 많지만, 이렇게 가정하면 계산이 훨씬 쉬워져요.
* **베이즈(Bayes):** 
    *   **베이즈 정리(Bayes' Theorem)**라는 확률 이론에 기반을 두고 있어요. 이 정리는 어떤 사건이 일어날 확률을 새로운 정보를 바탕으로 업데이트하는 방법이에요.

이 알고리즘을 이해하려면 **확률, 통계, 정규 분포, 평균, 분산** 같은 기본적인 수학 지식이 필요하다고 해요. 특히 다음 용어들을 알아두면 좋아요:

* **베이즈 정리**
* **확률**
* **변수 독립성**
* **정규 분포**
* **표준 편차**
* **분산**
* **평균**

### 9.2 베이즈 정리와 '나이브'한 가정

우리가 궁금한 건 '어떤 데이터를 봤을 때, 이게 특정 종류(클래스)에 속할 확률'이에요. 이걸 수학적으로는 $P(Y=K\|X)$라고 써요. (X라는 데이터를 봤을 때 Y가 K일 확률)

* **기본 베이즈 정리:** 
    *   $$P(Y=K|X) = \frac{P(Y=K) \times P(X|Y=K)}{P(X)}$$
    여기서 $P(Y=K)$는 **사전 확률(prior probability)**이라고 하는데, '어떤 데이터를 보기 전에 Y가 K일 확률'을 의미해요.

* **$P(X)$ 무시:** 
    *   우리가 두 가지 종류(예: '암' 또는 '정상') 중에서 더 확률이 높은 쪽을 고를 때는, 이 분모인 $P(X)$는 항상 같기 때문에 계산에 넣지 않아도 괜찮아요. 결국 우리는 $P(Y=K) \times P(X\|Y=K)$ 값이 가장 큰 클래스를 찾으면 돼요.

* **나이브 가정의 마법:** 
    *   $P(X|Y=K)$ 부분이 복잡해지는 것을 피하기 위해, 모든 개별 특징($X_j$)이 클래스 $Y=K$일 때 **서로 독립적**이라고 가정해요. 이렇게 하면 $P(X|Y=K)$는 각 특징의 확률 $P(X_j|Y=K)$를 **모두 곱하는** 형태로 아주 간단해져요. $$P(X|Y=K) = P(X_1|Y=K) \times P(X_2|Y=K) \times \dots \times P(X_n|Y=K)$$
    또는 수학 기호로는 다음과 같이 나타낼 수 있어요.
    $$P(X|Y=K) = \prod_{j=1}^{n} P(X_j|Y=K)$$
    여기서 $\prod$는 모든 $P(X_j|Y=K)$를 곱하라는 의미예요.

### 9.3 가우시안 분포의 적용

각각의 특징 $X_j$가 특정 종류 $K$에 속할 확률 $P(X_j\|Y=K)$는 **가우시안 분포(정규 분포)**를 따른다고 봐요. 이 가우시안 분포는 그 종류 $K$에 속하는 특징 $J$들의 **평균($\mu_{KJ}$)**과 **분산($\sigma_{KJ}^2$)**을 가지고 정의돼요.

* **가우시안 분포의 확률 밀도 함수(PDF) 공식:**
    *   $$P(X_j|Y=K) = \frac{1}{\sqrt{2\pi \sigma_{KJ}^2}} e^{-\frac{(X_j - \mu_{KJ})^2}{2\sigma_{KJ}^2}}$$
    여기서 $X_j$는 우리가 관측한 특징 값이에요. 이 공식이 바로 가우시안 분포의 종 모양 곡선을 나타내는 식이에요.


### 9.4 모델 파라미터 계산 (학습 과정)

컴퓨터가 데이터를 보고 스스로 배우는 과정을 '학습(Fit)'이라고 해요. 

* **사전 확률 (Prior Probability):** 
    *   주어진 클래스 $K$의 데이터 개수를 전체 데이터 개수로 나누어 계산해요.
    $$P(Y=K) = \frac{\text{클래스 K의 인스턴스 수}}{\text{전체 인스턴스 수}}$$
* **평균 (Mean, $\mu_{KJ}$):** 
    *   각 클래스 $K$ 내에서 특정 특징 $J$에 대한 모든 샘플 값들을 더한 후, 해당 클래스 $K$의 인스턴스 수로 나누어 계산해요.
    $$\mu_{KJ} = \frac{1}{N_K} \sum_{i \in \text{클래스 K}} X_{i,j}$$
    여기서 $N_K$는 클래스 $K$의 데이터 개수예요.
* **분산 (Variance, $\sigma_{KJ}^2$):** 
    *   각 클래스 $K$ 내에서 특정 특징 $J$에 대한 샘플 값들의 제곱 편차 합을 해당 클래스 $K$의 인스턴스 수로 나누어 계산해요.
    $$\sigma_{KJ}^2 = \frac{1}{N_K} \sum_{i \in \text{클래스 K}} (X_{i,j} - \mu_{KJ})^2$$

이 모든 값들은 우리가 컴퓨터에게 제공하는 **학습 데이터(X, Y)**를 가지고 계산해요.


### 9.5 언더플로우 문제와 로그 확률

* **문제:** 
    *   확률 값들은 보통 0과 1 사이의 아주 작은 숫자예요. 이런 작은 숫자들을 계속 곱하면 컴퓨터가 너무 작아서 제대로 계산하지 못하고 '언더플로우(underflow)'라는 오류가 날 수 있어요. 이는 수치적으로 불안정하게 만들어요.
* **해결책:** 
    *   곱셈을 덧셈으로 바꿔서 이 문제를 해결해요. 어떻게 하냐고요? 확률 값에 **로그(log)**를 씌우면 곱셈이 덧셈으로 바뀌어요. (수학적으로 $log(A \times B) = log(A) + log(B)$)

    *   따라서, 우리가 비교하려는 최종 확률 $P(Y=K\|X) \propto P(Y=K) \times \prod_{j=1}^{n} P(X_j\|Y=K)$에 로그를 적용하면:

    $$\log(P(Y=K\|X)) \propto \log(P(Y=K)) + \sum_{j=1}^{n} \log(P(X_j\|Y=K))$$
    *   여기서 $\sum$는 모든 $\log(P(X_j\|Y=K))$를 더하라는 의미예요.

* **가우시안 분포의 로그:**
    *    $P(X_j|Y=K)$는 가우시안 분포 공식이었죠? 여기에 로그를 적용하면 다음과 같이 간단해져요.
    $$\log(P(X_j\|Y=K)) = -\frac{1}{2} \log(2\pi \sigma_{KJ}^2) - \frac{(X_j - \mu_{KJ})^2}{2\sigma_{KJ}^2}$$

* **최종 목표:** 
    *   결국 우리가 할 일은 이 **로그 확률 값**을 **가장 크게 만드는** 종류(클래스 $K$)를 찾는 거예요. (수학적으로는 $\text{argmax}_K$)

### 9.6 파이썬 구현 상세

* **환경 설정:**
    * **NumPy:** 효율적인 배열 작업과 선형 대수 계산에 필요한 라이브러리예요.
    * **scikit-learn:** 데이터셋을 불러오고, 데이터를 학습/테스트용으로 나누고, 모델 성능을 평가할 때 사용해요. (하지만 실제 머신러닝 모델 구현에는 사용하지 않아요!)
* **클래스 구조:** `GaussianNB`라는 이름의 파이썬 클래스를 만들고, 이 클래스 안에 `fit` (학습)과 `predict` (예측)이라는 함수를 넣어요.

    * `fit(self, X, y)` 메서드:
        * 입력 데이터를 넘파이(NumPy) 배열로 변환해요.
        * 데이터에 있는 고유한 클래스(정답 종류)들을 식별하고, 클래스 수와 특징 수를 결정해요.
        * 각 클래스와 특징에 대한 **평균, 분산, 사전 확률**을 저장할 빈 넘파이 배열을 초기화해요.
        * 모든 클래스를 하나씩 살펴보면서, 각 클래스에 해당하는 데이터($X_k$)를 필터링하고, **해당 클래스의 평균, 분산, 사전 확률을 계산하여 저장**해요.

    * `_log_gaussian(self, X)` 메서드:
        * 주어진 데이터 $X$에 대해 **가우시안 분포의 로그 확률 값**을 계산하는 내부 헬퍼 함수예요.
        * 위에서 설명한 로그 가우시안 분포 공식
            $$-\frac{1}{2} \log(2\pi \sigma_{KJ}^2) - \frac{(X_j - \mu_{KJ})^2}{2\sigma_{KJ}^2}$$
            을 넘파이(NumPy) 연산으로 직접 구현해요.
        * 계산된 로그 확률들을 모두 **더해서** 반환해요.

    * `predict(self, X)` 메서드:
        * 새로운 입력 데이터 $X$를 넘파이 배열로 변환해요.
        * `_log_gaussian` 메서드를 사용하여 각 클래스에 대한 **로그 우도(log likelihood)**를 계산해요.
        * 학습 단계에서 계산한 **사전 확률에 대한 로그 값($\log(P(Y=K))$)**을 가져와요.
        * 각 클래스에 대해 로그 우도와 로그 사전 확률을 더한 후 (위에서 설명한 합산 공식), **`np.argmax`를 사용하여 이 합을 최대화하는 클래스를 최종 예측으로 반환**해요. 즉, 이 데이터가 어떤 클래스에 속할 확률이 가장 높은지 찾아주는 거죠.

* **예제 및 평가:** 
    *   **유방암(breast cancer) 데이터셋**을 사용하여 모델을 학습하고 (데이터를 학습용과 테스트용으로 나누어 활용), 예측을 수행한 후 **정확도(accuracy score)**를 계산하여 모델의 성능을 평가해요. 직접 구현된 모델은 **90% 이상의 정확도**를 보여준다고 해요!

## 10.파이썬으로 뇌종양 분류하기
- 출처:[Brain Tumor Classifier in Python ](https://www.youtube.com/watch?v=mGJ6OZmR-Xk)

PyTorch로 Convolutional Neural Network(CNN)를 구축하고 훈련하여 **MRI 스캔 이미지를 기반으로 뇌종양 유형을 분류**하는 과정을 보여주는 튜토리얼입니다. 이 프로젝트는 초보자 또는 중급 프로그래머에게 적합하며, 의학적 조언으로 사용되어서는 안 되는 프로그래밍 연습임을 강조합니다.

### 10.1 데이터셋 및 개발 환경 설정
*   **프로젝트 목표**: MRI 스캔을 사용하여 뇌종양 유무와 종양 유형(교모세포종, 수막종, 뇌하수체 종양)을 분류하는 CNN을 구축하고 훈련하는 것입니다.

*   **데이터셋**:
    *   Kaggle의 "뇌종양 MRI 데이터셋"을 사용합니다.
    *   데이터셋은 **'no tumor' (종양 없음)**, **'glioma' (교모세포종)**, **'meningioma' (수막종)**, **'pituitary' (뇌하수체 종양)**의 네 가지 하위 디렉토리를 포함하는 훈련(training) 및 테스트(testing) 디렉토리로 구성됩니다. 각 디렉토리에는 해당 유형의 스캔 이미지가 들어 있습니다.

*   **개발 환경 및 패키지 설치**:
    *   **Jupyter Lab** 또는 일반 IPython 노트북 사용을 권장합니다.
    *   UV 또는 pip를 사용하여 **PyTorch (torch), TorchVision (torchvision), Matplotlib (matplotlib)**를 설치해야 합니다.

### 10.2 초기 설정 
*   **초기 설정 및 데이터 준비**:
    *   **필수 라이브러리 임포트**: `torch`, `torch.nn`, `torch.optim`, `torchvision.datasets`, `torchvision.transforms`, `torch.utils.data.DataLoader`.
    *   **디바이스 설정**: 사용 가능한 경우 **CUDA GPU**를 사용하고, 그렇지 않으면 CPU로 대체하여 연산 속도를 최적화합니다.
    *   **이미지 변환 정의 (Transforms)**: 이미지를 **128x128 픽셀로 크기 조정**, **텐서로 변환**, 그리고 **정규화(0.5, 0.5, 0.5)**를 적용하여 신경망이 원시 픽셀 값 대신 정규화된 데이터로 작업하도록 합니다.
    *   **데이터 로더 생성**: 훈련 및 테스트 데이터셋을 위한 `DataLoader`를 생성합니다. 배치 크기는 32로 설정하고, 훈련 데이터는 섞고(shuffle), 데이터 로딩 속도 향상을 위해 4개의 워커와 `pin_memory=True`를 사용합니다.

    ```python
    # 1. 필수 라이브러리 임포트
    import os
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torchvision import datasets, transforms
    from torch.utils.data import DataLoader
    import matplotlib.pyplot as plt
    from torchvision.transforms.functional import to_pil_image
    import numpy as np

    # 2. 디바이스 설정
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}')

    # 3. 이미지 변환 정의 (128x128, Tensor 변환, 정규화)
    transform = transforms.Compose([
        transforms.Resize((128, 128)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)
    ])

    # 4. 데이터셋 및 데이터로더 설정
    train_dir = './dataset/Training'
    test_dir = './dataset/Testing'

    train_dataset = datasets.ImageFolder(train_dir, transform=transform)
    test_dataset = datasets.ImageFolder(test_dir, transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)

    class_names = train_dataset.classes  # ['glioma', 'meningioma', 'no_tumor', 'pituitary']
    print(f"Class names: {class_names}")

    ```

### 10.3 모델 정의

*   **모델 정의 (CNN 아키텍처)**:
    *   `nn.Sequential`을 사용하여 TensorFlow Keras와 유사한 방식으로 모델을 정의합니다.
    *   **합성곱(Convolutional) 레이어 (Conv2d)**: 이미지에서 특징을 추출하기 위해 필터(커널)를 사용합니다. 이 모델은 입력 채널 3개(RGB)에서 시작하여 **32, 64, 128 채널로 확장**됩니다.
    *   **활성화 함수 (ReLU)**: 비선형성을 추가합니다.
    *   **최대 풀링(Max Pooling) 레이어 (MaxPool2d)**: 특징 맵의 크기를 줄이고 정규화에 기여합니다.
    *   **플래튼(Flatten) 레이어**: 합성곱 레이어의 2D 출력을 1D 벡터로 평탄화합니다.
    *   **선형(Linear) 레이어 (Dense Layer)**: '추론' 또는 '논리'를 담당하는 완전 연결 레이어입니다. 마지막 선형 레이어는 **4개의 출력 뉴런**을 가지는데, 이는 **4가지 클래스(종양 없음, 교모세포종, 수막종, 뇌하수체 종양)**를 분류하기 위함입니다.
    *   **드롭아웃(Dropout) 레이어**: 과적합을 방지하기 위한 정규화 기법으로 0.5의 확률을 사용합니다.
    *   모델을 정의한 후에는 `model.to(device)`를 사용하여 모델을 GPU(사용 가능한 경우)로 이동시킵니다.

    ```python
    # 5. CNN 모델 정의
    model = nn.Sequential(
        nn.Conv2d(3, 32, kernel_size=3, padding=1),  # [B, 32, 128, 128]
        nn.ReLU(),
        nn.MaxPool2d(2),                            # [B, 32, 64, 64]

        nn.Conv2d(32, 64, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2),                            # [B, 64, 32, 32]

        nn.Conv2d(64, 128, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2),                            # [B, 128, 16, 16]

        nn.Flatten(),                               # [B, 128*16*16]
        nn.Dropout(0.5),
        nn.Linear(128 * 16 * 16, 256),
        nn.ReLU(),
        nn.Linear(256, 4)  # 4-class classification
    ).to(device)
    ```

### 10.4 손실 함수 및 훈련 루프

*   **옵티마이저 및 손실 함수**:
    *   **옵티마이저**: `optim.AdamW` (가중치 감소 포함)를 사용하고 학습률은 1e-4로 설정합니다.
    *   **손실 함수**: **`nn.CrossEntropyLoss`**를 사용하여 다중 클래스 분류에 적합한 손실을 계산합니다.

    ```python
    # 6. 손실 함수 및 옵티마이저 정의
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)

    ```

*   **훈련 루프**:
    *   총 **25 에폭(epochs)** 동안 모델을 훈련합니다.
    *   각 배치마다 옵티마이저의 기울기를 0으로 설정하고, 예측을 수행하며, 손실을 계산하고, **역전파(loss.backward())**를 통해 기울기를 업데이트한 다음, **옵티마이저 스텝(opt.step())**을 수행하여 모델 파라미터를 업데이트합니다.
    *   에폭별 손실을 출력하여 훈련 진행 상황을 모니터링합니다.

    ```python
    # 7. 훈련 루프
    epochs = 25
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)

            loss.backward()
            optimizer.step()
            running_loss += loss.item() * images.size(0)

        epoch_loss = running_loss / len(train_loader.dataset)
        print(f"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}")

    ```

### 10.4 평가 및 시각화

*   **평가 및 정확도 측정**:
    *   훈련 후 모델을 **평가 모드(`model.eval()`)**로 전환하고 기울기 계산을 비활성화(`torch.no_grad()`)합니다.
    *   테스트 데이터 로더를 사용하여 **테스트 손실과 정확도**를 계산합니다.
    *   이 모델은 **96.49%의 테스트 정확도와 0.12의 손실**을 달성했습니다. 4개의 클래스를 분류하는 경우 무작위 추측 정확도는 25%이므로 이 결과는 매우 우수합니다.
    
    ```python
    # 8. 평가 (테스트 정확도)
    model.eval()
    correct = 0
    total = 0
    test_loss = 0.0

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            test_loss += loss.item() * images.size(0)

            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

    avg_test_loss = test_loss / len(test_loader.dataset)
    accuracy = correct / total * 100
    print(f"Test Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.2f}%")
    ```

*   **결과 시각화**:
    *   `matplotlib`과 `torchvision.transforms.functional.to_pil_image`를 사용하여 임의의 이미지를 선택하고, 모델이 예측한 클래스와 실제 클래스를 시각적으로 비교하여 보여줍니다.

    ```python
    # 9. 결과 시각화
    def visualize_prediction():
        model.eval()
        images, labels = next(iter(test_loader))
        idx = np.random.randint(0, images.size(0))
        image, label = images[idx], labels[idx]

        with torch.no_grad():
            output = model(image.unsqueeze(0).to(device))
            _, predicted = torch.max(output, 1)

        plt.imshow(to_pil_image(image))
        plt.title(f"True: {class_names[label]}, Pred: {class_names[predicted.item()]}")
        plt.axis('off')
        plt.show()

    visualize_prediction()
    ```

*   **향후 개선 사항**:
    *   하이퍼파라미터 튜닝, 다른 아키텍처 시도, 다른 옵티마이저 또는 학습률 스케줄링 사용, 모델 체크포인팅 등을 통해 결과를 더 개선할 수 있습니다.

## 11. 사이킷런 시작하기
- 출처:[Scikit-Learn Full Crash Course](https://www.youtube.com/watch?v=SIEaLBXr0rk
)
Scikit-learn은 신경망을 사용하지 않는 전통적인 기계 학습(예: 회귀, 분류, 클러스터링, PCA)을 위한 **파이썬 툴킷**입니다. 이 강좌는 기계 학습 방법론 자체를 깊이 다루기보다는 **Scikit-learn 툴킷을 사용하는 방법**에 중점을 둡니다.


### 11.1 **선수 지식 및 환경 설정**:
*   **선수 지식**: Python 코딩에 익숙하고 NumPy, Pandas, Matplotlib 라이브러리에 대한 기본적인 이해가 필요합니다.
*   **패키지 설치**: `pip` 또는 `uv`와 같은 패키지 관리자를 사용하여 `scikit-learn`, `numpy`, `pandas`, `matplotlib`을 설치해야 합니다.
*   **개발 환경**: Jupyter Lab과 같은 인터랙티브 Python 노트북 환경에서 작업하는 것이 권장됩니다. 이는 코드의 개별 부분을 실행하고 변경 사항을 빠르게 테스트하는 데 편리합니다.

### 11.2 **데이터 다루기**:
*   **데이터셋 로드/가져오기/생성**: Scikit-learn의 `datasets` 모듈은 다양한 데이터셋을 제공합니다.
    *   `load` 함수: 패키지 자체에서 데이터셋을 로드합니다 (예: `load_breast_cancer`, `load_iris`).
    *   `fetch` 함수: 인터넷에서 데이터셋을 다운로드합니다 (예: `fetch_california_housing`, `fetch_openml`).
    *   `make` 함수: 특정 분포 또는 구조에 따라 데이터를 무작위로 생성합니다 (예: `make_blobs`, `make_moons`). `random_state` 매개변수를 사용하여 데이터 생성의 일관성을 유지할 수 있습니다.
*   **데이터 형식**: `return_X_y=True` 옵션을 사용하여 데이터를 피처(X)와 타겟(Y)의 NumPy 배열로 반환받거나, `as_frame=True` 옵션으로 Pandas DataFrame 형식으로 로드할 수 있습니다.

### 11.3 **데이터 분할 (`sklearn.model_selection`)**:
*   **훈련-테스트 분할 (Train-Test Split)**: 데이터를 훈련 세트와 테스트 세트로 나눕니다. 이는 모델이 훈련 데이터를 암기(과적합)하는 대신 일반화 능력을 학습했는지 평가하기 위해 필수적입니다.
*   **계층적 셔플 분할 (Stratified Shuffle Split)**: 훈련 및 테스트 데이터 세트에서 클래스의 분포(비율)가 유사하도록 보장하여 불균형한 데이터 분할로 인한 문제를 방지합니다.

### 11.4 **데이터 전처리 (`sklearn.preprocessing`)**:
*   **스케일링 (Scaling)**:
    *   **Standard Scaler**: 데이터를 제로 중심(평균을 0으로) 및 단위 분산(분산을 1로)으로 스케일링합니다. 이는 유클리드 거리와 같은 거리 측정에 기반한 알고리즘(예: K-Nearest Neighbors, Linear Regression, SVM)에 매우 중요합니다.
    *   **Min-Max Scaler**: 모든 피처 값을 0과 1 사이로 압축합니다.
    *   **주의**: 결정 트리 기반 알고리즘(예: Random Forest, Decision Tree)은 스케일링에 덜 민감합니다.
*   **인코딩 (Encoding)**: 비수치적 피처를 기계 학습 알고리즘이 이해할 수 있는 수치 형식으로 변환합니다.
    *   **Ordinal Encoder**: '작음', '중간', '큼'과 같이 순서가 있는 범주형 데이터를 숫자로 인코딩합니다.
    *   **One-Hot Encoder**: 순서가 없는 범주형 데이터를 여러 개의 이진(0 또는 1) 컬럼으로 변환합니다 (예: '직업', '인종').

### 11.5 **기계 학습 모델 (`sklearn.neighbors`, `sklearn.linear_model`, `sklearn.svm`, `sklearn.ensemble`, `sklearn.tree`, `sklearn.naive_bayes`, `sklearn.cluster`, `sklearn.decomposition`)**:
*   **분류 (Classification)**: 데이터를 이산적인 클래스나 레이블로 예측합니다 (예: 암 양성/음성).
    *   **K-Nearest Neighbors Classifier**: 가장 가까운 K개 이웃의 다수결로 분류합니다.
    *   **Logistic Regression**: 선형 회귀의 원리를 사용하여 분류를 수행합니다.
    *   **Decision Tree Classifier**: 결정 규칙의 트리를 사용하여 분류합니다.
    *   **Support Vector Machine (SVC)**: 데이터를 분리하는 최적의 초평면을 찾습니다.
    *   **Random Forest Classifier**: 여러 결정 트리의 앙상블을 사용합니다.
    *   **Gaussian Naive Bayes**: 베이즈 정리를 기반으로 분류합니다.
*   **회귀 (Regression)**: 연속적인 숫자 값을 예측합니다 (예: 주택 가격).
    *   **Linear Regression**: 선형 관계를 모델링합니다.
    *   **Lasso, Ridge, Elastic Net Regression**: 선형 회귀에 정규화를 추가합니다.
    *   **K-Nearest Neighbors Regressor, Support Vector Regressor (SVR), Random Forest Regressor, Decision Tree Regressor**: 분류 모델의 회귀 버전입니다.
*   **모델 사용 공통 워크플로우**: 모든 모델은 일반적으로 다음 단계를 따릅니다:
    1.  모델 인스턴스 생성 (`KNeighborsClassifier()`)
    2.  훈련 데이터로 모델 학습 (`.fit(X_train_scaled, y_train)`)
    3.  모델 성능 평가 (`.score(X_test_scaled, y_test)`)
    4.  새로운 데이터에 대한 예측 생성 (`.predict(new_instance)`)
    5.  일부 모델은 예측 확률(`predict_proba`)도 제공합니다.
*   **하이퍼파라미터**: 각 알고리즘은 성능 최적화를 위해 조정할 수 있는 고유한 하이퍼파라미터를 가집니다 (예: K-Nearest Neighbors의 `n_neighbors`, Random Forest의 `n_estimators`, `max_depth`).

### 11.6 **비지도 학습 (`sklearn.cluster`, `sklearn.decomposition`)**:
*   **클러스터링 (Clustering)**: 레이블이 없는 데이터를 유사한 그룹으로 묶습니다.
    *   **K-Means**: 데이터를 K개의 클러스터로 분할합니다.
    *   **DBSCAN**: 밀도 기반 클러스터링으로, 불규칙한 모양의 클러스터를 찾는 데 유용합니다.
*   **차원 축소 (Dimensionality Reduction)**:
    *   **주성분 분석 (PCA - Principal Component Analysis)**: 고차원 데이터를 가장 많은 분산을 설명하는 새로운 방향(주성분)으로 줄여 차원을 축소합니다. 이는 전처리 단계로 복잡성을 줄이거나 속도를 높이는 데 사용될 수 있습니다.

### 11.7 **모델 평가 (`sklearn.metrics`)**:
*   **분류 지표**:
    *   **Accuracy Score**: 전체 예측 중 올바른 예측의 비율.
    *   **Precision Score**: 모델이 양성이라고 예측한 것 중 실제로 양성인 비율.
    *   **Recall Score**: 실제 양성인 것 중 모델이 올바르게 양성으로 예측한 비율.
    *   **F1 Score**: Precision과 Recall의 조화 평균.
*   **회귀 지표**:
    *   **R2 Score (결정 계수)**: 모델이 타겟 변동성의 얼마를 설명하는지 나타냅니다.
    *   **Mean Absolute Error (MAE)**: 예측 오차의 절댓값 평균.
    *   **Mean Squared Error (MSE)**: 예측 오차 제곱의 평균.
    *   **Root Mean Squared Error (RMSE)**: MSE의 제곱근.

### 11.8 **교차 검증 (Cross-Validation) (`sklearn.model_selection`)**:
*   데이터를 여러 '폴드(fold)'로 분할하고, 한 폴드를 테스트 세트로 사용하고 나머지를 훈련 세트로 사용하여 모델을 여러 번 훈련하고 평가합니다.
*   이를 통해 단일 훈련-테스트 분할보다 모델 성능에 대한 더 견고한 추정치를 얻을 수 있습니다.

### 11.9 **하이퍼파라미터 튜닝 (Hyperparameter Tuning) (`sklearn.model_selection`)**:
*   **Grid Search Cross Validation**: 미리 정의된 하이퍼파라미터 값들의 모든 가능한 조합을 체계적으로 탐색하여 최적의 조합을 찾습니다. 교차 검증을 사용하여 각 조합의 성능을 평가합니다.
*   **중요**: 하이퍼파라미터 튜닝에는 **검증 세트**를 사용하며, 모델의 최종 성능을 평가하는 **테스트 세트**는 튜닝에 사용하지 않습니다.

### 11.10 **파이프라인 (Pipelines) (`sklearn.pipeline`)**:
*   데이터 전처리(스케일링, PCA 등) 및 모델 훈련과 같은 일련의 변환 및 추정 단계를 하나의 Scikit-learn 객체로 결합하는 편리한 방법입니다.
*   이를 통해 코드의 일관성을 유지하고 워크플로우를 간소화할 수 있습니다.

Scikit-learn의 핵심 기능을 다루지만, 개별 기계 학습 알고리즘의 심층적인 이론이나 모든 사용 가능한 메서드를 다루지는 않습니다. 추가적인 학습을 위해 Scikit-learn 문서 참조를 권장합니다.