---
title: 1차시 11(빅데이터 분석):EDA (실습 예제)
layout: single
classes: wide
categories:
  - EDA
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

# 1. 온라인 카지노 플레이어 데이터 분석 입문 가이드

- 출처: [Exploratory Data Analysis In Python: Machine Learning Project Transactional Data](https://www.youtube.com/watch?v=Pi_OcqLzF64)

## 1. 탐색적 데이터 분석(EDA)이란?

**탐색적 데이터 분석(Exploratory Data Analysis, EDA)**은 데이터를 처음 받았을 때 "이 데이터가 어떤 모습인지, 어떤 패턴이 있는지" 알아보는 과정입니다. 마치 새로운 동네에 이사했을 때 주변을 둘러보는 것

* EDA의 목적
  - 데이터의 구조와 특성 파악
  - 이상한 값(오류, 결측값) 발견
  - 숨겨진 패턴과 인사이트 발견
  - 머신러닝 모델링을 위한 준비

## 2. 필요한 도구들 (라이브러리)

```python
import pandas as pd              # 데이터 처리의 핵심 도구
import numpy as np               # 수치 계산
import matplotlib.pyplot as plt  # 기본 그래프
import seaborn as sns           # 예쁜 그래프

```

**각 라이브러리의 역할:**
- **Pandas**: 엑셀 같은 표 형태 데이터 처리
- **NumPy**: 수학적 계산
- **Matplotlib**: 기본적인 그래프 그리기
- **Seaborn**: 더 예쁜 통계 그래프

## 3. 데이터 불러오기 및 첫 탐색

```python
# 1. 데이터 불러오기
df = pd.read_csv('casino_data.csv')

# 2. 데이터 둘러보기
print("📋 데이터 크기:", df.shape)  # (행 수, 열 수)
print("\n👀 첫 5행 확인:")
print(df.head())

# 3. 데이터 타입 확인
print("\n📈 데이터 타입:")
print(df.dtypes)

# 4. 기본 정보 확인
print("\n📊 기본 정보:")
df.info()
```


- 컬럼명 수정
```python
df.columns = ['request_time_UTC', 'transaction_type','transaction_amount','status','account_identifier']
```

- 중요한 데이터 변환: 날짜 형식 맞추기
```python
# 날짜 컬럼이 문자열로 되어 있으면 날짜 형식으로 변환
df['request_time_UTC'] = pd.to_datetime(df['request_time_UTC'])
```

```
request_time_UTC	transaction_type	transaction_amount	status	account_identifier
0	2019-07-05 05:02:28+00:00	LOYALTYCARDDEBIT	10.0	APPROVED	customer1734
1	2019-07-05 05:26:00+00:00	LOYALTYCARDDEBIT	10.0	APPROVED	customer1734
2	2019-07-05 04:54:35+00:00	LOYALTYCARDCREDITCL	10.0	APPROVED	customer1734
```


## 4. 데이터 요약 통계

```python
# 숫자 데이터 요약
print("💰 거래 금액 통계:")
print(df['transaction_amount'].describe())

# 범주형 데이터 확인
print("\n🔍 거래 유형별 개수:")
print(df['transaction_type'].value_counts())

print("\n✅ 승인 상태별 개수:")
print(df['status'].value_counts())

print("\n👥 총 고객 수:")
print(df['account_identifier'].nunique())
```

## 5. 데이터 정리하기

### 5.1 거래 유형 이름 바꾸기 (이해하기 쉽게)

```python
# 복잡한 이름을 간단하게 바꾸기
# Transactions labeled as “LOYALTYCARDDEBIT” are Level 2 deposits. 
# Transactions labeled as “LOYALTYCARDCREDIT” are Level 2 withdrawals. 
# Transactions labeled as “LOYALTYCARDCREDITCL” or “LOYALTYCARDCREDITACH” are Level 1 deposits made via a card or ACH respectively. 

transaction_mapping = {
    'LOYALTYCARDDEBIT': 'L2 deposit',      # 레벨2 입금
    'LOYALTYCARDCREDIT': 'L2 withdrawal',   # 레벨2 출금  
    'LOYALTYCARDCREDITCL': 'L1 deposit'     # 레벨1 입금
}

# map() 함수는 시리즈의 각 요소에 함수나 딕셔너리를 적용
df['transaction_type'] = df['transaction_type'].map(transaction_mapping)
print("🔄 변경된 거래 유형:")
print(df['transaction_type'].value_counts())
```

### 5.2 필요한 데이터만 선택하기
```python
# 승인된 L2 입금 거래만 분석 (예시)
filtered_df = df[(df['status'] == 'Approved') & 
                 (df['transaction_type'] == 'L2 deposit')]

print(f"📝 필터링 결과: {len(filtered_df)}건의 데이터")
```

## 6. 고객 활동 시각화

### 6.1 고객별 거래 횟수 분석
```python
# 고객별 총 거래 수 계산
customer_transactions = df.groupby('account_identifier').size().reset_index()
customer_transactions.columns = ['customer_id', 'total_transactions']

# 히스토그램으로 분포 확인
plt.figure(figsize=(10, 6))
plt.hist(customer_transactions['total_transactions'], bins=50, alpha=0.7)
plt.title('고객별 총 거래 횟수 분포')
plt.xlabel('거래 횟수')
plt.ylabel('고객 수')
plt.show()
```

### 6.2 거래 유형별 분포
```python
plt.figure(figsize=(12, 6))
sns.histplot(data=df, x='transaction_amount', hue='transaction_type', 
             bins=30, alpha=0.6)
plt.title('거래 유형별 금액 분포')
plt.xlabel('거래 금액')
plt.ylabel('빈도')
plt.show()
```

### 6.3 VIP 고객 찾기
```python
# 가장 자주 거래하는 상위 20명
top_frequent = df.groupby('account_identifier').size().sort_values(ascending=False).head(20)

# 가장 많이 돈을 쓴 상위 20명  
top_spenders = df.groupby('account_identifier')['transaction_amount'].sum().sort_values(ascending=False).head(20)

# 나란히 비교 시각화
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

top_frequent.plot(kind='bar', ax=ax1, color='skyblue')
ax1.set_title('거래 횟수 Top 20')
ax1.set_ylabel('거래 횟수')
# ax1 그래프의 첫 번째 막대 그룹(컨테이너)을 가리킵
ax1.bar_label(ax1.containers[0], fmt='%d', padding=5)  # 막대 위에 숫자 표시

top_spenders.plot(kind='bar', ax=ax2)
ax2.set_title('거래 금액 Top 20')  
ax2.set_ylabel('총 거래 금액')

plt.tight_layout()
plt.show()
```

## 7. 시간 패턴 분석

### 7.1 시간 정보 추출
```python
# 날짜에서 여러 정보 추출
df['year'] = df['request_time_UTC'].dt.year
df['month'] = df['request_time_UTC'].dt.month  
df['day'] = df['request_time_UTC'].dt.day
df['hour'] = df['request_time_UTC'].dt.hour
df['weekday'] = df['request_time_UTC'].dt.day_name()  # 요일 이름

print("📅 추출된 시간 정보:")
print(df[['request_time_UTC', 'year', 'month', 'day', 'hour', 'weekday']].head())
```

### 7.2 활동 시간대 히트맵
```python
# 시간대별, 요일별 활동 패턴
activity_pattern = df.groupby(['weekday', 'hour']).size().reset_index()
activity_pattern.columns = ['weekday', 'hour', 'transaction_count']

# 히트맵용 데이터 변환
# pivot()은 DataFrame에서만 사용(행,열,값)
heatmap_data = activity_pattern.pivot(index='weekday', columns='hour', values='transaction_count')

# 요일 순서 정렬
weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
heatmap_data = heatmap_data.reindex(weekday_order)

# 히트맵 그리기
plt.figure(figsize=(15, 8))
#g:표시되는 숫자의 형식을 일반 형식으로 설정 (불필요한 0 제거)
#'YlOrRd': yello, orange, red
sns.heatmap(heatmap_data, annot=True, fmt='g', cmap='YlOrRd')
plt.title('시간대별, 요일별 거래 활동 히트맵')
plt.xlabel('시간 (24시간)')
plt.ylabel('요일')
plt.show()
```


## 8. 고객 생애주기 분석

### 8.1 월별 고객 활동 추적
```python
# 각 고객의 첫 거래와 마지막 거래 월 찾기
customer_lifecycle = df.groupby('account_identifier')['request_time_UTC'].agg(['min', 'max']).reset_index()
customer_lifecycle['start_month'] = customer_lifecycle['min'].dt.to_period('M')
customer_lifecycle['end_month'] = customer_lifecycle['max'].dt.to_period('M')

# "빠른 이탈자" (한 달만 활동) 식별
customer_lifecycle['is_fast_quitter'] = (customer_lifecycle['start_month'] == customer_lifecycle['end_month'])

print("🏃‍♂️ 빠른 이탈자 비율:", customer_lifecycle['is_fast_quitter'].mean() * 100, "%")
```

### 8.2 월별 고객 수 변화
```python
# 월별 전체 고객 수와 이탈 고객 수
monthly_stats = customer_lifecycle.groupby('start_month').agg({
    'account_identifier': 'count',  # 전체 신규 고객
    'is_fast_quitter': 'sum'        # 빠른 이탈자 수
}).reset_index()

monthly_stats.columns = ['month', 'total_customers', 'fast_quitters']

# 막대 차트로 비교
# melt(): 여러 개의 열(column)을 하나의 열로 녹여서(melt), 행(row)의 수를 늘리는 방식
monthly_stats_melted = monthly_stats.melt(
          id_vars=['month'], # 월 정보 유지
          value_vars=['total_customers', 'fast_quitters'],# 변환할 열들
          var_name='customer_type', # 새로운 변수 이름 열
          value_name='count' # 새로운 값 열
          )
# value_vars=['total_customers', 'fast_quitters'] → 두 컬럼을 한 컬럼(customer_type)으로 병합
# value_name='count' → 실제 값은 count 컬럼에 저장

plt.figure(figsize=(12, 6))
ax = sns.barplot(data=monthly_stats_melted, x='month',y='count',hue='customer_type')
# 각 bar에 레이블 추가
for container in ax.containers:
  ax.bar_label(container, fmt='%.0f', padding=3)

plt.title('월별 신규 고객 vs 빠른 이탈 고객')
plt.xticks(rotation=45)
plt.ylabel('고객 수')
plt.show()
```

## 9. 비즈니스 인사이트

### 9.1 발견할 수 있는 패턴들:
- **시간 패턴**: 언제 고객들이 가장 활발한가?
- **고객 세분화**: 자주 거래하는 고객 vs 큰 금액 거래하는 고객
- **이탈 패턴**: 어떤 고객들이 빨리 떠나는가?

### 9.2 다음 단계 머신러닝 활용:
```python
# 예시: 고객 세분화를 위한 특성 생성
customer_features = df.groupby('account_identifier').agg({
    'transaction_amount': ['sum', 'mean', 'count'],  # 총액, 평균, 횟수
    'hour': 'mean',  # 평균 거래 시간
}).reset_index()

customer_features.columns = ['customer_id', 'total_amount', 'avg_amount', 'frequency', 'avg_hour']

print("🎯 머신러닝을 위한 고객 특성:")
print(customer_features.head())
```

## 10. 핵심 요약

1. **EDA는 데이터 탐정 놀이**: 데이터 속 숨겨진 이야기를 찾아내는 과정
2. **시각화가 핵심**: 숫자보다는 그래프로 패턴을 파악
3. **단계적 접근**: 전체 → 부분 → 세부사항 순으로 분석
4. **비즈니스 관점 유지**: 기술적 분석을 실제 비즈니스 의사결정으로 연결

* **다음 단계**
  - **고객 세분화 (클러스터링)**: 비슷한 행동 패턴의 고객 그룹 찾기  
  - **이탈 예측 모델**: 어떤 고객이 떠날 확률이 높은지 예측
  - **추천 시스템**: 개인화된 서비스 제공

---

# 2. 스마트 미터 전력 소비량 데이터 분석

탐색적 데이터 분석(EDA)은 데이터를 깊이 파고들어 숨겨진 패턴, 특이점, 관계를 찾아내는 과정이에요. 포르투갈 건물의 스마트 미터에서 수집된 실제 전력 소비량 데이터를 가지고 이 과정을 자세히 설명한다.

- 출처:[Exploratory Data Analysis For Time Series: Machine Learning Project Energy Consumption Data](https://www.youtube.com/watch?v=jCYjcEaNfzc)

## 1. 스마트 미터 전력 소비량 데이터 분석: 핵심 요약

우리가 분석할 데이터는 포르투갈 건물에 설치된 스마트 미터에서 15분 간격으로 수집된 전력 소비량 데이터예요. 쉽게 말해, 건물의 전기 사용량을 15분마다 기록한 거죠. 이 데이터를 가지고 어떤 분석을 했고 어떤 점을 알아냈는지 살펴볼까요?

### 1.1 데이터 불러오기 및 첫인상 확인

  * **데이터는 어떻게 생겼을까?**
      * `pandas` 라이브러리의 `read_csv` 함수를 사용해서 데이터를 불러왔어요. 마치 엑셀 파일을 열어보는 것과 비슷해요.
      * 데이터를 살펴보니 약 46,000개의 행(시간별 측정값)과 173개의 열(시간 정보 1개 + 스마트 미터 172개)로 이루어져 있었어요.
      * `Time`이라는 열은 측정 시각을, `energy meter 1`부터 `172`까지의 열은 각 스마트 미터의 전력 소비량을 나타내요.
  
    ```python
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.preprocessing import StandardScaler

    # --- 1. 데이터 로드 및 초기 확인 ---
    print("--- 1. 데이터 로드 및 초기 확인 ---")
    df = pd.read_csv('your_energy_data.csv')
    print(f"데이터셋 크기: {df.shape}")
    print("\n초기 데이터 미리보기:")
    print(df.head())
    print("\n데이터 타입 확인:")
    print(df.info())
    ```

    ```
    	Time	Energy_Meter_1	Energy_Meter_2	Energy_Meter_3	Energy_Meter_4	Energy_Meter_5	Energy_Meter_6	Energy_Meter_7	Energy_Meter_8	Energy_Meter_9	...	
0	2022-05-05 12:00:00	0.068	NaN	0.034	0.038	0.000	NaN	NaN	NaN	NaN	...	
1	2022-05-05 12:15:00	0.070	NaN	0.044	0.062	0.006	NaN	NaN	NaN	NaN	...	
2	2022-05-05 12:30:00	0.122	NaN	0.037	0.045	0.000	NaN	NaN	NaN	NaN
    ```
    
  * **데이터 타입은 괜찮을까?**
      * 전력 소비량 데이터는 `float64`라는 숫자 타입이라 괜찮았지만, `Time` 열은 `object` (문자열) 타입이었어요. 시간을 가지고 분석하려면 `datetime`이라는 시간 전용 타입으로 바꿔줘야 해요.

### 1.2 시간 데이터 변환

  * `pandas`의 `to_datetime` 함수를 써서 `Time` 열을 `datetime` 타입으로 바꿨어요. 이제 시간을 기준으로 데이터를 다룰 수 있게 됐죠.

  ```python
  # --- 2. 데이터 타입 변환 (time 열이 object인 경우) ---
  print("\n--- 2. 데이터 타입 변환 ---")
  if df['Time'].dtype == 'object':
      df['Time'] = pd.to_datetime(df['Time'])
      print("\n'Time' 열이 datetime 타입으로 변환되었습니다.")
      print(df.info())
  ```

### 1.3 데이터 요약 통계 확인

  * `describe()` 메서드를 사용해서 각 스마트 미터의 데이터가 대략 어떤 범위에 있고, 평균적으로 얼마를 사용했는지 등 기본적인 통계를 확인했어요.
  * `Time` 열의 최소/최대 날짜를 보니 데이터가 수집된 전체 기간을 알 수 있었어요.
  * **놀라운 점**: 어떤 스마트 미터는 전체 기간 동안 데이터를 꾸준히 수집했지만, 어떤 미터는 아주 적은 기간만 데이터를 수집했더라고요. 마치 어떤 친구는 개근했는데, 어떤 친구는 결석이 많은 것처럼요.

  ```python
  # --- 3. 요약 통계 확인 ---
  print("\n--- 3. 요약 통계 확인 ---")
  print(df.describe())
  print(f"\n'Time' 열 최소 날짜: {df['Time'].min()}")
  print(f"'Time' 열 최대 날짜: {df['Time'].max()}")
  ```


### 1.4 전체 날짜 범위 및 중복/누락 확인

  * `pandas.date_range`를 사용해서 데이터가 수집된 시작 날짜부터 끝 날짜까지 15분 간격으로 모든 시간대를 만들어봤어요.
  * 데이터셋의 측정치 수와 우리가 만든 전체 시간대 수가 일치하는지 확인해서 **누락되거나 중복된 시간대가 없다**는 것을 검증했어요.

  ```python
  # --- 4. 전체 날짜 범위 및 중복 타임스탬프 확인 ---
  print("\n--- 4. 전체 날짜 범위 및 중복 타임스탬프 확인 ---")
  start_date = df['Time'].min()
  end_date = df['Time'].max()
  full_date_range = pd.date_range(start=start_date, end=end_date, freq='15min')

  print(f"생성된 전체 날짜 범위 길이: {len(full_date_range)}")
  print(f"데이터셋 관측치 수: {len(df)}")

  try:
      # Time 열을 인덱스로 설정하여 정렬 후 비교
      df_sorted = df.set_index('Time').sort_index()
      assert df_sorted.index.equals(full_date_range), "누락되거나 중복된 타임스탬프가 있습니다!"
      print("성공: 누락되거나 중복된 타임스탬프가 없습니다.")
  except AssertionError as e:
      print(f"오류: {e}")
      
  # 다시 Time 열을 컬럼으로 되돌립니다.
  df = df.reset_index(drop=True)
  ```

### 1.5 스마트 미터별 데이터 수 히스토그램

  * 각 스마트 미터가 얼마나 많은 데이터를 수집했는지 막대 그래프(히스토그램)로 그려봤어요.
  * 이 그래프를 보니 172개 미터 중 약 90개는 거의 모든 기간의 데이터를 수집했지만, 일부 미터는 5,000개 미만의 적은 데이터만 수집했다는 것을 알 수 있었어요.

  ```python
  # 한글 폰트 설정 (선택사항)
  plt.rcParams['font.family'] = 'Nanum Gothic'
  plt.rcParams['axes.unicode_minus'] = False

  # --- 5. 장치별 관측치 수 히스토그램 ---
  print("\n--- 5. 장치별 관측치 수 히스토그램 ---")
  # 결측값이 없는 관측치 수 계산 (Time 열 제외)
  observation_counts = df.drop(columns='Time').count()

  plt.figure(figsize=(12, 6))
  sns.histplot(observation_counts, bins=20, kde=True)

  plt.title('각 스마트 미터별 유효 관측치 수 히스토그램')
  plt.xlabel('유효 관측치 수')
  plt.ylabel('스마트 미터 개수')
  plt.grid(True, linestyle='--', alpha=0.7)
  plt.show()
  ```

### 1.6 결측값(비어있는 데이터) 분석

  * `Time` 열을 데이터프레임의 '인덱스'로 설정해서 시간을 기준으로 데이터를 쉽게 찾을 수 있게 했어요.
  * **충격적인 발견**: 전체 데이터 셀의 무려 **25%가 비어있었어요\!** 즉, 4개 중 1개는 데이터가 없다는 뜻이죠.
  * **스마트 미터별 결측값 비율**: 어떤 미터는 60% 또는 80% 이상의 데이터가 비어있기도 했어요. 이는 장치 문제, 뒤늦게 작동 시작, 중간에 고장 등 여러 이유 때문일 수 있어요.
  * **시간대별 결측값 수**: 특정 시간대에 모든 스마트 미터가 작동하지 않은 경우도 있었어요. 그래프를 보니 **어떤 시간에는 100%의 장치가 다운된 상황**도 발생했음을 알 수 있었어요. 특히 데이터 수집 기간의 마지막 부분에서는 이런 현상이 더 심해졌는데, 이는 정전이나 장치 업데이트와 같은 동시 발생 이벤트와 관련이 있을 수 있어요.

  ```python
  # --- 6. 결측값 분석 ---
  print("\n--- 6. 결측값 분석 ---")
  # Time 열을 인덱스로 설정
  df_indexed = df.set_index('Time')

  # 데이터프레임 전체 결측값 비율
  total_missing_percentage = df_indexed.isnull().sum().sum() / (df_indexed.shape[0] * df_indexed.shape[1]) * 100
  print(f"데이터프레임 전체 셀 중 결측값 비율: {total_missing_percentage:.2f}%")

  ```
  ```python
  # 열(장치)별 결측값 비율 히스토그램
  missing_by_column_percentage = df_indexed.isnull().sum() / len(df_indexed) * 100
  plt.figure(figsize=(12, 6))
  sns.histplot(missing_by_column_percentage, bins=20, kde=True)
  plt.title('스마트 미터별 결측값 비율 히스토그램')
  plt.xlabel('결측값 비율 (%)')
  plt.ylabel('스마트 미터 개수')
  plt.grid(True, linestyle='--', alpha=0.7)
  plt.show()
  ```

  ```python
  # 행(시간)별 결측값 수 및 비율
  missing_by_row_count = df_indexed.isnull().sum(axis=1)
  missing_by_row_percentage = missing_by_row_count / df_indexed.shape[1] * 100

  plt.figure(figsize=(15, 7))
  plt.plot(missing_by_row_percentage.index, missing_by_row_percentage.values)
  plt.title('시간대별 스마트 미터 결측값 비율')
  plt.xlabel('시간')
  plt.ylabel('결측값 비율 (%)')
  plt.grid(True, linestyle='--', alpha=0.7)
  plt.show()
  ```

### 1.7 상관관계 분석

  * **상관관계**는 두 변수가 얼마나 비슷한 움직임을 보이는지 나타내는 척도예요. `Pearson 상관계수`를 사용해서 스마트 미터들끼리 얼마나 비슷한 전력 소비 패턴을 보이는지 알아봤어요.
  * **히트맵**: 상관관계를 색상으로 시각화한 그림이에요. 대각선은 자기 자신과의 상관관계라서 항상 1(완벽하게 같음)이 나와요. 히트맵을 통해 시각적으로 어떤 미터들이 비슷한 패턴을 보이는지 쉽게 파악할 수 있었어요.
  * 상관관계가 높은 (예: 0.5 이상 또는 -0.5 미만) 스마트 미터 쌍을 찾아 시각화해보니, Energy Meter 49와 32는 63%의 상관관계를 보여 유사한 에너지 소비 패턴을 가지고 있었어요.

  ```python
  # --- 7. 상관관계 분석 ---
  print("\n--- 7. 상관관계 분석 ---")
  # 결측값을 제거하거나 적절히 채워야 상관관계 계산이 정확합니다. NaN이 있는 행을 제거하거나,
  # 실제 분석에서는 결측치 처리 전략(평균, 중앙값, 이전/이후 값 채우기 등)을 신중하게 선택해야 합니다.
  # df_corr = df_indexed.dropna(axis=1, how='any') # 모든 결측값이 있는 열 제거 (예시)
  df_corr = df_indexed.fillna(df_indexed.mean()) # 평균으로 결측값 채우기

  correlation_matrix = df_corr.corr(method='pearson')

  plt.figure(figsize=(15, 12))
  sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False, fmt=".2f", linewidths=.5)
  plt.title('스마트 미터 간의 Pearson 상관관계 히트맵')
  plt.show()

  # 중복 정보 제거를 위한 상삼각 행렬 시각화
  mask = np.triu(correlation_matrix)
  plt.figure(figsize=(15, 12))
  sns.heatmap(correlation_matrix, mask=mask, cmap='coolwarm', annot=False, fmt=".2f", linewidths=.5)
  plt.title('스마트 미터 간의 Pearson 상관관계 (상삼각 행렬)')
  plt.show()

  # 상관관계가 높은 (예: 0.7 이상) 장치 쌍 추출
  high_corr_pairs = []
  for i in range(len(correlation_matrix.columns)):
      for j in range(i + 1, len(correlation_matrix.columns)):
          if abs(correlation_matrix.iloc[i, j]) > 0.7: # 상관계수 임계값 설정
              high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))

  if high_corr_pairs:
      print("\n상관관계가 높은 (절대값 0.7 이상) 장치 쌍:")
      for pair in high_corr_pairs:
          print(f"- {pair[0]} 와 {pair[1]}: 상관계수 {pair[2]:.2f}")
          # 시각화 예시: 높은 상관관계를 가진 두 장치의 데이터 플로팅
          plt.figure(figsize=(10, 5))
          plt.plot(df_corr[pair[0]], label=pair[0], alpha=0.7)
          plt.plot(df_corr[pair[1]], label=pair[1], alpha=0.7)
          plt.title(f'{pair[0]} vs {pair[1]} (상관계수: {pair[2]:.2f})')
          plt.xlabel('시간')
          plt.ylabel('에너지 소비량')
          plt.legend()
          plt.grid(True, linestyle='--', alpha=0.7)
          plt.show()
  else:
      print("\n상관관계가 높은 장치 쌍을 찾지 못했습니다 (임계값 0.7 기준).")

  ```


### 1.8 계절성 분석 (월별 및 주별 패턴)

  * 데이터의 **값들을 표준화**했어요. 이는 다양한 범위의 데이터를 비교하기 위해 마치 시험 점수를 등급으로 바꾸는 것과 비슷하다고 생각하시면 돼요.
  * `month_of_year`(월)와 `day_of_the_week`(요일) 같은 시간 관련 정보를 추가해서 월별, 주별 패턴을 분석했어요.

  ```python
  # --- 8. 계절성 분석 (월별 및 주별 패턴) ---
  print("\n--- 8. 계절성 분석 (월별 및 주별 패턴) ---")

  # 'Time' 열을 인덱스로 설정
  df_seasonality = df.set_index('Time').copy()

  # 데이터 표준화 (결측치가 있는 경우 처리 후 진행)
  # 여기서는 간단히 결측치를 0으로 채우거나(권장하지 않음) 평균으로 채운 후 표준화합니다.
  # 실제 분석에서는 더 정교한 결측치 처리 전략이 필요합니다.
  df_seasonality_filled = df_seasonality.fillna(df_seasonality.mean()) # 평균으로 결측치 채우기

  scaler = StandardScaler()
  # 시간 컬럼을 제외하고 모든 에너지 미터 컬럼에 대해 스케일링 적용
  # fit_transform()이 NumPy 배열을 반환하므로, DataFrame의 메타정보(컬럼명, 인덱스)와 pandas의 기능을 활용하기 위해 다시 DataFrame으로 변환
  scaled_data = scaler.fit_transform(df_seasonality_filled.select_dtypes(include=np.number))
  df_scaled = pd.DataFrame(scaled_data, columns=df_seasonality_filled.select_dtypes(include=np.number).columns, index=df_seasonality_filled.index)

  # 시간 관련 파생 변수 생성
  df_scaled['month_of_year'] = df_scaled.index.strftime('%m_') + df_scaled.index.month_name()
  df_scaled['day_of_the_week'] = df_scaled.index.strftime('%w_') + df_scaled.index.day_name() # 0:일요일, 6:토요일
  ```

  ```python
                      Energy_Meter_1  Energy_Meter_2  Energy_Meter_3  ...  Energy_Meter_172  month_of_year  day_of_the_week
  Time                                                                 ...
  2022-05-05 12:00:00       -0.248048   -2.231273e-16       -0.694518  ...         -0.597342         05_May       4_Thursday
  2022-05-05 12:15:00       -0.227917   -2.231273e-16       -0.610187  ...         -0.231819         05_May       4_Thursday
  2022-05-05 12:30:00        0.295488   -2.231273e-16       -0.669219  ...         -0.406965         05_May       4_Thursday
  2022-05-05 12:45:00        0.396143   -2.231273e-16       -0.854747  ...         -0.186129         05_May       4_Thursday
  2022-05-05 13:00:00        0.325685   -2.231273e-16       -0.854747  ...         -0.475501         05_May       4_Thursday
  ```
  * **월별 주기성**:
      * 월별 평균 전력 소비량을 그래프로 그려보니, 1월에 줄어들다가 4\~5월에 가장 낮아지고, 6월부터 다시 늘어나서 7월과 9\~10월에 최고점을 찍고 다시 줄어드는 **계절적인 패턴**을 발견할 수 있었어요.
      * 7월과 8월에는 변동성이 컸는데, 이는 특정 달에 데이터에 이상치(평균에서 크게 벗어나는 값)가 많았음을 의미해요.
      * **박스 플롯**을 통해 더 자세히 살펴보니, 12월, 1월, 2월에 전력 사용량이 많았고, 5월, 6월에는 적게 사용했어요. 특히 2022년 7월에는 이상치가 많아서 평균과 변동성에 큰 영향을 미쳤다는 것도 알 수 있었죠.

  ```python 
    # 월별 주기성
  # 모든 스마트 미터의 평균 전력 소비량 계산
  
  monthly_avg = df_scaled.drop(columns=['month_of_year', 'day_of_the_week'], errors='ignore').mean(axis=1).groupby(df_scaled['month_of_year']).mean()
  monthly_std = df_scaled.drop(columns=['month_of_year', 'day_of_the_week'], errors='ignore').mean(axis=1).groupby(df_scaled['month_of_year']).std()

  
  plt.figure(figsize=(12, 6))
  monthly_avg.sort_index().plot(kind='bar', yerr=monthly_std.sort_index())
  #막대 그래프: 월별 평균값의 높낮이,전체 12개월의 평균 전력 소비량을 기준으로 해당 월의 평균이 얼마나 높거나 낮은지
  #에러 바(yerr): 각 월별 변동성의 크기(표준편차),해당 월의 표준편차
  #해석: 에러 바가 길수록 해당 월의 데이터 변동성이 큼
  #즉, 계절성 패턴의 강도와 안정성을 동시에 파악하기 위한 분석입니다.:


  plt.title('월별 평균 전력 소비량 (표준 편차 포함)')
  plt.xlabel('월')
  plt.ylabel('표준화된 에너지 소비량')
  plt.xticks(rotation=45)
  plt.grid(axis='y', linestyle='--', alpha=0.7)
  plt.tight_layout()
  plt.show()

  # 박스 플롯으로 월별 데이터 분포 심층 분석
  # 모든 스마트 미터의 평균 전력 소비량을 기반으로 월별 박스 플롯 생성
  df_scaled['overall_avg_consumption'] = df_scaled.drop(columns=['month_of_year', 'day_of_the_week'], errors='ignore').mean(axis=1)
  plt.figure(figsize=(14, 7))
  sns.boxplot(x='month_of_year', y='overall_avg_consumption', data=df_scaled.sort_values('month_of_year'))
  plt.title('월별 전력 소비량 분포 (박스 플롯)')
  plt.xlabel('월')
  plt.ylabel('표준화된 에너지 소비량')
  plt.xticks(rotation=45)
  plt.grid(axis='y', linestyle='--', alpha=0.7)
  plt.tight_layout()
  plt.show()
  ```
  * **주별 주기성**:
      * 주별 에너지 소비량을 확인해보니 **토요일에 전력 사용량이 높고, 일요일에 가장 낮으며, 주중에는 비교적 일정한 패턴**을 보였어요. 주말의 활동 패턴이 전력 사용량에 영향을 미치는 것이죠.

  ```python
  # 주별 주기성
  daily_avg = df_scaled.drop(columns=['month_of_year', 'day_of_the_week'], errors='ignore').mean(axis=1).groupby(df_scaled['day_of_the_week']).mean()
  daily_std = df_scaled.drop(columns=['month_of_year', 'day_of_the_week'], errors='ignore').mean(axis=1).groupby(df_scaled['day_of_the_week']).std()

  plt.figure(figsize=(10, 6))
  daily_avg.sort_index().plot(kind='bar', yerr=daily_std.sort_index())
  plt.title('주중/주말별 평균 전력 소비량 (표준 편차 포함)')
  plt.xlabel('요일')
  plt.ylabel('표준화된 에너지 소비량')
  plt.xticks(rotation=45)
  plt.grid(axis='y', linestyle='--', alpha=0.7)
  plt.tight_layout()
  plt.show()

  print("\n--- 분석 완료 ---")
  ```


## 2. 앞으로의 프로젝트 방향

이런 탐색적 데이터 분석을 통해 얻은 정보들을 바탕으로 다음과 같은 흥미로운 프로젝트들을 기획해볼 수 있어요.

  * **예측 유지보수 시스템**: 스마트 미터가 언제쯤 고장 날지 미리 예측해서 유지보수 담당자에게 알려주는 시스템을 만들 수 있어요. 결측값 분석을 통해 특정 장치의 잦은 다운타임을 파악했다면, 이러한 시스템을 구축하는 데 중요한 단서가 되겠죠?
  * **에너지 소비량 예측**: 특정 건물이나 전체 지역의 미래 에너지 소비량을 예측하는 모델을 만들 수 있어요. 월별, 주별 패턴을 파악한 것이 중요한 예측 요소가 될 거예요.
  * **스마트 미터 클러스터링**: 비슷한 전력 소비 패턴을 보이는 스마트 미터들을 그룹으로 묶을 수 있어요. 이렇게 그룹화하면 개별 미터마다 모델을 만드는 대신, 그룹별로 모델을 만들어서 효율을 높일 수 있습니다. 상관관계 분석을 통해 얻은 정보가 여기에 활용될 수 있습니다.
