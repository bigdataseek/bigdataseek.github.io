---
title: 12차시 3 :StatQuest(Machin Learning 3)
layout: single
classes: wide
categories:
  - Machine Learning
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 52. 랜덤 포레스트(Random Forests)

### **52.1 랜덤 포레스트란 무엇인가요?**
랜덤 포레스트는 **의사결정 트리(decision trees)**를 기반으로 구축됩니다. 의사결정 트리는 만들고, 사용하고, 해석하기 쉽지만, 새로운 샘플을 분류할 때 유연성이 부족하여 예측 정확도가 떨어진다는 단점이 있습니다. 랜덤 포레스트는 이러한 의사결정 트리의 단순성에 유연성을 결합하여 **정확도를 크게 향상**시킵니다. 다양한 트리를 생성하는 것이 개별 의사결정 트리보다 랜덤 포레스트를 더 효과적으로 만듭니다.

### **52.2 랜덤 포레스트 구축 방법**
랜덤 포레스트는 다음 단계에 따라 구축됩니다:

*   **1단계: 부트스트랩 데이터셋 생성 (Create a bootstrap data set)**.
    *   원본 데이터셋과 **동일한 크기**의 부트스트랩 데이터셋을 만듭니다.
    *   원본 데이터셋에서 **샘플을 무작위로 선택**하며, **동일한 샘플을 여러 번 선택할 수 있습니다**.

*   **2단계: 의사결정 트리 생성 (Create a decision tree)**.
    *   각 단계에서 **무작위로 선택된 변수(또는 열)의 부분집합만 사용**하여 의사결정 트리를 구축합니다. 예를 들어, 루트 노드를 분할할 때 모든 변수를 고려하는 대신 무작위로 두 개의 변수만 선택할 수 있습니다.
    *   이러한 방식으로 트리를 생성하면 **다양한 종류의 트리**가 만들어집니다.

*   **반복 (Repetition)**:
    *   위의 1단계와 2단계를 **수백 번 반복하여** 여러 개의 의사결정 트리를 만듭니다. 각각의 트리마다 새로운 부트스트랩 데이터셋을 만들고, 각 단계에서 변수의 부분집합만 고려합니다.

### **52.3 랜덤 포레스트 사용 방법**
새로운 데이터(예: 새로운 환자 정보)가 들어오면, 이를 사용하여 예측하거나 분류합니다.

*   새로운 데이터를 **만든 모든 의사결정 트리에 각각 통과시킵니다**.
*   각 트리는 분류 결과를 내놓습니다 (예: "심장병 있음" 또는 "심장병 없음").
*   모든 트리의 분류 결과 중에서 **가장 많은 표를 얻은 옵션을 최종 분류 결과로 결정**합니다.
*   이러한 방식으로 부트스트랩 데이터를 사용하고 결과를 취합하여 결정을 내리는 것을 **배깅(bagging)**이라고 합니다.

### **52.4 랜덤 포레스트 정확도 평가 방법**
랜덤 포레스트의 성능은 **OOB(Out-Of-Bag) 데이터셋**을 사용하여 추정할 수 있습니다.

*   **OOB 데이터셋 생성**: 부트스트랩 데이터셋을 만들 때 **포함되지 않은 원본 데이터의 샘플**들이 있습니다. 이들을 OOB 데이터셋이라고 부릅니다. 일반적으로 원본 데이터의 약 3분의 1이 부트스트랩 데이터셋에 포함되지 않습니다.
*   **OOB 에러 계산**:
    *   특정 트리를 만들 때 사용되지 않은 OOB 샘플을 해당 트리에 통과시킵니다.
    *   이 OOB 샘플을 **해당 샘플을 만드는 데 사용되지 않은 모든 다른 트리에 통과**시킵니다.
    *   가장 많은 표를 얻은 레이블을 OOB 샘플의 랜덤 포레스트 분류 결과로 지정합니다.
    *   모든 OOB 샘플에 대해 이 과정을 반복하여 랜덤 포레스트가 **정확하게 분류한 OOB 샘플의 비율**로 정확도를 측정하고, **잘못 분류한 비율**을 OOB 에러(Out-of-Bag error)라고 합니다.

### **52.5 랜덤 포레스트 최적화**
OOB 에러를 사용하여 랜덤 포레스트의 하이퍼파라미터(hyperparameters)를 최적화할 수 있습니다.

*   예를 들어, 각 단계에서 고려할 **변수의 수를 다르게 설정하여** 여러 개의 랜덤 포레스트를 구축하고 각각의 OOB 에러를 비교합니다.
*   가장 낮은 OOB 에러를 보이는 랜덤 포레스트를 선택하여 최적의 모델을 찾습니다.
*   일반적으로 변수 수의 제곱근 값을 기준으로 시작하여 그 값의 위아래로 몇 가지 설정을 시도해봅니다.

## 53. 랜덤 포레스트와 결측치 처리 및 샘플 클러스터링
**랜덤 포레스트(Random Forests)**가 **결측치(missing data)**를 어떻게 다루고, 샘플 간의 **유사성(similarity) 및 클러스터링**에 어떻게 활용될 수 있는지 명확하게 설명합니다.

랜덤 포레스트는 다음 두 가지 유형의 결측치를 처리할 수 있습니다:
1.  **랜덤 포레스트를 구축하는 데 사용되는 원본 데이터셋 내의 결측치**.
2.  **분류하려는 새로운 샘플 내의 결측치**.

### **53.1 원본 데이터셋 내 결측치 처리 (반복적 추정 방법)**

이 방법은 초기 추정을 하고 점진적으로 이를 개선하는 방식으로 진행됩니다.

*   **초기 추정 (Initial Guess):**
    *   **범주형 데이터** (예: 동맥 막힘 여부): 해당 범주 내에서 **가장 흔한 값**을 초기 추정치로 사용합니다. 예를 들어, 심장병이 없는 환자들 중 동맥이 막히지 않은 경우가 가장 많다면 '아니요'로 추정합니다.
    *   **수치형 데이터** (예: 체중): 해당 범주 내에서 **중간값(median value)**을 초기 추정치로 사용합니다.
*   **추정치 개선 (Refining Guesses):**
    *   **1단계: 랜덤 포레스트 구축 및 데이터 실행:**
        *   결측치가 채워진(초기 추정치로) 데이터셋으로 랜덤 포레스트를 구축합니다.
        *   모든 데이터를 **모든 의사결정 트리**에 통과시킵니다.
    *   **2단계: 근접성 행렬(Proximity Matrix) 계산:**
        *   근접성 행렬은 각 샘플에 대한 행과 열을 가집니다.
        *   두 샘플이 **같은 리프 노드(leaf node)**에 도달할 때마다 해당 샘플 쌍의 근접성 행렬 값에 1을 더합니다.
        *   모든 트리에 대해 이 과정을 거친 후, 각 근접성 값을 **총 트리 수로 나누어 정규화**합니다. 이 값은 두 샘플이 얼마나 자주 같은 리프 노드에 도달했는지를 나타내며, 이는 샘플 간의 유사성을 의미합니다.
    *   **3단계: 근접성 행렬을 이용한 결측치 재추정:**
        *   **범주형 데이터**: 근접성 값을 **가중치(weights)**로 사용하여 '예'와 '아니요'의 **가중 빈도(weighted frequency)**를 계산합니다. 가중 빈도가 더 높은 쪽이 새로운 추정치가 됩니다.
        *   **수치형 데이터**: 근접성 값을 가중치로 사용하여 **가중 평균(weighted average)**을 계산합니다.
    *   **4단계: 반복 (Iteration):**
        *   이 모든 과정을 **6~7회 반복**하여 결측치 추정치가 더 이상 변하지 않을 때(수렴할 때)까지 수행합니다.

### **53.2 근접성 행렬을 이용한 샘플 클러스터링 및 유사성 분석**

근접성 행렬은 샘플 간의 관계를 시각화하는 데 사용될 수 있습니다.

*   **거리 행렬(Distance Matrix)로 변환:**
    *   정규화된 근접성 행렬에서 **'1 - 근접성 값'**을 계산하면 이는 **거리 값**이 됩니다. 근접성이 1이라는 것은 두 샘플이 모든 트리에서 같은 리프 노드에 있었다는 의미이며, 이는 거리가 0이라는 뜻입니다.
*   **시각화:**
    *   이 거리 행렬을 사용하여 **히트맵(Heatmap)**을 그리거나 **MDS(Multidimensional Scaling) 플롯**을 생성할 수 있습니다.
    *   이러한 시각화는 데이터 유형(순위, 객관식, 수치 등)에 관계없이 샘플들이 서로 어떻게 관련되어 있는지를 보여줄 수 있어 매우 유용합니다.

### **53.3 분류하려는 새로운 샘플 내 결측치 처리**

새로운 환자(또는 샘플)를 분류하고 싶은데, 그 환자의 일부 데이터(예: 동맥 막힘 여부)가 없는 경우입니다.

*   **1단계: 두 개의 복사본 생성:**
    *   새로운 환자 데이터의 두 가지 복사본을 만듭니다. 하나는 '심장병 있음'으로 가정하고, 다른 하나는 '심장병 없음'으로 가정합니다.
*   **2단계: 결측치 추정:**
    *   이 두 복사본 각각에 대해 위에서 설명한 **반복적 추정 방법**을 사용하여 결측치를 채웁니다.
*   **3단계: 랜덤 포레스트를 통한 분류 및 최종 결정:**
    *   결측치가 채워진 이 두 샘플을 **이미 구축된 랜덤 포레스트의 모든 트리**에 통과시킵니다.
    *   두 옵션(예: '심장병 있음' 또는 '심장병 없음') 중 어느 쪽이 랜덤 포레스트에 의해 **더 많이 정확하게 분류되었는지**를 확인합니다. 더 많이 표를 얻은 옵션이 최종 분류 결과가 됩니다.

요약하자면, 랜덤 포레스트는 결측치를 추정하고, 샘플 간의 유사성을 파악하며, 이를 통해 데이터를 더 효과적으로 분석하고 분류할 수 있는 강력한 도구입니다.

## 54. R에서 Random Forests 구축, 사용 및 평가

R 프로그래밍 언어를 사용하여 **랜덤 포레스트(Random Forests) 모델을 구축, 사용 및 평가하는 방법**을 단계별로 설명합니다. 

### **54.1 초기 설정 및 데이터 로드**
*   **필요 라이브러리 로드**: 화려한 그래프를 그리기 위한 `ggplot2`, `ggplot2`의 기본 설정을 개선하는 `cowplot`, 그리고 랜덤 포레스트 모델을 생성하기 위한 `randomForest` 라이브러리를 로드합니다.
*   **데이터셋 준비**: UCI 머신러닝 저장소에서 심장 질환(Heart Disease) 실제 데이터셋을 가져옵니다. 이 데이터는 URL을 통해 R로 직접 읽어들일 수 있습니다.
*   **데이터 초기 확인**: `head()` 함수를 사용하여 데이터의 첫 6행을 확인합니다. 초기에는 열 이름이 지정되어 있지 않아 데이터를 파악하기 어렵습니다.

### **54.2 데이터 전처리 및 정리**
*   **열 이름 지정**: UCI 웹사이트에 명시된 이름에 따라 데이터셋의 열 이름을 지정합니다.
*   **데이터 구조 파악 및 문제점 확인**: `str()` 함수를 사용하여 데이터의 구조를 파악합니다.
    *   'sex' 열은 0(여성)과 1(남성)을 나타내는 팩터(factor)여야 하지만, 초기에는 그렇지 않습니다.
    *   'CP'(chest pain) 열도 팩터여야 하며, 1-3은 다양한 통증 유형을, 4는 통증 없음을 나타내야 합니다.
    *   'CA'와 'Thal' 열은 팩터로 잘못 인식되거나, '?' 값을 포함하고 있어 `NA`로 변경해야 합니다.
*   **데이터 클리닝 작업**:
    *   '?' 값을 `NA` (결측값)로 변경합니다.
    *   'sex' 열의 0을 'F'(여성)로, 1을 'M'(남성)으로 변환한 후 팩터로 만듭니다.
    *   다른 여러 열(CP, restecg, fbs, exang, slope 등)도 팩터로 변환합니다.
    *   'CA'와 'Thal' 열은 처음에 문자열로 인식될 수 있으므로, 정수형으로 변환한 후 팩터로 만듭니다.
    *   'HD'(heart disease) 열의 0을 'healthy'(건강), 1을 'unhealthy'(건강하지 않음)로 변환하여 팩터로 만듭니다. 이는 데이터 가독성을 높입니다.
*   **정리된 데이터 확인**: `str()` 함수를 다시 사용하여 데이터가 올바르게 변경되었는지 확인합니다.

### **54.3 결측값 보간(Imputation)**
*   **재현성을 위한 시드 설정**: 랜덤 넘버 생성기의 시드(seed)를 설정하여 나중에 결과를 재현할 수 있도록 합니다.
*   **`rfImpute`를 이용한 결측값 보간**: `rfImpute` 함수를 사용하여 데이터셋의 `NA` 값을 예측하여 채웁니다.
    *   `HD ~ .`는 'HD' 열(심장 질환)을 다른 모든 열의 데이터를 사용하여 예측한다는 의미입니다.
    *   `data` 인수는 사용할 데이터셋을 지정합니다.
    *   `iter` 인수는 결측값을 추정하기 위해 `rfImpute`가 구축할 랜덤 포레스트의 수를 지정합니다 (이 예시에서는 20으로 설정했지만, 일반적으로 4-6회 반복으로 충분합니다).
*   **OOB 오류율 모니터링**: 각 반복 후에 `rfImpute`는 OOB(Out-of-Bag) 오류율을 출력하며, 이 값이 감소하면 추정치가 개선되고 있음을 나타냅니다. 안정되면 추정치가 최적에 도달했음을 의미합니다.
*   **결과 저장**: 결측값이 채워진 데이터셋은 `data.imputed`로 저장됩니다.

### **54.4 랜덤 포레스트 모델 구축**
*   **`randomForest` 함수 사용**: `randomForest` 함수를 사용하여 실제 랜덤 포레스트 모델을 구축합니다.
    *   이전과 같이 `HD ~ .`를 통해 'HD'를 예측 대상으로 지정합니다.
    *   데이터셋으로는 결측값이 보간된 `data.imputed`를 사용합니다.
    *   `proximity=TRUE` 옵션을 통해 근접도 행렬(proximity matrix)을 반환하도록 설정합니다. 이는 나중에 샘플들을 클러스터링하는 데 사용됩니다.
*   **모델 저장**: 구축된 랜덤 포레스트 모델과 근접도 행렬은 `model`이라는 변수로 저장됩니다.

### **54.5 모델 평가 및 해석**
*   **모델 요약 보기**: `model` 변수를 출력하면 랜덤 포레스트의 요약과 성능을 확인할 수 있습니다.
    *   **호출(Call)**: 모델 구축에 사용된 원본 함수 호출을 보여줍니다.
    *   **유형**: 모델이 **분류(classification)**, 회귀(regression), 또는 비지도(unsupervised) 모델인지 표시합니다.
    *   **트리 수**: 랜덤 포레스트에 포함된 의사결정 트리의 수를 보여줍니다 (기본값은 500개).
    *   **노드당 변수 수 (mtry)**: 각 내부 노드에서 고려된 변수(열)의 수를 나타냅니다. 분류 트리의 기본값은 변수 수의 제곱근입니다.
    *   **OOB 오류 추정치**: OOB 샘플 중 랜덤 포레스트가 올바르게 분류한 비율(이 예시에서는 83.5%가 올바르게 분류됨)을 나타냅니다.
    *   **혼동 행렬(Confusion Matrix)**: 모델의 예측 성능을 상세하게 보여줍니다.
        *   정확하게 'healthy'로 분류된 환자 수 (예: 141명).
        *   'unhealthy'이지만 'healthy'로 잘못 분류된 환자 수 (예: 27명).
        *   'healthy'이지만 'unhealthy'로 잘못 분류된 환자 수 (예: 23명).
        *   정확하게 'unhealthy'로 분류된 환자 수 (예: 112명).

*   **최적의 트리 수 확인**:
    *   `model$err.rate` 매트릭스를 활용하여 트리 수에 따른 오류율(OOB, healthy, unhealthy)을 그래프로 시각화합니다.
    *   그래프를 통해 **500개 트리 이후에 오류율이 안정화**되는 것을 확인할 수 있습니다. 즉, 더 많은 트리를 추가해도 분류 성능이 크게 개선되지 않습니다.

*   **최적의 `mtry` (노드당 변수 수) 확인**:
    *   1부터 10까지의 `mtry` 값(각 내부 노드에서 고려할 변수의 수)을 변경하면서 여러 랜덤 포레스트를 구축하는 루프를 실행합니다.
    *   각 `mtry` 값에 대한 OOB 오류율을 저장합니다.
    *   오류율을 출력하여 **가장 낮은 OOB 오류율을 보이는 `mtry` 값**을 찾습니다 (이 예시에서는 기본값인 `mtry=3`이 최적이었습니다).

### **54.6 MDS 플롯으로 샘플 관계 시각화**
*   **MDS(Multidimensional Scaling) 플롯**: 랜덤 포레스트의 근접도 행렬(proximity matrix)을 사용하여 샘플들 간의 관계를 시각적으로 보여주는 MDS 플롯을 생성합니다.
    *   근접도 행렬에서 1을 뺀 값으로 거리 행렬(distance matrix)을 만듭니다.
    *   `CMDScale` 함수를 거리 행렬에 적용합니다.
    *   X축과 Y축이 거리 행렬의 분산을 얼마나 설명하는지 계산합니다.
*   **플롯 해석**: 생성된 MDS 플롯은 샘플들이 어떻게 서로 관련되어 있는지를 시각적으로 나타냅니다. 예를 들어, 'unhealthy' 샘플들은 왼쪽에, 'healthy' 샘플들은 오른쪽에 군집화되어 나타날 수 있습니다. 이를 통해 새로운 환자 데이터가 주어졌을 때 어느 그룹에 속할 가능성이 높은지 추론할 수 있습니다.

## 55. 연쇄 법칙(The Chain Rule)

복잡한 함수에서 미분(derivative)을 계산하는 데 필수적인 **연쇄 법칙**을 다양한 예시를 통해 명확하게 설명합니다. 특히 머신러닝의 손실 함수(loss function) 최적화에 어떻게 적용되는지 보여줍니다.

### **55.1 미분(Derivative)의 기본 개념 복습**
*   **미분**: 곡선 위의 한 점에서의 접선의 기울기를 나타내며, 한 변수가 다른 변수에 대해 얼마나 빠르게 변화하는지 알려줍니다.
*   **예시**: 'StatQuest를 좋아하는 정도'와 '멋짐(awesomeness)'의 관계가 `awesomeness = likes statquest^2`이라는 포물선으로 표현될 때, 미분은 'StatQuest를 좋아하는 정도'에 따른 '멋짐'의 변화율(기울기)을 알려줍니다. '멱법칙(power rule)'을 사용하면 이 미분은 `2 * likes statquest`가 됩니다.

### **55.2 연쇄 법칙의 핵심 아이디어: 연결된 변화율**
*   **중간 단계를 통한 연결**: 연쇄 법칙은 여러 변수가 사슬처럼 연결되어 있을 때, 최종 결과가 시작 변수에 대해 어떻게 변하는지 계산하는 방법입니다.
*   **간단한 예시 (체중 → 키 → 신발 사이즈)**:
    *   **체중과 키의 관계**: 체중이 1단위 증가할 때 키가 2단위 증가한다면, '키의 체중에 대한 미분'은 2입니다. (`height = 2 * weight`).
    *   **키와 신발 사이즈의 관계**: 키가 1단위 증가할 때 신발 사이즈가 1/4단위 증가한다면, '신발 사이즈의 키에 대한 미분'은 1/4입니다. (`shoe_size = 1/4 * height`).
    *   **연쇄 법칙 적용**: 체중이 신발 사이즈에 미치는 변화를 알고 싶을 때, **'신발 사이즈의 체중에 대한 미분'은 '신발 사이즈의 키에 대한 미분'과 '키의 체중에 대한 미분'을 곱한 것과 같습니다**. 즉, `(1/4) * 2 = 1/2`. 이는 체중 1단위 증가당 신발 사이즈가 1/2단위 증가함을 의미합니다.
    *   **핵심**: 중간 변수(키)가 시작 변수(체중)와 최종 변수(신발 사이즈)를 연결하기 때문에 각 단계의 미분을 곱하는 것입니다.

### **55.3 복잡한 함수에 대한 적용**
*   **예시 (간식 후 시간 → 배고픔 → 아이스크림 갈망)**:
    *   '간식 후 시간'이 길어질수록 '배고픔'이 기하급수적으로 증가하고, '배고픔'이 증가할수록 '아이스크림 갈망'이 점차 완화되는(제곱근 함수) 복잡한 관계가 있습니다.
    *   이러한 관계를 직접 하나의 식으로 합쳐 미분하는 것은 매우 어렵습니다.
    *   **해결책**: 연쇄 법칙을 사용하여 '아이스크림 갈망'의 '간식 후 시간'에 대한 미분을 **('아이스크림 갈망'의 '배고픔'에 대한 미분) * ('배고픔'의 '간식 후 시간'에 대한 미분)**으로 분리하여 계산합니다. 이를 통해 쉽게 미분을 구할 수 있습니다.

### **55.4 "내부의 것(Stuff Inside)" 개념을 이용한 연쇄 법칙 적용**
*   **함수가 내부에 중첩된 경우**: 실제 문제에서는 두 개의 분리된 방정식이 아니라, 하나의 복잡한 방정식 안에 다른 방정식이 '끼워져' 있는 경우가 많습니다.
*   **괄호 활용**: 이럴 때는 괄호를 사용하여 **내부 함수(stuff inside)**를 식별합니다.
*   **적용 방법**: '전체 함수의 시간에 대한 미분'은 '전체 함수의 내부 함수에 대한 미분'과 '내부 함수의 시간에 대한 미분'을 곱한 것과 같습니다. 멱법칙을 활용하여 각 부분을 미분한 후 곱하면 됩니다.

### **55.5 머신러닝 손실 함수(Loss Function) 최적화에서의 연쇄 법칙**
*   **잔차 제곱합(Residual Sum of Squares)**: 머신러닝에서 모델이 데이터에 얼마나 잘 맞는지 평가하는 데 사용되는 일반적인 손실 함수입니다. 이 값을 최소화하는 모델 파라미터(예: 절편)를 찾아야 합니다.
*   **최적화 과정**: 손실 함수를 최소화하기 위해, 손실 함수의 미분 값이 0이 되는 지점(최저점)을 찾습니다.
*   **예시 (절편에 대한 잔차 제곱합의 미분)**:
    *   예측 키(`predicted height = intercept + 1 * weight`)와 실제 키(`observed height`)의 차이인 잔차(`residual`)가 있습니다.
    *   **잔차 제곱합(`residual^2`)**을 최소화하는 절편(intercept) 값을 찾기 위해 '잔차 제곱합의 절편에 대한 미분'을 계산해야 합니다.
    *   **연쇄 법칙 적용**: '잔차 제곱합의 절편에 대한 미분'은 **('잔차 제곱합의 잔차에 대한 미분') * ('잔차의 절편에 대한 미분')**과 같습니다.
    *   **계산**:
        *   '잔차 제곱합의 잔차에 대한 미분'은 멱법칙에 따라 `2 * residual`입니다.
        *   '잔차의 절편에 대한 미분'은 식을 풀어서 계산하면 `-1`이 됩니다.
        *   두 미분을 곱하여 '잔차 제곱합의 절편에 대한 미분'을 얻습니다.
    *   **"내부의 것" 방법 재적용**: 이 복잡한 경우에도 '잔차 제곱합'을 '내부의 것(관측값 - 절편 - 1 * 체중)'의 제곱으로 보고 연쇄 법칙을 적용할 수 있으며, 동일한 결과를 얻습니다.
    *   **최적의 절편 찾기**: 구해진 미분 값을 0으로 설정하고 풀면, 잔차 제곱합을 최소화하는 최적의 절편 값을 찾을 수 있습니다. (예시에서는 절편이 1일 때 최소화됨).

연쇄 법칙은 단순히 미분 계산 기술을 넘어, **신경망의 학습 과정(특히 역전파, backpropagation)**에서 가중치를 업데이트하고 손실 함수를 최소화하는 데 핵심적인 역할을 합니다.

## 56. 경사 하강법 (Gradient Descent)

**경사 하강법**은 통계학, 머신러닝, 데이터 과학 분야에서 **파라미터(매개변수)를 최적화**하는 데 사용되는 강력한 알고리즘입니다. 선형 회귀에서 절편과 기울기를 최적화하거나, 로지스틱 회귀에서 곡선을 최적화하고, t-SNE에서 클러스터를 최적화하는 등 다양한 문제에 적용될 수 있습니다. 경사 하강법은 최적의 해를 찾기 위해 반복적인 단계를 통해 나아가는 방식입니다.

### **56.1 경사 하강법 작동 원리 (절편 단일 파라미터 예시)**

1.  **초기 추정값 설정**: 먼저 최적화하려는 파라미터(예: **절편**)에 대해 **무작위 초기 값**을 선택합니다. 이는 경사 하강법이 개선할 수 있는 시작점 역할을 합니다. (예시에서는 절편을 0으로 설정).

2.  **손실 함수 (Loss Function) 정의**: 모델이 데이터에 얼마나 잘 맞는지 평가하기 위한 **손실 함수**를 정의합니다. 선형 회귀 예시에서는 **잔차 제곱합 (Sum of Squared Residuals)**이 사용됩니다. 머신러닝 용어로는 잔차 제곱합이 손실 함수의 한 유형입니다.

3.  **손실 함수의 도함수 (Derivative) 계산**: 현재 파라미터 값에서 손실 함수의 **기울기**를 알아내기 위해 파라미터에 대한 손실 함수의 **도함수**를 계산합니다. 이 기울기는 우리가 가야 할 방향과 얼마나 빨리 가야 하는지를 알려줍니다.

4.  **단계 크기 (Step Size) 계산**: 도함수 값(기울기)에 **학습률 (Learning Rate)**이라는 작은 수를 곱하여 **단계 크기**를 결정합니다.
    *   **단계 크기 = 기울기 × 학습률**
    *   기울기가 0에 가까우면(최적 값에 가까우면) **아주 작은 단계(baby steps)**를 밟고.
    *   기울기가 0에서 멀면(최적 값에서 멀면) **큰 단계(big steps)**를 밟습니다.

5.  **새로운 파라미터 값 계산**: 이전 파라미터 값에서 계산된 단계 크기를 빼서 **새로운 파라미터 값**을 업데이트합니다.
    *   **새로운 절편 = 이전 절편 - 단계 크기**

6.  **반복**: 새로운 절편 값을 다시 도함수에 대입하고, 단계 크기를 계산하고, 절편을 업데이트하는 과정을 **반복**합니다.

7.  **멈추는 조건**:
    *   **단계 크기가 0에 매우 가까워질 때 (예: 0.001 또는 더 작을 때)** 멈춥니다. 이는 기울기가 0에 매우 가깝다는 의미이며, 최적의 값에 도달했음을 나타냅니다.
    *   또는 **최대 반복 횟수 (예: 1000회 이상)에 도달**했을 때 멈춥니다.

### 56.2 **경사 하강법 작동 원리 (절편 및 기울기 다중 파라미터 예시)**

여러 파라미터를 최적화할 때도 기본 원리는 동일합니다.

1.  **손실 함수 정의**: 마찬가지로 잔차 제곱합을 손실 함수로 사용합니다.
2.  **손실 함수의 도함수 (Gradient) 계산**: 이번에는 각 파라미터(예: 절편과 기울기)에 대해 **각각 도함수**를 계산합니다. 이처럼 **두 개 이상의 동일 함수에 대한 도함수들을 묶어 "경사 (Gradient)"**라고 부르며, 이 경사를 이용하여 손실 함수의 가장 낮은 지점으로 "하강 (Descent)"하기 때문에 **경사 하강법 (Gradient Descent)**이라는 이름이 붙었습니다.
3.  **초기 추정값 설정**: 절편과 기울기 각각에 대해 무작위 초기 값을 선택합니다.
4.  **단계 크기 계산**: 각 파라미터에 대한 도함수 값과 학습률을 사용하여 각각의 **단계 크기**를 계산합니다.
5.  **새로운 파라미터 값 계산**: 이전 절편과 이전 기울기에서 각각의 단계 크기를 빼서 **새로운 절편과 새로운 기울기**를 계산합니다.
6.  **반복 및 멈추는 조건**: 모든 단계 크기가 매우 작아지거나 최대 반복 횟수에 도달할 때까지 과정을 반복합니다.

**학습률 (Learning Rate)**은 경사 하강법의 민감도에 큰 영향을 미칩니다. 실전에서는 학습률이 자동으로 조정되기도 합니다.

### **56.3 경사 하강법 일반적인 단계 요약**

어떤 손실 함수를 사용하더라도 경사 하강법은 다음과 같이 작동합니다:

1.  **손실 함수의 도함수(경사) 계산**: 손실 함수에 포함된 각 파라미터에 대해 도함수를 계산합니다. 이를 **손실 함수의 경사(gradient)**라고 부릅니다.
2.  **파라미터의 무작위 초기 값 선택**.
3.  **경사에 파라미터 값 대입**.
4.  **단계 크기 계산**.
5.  **새로운 파라미터 값 계산**.
6.  **단계 크기가 매우 작아지거나 최대 반복 횟수에 도달할 때까지 3단계부터 반복**.

### **56.4 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)**

경사 하강법은 데이터 포인트가 많을 경우(수백만 개) 도함수 계산에 많은 시간이 소요될 수 있습니다. 이러한 문제를 해결하기 위해 **확률적 경사 하강법 (Stochastic Gradient Descent, SGD)**이 사용됩니다. SGD는 전체 데이터셋 대신 **각 단계에서 무작위로 선택된 데이터의 부분집합**을 사용하여 도함수를 계산함으로써 시간을 절약합니다.

## 57. 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)

**확률적 경사 하강법 (SGD)**은 기존의 경사 하강법(Gradient Descent, GD)이 **대규모 데이터셋과 많은 파라미터를 다룰 때 발생하는 계산상의 비효율성 문제를 해결**하기 위해 고안된 강력한 최적화 알고리즘입니다. SGD는 각 단계에서 전체 데이터 대신 **무작위로 선택된 샘플 하나 또는 데이터의 부분집합(미니 배치)**만을 사용하여 파라미터를 업데이트합니다.

### **57.1 경사 하강법 (Gradient Descent) 복습 및 문제점**

경사 하강법은 모델의 파라미터(예: 선형 회귀의 절편과 기울기)를 최적화하여 **손실 함수(Loss Function)** 값을 최소화하는 알고리즘입니다.

**경사 하강법의 작동 방식 (간단한 선형 회귀 예시)**:
1.  **손실 함수 정의**: 모델이 데이터에 얼마나 잘 맞는지 평가하기 위해 **잔차 제곱합(Sum of Squared Residuals)**과 같은 손실 함수를 사용합니다.
2.  **파라미터 초기화**: 절편과 기울기에 대한 초기 값을 설정합니다 (예: 절편=0, 기울기=1).
3.  **도함수 계산**: 손실 함수를 각 파라미터(절편, 기울기)에 대해 **도함수**를 계산합니다.
4.  **파라미터 업데이트**: 계산된 도함수 값, 관측된 데이터 값, 그리고 초기 추정치를 사용하여 **단계 크기(step size)**를 계산하고, 이를 **학습률(learning rate)**과 곱하여 새로운 파라미터 값을 업데이트합니다.
5.  **반복**: 단계 크기가 매우 작아지거나 미리 설정된 최대 반복 횟수에 도달할 때까지 이 과정을 반복합니다.

**경사 하강법의 문제점**:
*   간단한 데이터셋 (예: 3개의 데이터 포인트와 2개의 파라미터)에서는 각 단계에서 많은 계산이 필요하지 않습니다.
*   하지만 **복잡한 모델 (예: 23,000개의 유전자를 사용하는 로지스틱 회귀)**과 **대규모 데이터셋 (예: 100만 개의 샘플)**에서는 이야기가 달라집니다.
    *   이 경우, **각 단계에서 23,000개의 도함수에 대해 100만 개의 항을 계산**해야 합니다.
    *   이는 **한 단계마다 230억 개의 항**을 계산해야 함을 의미하며, 최소 1,000단계를 수행한다고 가정하면 **2.3조 개 이상의 항**을 계산해야 합니다.
    *   따라서, **빅데이터 환경에서 일반적인 경사 하강법은 매우 느려서 현실적으로 계산이 불가능**할 수 있습니다.

### **57.2 확률적 경사 하강법 (SGD)의 작동 원리**

SGD는 위에서 언급된 계산 문제를 해결하기 위해 다음과 같은 방식으로 작동합니다.

*   **샘플링 기반 계산**: 각 단계에서 전체 데이터셋 대신 **무작위로 하나의 샘플**을 선택하고, 오직 이 샘플만을 사용하여 도함수를 계산하고 파라미터를 업데이트합니다.
    *   예를 들어, 100만 개의 샘플이 있는 경우 SGD는 각 단계에서 계산할 항의 수를 **100만 분의 1**로 줄여줍니다. 이는 계산 시간을 크게 절약합니다.
*   **데이터의 중복성 활용**: 데이터에 중복성(redundancy)이 많을 때 특히 유용합니다. 예를 들어, 12개의 데이터 포인트가 있지만 3개의 클러스터를 형성하는 경우, 모든 데이터를 사용할 필요 없이 무작위로 한 점을 선택하여 경사를 계산할 수 있습니다.
*   **학습률 (Learning Rate)**: SGD 또한 일반 경사 하강법처럼 학습률 값에 민감합니다.
    *   일반적인 전략은 **상대적으로 큰 학습률로 시작하여 각 단계마다 점차 작게 만드는 것**입니다.
    *   이러한 학습률 변화 방식을 **스케줄(schedule)**이라고 부릅니다. 많은 SGD 구현에서는 이 스케줄을 기본적으로 자동으로 처리해 줍니다.
    *   만약 파라미터 추정값이 수렴하지 않는다면, 이 스케줄 설정을 조정해 볼 필요가 있습니다.

### **57.3 미니 배치 경사 하강법 (Mini-Batch Gradient Descent)**

*   **정의**: SGD의 엄격한 정의는 각 단계에서 **하나의 샘플**만을 사용하는 것이지만, 실제로 더 흔하게 사용되는 방식은 **작은 데이터 부분집합, 즉 미니 배치(mini-batch)**를 각 단계에 사용하는 것입니다.
*   **장점**: 미니 배치를 사용하면 **하나의 샘플만 사용하는 것과 전체 데이터를 사용하는 것의 장점을 모두 취할 수 있습니다**.
    *   **더 안정적인 추정**: 전체 데이터를 사용하는 것처럼 **파라미터에 대한 더 안정적인 추정치**를 더 적은 단계로 얻을 수 있습니다.
    *   **더 빠른 계산**: 하나의 샘플만 사용하는 것처럼 **전체 데이터를 사용하는 것보다 훨씬 빠릅니다**.
    *   예를 들어, 3개의 샘플로 미니 배치를 사용한 결과는 하나의 샘플만 사용했을 때보다 황금 표준(최적값)에 더 가까운 추정치를 얻었습니다.

### **57.4 새로운 데이터에 대한 파라미터 업데이트 용이성**

*   SGD의 또 다른 유용한 특징은 **새로운 데이터가 들어왔을 때 기존 파라미터 추정치를 쉽게 업데이트**할 수 있다는 점입니다.
*   **처음부터 다시 시작할 필요 없이**, 가장 최근의 파라미터 추정치에서 시작하여 새로운 샘플을 사용하여 한 단계 더 나아가 파라미터를 업데이트할 수 있습니다. 이는 모델을 지속적으로 최신 데이터로 업데이트해야 하는 시나리오에서 매우 큰 장점입니다.

**확률적 경사 하강법 (SGD)**은 방대한 데이터와 많은 파라미터를 다루는 AI 및 머신러닝 모델에서 **계산 효율성을 극대화**하는 필수적인 최적화 기법입니다. 특히 일반적인 경사 하강법이 계산적으로 불가능할 때 매우 유용하며, 새로운 데이터가 들어올 때 파라미터를 쉽게 업데이트할 수 있다는 장점도 있습니다.

## 58. AdaBoost (Adaptive Boosting)

AdaBoost는 약한 학습기(Weak Learners)들을 결합하여 강력한 분류기(Strong Classifier)를 만드는 **앙상블 학습(Ensemble Learning)** 방법 중 하나입니다. 이 영상에서는 AdaBoost를 **결정 스텀프(Decision Stumps)**와 결합하는 가장 일반적인 방식을 설명하며, 이는 AdaBoost의 기본 개념을 이해하는 데 매우 중요합니다.

### **58.1 AdaBoost의 세 가지 핵심 아이디어**

AdaBoost의 작동 방식을 이해하기 위한 세 가지 주요 개념은 다음과 같습니다. 랜덤 포레스트(Random Forest)와 대조하여 설명하면 그 특징이 더욱 명확해집니다.

1.  **많은 약한 학습기(Weak Learners) 결합**:
    *   AdaBoost는 정확도가 낮은 약한 학습기들을 많이 결합하여 분류를 수행합니다.
    *   이때 약한 학습기는 거의 항상 **스텀프(Stump)**입니다. 스텀프는 하나의 노드와 두 개의 리프(잎)만을 가진 매우 단순한 결정 트리입니다.
    *   스텀프는 하나의 변수만을 사용하여 결정을 내리기 때문에 (예: 흉통, 혈액 순환, 막힌 동맥, 체중 중 하나만 사용) 단독으로는 정확한 분류를 잘 하지 못합니다. 그래서 이를 '약한 학습기'라고 부릅니다.
    *   **랜덤 포레스트와의 차이점**: 랜덤 포레스트에서는 완전한 크기의 결정 트리를 만드는 반면, AdaBoost에서는 보통 스텀프를 사용합니다.

2.  **일부 스텀프가 최종 분류에 더 큰 영향력(Say)을 가짐**:
    *   AdaBoost로 만들어진 스텀프들의 숲에서는 **일부 스텀프가 최종 분류에 더 많은 영향력(Say)을 가집니다**.
    *   이 영향력은 해당 스텀프가 얼마나 잘 분류했는지에 따라 결정됩니다.
    *   **랜덤 포레스트와의 차이점**: 랜덤 포레스트의 각 트리는 최종 분류에 동등한 투표권을 가집니다.

3.  **이전 스텀프의 실수(Mistakes)를 고려하여 각 스텀프 생성**:
    *   AdaBoost에서는 스텀프가 만들어지는 순서가 중요합니다.
    *   **첫 번째 스텀프가 저지른 오류는 두 번째 스텀프가 만들어지는 방식에 영향을 미칩니다.** 마찬가지로 두 번째 스텀프의 오류는 세 번째 스텀프에 영향을 주는 식으로 진행됩니다.
    *   **랜덤 포레스트와의 차이점**: 랜덤 포레스트의 각 결정 트리는 서로 독립적으로 만들어집니다.

### **58.2 AdaBoost를 사용한 스텀프 숲 생성 과정 (세부 사항)**

AdaBoost는 데이터를 사용하여 스텀프 숲을 만들고, 심장 질환 여부 등을 예측하는 데 사용될 수 있습니다.

1.  **초기 샘플 가중치 부여**:
    *   처음에는 모든 샘플에 동일한 가중치(예: 전체 샘플 수의 1/N)를 부여하여 모든 샘플이 동등하게 중요하게 취급됩니다.

2.  **첫 번째 스텀프 생성**:
    *   주어진 변수(예: 흉통, 막힌 동맥, 체중) 중 샘플을 가장 잘 분류하는 변수를 찾아 첫 번째 스텀프를 만듭니다.
    *   이때는 모든 샘플 가중치가 동일하므로 무시하고 진행할 수 있습니다. 최적의 분류 변수를 결정하기 위해 **Gini 지수**와 같은 기법을 사용할 수 있습니다.

3.  **스텀프의 영향력(Amount of Say) 결정**:
    *   스텀프가 최종 분류에 얼마나 많은 영향력을 가질지 결정해야 합니다. 이는 스텀프가 샘플을 얼마나 잘 분류했는지에 따라 결정됩니다.
    *   **총 오류(Total Error)** 계산: 잘못 분류된 샘플들의 가중치를 합산하여 총 오류를 계산합니다. 총 오류는 0(완벽한 스텀프)과 1(끔찍한 스텀프) 사이의 값을 가집니다.
    *   **영향력 공식**: `영향력 = 1/2 * log((1 - 총 오류) / 총 오류)` 공식을 사용하여 영향력을 계산합니다.
        *   **총 오류가 작을수록(잘 분류할수록) 영향력은 크고 긍정적인 값**을 가집니다.
        *   총 오류가 0.5(동전 던지기와 비슷)이면 영향력은 0이 됩니다.
        *   **총 오류가 클수록(잘못 분류할수록) 영향력은 크고 음수 값**을 가집니다. 음수 영향력은 스텀프의 투표를 반대로 해석하게 만듭니다.

4.  **샘플 가중치 수정**:
    *   이전 스텀프의 실수를 다음 스텀프가 고려하도록 샘플 가중치를 수정합니다.
    *   **잘못 분류된 샘플의 가중치 증가**: 잘못 분류된 샘플의 가중치는 `이전 샘플 가중치 * e^(스텀프의 영향력)` 공식을 사용하여 증가시킵니다. 영향력이 클수록 가중치가 더 많이 증가합니다.
    *   **정확하게 분류된 샘플의 가중치 감소**: 정확하게 분류된 샘플의 가중치는 `이전 샘플 가중치 * e^(-스텀프의 영향력)` 공식을 사용하여 감소시킵니다. 영향력이 클수록 가중치가 더 많이 감소합니다.
    *   **가중치 정규화**: 새로운 샘플 가중치들의 합이 1이 되도록 정규화합니다.

5.  **다음 스텀프 생성**:
    *   수정된 샘플 가중치를 사용하여 두 번째 스텀프를 만듭니다.
    *   **가중치 사용 방식**:
        *   **가중 Gini 함수 사용**: 가중 Gini 지수를 계산하여 다음 스텀프를 분할할 변수를 결정합니다. 이때 가중치는 이전에 잘못 분류되어 가중치가 높아진 샘플에 더 큰 중요성을 부여합니다.
        *   **새로운 데이터셋 생성**: 가중치가 높은 샘플의 복사본을 포함하는 새로운 데이터셋을 만듭니다. 이렇게 하면 높은 가중치를 가진 샘플이 더 자주 나타나게 되어, 다음 스텀프가 이 샘플들을 올바르게 분류하는 데 더 집중하도록 유도합니다.
    *   이후 동일한 과정(최적 스텀프 찾기, 영향력 계산, 가중치 업데이트)을 반복하여 스텀프 숲을 만듭니다.

### **58.3 AdaBoost 스텀프 숲을 이용한 분류 방법**

AdaBoost로 만들어진 스텀프 숲이 실제 분류를 수행하는 과정은 다음과 같습니다.

1.  **각 스텀프의 예측과 영향력 합산**:
    *   환자를 '심장 질환이 있음'으로 분류한 스텀프들의 영향력을 모두 합산합니다.
    *   환자를 '심장 질환이 없음'으로 분류한 스텀프들의 영향력을 모두 합산합니다.
2.  **최종 분류 결정**:
    *   두 그룹 중 **영향력 합계가 더 큰 쪽으로 환자를 최종 분류**합니다. 예를 들어, '심장 질환이 있음' 그룹의 합이 더 크면 해당 환자는 심장 질환이 있는 것으로 분류됩니다.

요약하자면, AdaBoost는 여러 약한 학습기(주로 스텀프)를 순차적으로 학습시키면서, 이전 학습기가 잘못 분류한 데이터에 더 큰 가중치를 부여하여 다음 학습기가 해당 오류를 개선하도록 유도하고, 각 학습기의 성능에 따라 최종 분류에 미치는 영향력을 다르게 부여하는 강력한 앙상블 학습 기법입니다.

## 59. 그레이디언트 부스팅 (Gradient Boosting) 1/4: 회귀 기본 아이디어
그레이디언트 부스팅(Gradient Boosting)의 기본 아이디어를 설명하며, 특히 **회귀(Regression)** 문제에 적용되는 방식에 초점을 맞춥니다.

그레이디언트 부스팅 머신러닝 알고리즘이 어떻게 작동하는지 설명하는 시리즈의 첫 번째 부분입니다. 특히, 몸무게와 같은 **연속적인 값(Continuous Value)을 예측하는 회귀 문제**에 그레이디언트 부스팅을 사용하는 가장 일반적인 방법을 다룹니다.

### **59.1 선수 지식**

이 영상의 내용을 완전히 이해하기 위해서는 다음 개념에 대한 사전 지식이 필요하다고 언급됩니다:
*   **결정 트리(Decision Trees)**
*   **AdaBoost**
*   **편향-분산 트레이드오프(Bias-Variance Trade-off)**

### **59.2 사용 데이터 예시**

영상에서는 6명의 사람들의 다음 측정값을 사용하여 설명합니다:
*   키 (Height)
*   가장 좋아하는 색 (Favorite Colors)
*   성별 (Genders)
*   몸무게 (Weights)
    
이 데이터를 사용하여 몸무게를 예측하는 과정을 단계별로 설명합니다.

### **59.3 AdaBoost와 그레이디언트 부스팅의 비교 및 대조**

그레이디언트 부스팅은 AdaBoost와 많은 유사점을 가지고 있지만, 중요한 차이점도 있습니다.

**유사점:**
*   **이전 트리의 오류 기반으로 다음 트리 구축**: 두 알고리즘 모두 이전 트리가 만든 오류를 기반으로 다음 트리를 구축합니다.
*   **스케일링**: 두 알고리즘 모두 트리의 기여도를 스케일링(조정)합니다.

**차이점:**
*   **시작점**:
    *   **AdaBoost**: 훈련 데이터에서 매우 짧은 트리인 **스텀프(Stump)**를 구축하며 시작합니다.
    *   **그레이디언트 부스팅**: 트리나 스텀프 대신 **단일 리프(Single Leaf)**로 시작합니다. 이 리프는 모든 샘플에 대한 예측하려는 값(예: 몸무게)의 **평균값**을 나타냅니다.
*   **트리 크기**:
    *   **AdaBoost**: 일반적으로 스텀프(단일 분할)를 사용합니다.
    *   **그레이디언트 부스팅**: AdaBoost와 달리 트리가 일반적으로 스텀프보다 큽니다. 하지만 여전히 트리의 크기를 제한합니다. 예시에서는 최대 4개의 리프를 가진 트리를 만들지만, 실제로는 8개에서 32개의 리프를 가지도록 설정하는 경우가 많습니다.
*   **스케일링 방식**:
    *   **AdaBoost**: 새로운 스텀프가 최종 결과에 미치는 **영향력(amount of say)**은 이전 오류를 얼마나 잘 보정했는지에 따라 달라집니다.
    *   **그레이디언트 부스팅**: **모든 트리(All Trees)**를 **동일한 양**으로 스케일링합니다. 이 역할을 하는 것이 **학습률(Learning Rate)**입니다.

### **59.4 그레이디언트 부스팅을 이용한 회귀 과정 (단계별 설명)**

그레이디언트 부스팅이 훈련 데이터를 사용하여 몸무게를 예측하는 가장 일반적인 과정은 다음과 같습니다.

1.  **초기 예측 (평균값)**:
    *   예측하려는 변수(예: 몸무게)의 **평균값**을 계산합니다. 이것이 모든 사람의 몸무게에 대한 첫 번째 예측이 됩니다. (예: 71.2 kg)

2.  **가상 잔차 (Pseudo Residuals) 계산**:
    *   첫 번째 트리의 오류를 기반으로 다음 트리를 구축하기 위해 **관측된 값(Observed Weights)**과 **현재 예측된 값(Predicted Weight)** (즉, 평균값)의 **차이**를 계산합니다.
    *   이 차이를 **"가상 잔차(Pseudo Residual)"**라고 부르며, 선형 회귀의 잔차와 유사하지만 그레이디언트 부스팅에서 사용된다는 점을 상기시키는 용어입니다.
    *   모든 훈련 데이터 샘플에 대해 이 잔차를 계산합니다.

3.  **잔차를 예측하는 트리 구축**:
    *   이제 키, 좋아하는 색, 성별과 같은 특징을 사용하여 **이전에 계산된 잔차를 예측하는 트리**를 구축합니다.
    *   예시에서는 최대 4개의 리프만 허용하지만, 실제로는 8개에서 32개의 리프를 허용하는 경우가 많습니다.
    *   만약 여러 샘플이 같은 리프로 가게 되면, 해당 리프의 잔차들을 **평균값**으로 대체합니다.

4.  **학습률 (Learning Rate)을 이용한 트리 스케일링**:
    *   새로운 트리의 기여도를 스케일링하기 위해 **학습률**을 사용합니다.
    *   학습률은 0과 1 사이의 값이며, 예시에서는 0.1로 설정되었습니다.
    *   **오버피팅(Overfitting)** 문제를 해결하고, "올바른 방향으로 작은 단계"를 밟기 위해 이 스케일링을 사용합니다. 여러 번의 작은 단계가 테스트 데이터셋에서 더 나은 예측(낮은 분산)을 가져온다는 경험적 증거가 있습니다.

5.  **새로운 예측 업데이트**:
    *   **초기 예측 (평균값)**에 **학습률로 스케일링된 새로운 트리의 예측값**을 더하여 새로운 예측값을 만듭니다.
    *   (예: 71.2 + 0.1 * 16.8 = 72.9)

6.  **과정 반복**:
    *   업데이트된 예측값과 관측된 값 사이의 **새로운 가상 잔차**를 다시 계산합니다.
    *   이 새로운 잔차를 예측하는 또 다른 트리를 구축하고, 학습률로 스케일링하여 전체 예측에 추가합니다.
    *   이 과정은 미리 지정된 최대 트리의 수에 도달하거나, 추가적인 트리가 잔차의 크기를 더 이상 유의미하게 줄이지 못할 때까지 반복됩니다.
    *   각 트리를 추가할 때마다 잔차가 줄어들며, 이는 좋은 예측을 향해 작은 단계를 밟고 있다는 의미입니다.

### **59.5 최종 예측 방법**

그레이디언트 부스팅으로 모델이 완성되면, 새로운 측정값이 주어졌을 때 다음과 같이 예측합니다:
*   **초기 예측 (평균값)**에서 시작하여,
*   **학습률로 스케일링된 첫 번째 트리의 예측값**, **두 번째 트리의 예측값**, **세 번째 트리의 예측값** 등을 **모두 더합니다**.

이러한 과정을 통해 최종적인 예측값이 산출됩니다.

요약하자면, 그레이디언트 부스팅은 초기 평균값 예측에서 시작하여, 이전 트리가 만든 오류(잔차)를 예측하는 약한 학습기(트리)들을 순차적으로 구축하고, 각 트리의 기여도를 학습률로 스케일링하여 최종 예측값을 점진적으로 개선해 나가는 앙상블 학습 기법입니다.

## 60. 그레이디언트 부스팅 (Gradient Boosting) 2/4, 회귀 상세

Gradient Boost 알고리즘이 **회귀(Regression)** 문제에 어떻게 적용되는지에 대한 상세한 설명을 다룹니다. 

### **60.1 훈련 데이터셋 및 손실 함수(Loss Function)**

*   **훈련 데이터셋**: 동영상에서는 키, 좋아하는 색상, 성별, 체중 정보가 있는 세 사람의 **매우 간단한 훈련 데이터셋**을 예시로 사용합니다. 여기서 $x_i$는 각 사람의 측정값(예측에 사용)을, $y_i$는 각 사람의 체중(예측하려는 값)을 나타냅니다. 총 데이터 샘플 수 $n$은 3입니다.
*   **손실 함수(Loss Function)**: 모델이 훈련 데이터에 얼마나 잘 맞는지를 평가하는 데 사용되는 **미분 가능한 손실 함수**가 필요합니다. Gradient Boost의 회귀 문제에서 가장 일반적으로 사용되는 손실 함수는 **$\frac{1}{2} \times (\text{관측값} - \text{예측값})^2$** 입니다.
    *   이 손실 함수는 선형 회귀에서 사용되는 **제곱 오차 합(sum of squared residuals)**과 관련이 있습니다.
    *   $\frac{1}{2}$이 곱해지는 이유는 나중에 **미분할 때 수학을 더 쉽게** 만들기 위함입니다. 예측값에 대해 미분하면 $-1 \times (\text{관측값} - \text{예측값})$, 즉 **음의 잔차(negative residual)**가 남게 됩니다. Gradient Boost는 미분(gradient)을 많이 사용하므로 이 형태가 편리합니다.
    *   $y_i$는 관측값을 나타내고, $F(x)$는 예측값을 제공하는 함수입니다.


### **60.2 Gradient Boost 알고리즘의 단계별 설명**

Gradient Boost 알고리즘은 크게 세 단계로 구성됩니다.

1.단계 1: 모델을 상수 값으로 초기화 (Initialize the model with a constant value)
*   이 단계에서는 모델 $F_0(x)$를 **상수 값으로 초기화**합니다.
*   이 상수 값은 **손실 함수의 합계를 최소화하는 예측값** $\gamma$를 찾는 방식으로 결정됩니다.
*   주어진 손실 함수($\frac{1}{2} \times (\text{관측값} - \text{예측값})^2$)의 경우, 이 값을 최소화하는 $\gamma$는 **관측값들의 평균**이 됩니다.
*   예시에서는 관측된 체중들의 평균인 **73.3**으로 $F_0(x)$가 초기화됩니다. 이는 모든 샘플이 73.3의 체중을 가질 것이라고 예측하는 하나의 **리프(leaf)**를 생성하는 것과 같습니다.

2.단계 2: M개의 트리를 생성하는 반복 (Loop to make M trees)
이 단계는 실제로는 대부분의 사람들이 $M$을 100개 또는 그 이상으로 설정하는 루프입니다. 
동영상에서는 예시를 위해 $M=2$로 설정합니다.

*   **A. 음의 기울기(Negative Gradient) 계산 (Calculate negative gradient)**
    *   이 부분은 **손실 함수를 예측값에 대해 미분한 값에 -1을 곱한 것**입니다.
    *   앞서 설명했듯이, 이 값은 **(관측값 - 예측값)**, 즉 **잔차(residual)**와 동일합니다.
    *   Gradient Boost는 이 잔차를 **"가상 잔차(pseudo-residuals)"**라고 부르며, $R_{i,m}$으로 표기합니다.
    *   초기 $m=1$일 때는 $F_0(x)$ (73.3)를 현재 예측값으로 사용하여 각 샘플에 대한 잔차를 계산합니다.
        *   예: 첫 번째 샘플의 잔차 $R_{1,1}$ = 관측값 - 73.3 = 88 - 73.3 = 14.7.
*   **B. 잔차를 예측하기 위한 회귀 트리 구축 (Build a regression tree to predict the residuals)**
    *   이제 계산된 **잔차를 예측하기 위해 새로운 회귀 트리**를 구축합니다. 이 트리는 원래의 예측 변수(키, 좋아하는 색상, 성별)를 사용하여 잔차를 예측합니다.
    *   동영상 예시에서는 세 개의 샘플만 있기 때문에, 설명을 위해 일반적으로 사용되지 않는 **스텀프(stump)**(하나의 분할만 있는 매우 작은 트리)를 사용합니다. 실제 Gradient Boost에서는 더 큰 트리를 사용합니다.
    *   트리의 각 **리프(leaf)**는 **터미널 영역(terminal region)** $R_{j,m}$을 나타냅니다.
*   **C. 각 리프의 출력값($\gamma$) 결정 (Determine output values for each leaf)**
    *   새로 만든 트리의 각 리프에 대해 **출력값 $\gamma_{j,m}$**을 계산합니다.
    *   이 $\gamma$ 값은 해당 리프에 속한 샘플들의 **이전 예측값과 현재 잔차를 사용하여 손실 함수의 합계를 최소화하는 값**으로 결정됩니다.
    *   주어진 손실 함수의 경우, 이 $\gamma$ 값은 해당 리프에 속한 **잔차들의 평균**이 됩니다.
        *   예시에서 리프 $R_{1,1}$의 출력값은 해당 리프의 잔차 평균인 -7.3입니다.
        *   리프 $R_{2,1}$의 출력값은 해당 리프의 잔차 평균인 8.7입니다.\\
*   **D. 새로운 예측값($F_m(x)$) 생성 (Make new predictions)**
    *   각 샘플에 대한 **새로운 예측값 $F_m(x)$**을 생성합니다.
    *   새로운 예측값은 **이전 예측값 $F_{m-1}(x)$**에 **학습률(learning rate)** $\nu$를 곱한 **새로운 트리의 리프 출력값 $\gamma_{j,m}$**을 더하여 계산됩니다.
        *   **$F_m(x) = F_{m-1}(x) + \nu \times \gamma_{j,m}$**.
    *   **학습률 $\nu$**는 0과 1 사이의 값이며, 각 트리가 최종 예측에 미치는 영향을 줄여서 **장기적으로 정확도를 향상**시키는 역할을 합니다. 예시에서는 $\nu = 0.1$로 설정됩니다.
    *   이 과정을 통해 새로운 예측값들은 이전 예측값보다 **관측값에 약간 더 가까워집니다**.
    *   이 모든 과정(A-D)은 $m=1$에 대해 완료되었으며, 이제 $m=2$로 설정하고 이 과정을 다시 반복하여 새로운 잔차, 새로운 트리, 새로운 출력값, 그리고 새로운 예측값 $F_2(x)$를 계산합니다.

3.**단계 3: 최종 예측 (Final Prediction)**
*   $M$번의 반복이 모두 끝나면, **마지막 예측값 $F_M(x)$**가 Gradient Boost 알고리즘의 최종 결과물이 됩니다.
*   새로운 데이터가 주어졌을 때, 이 $F_M(x)$를 사용하여 체중을 예측할 수 있습니다.
*   예시에서는 $M=2$일 때, 최종 예측은 $F_2(x)$가 되며, 이는 $F_0(x)$에 첫 번째 트리의 출력값($\gamma_{j,1}$)과 두 번째 트리의 출력값($\gamma_{j,2}$)에 각각 학습률을 곱한 것을 더한 값이 됩니다.

Gradient Boost는 **이전 예측의 잔차를 학습하는 여러 개의 약한 예측 모델(대부분 결정 트리)을 순차적으로 결합**하여 강력한 예측 모델을 구축하는 앙상블 학습 기법입니다. 각 트리는 이전 트리가 잘못 예측한 부분을 보정하며, 학습률을 통해 모델의 과적합을 방지하고 일반화 성능을 향상시킵니다.

## 61. Gradient Boost Part 3 (of 4): Classification

### **61.1 훈련 데이터셋 및 초기 예측**

*   **훈련 데이터셋**: 동영상에서는 6명의 연령, 좋아하는 색상, 팝콘 선호도, 그리고 영화 "Troll Two"를 좋아했는지 여부가 기록된 **훈련 데이터**를 예시로 사용합니다. 목표는 "Troll Two"를 좋아하는지 여부를 분류하는 것입니다.
*   **초기 예측 (Initial Prediction)**: Gradient Boost를 분류에 사용할 때, 모든 개인에 대한 초기 예측은 **로그 오즈(log of the odds)**입니다.
    *   로그 오즈는 로지스틱 회귀에서 평균과 유사한 개념입니다.
    *   훈련 데이터셋에서 4명이 "Troll Two"를 좋아하고 2명이 좋아하지 않는 경우, "Troll Two"를 좋아할 로그 오즈는 **log(좋아하는 사람 수 / 좋아하지 않는 사람 수)**, 즉 log(4/2) = 0.7이 됩니다. 이 값이 초기 예측값으로 사용됩니다.

*   **로그 오즈를 확률로 변환**: 분류를 위해 로그 오즈를 **확률(Probability)**로 변환하는 가장 쉬운 방법은 **로지스틱 함수(Logistic Function)**를 사용하는 것입니다.
    *   확률 = $e^{\text{로그 오즈}} / (1 + e^{\text{로그 오즈}})$.
    *   초기 로그 오즈 0.7을 로지스틱 함수에 넣으면 0.7의 확률이 나옵니다 (반올림으로 인해 로그 오즈 값과 동일하게 보일 수 있음).
*   **분류 결정**: 일반적으로 0.5보다 큰 확률은 긍정 클래스로 분류합니다. 초기 확률 0.7은 0.5보다 크므로, 모든 사람을 "Troll Two"를 좋아한다고 분류할 수 있습니다. 하지만 훈련 데이터에는 좋아하지 않는 사람도 있으므로 이 초기 예측은 "형편없다(lame)"고 할 수 있습니다.

### **61.2 의사 잔차(Pseudo-Residuals) 계산**

*   초기 예측이 얼마나 나쁜지 측정하기 위해 **의사 잔차(pseudo-residuals)**를 계산합니다.
*   의사 잔차는 **관측값과 예측값의 차이**입니다.
    *   그래프를 통해 이해하는 것이 더 쉽습니다. Y축은 "Troll Two"를 좋아할 확률(0 또는 1)을 나타내고, 점선은 예측 확률 0.7을 나타냅니다.
    *   관측값(Observed Value)은 0 (싫어함) 또는 1 (좋아함)입니다.
    *   잔차 = **관측값 - 예측값**.
    *   예를 들어, 첫 번째 샘플의 관측값이 1이고 예측값이 0.7이면, 잔차는 1 - 0.7 = 0.3이 됩니다. 이 잔차들은 새로운 열에 저장됩니다.


### **61.3 반복적인 트리 생성 및 예측 업데이트**

Gradient Boost는 여러 개의 트리를 반복적으로 생성하여 예측을 개선합니다.

1. **잔차를 예측하기 위한 회귀 트리 구축**
*   이제 계산된 **잔차를 예측하기 위해 새로운 회귀 트리**를 구축합니다. 이 트리는 원래의 예측 변수(팝콘 선호도, 연령, 좋아하는 색상)를 사용하여 잔차를 예측합니다.
*   이 예시에서는 간단하게 3개의 리프(leaf)만 허용되는 작은 트리를 사용합니다. 실제로는 보통 **8개에서 32개 사이의 리프**를 가지는 트리를 사용합니다.

2. **각 리프의 출력값($\gamma$) 결정**
*   새로 만든 트리의 각 리프에 대해 **출력값 $\gamma_{j,m}$**을 계산합니다.
*   분류를 위한 Gradient Boost에서 리프의 출력값 계산은 회귀의 경우보다 더 복잡합니다. 예측이 로그 오즈 형태이고 리프가 확률에서 파생되기 때문입니다.
*   가장 일반적인 변환 공식은 다음과 같습니다:
    **분자**: 해당 리프에 속한 모든 잔차의 합계
    **분모**: 각 잔차에 대해 **(이전 예측 확률 $\times$ (1 - 이전 예측 확률))**의 합계
    *   이 공식의 유도는 기술적이며 "Gradient Boost Part 4"에서 다룹니다.
*   예시에서, 리프에 하나의 잔차만 있는 경우 이 공식에 해당 잔차와 초기 예측 확률(0.7)을 대입하여 계산합니다. 예를 들어, -3.3과 같은 값이 나옵니다.
*   리프에 여러 개의 잔차가 있는 경우, 분자에는 해당 잔차들을 모두 더하고, 분모에는 각 잔차에 해당하는 이전 예측 확률(초기 트리의 경우 0.7)을 이용하여 계산한 값을 모두 더합니다. 예를 들어, -1 또는 1.4와 같은 값이 나옵니다.

3. **새로운 예측값($F_m(x)$) 생성**
*   새로운 로그 오즈 예측값은 **이전 로그 오즈 예측값**에 **학습률(learning rate) $\nu$**를 곱한 **새로운 트리의 리프 출력값 $\gamma_{j,m}$**을 더하여 계산됩니다.
    *   **$F_m(x) = F_{m-1}(x) + \nu \times \gamma_{j,m}$**.
*   학습률 $\nu$는 0에서 1 사이의 값으로, 각 트리가 최종 예측에 미치는 영향을 조절합니다. 예시에서는 0.8을 사용했지만, **0.1이 더 흔하게 사용**됩니다.
*   계산된 새로운 로그 오즈 예측값을 다시 로지스틱 함수에 넣어 새로운 예측 확률로 변환합니다. 이를 통해 이전 예측값보다 관측값에 약간 더 가까워지는 방향으로 작은 개선이 이루어집니다.
*   새로운 예측 확률이 이전보다 나빠질 수도 있는데, 이는 많은 트리를 만드는 이유 중 하나입니다.

4. **새로운 의사 잔차 계산 및 반복**
*   새로운 예측 확률을 바탕으로 **새로운 의사 잔차**를 계산합니다.
*   이전과 마찬가지로 잔차는 관측된 확률(0 또는 1)과 새로운 예측 확률 간의 차이입니다.
*   이 과정 (잔차 계산, 새로운 트리 구축, 리프 출력값 결정, 예측 업데이트)은 설정된 **최대 트리 수**에 도달하거나 잔차가 매우 작아질 때까지 **반복**됩니다.

### **61.4 최종 예측**

*   최대 트리 수($M$)만큼 반복이 완료되면, **최종 로그 오즈 예측값 $F_M(x)$**가 생성됩니다.
*   새로운 사람이 주어졌을 때, 이 사람의 데이터를 초기 예측 리프와 $M$개의 트리에 순차적으로 통과시킵니다.
    *   초기 리프의 예측값에, 각 트리를 통해 얻은 스케일링된(학습률이 곱해진) 리프 출력값을 계속 더해 나갑니다.
*   이렇게 얻은 최종 로그 오즈 예측값(예: 2.3)을 **로지스틱 함수에 넣어 확률로 변환**합니다 (예: 0.9).
*   이 확률(예: 0.9)이 분류 임계값(예: 0.5)보다 크면, 해당 사람은 긍정 클래스("Troll Two"를 좋아함)로 분류됩니다.

Gradient Boost는 분류 문제에서도 회귀와 유사하게 **이전 모델이 잘못 예측한 부분을 보정하기 위해 잔차(여기서는 의사 잔차)를 학습하는 약한 분류 모델(대부분 결정 트리)을 순차적으로 결합**합니다. 특히, 분류에서는 예측값을 **로그 오즈** 형태로 유지하고, 이를 **로지스틱 함수**를 통해 확률로 변환하며, 각 리프의 출력값을 계산할 때 로그 오즈의 특성을 반영하는 **특정 공식**을 사용하여 예측을 점진적으로 개선합니다. 학습률을 통해 모델의 과적합을 방지하고 일반화 성능을 향상시키는 것이 중요합니다.

## 62. Gradient Boost Part 4 (of 4): Classification Details

### 62.1 **분류를 위한 그래디언트 부스트(Gradient Boost for Classification) 개요**

이 영상은 그래디언트 부스트가 분류 문제에 어떻게 적용되는지 깊이 있게 다루고 있습니다. 설명을 이해하기 위해서는 로지스틱 회귀(Logistic Regression)에서 **로그 오즈(log odds)**와 **로그 우도(log likelihood)**의 역할에 대한 기본적인 이해가 필요합니다. 실습 예제에서는 알고리즘의 세부 사항에 집중하기 위해 매우 작은 훈련 데이터 세트와 스텀프(stumps)를 사용하지만, 실제로는 8개에서 32개의 리프(leaves)를 가진 트리를 사용하는 것이 일반적입니다.

**훈련 데이터 세트 예시**:
세 명의 사람이 팝콘을 좋아하는지 여부, 나이, 좋아하는 색상, 그리고 영화 '트롤 2'를 좋아하는지 여부(예측 목표)를 포함하는 간단한 데이터 세트를 사용합니다. `X_i`는 예측에 사용되는 측정값 행을, `y_i`는 '트롤 2'를 좋아하는지 여부를 나타냅니다.

### 62.2 **주요 개념 및 구성 요소**

1.  **미분 가능한 손실 함수 (Differentiable Loss Function)**
    *   분류를 위해 가장 일반적으로 사용되는 손실 함수는 **음의 로그 우도(Negative Log Likelihood)**입니다.
    *   로그 우도는 예측이 좋을수록 값이 커지므로, 손실 함수로 사용하려면 음수 1을 곱하여 작은 값이 더 나은 모델을 나타내도록 합니다.
    *   이 음의 로그 우도 함수는 예측 확률(P) 대신 **예측 로그 오즈(predicted log odds)**의 함수로 변환됩니다. 이는 계산을 용이하게 하기 위함입니다. 최종 손실 함수는 다음과 같은 형태로 유도됩니다:
        `손실 함수 = -(observed * log(P)) - ((1 - observed) * log(1 - P))` 에서 `predicted log odds`의 함수로 변환됩니다.

2.  **손실 함수의 1차 미분 (First Derivative of the Loss Function)**
    *   손실 함수를 예측 로그 오즈에 대해 미분할 수 있으며, 이 미분 값은 **`-(observed - P)`** (관측값 - 예측 확률) 또는 `observed - predicted probability`와 관련이 있습니다.
    *   이는 **유사 잔차(pseudo residual)**를 계산하는 데 사용됩니다.
    *   경우에 따라 예측 로그 오즈 함수를 사용하는 것이 더 쉽고, 경우에 따라 예측 확률 P 함수를 사용하는 것이 더 쉽습니다.

3.  **손실 함수의 2차 미분 (Second Derivative of the Loss Function)**
    *   손실 함수의 2차 미분은 나중에 감마(gamma) 값을 계산하는 데 필요합니다.
    *   복잡한 대수 과정을 거쳐 **`P * (1 - P)`** 라는 간단한 형태로 정리됩니다.


### 62.3 **그래디언트 부스트 분류 알고리즘 단계**

그래디언트 부스트는 여러 개의 약한 학습기(weak learners)를 순차적으로 구축하여 이전 학습기의 오류를 보정하는 방식으로 작동합니다.

**1단계: 모델 초기화 (Initialize the Model)**
*   **초기 예측값(Initial Prediction) 계산**: 모든 샘플에 대한 **최적의 초기 예측 로그 오즈(F₀(X))**를 찾아 모델을 초기화합니다.
*   이 값은 손실 함수의 합을 최소화하는 로그 오즈 값 `gamma`를 찾는 과정을 통해 결정됩니다.
*   손실 함수의 합을 로그 오즈에 대해 미분한 후, 그 합이 0이 되는 값을 찾아 `gamma` (여기서는 예측 확률 P)를 계산합니다. 예시에서는 `2/3`가 초기 예측 확률 P로 계산됩니다.
*   이 예측 확률은 다시 **예측 로그 오즈**로 변환됩니다. 예시에서는 `log(2/1) = 0.69`가 초기 예측 로그 오즈 `F₀(X)`가 됩니다.

**2단계: 트리를 구축하는 반복 과정 (Build Trees - Iterative Process)**
`m`번째 트리(`m = 1, 2, ... , M`)를 구축하기 위해 다음 단계를 반복합니다.

*   **A. 유사 잔차(Pseudo Residuals) 계산**
    *   **유사 잔차 `R_i,m`**는 현재 모델의 예측 오류를 나타냅니다.
    *   이는 **손실 함수의 1차 미분 값에 -1을 곱한 값**으로 계산됩니다.
    *   수식적으로는 `observed - predicted probability`와 같습니다.
    *   가장 최근의 예측 로그 오즈(`F_{m-1}(X)`)를 사용하여 각 샘플에 대한 예측 확률을 계산하고, 이를 기반으로 유사 잔차를 계산합니다.

*   **B. 회귀 트리 구축 (Build a Regression Tree)**
    *   앞서 계산된 **유사 잔차를 예측 목표로 하는 회귀 트리**를 구축합니다.
    *   이 트리는 입력 특성(나이, 좋아하는 색상 등)을 사용하여 잔차를 예측합니다.
    *   트리의 터미널 리전(terminal regions), 즉 **리프(leaves)**에 레이블을 지정합니다.

*   **C. 리프 출력 값 (Gamma) 계산**
    *   새로 생성된 트리의 각 리프(`R_j,m`)에 대한 **출력 값 `gamma_j,m`**을 계산합니다.
    *   이 `gamma` 값은 해당 리프에 속한 샘플들에 대한 손실 함수의 합을 최소화하는 값입니다.
    *   **2차 테일러 다항식(second-order Taylor polynomial)**으로 손실 함수를 근사하여 `gamma`를 쉽게 찾습니다.
    *   근사된 손실 함수를 `gamma`에 대해 미분하고 0으로 설정하여 `gamma`를 풀면 다음과 같습니다:
        `gamma = - (손실 함수의 1차 미분) / (손실 함수의 2차 미분)`.
    *   이전에 계산된 1차 미분(`observed - predicted probability`)과 2차 미분(`P * (1 - P)`)을 대입하면:
        `gamma = (잔차) / (P * (1 - P))`.
        *   **단일 샘플 리프**: 해당 샘플의 잔차를 `P * (1 - P)`로 나눈 값이 됩니다.
        *   **여러 샘플이 있는 리프**: 해당 리프의 **모든 샘플에 대한 잔차의 합**을 **모든 샘플에 대한 `P * (1 - P)`의 합**으로 나눈 값이 됩니다.

*   **D. 새로운 예측값 생성 (Make New Predictions)**
    *   각 샘플에 대한 **새로운 예측 로그 오즈 `F_m(X)`**를 업데이트합니다.
    *   업데이트 공식은 다음과 같습니다:
        **`F_m(X) = F_{m-1}(X) + 학습률(nu) * gamma_j,m`**
        *   `F_{m-1}(X)`: 이전 단계의 예측 로그 오즈.
        *   `nu` (학습률): 모델의 학습 속도를 조절하는 값 (예시에서는 0.8).
        *   `gamma_j,m`: 현재 구축된 트리의 해당 리프 출력 값.
    *   이 과정을 통해 새로운 예측은 이전 예측보다 더 나은 예측을 제공하도록 조정됩니다.

이 2단계는 `M`번 반복됩니다. 실제 적용에서는 `M`이 100회 이상인 경우가 많습니다.

**3단계: 최종 출력 (Final Output)**
*   모든 `M`번의 반복이 끝나면, **최종 예측 `F_M(X)`**가 그래디언트 부스트 알고리즘의 출력이 됩니다.
*   새로운 데이터가 들어오면, 이 `F_M(X)`를 사용하여 예측 로그 오즈를 계산합니다.
*   계산된 예측 로그 오즈는 **예측 확률**로 변환할 수 있습니다.
*   예측 확률을 0.5와 같은 특정 임계값(threshold)과 비교하여 최종 분류(예: '트롤 2'를 좋아하는지 여부)를 결정합니다.

## 63. Troll 2

## 64. XGBoost  Part 1 (of 4): 회귀 

XGBoost는 **eXtreme Gradient Boost**의 약자로, 많은 구성 요소를 가진 강력한 머신러닝 알고리즘입니다. 각 구성 요소는 이해하기 쉽고, 이 영상에서는 XGBoost가 독특한 회귀 트리를 사용하여 회귀를 수행하는 방법에 대한 직관을 구축하는 데 초점을 맞춥니다. XGBoost는 크고 복잡한 데이터셋에 사용하도록 설계되었습니다.

### **64.1 XGBoost 회귀 트리 구축 과정**

XGBoost는 여러 단계를 거쳐 회귀 트리를 구축합니다.

**1.1. 초기 예측 설정**
*   가장 첫 번째 단계는 초기 예측을 하는 것입니다. 이 예측은 어떤 값도 될 수 있지만, 기본적으로 **0.5**로 설정됩니다.
*   잔차(residuals, 관측값과 예측값의 차이)는 초기 예측의 정확도를 보여줍니다.

**1.2. 잔차에 회귀 트리 적합**
*   XGBoost는 일반적인 회귀 트리 대신 **고유한 회귀 트리(XGBoost tree)**를 잔차에 적합시킵니다.
*   각 트리는 하나의 리프(leaf)로 시작하며, 모든 잔차가 이 리프로 이동합니다.

**1.3. 유사성 점수(Similarity Score) 계산**
*   리프에 있는 잔차들의 **품질 점수** 또는 **유사성 점수**를 계산합니다.
*   **공식**: `(잔차의 합계)² / (잔차의 개수 + 람다)`.
    *   **람다(lambda)**는 정규화(regularization) 파라미터로, 초기에는 0으로 설정됩니다.
    *   분자에서 잔차를 더하기 전에 제곱하지 않으므로, 양수 잔차와 음수 잔차는 서로 상쇄될 수 있습니다.

**1.4. 데이터 분할 및 게인(Gain) 계산**
*   유사한 잔차들을 더 잘 그룹화하기 위해 데이터를 두 그룹으로 분할할 수 있는지 확인합니다.
*   특정 임계값(예: 약물 복용량)을 기준으로 데이터를 분할한 후, 새로 생성된 각 리프에 대해 유사성 점수를 계산합니다.
*   분할이 얼마나 더 나은지 정량화하기 위해 **게인(Gain)**을 계산합니다.
*   **공식**: `게인 = (왼쪽 리프의 유사성 점수) + (오른쪽 리프의 유사성 점수) - (루트의 유사성 점수)`.
*   가장 큰 게인을 제공하는 임계값을 해당 분기로 선택합니다.
*   트리 깊이는 기본적으로 최대 6단계로 제한됩니다.

### **64.2 트리 가지치기(Pruning)**

XGBoost 트리는 계산된 게인 값을 기반으로 가지치기됩니다.

*   **감마(gamma)**라는 사용자 정의 트리 복잡성 파라미터를 설정합니다 (예: 130).
*   트리의 **가장 낮은 분기(lowest branch)의 게인**과 **감마 값**의 차이를 계산합니다.
    *   **차이가 음수**이면 해당 분기를 **제거(가지치기)**합니다.
    *   **차이가 양수**이면 해당 분기를 **유지**합니다.
*   가지치기 결정이 내려지면, 다음 상위 게인 값과 감마의 차이를 계산하여 트리를 위로 거슬러 올라가면서 가지치기 여부를 결정합니다.
*   예시로, 감마를 150으로 설정하면 게인(140.17)보다 커서 가지치기가 발생하여 트리가 제거될 수 있습니다.

### **64.3 람다(Lambda)의 역할 (정규화 파라미터)**

람다는 예측의 민감도를 개별 관측값에 대해 줄이는 **정규화 파라미터**입니다.

*   **유사성 점수에 미치는 영향**: 람다가 0보다 크면 유사성 점수가 작아집니다. 감소량은 노드의 잔차 수에 반비례하며, 잔차가 적을수록 유사성 점수 감소율이 더 큽니다 (예: 잔차 1개일 때 50% 감소).
*   **게인에 미치는 영향**: 람다가 0보다 크면 게인 값 또한 작아집니다. 게인 값이 작아지면 가지치기(pruning)가 더 쉬워집니다.
*   **과적합 방지**: 감마를 0으로 설정하는 것이 가지치기를 완전히 끄는 것은 아니지만, 람다를 0보다 크게 설정하는 것은 **과적합(overfitting)**을 방지하는 데 도움을 줍니다.

### **64.4 리프의 출력값(Output Value) 계산**

*   리프의 출력값은 **공식**: `(잔차의 합계) / (잔차의 개수 + 람다)` 입니다.
*   유사성 점수 계산과 달리, 잔차의 합계를 제곱하지 않습니다.
*   **람다가 0**일 경우, 리프의 출력값은 해당 리프에 있는 **잔차의 평균**과 같습니다.
*   **람다가 0보다 크면**, 이 개별 관측값이 전체 예측에 추가하는 양을 줄여 예측 민감도를 낮춥니다. 기본값은 람다=0입니다.

### **64.5 새로운 예측 생성**

*   첫 번째 트리가 완성되면 새로운 예측을 만들 수 있습니다.
*   **새로운 예측값 = 초기 예측 + (학습률 * 트리의 출력)**.
*   여기서 **학습률(learning rate)**은 XGBoost에서 **에타(Etta)**라고 불리며, 기본값은 0.3입니다.
*   새로운 예측은 이전 잔차보다 작은 새로운 잔차를 생성하여, 모델이 올바른 방향으로 작은 단계를 밟았음을 보여줍니다.
*   이러한 과정(새로운 잔차에 기반하여 다른 트리를 구축하고 예측을 업데이트하는 과정)은 잔차가 매우 작아지거나 설정된 최대 트리 수에 도달할 때까지 **반복**됩니다.

**요약하자면, XGBoost 회귀 트리를 구축하는 주요 단계는 다음과 같습니다:**

1.  **유사성 점수**와 **게인(Gain)**을 계산하여 데이터를 분할하는 방법을 결정합니다.
2.  게인 값과 사용자 정의 트리 복잡성 파라미터인 **감마(Gamma)**의 차이를 계산하여 트리를 **가지치기(Prune)**합니다. 차이가 음수이면 가지치기하고, 양수이면 가지치기하지 않습니다.
3.  남아 있는 리프에 대한 **출력값**을 계산합니다.
4.  **람다(Lambda)**는 정규화 파라미터로, 0보다 클 때 유사성 점수를 줄여 더 많은 가지치기를 유도하고, 리프의 출력값을 더 작게 만듭니다.

## 65. XGBoost Part 2 (of 4): Classification

### **65.1 초기 예측 (Initial Prediction)**

*   **시작점**: XGBoost를 훈련 데이터에 적용하는 가장 첫 단계는 **초기 예측(initial prediction)**을 하는 것입니다.
*   **기본값**: 이 초기 예측값은 무엇이든 될 수 있지만, 기본적으로는 0.5입니다. 이는 약물 복용량과 관계없이 약물이 효과적일 확률이 50%라는 것을 의미합니다.
*   **잔차(Residuals)**: 관측된 값과 초기 예측값 사이의 차이를 **잔차(residuals)**라고 하며, 이 잔차는 초기 예측이 얼마나 좋은지를 보여줍니다.

### **65.2 XGBoost 트리 구축 (Building XGBoost Trees)**

*   **잔차에 트리 맞추기**: 회귀와 마찬가지로, XGBoost 트리는 이 잔차에 맞춰 구축됩니다.
*   **분류를 위한 유사성 점수 (Similarity Scores for Classification)**:
    *   분류를 위해 새로운 **유사성 점수(similarity scores)** 공식이 사용됩니다.
    *   **분자(Numerator)**: 이 점수의 분자는 잔차의 제곱 합으로, 회귀에서의 분자와 동일합니다.
    *   **분모(Denominator)**: 분모는 정규화 매개변수인 람다(lambda)를 포함하지만, 나머지는 회귀와 다릅니다. 분류의 경우, 분모는 **각 관측값의 '이전 예측 확률(previously predicted probability) * (1 - 이전 예측 확률)'의 합**입니다.
*   **트리 분할 (Tree Splitting)**:
    *   각 트리는 단일 리프(leaf)로 시작하며 모든 잔차가 이 리프로 이동합니다.
    *   이후 특정 임계값(threshold)을 기준으로 잔차들을 두 그룹으로 분할하여 유사한 잔차들을 더 잘 클러스터링할 수 있는지 결정합니다.
*   **게인 (Gain)**:
    *   각 분할에서 **게인(gain)** 값을 계산합니다. 게인은 분할을 통해 얻는 정보량 또는 성능 향상을 나타냅니다.
    *   게인 값이 가장 큰 임계값이 트리의 첫 번째 가지(branch)로 선택됩니다. 이 과정은 트리의 수준(level) 제한 또는 리프의 최소 잔차 수(cover)와 같은 기준에 도달할 때까지 반복됩니다.

### **65.3 트리 가지치기 및 정규화 (Tree Pruning and Regularization)**

*   **가지치기 (Pruning)**:
    *   트리 가지치기는 **감마(gamma)**라는 사용자가 정의한 트리 복잡도 매개변수를 사용하여 수행됩니다.
    *   가장 낮은 가지의 게인 값과 감마의 차이를 계산하여, 이 차이가 음수이면 해당 가지를 가지치기합니다.
*   **람다 (Lambda)**:
    *   **람다(lambda)**는 정규화(regularization) 매개변수입니다.
    *   람다는 유사성 점수를 줄여 궁극적으로 리프를 가지치기 더 쉽게 만듭니다.
    *   람다 값이 0보다 크면 개별 관측값에 대한 트리의 민감도를 줄이고 가지치기를 통해 다른 관측값과 결합하는 효과를 줍니다. 또한, 리프의 출력 값을 더 작게 만듭니다.
*   **커버 (Cover)**:
    *   각 리프에 있어야 할 **잔차의 최소 수를 결정하는 '커버(cover)'**라는 개념이 있습니다.
    *   분류에서 커버는 유사성 점수의 분모에서 람다를 뺀 값, 즉 **'이전 예측 확률 * (1 - 이전 예측 확률)'의 합**과 동일합니다.
    *   기본적으로 최소 커버 값은 1이지만, 이 예시에서는 트리가 너무 작아지는 것을 방지하기 위해 최소 커버 값을 0으로 설정했습니다 (이는 `min_child_weight` 매개변수를 0으로 설정하는 것과 같습니다). 회귀와 달리 분류에서 커버는 이전 예측 확률에 따라 달라지므로 더 복잡합니다.

### **65.4 리프의 출력 값 계산 (Calculating Output Values for Leaves)**

*   **분류 리프 출력 공식**: 분류를 위한 리프의 출력 값은 **잔차의 합을 '각 리프 내 잔차의 (이전 예측 확률 * (1 - 이전 예측 확률))의 합 + 람다'로 나눈 값**입니다.
*   **람다의 영향**: 람다가 0이면 정규화가 없어 출력 값이 더 커지지만, 람다가 0보다 크면 출력 값을 0에 가깝게 줄여 이 단일 관측값이 새로운 예측에 추가하는 양을 줄입니다. 즉, 격리된 관측값에 대한 예측 민감도를 줄입니다.

### **65.5 새로운 예측 생성 (Making New Predictions)**

*   **로그 오즈로 변환**: 새로운 예측은 초기 예측에서 시작됩니다. XGBoost는 분류 시 확률을 **로그 오즈(log odds) 값**으로 변환해야 합니다. 기본 초기 예측 0.5는 로그 오즈 0에 해당합니다.
*   **학습률 적용**: 초기 예측의 로그 오즈 값에 **학습률(learning rate)** (XGBoost에서는 **에타(etta)**라고 하며 기본값은 0.3)을 곱한 트리의 출력 값을 더합니다.
*   **확률로 변환**: 이렇게 얻은 새로운 로그 오즈 값을 **로지스틱 함수(logistic function)**에 넣어 다시 확률로 변환합니다.
*   **잔차 감소**: 이 과정을 통해 새로운 예측 확률이 생성되고, 이전보다 **더 작은 잔차**를 얻게 됩니다. 이는 올바른 방향으로 나아가는 작은 단계에 해당합니다.

### **65.6 반복적인 트리 구축 (Iterative Tree Building)**

*   새로운 잔차가 생성되면, 이 새로운 잔차에 맞춰 **두 번째 트리를 구축**합니다. 이 때, 이전 예측 확률이 모든 관측값에 대해 동일하지 않으므로 유사성 점수 계산이 더 복잡해집니다.
*   이러한 트리를 구축하고 새로운 예측을 생성하는 과정을 잔차가 매우 작아지거나 설정된 **최대 트리 수(maximum number of trees)**에 도달할 때까지 **반복**합니다.

XGBoost 분류 트리를 구축할 때는 유사성 점수와 게인을 계산하여 데이터를 분할하고, 게인 값과 사용자가 정의한 감마 매개변수 간의 차이를 계산하여 트리를 가지치기합니다. 리프의 출력 값을 계산하고, 람다(정규화 매개변수)는 유사성 점수를 줄이고 리프의 출력 값을 작게 하여 가지치기를 강화합니다. 또한, 분류 시 리프의 최소 잔차 수는 커버(cover)라는 지표와 관련이 있습니다.

## 66. XGBoost Part 3 (of 4): 수학적 세부사항

### **66.1 XGBoost의 목표: 손실 함수 최소화**
XGBoost는 회귀(regression)와 분류(classification) 모두에 대해 유사도 점수(similarity scores)를 사용해 트리를 구축하고, 리프(leaf)에 대한 출력값(output values)을 계산합니다. 핵심 목표는 다음 방정식을 최소화하는 출력값을 찾는 것입니다:

*   **손실 함수(Loss Function)**: 예측이 얼마나 좋은지 정량화합니다.
    *   **회귀(Regression)**: 일반적으로 `1/2 * (잔차)^2`를 사용합니다. `y_i`는 관측값, `P_i`는 예측값입니다.
    *   **분류(Classification)**: 일반적으로 음의 로그 가능도(negative log-likelihood)를 사용합니다. `y_i`는 0 또는 1, `P_i`는 0과 1 사이의 예측값입니다.
*   **정규화 항(Regularization Term)**: `1/2 * lambda * (출력값)^2` 형태로 구성됩니다. 이는 릿지 회귀와 유사하게 출력값을 0으로 수축시켜 과적합을 방지하고 가지치기(pruning)를 장려합니다. `lambda` 값이 커질수록 최적 출력값은 0에 가까워집니다. (참고: 원래 XGBoost 논문에는 가지치기를 장려하는 `gamma * T` 항이 있지만, 이 영상에서는 출력값 및 유사도 점수 도출에 영향을 미치지 않아 생략되었습니다).

### **66.2 최적 출력값(Optimal Output Value) 도출 과정**

*   **초기 예측 및 잔차**: XGBoost는 일반적으로 0.5의 초기 예측으로 시작하며, 관측값과 예측값의 차이인 잔차(residuals)를 사용합니다.
*   **최소화 방법**:
    1.  **간단한 예시**: `lambda`를 0으로 설정하고 여러 출력값을 대입하여 손실 함수 + 정규화 항이 최소가 되는 지점을 그래프로 찾아볼 수 있습니다. 이 지점은 포물선의 바닥으로, 미분값이 0이 되는 곳입니다.
    2.  **2차 테일러 근사(Second-Order Taylor Approximation)**: 실제 XGBoost에서는 손실 함수를 2차 테일러 근사로 단순화하여 수학적 계산을 용이하게 합니다. 이는 그래디언트 부스트가 분류 문제에서만 사용했던 방법과 달리, XGBoost는 회귀와 분류 모두에 이 방법을 적용합니다.
        *   이 근사는 이전 예측에 대한 손실 함수, 손실 함수의 1차 미분(그래디언트 G로 표현), 2차 미분(헤시안 H로 표현)으로 구성됩니다.
    3.  **최적 출력값 공식 도출**:
        *   테일러 근사된 손실 함수와 정규화 항을 결합한 식에서 출력값과 관련 없는 상수항을 제거합니다.
        *   남은 식을 출력값에 대해 미분하고, 그 미분값을 0으로 설정하여 최적 출력값을 구합니다.

*   **최적 출력값 공식 (일반식)**:
    `-(시그마 G) / (시그마 H + lambda)`

*   **회귀(Regression)를 위한 특정 공식**:
    *   **G (1차 미분)**: `-1 * (y_i - P_i)` (음의 잔차). 따라서 분자는 잔차의 합이 됩니다.
    *   **H (2차 미분)**: `1`. 따라서 분모는 `(잔차의 개수 + lambda)`가 됩니다.
    *   **회귀 출력값**: `(시그마 잔차) / (잔차의 개수 + lambda)`.

*   **분류(Classification)를 위한 특정 공식**:
    *   **G (1차 미분)**: 음의 잔차. 따라서 분자는 잔차의 합이 됩니다.
    *   **H (2차 미분)**: `P_i * (1 - P_i)` (이전 예측 확률).
    *   **분류 출력값**: `(시그마 잔차) / (시그마 (P_i * (1 - P_i)) + lambda)`.

### **66.3 유사도 점수(Similarity Score) 도출 과정**

*   **목표**: 트리를 성장시키기 위해 유사도 점수를 계산합니다.
*   **도출 방법**: 최적 출력값을 찾기 위해 사용했던 **단순화된 식** (2차 테일러 근사 및 상수 제거 후 남은 식)에 최적 출력값을 다시 대입합니다.
    *   이 식을 `-1`을 곱하여 포물선을 뒤집으면, 최적 출력값은 포물선의 가장 높은 지점의 X축 좌표가 되고, 이 때의 Y축 좌표가 유사도 점수가 됩니다.
    *   XGBoost 구현체에서는 계산량 감소를 위해 이 점수에 `1/2`이 생략된 형태로 사용됩니다.

*   **유사도 점수 공식 (일반식)**:
    `(시그마 G)^2 / (시그마 H + lambda)`

*   **회귀(Regression)를 위한 특정 공식**:
    *   **분자**: 잔차의 합의 제곱.
    *   **분모**: `(잔차의 개수 + lambda)`.

*   **분류(Classification)를 위한 특정 공식**:
    *   **분자**: 잔차의 합의 제곱.
    *   **분모**: `(시그마 (P_i * (1 - P_i)) + lambda)`.

### **66.4 커버(Cover)**
*   **정의**: 커버는 리프에 있는 최소 잔차 수와 관련이 있으며, 유사도 점수 분모에서 `lambda`를 뺀 값, 즉 헤시안(H_i)의 합과 같습니다.
*   **회귀**: 헤시안은 `1`이므로, 커버는 리프의 잔차 개수와 같습니다.
*   **분류**: 헤시안은 `P * (1 - P)`이므로, 커버는 `P_i * (1 - P_i)`의 합과 같습니다.

XGBoost는 손실 함수와 정규화 항으로 구성된 식을 최소화하여 트리를 구축합니다. 이 과정에서 2차 테일러 근사를 사용하여 최적의 출력값을 계산하고, 이를 다시 단순화된 식에 대입하여 유사도 점수를 도출합니다. 이러한 수학적 세부사항은 XGBoost가 회귀 및 분류 문제에서 강력한 성능을 발휘하는 핵심적인 이유입니다.

## 67. XGBoost Part 4 (of 4): 최적화
**XGBoost의 최적화 기법**은 대규모 훈련 데이터셋을 효율적으로 처리하기 위한 여러 부분으로 구성됩니다.

### **67.1 근사 탐욕 알고리즘 (Approximate Greedy Algorithm)**

*   **기존 탐욕 알고리즘의 문제점**: XGBoost는 기본적으로 트리를 구축할 때 **탐욕 알고리즘(greedy algorithm)**을 사용합니다. 이는 가능한 모든 임계값(threshold)에 대해 유사도 점수(similarity scores)와 이득(gain)을 계산하고, 가장 큰 이득을 제공하는 임계값을 선택하는 방식입니다. 이 결정은 이후 리프가 어떻게 분할될지에 대해 미리 걱정하지 않고 즉각적으로 이루어지며, 이를 통해 트리를 빠르게 구축할 수 있습니다.
*   **대규모 데이터셋의 한계**: 그러나 측정값이 많을 경우, 모든 가능한 임계값을 확인하는 것은 매우 느려집니다. 예를 들어, 여러 변수를 사용하는 경우 모든 임계값을 확인하는 것은 엄청난 시간이 소요될 수 있습니다.
*   **근사 탐욕 알고리즘 도입**: 이 문제를 해결하기 위해 **근사 탐욕 알고리즘**이 사용됩니다.
    *   **작동 방식**: 모든 임계값을 테스트하는 대신, 데이터를 **분위수(quantiles)**로 나누고 이 분위수를 후보 임계값으로만 사용합니다.
    *   **정확도와 속도 간의 균형**: 분위수(예: 1개, 2개, 5개)의 수가 늘어날수록 예측의 정확도는 높아지지만, 테스트해야 할 임계값이 많아져 트리를 구축하는 데 시간이 더 오래 걸립니다.
    *   **기본 설정**: XGBoost의 근사 탐욕 알고리즘은 기본적으로 약 **33개의 분위수**를 사용합니다.

### **67.2 가중치 분위수 스케치 (Weighted Quantile Sketch) 및 병렬 학습 (Parallel Learning)**

*   **대규모 데이터 처리 문제**: 컴퓨터 메모리에 모든 데이터를 한 번에 담을 수 없을 정도로 데이터가 방대할 때, 숫자 정렬이나 분위수 찾기와 같은 간단해 보이는 작업도 매우 느려질 수 있습니다.
*   **스케치 알고리즘의 역할**: 이 문제를 우회하기 위해 **스케치 알고리즘(sketch algorithms)**이라는 종류의 알고리즘은 근사적인 해답을 빠르게 생성할 수 있습니다.
*   **병렬 학습**: XGBoost는 데이터를 작은 조각으로 나누어 여러 컴퓨터에서 동시에 작업할 수 있도록 **병렬 학습**을 활용합니다.
*   **가중치 분위수 스케치**:
    *   **작동 방식**: 분위수 스케치 알고리즘은 각 컴퓨터의 값을 결합하여 **근사 히스토그램(approximate histogram)**을 만들고, 이를 사용하여 **근사 분위수(approximate quantiles)**를 계산합니다. XGBoost는 이보다 발전된 **가중치 분위수 스케치(weighted quantile sketch)**를 사용합니다.
    *   **가중치 분위수**: 일반적인 분위수는 각 분위수에 동일한 수의 관측치가 들어가도록 설정되지만, **가중치 분위수**에서는 각 관측치에 해당 **가중치(weight)**가 있으며, **각 분위수 내 가중치의 합**이 동일하게 설정됩니다.
    *   **가중치 계산**: 이 가중치는 시리즈 2부와 3부에서 논의된 **커버(cover) 측정항목**에서 파생됩니다. 구체적으로, 각 관측치에 대한 가중치는 손실 함수의 **2차 미분(second derivative of the loss function)**, 즉 **헤시안(Hessian)**입니다.
        *   **회귀(Regression)**: 헤시안이 `1`이므로, 모든 가중치는 `1`과 같습니다. 따라서 가중치 분위수는 일반 분위수와 동일하게 각 관측치 수가 동일합니다.
        *   **분류(Classification)**: 가중치는 `이전 예측 확률 * (1 - 이전 예측 확률)`입니다.
    *   **가중치의 영향 (분류)**:
        *   이전 예측 확률이 `0` 또는 `1`에 매우 가까울 때(즉, 분류에 대한 확신이 높을 때), 가중치는 상대적으로 작습니다.
        *   이전 예측 확률이 `0.5`에 매우 가까울 때(즉, 분류에 대한 확신이 낮을 때), 가중치는 상대적으로 큽니다.
        *   XGBoost는 가중치의 합이 유사하도록 분위수를 나눔으로써, **낮은 확신도 예측(low confidence predictions)을 가진 관측치들을 더 적은 관측치를 포함하는 분위수(더 작은 분위수)에 배치**합니다. 이는 필요한 경우 더 작은 분위수를 얻는 이점이 있습니다.
*   **적용 조건**: 이러한 근사 탐욕 알고리즘, 병렬 학습, 가중치 분위수 스케치는 **훈련 데이터셋이 매우 클 때만** 사용됩니다. 데이터셋이 작을 때는 일반적인 탐욕 알고리즘이 사용됩니다.

### **67.3 희소성 인식 분할 찾기 (Sparsity-Aware Split Finding)**

*   **결측값 처리**: XGBoost는 결측값(missing values)이 있는 경우에도 트리를 구축하고 새로운 관측치를 처리하는 방법을 제공합니다.
*   **과정**:
    1.  **데이터 분할**: 결측값이 있는 관측치와 없는 관측치로 데이터를 두 개의 테이블로 나눕니다.
    2.  **임계값 테스트**: 결측값이 없는 테이블을 사용하여 낮은 값에서 높은 값으로 정렬된 잔차(residuals)를 기준으로 후보 임계값을 테스트합니다.
    3.  **이득 계산**: 각 후보 임계값에 대해 두 가지 이득 값(`gain_left`, `gain_right`)을 계산합니다.
        *   `gain_left`: 결측값이 있는 모든 잔차를 왼쪽 리프에 배치하여 계산.
        *   `gain_right`: 결측값이 있는 모든 잔차를 오른쪽 리프에 배치하여 계산.
    4.  **최대 이득 선택**: 모든 임계값과 `gain_left`, `gain_right` 조합 중에서 **가장 큰 전체 이득 값**을 주는 임계값을 선택합니다.
    5.  **기본 경로 설정**: 선택된 임계값에 대해 결측값이 있는 관측치가 이동하는 경로(예: 왼쪽 리프)가 향후 모든 결측값 관측치를 위한 **기본 경로(default path)**가 됩니다.

### **67.4 캐시 인식 접근 (Cache-Aware Access)**

*   **컴퓨터 메모리 계층**: 컴퓨터는 CPU, 캐시 메모리(CPU가 가장 빠르게 접근), 메인 메모리(캐시보다 크지만 느림), 하드 드라이브(가장 크지만 가장 느림)로 구성됩니다.
*   **최적화 목표**: 프로그램 실행 속도를 높이려면 **캐시 메모리의 활용을 극대화**해야 합니다.
*   **XGBoost의 방식**: XGBoost는 **그래디언트(gradients)**와 **헤시안(Hessians)**을 캐시에 배치하여 유사도 점수와 출력값을 빠르게 계산할 수 있도록 합니다.

### **67.5 아웃-오브-코어 연산을 위한 블록 (Blocks for Out-of-Core Computation)**

*   **문제**: 데이터셋이 너무 커서 캐시와 메인 메모리에 모두 담을 수 없을 때, 일부 데이터는 하드 드라이브에 저장되어야 합니다. 하드 드라이브 읽기/쓰기 작업은 매우 느립니다.
*   **XGBoost의 방식**:
    *   **데이터 압축**: XGBoost는 데이터를 **압축**하여 하드 드라이브 접근을 최소화합니다. CPU가 데이터를 압축 해제하는 데 시간이 걸리지만, 이는 하드 드라이브가 데이터를 읽는 시간보다 빠릅니다.
    *   **샤딩(Sharding)**: 여러 하드 드라이브를 사용할 수 있을 때, XGBoost는 **샤딩**이라는 데이터베이스 기술을 사용하여 디스크 접근 속도를 높입니다. 데이터셋을 분할하여 각 드라이브가 고유한 레코드 세트를 가지게 함으로써, CPU가 데이터를 필요로 할 때 두 드라이브가 동시에 데이터를 읽을 수 있습니다.

### **67.6 추가적인 속도 향상 기법**
*   **데이터의 랜덤 서브셋 사용**: 각 트리를 구축할 때 **훈련 데이터의 무작위 서브셋**만을 사용하여 속도를 높일 수 있습니다.
*   **특징의 랜덤 서브셋 사용**: 분할 방법을 결정할 때 **특징(features)의 무작위 서브셋**만 고려하여 트리를 빠르게 구축할 수 있습니다.

XGBoost는 통계적 기법을 넘어선 다양한 최적화(하드웨어 고려 포함)를 통해 빠른 속도를 자랑합니다. 이러한 최적화는 대규모 데이터셋을 효율적으로 처리하고, 결측값을 지능적으로 다루며, 컴퓨터 하드웨어의 이점을 최대한 활용하여 강력한 성능을 제공하는 핵심적인 이유입니다.

## 68. 파이썬으로 XGBoost 구현하기

**XGBoost**라는 매우 유용한 머신러닝 방법론을 사용하여 **고객 이탈(customer churn)을 예측**하는 과정을 파이썬 주피터 노트북 환경에서 처음부터 끝까지 다룹니다. XGBoost는 정확도를 유지하면서도 모델을 이해하고 해석하기 쉽다는 장점이 있습니다.

**이 장을 통해 배우게 될 주요 내용은 다음과 같습니다:**
*   파일에서 데이터 가져오기
*   누락된 데이터 처리하기 (XGBoost만의 독특한 방식 포함)
*   XGBoost용 데이터 형식 지정 (원-핫 인코딩 사용 포함)
*   예비 XGBoost 모델 구축
*   교차 검증 및 그리드 서치를 통한 매개변수 최적화
*   최적화된 XGBoost 모델 시각화, 검증 및 해석

### **68.1 데이터 가져오기 및 초기 전처리**

1.  **필요 모듈 로드:** Pandas, NumPy, Scikit-learn, XGBoost 등 파이썬 모듈을 로드합니다.
2.  **데이터 불러오기:** IBM Base Samples 웹사이트에서 **Telco Churn 데이터셋**을 Pandas DataFrame으로 로드합니다. 이 데이터셋은 고객의 연속형 및 범주형 데이터를 사용하여 고객이 통신 서비스를 중단할지 여부를 예측하는 데 사용됩니다.
3.  **불필요한 열 제거:**
    *   `churn_reason`, `cltv`, `churn_score` 등의 **사후(exit interview) 데이터**는 이탈 후 수집되므로 예측에 사용하면 완벽한 예측 능력을 주어 모델의 일반화를 방해하므로 제거합니다.
    *   모든 값이 동일하여 예측에 도움이 되지 않는 `count`, `country`, `state`와 같은 열을 제거합니다.
    *   개인 식별 정보이며 예측에 유용하지 않은 `customer_id`를 제거합니다.
    *   `lat_long`처럼 이미 분리된 `latitude`와 `longitude` 열이 있으므로 중복되는 열을 제거합니다.
4.  **공백 문자 처리:**
    *   시티 이름(`city` 컬럼)과 열 이름(`column names`)에 포함된 공백을 나중에 트리를 시각화하기 위해 밑줄(`_`)로 바꿉니다. XGBoost 자체는 공백을 신경 쓰지 않지만, 트리를 예쁘게 그리기 위해서는 필요합니다.

### **68.2 누락된 데이터 처리 (XGBoost 방식)**

1.  **누락된 데이터 식별:** 누락된 데이터는 수집되지 않은 특성을 나타내는 빈 공간 또는 `NA`와 같은 대리 값입니다.
2.  **XGBoost의 독특한 처리:** XGBoost는 **누락된 데이터에 대한 기본 동작이 있으며, 이를 0으로 설정하면 잘 처리합니다**. 심지어 데이터셋에 원래 0이 있는 경우에도 XGBoost는 성능에 영향을 주지 않고 잘 작동한다고 개발자가 언급했습니다.
3.  **`TotalCharges` 열 처리 예시:**
    *   `TotalCharges` 열의 데이터 유형이 `object`로 나타나는데, 이는 숫자와 문자열이 혼합되어 있음을 의미합니다.
    *   숫자형으로 변환을 시도하면 **빈 공간** 때문에 오류가 발생합니다.
    *   빈 공간을 가진 행의 수를 확인한 결과 단 11개에 불과했습니다. 이들은 `tenure_months`가 0인, 즉 막 가입했지만 아직 요금이 청구되지 않은 고객들입니다.
    *   이 11개 행의 `TotalCharges` 값을 **0으로 설정**합니다.
    *   이후 `TotalCharges` 열의 데이터 유형을 `float64`로 변환하여 XGBoost가 처리할 수 있도록 합니다.
4.  **남아있는 모든 공백을 밑줄로 대체:** 시각화를 위해 모든 열의 남아있는 공백을 밑줄로 대체합니다.

### **68.3 XGBoost를 위한 데이터 형식 지정 (원-핫 인코딩)**

1.  **특성(X) 및 타겟(y) 분리:**
    *   모델 예측에 사용할 열들을 대문자 **X** (`X_encoded`)로, 예측하고자 하는 `churn_value` 열을 소문자 **y**로 분리합니다. `churn_value`는 이탈 고객은 1, 비이탈 고객은 0입니다.
2.  **데이터 유형 확인:** `latitude`, `longitude`, `monthly_charges`, `total_charges`와 같은 **연속형 데이터**는 `float64` 유형으로 잘 되어있습니다. 하지만 `object` 유형의 열들은 **범주형 데이터**이므로 추가 처리가 필요합니다.
3.  **원-핫 인코딩(One-Hot Encoding):**
    *   **필요성:** XGBoost를 포함한 많은 머신러닝 알고리즘은 연속형 데이터를 잘 지원하지만, `phone_service`와 같은 범주형 데이터는 직접 지원하지 않습니다. 범주형 데이터를 연속형처럼 숫자로 변환하면 잘못된 유사성을 부여할 수 있습니다 (예: 결제 방법 1, 2, 3, 4를 사용하면 4가 3에 더 가깝다고 가정하게 됨).
    *   **방법:** 원-핫 인코딩은 각 범주를 별개의 독립적인 범주로 처리합니다. `pd.get_dummies` 함수를 사용하여 각 범주 값을 새로운 이진(0 또는 1) 열로 변환합니다. 예를 들어, `payment_method` 열은 `payment_method_bank_transfer`, `payment_method_credit_card` 등의 여러 새 열로 분할되어 해당 결제 방법을 사용하면 1, 아니면 0이 됩니다.
    *   **누락된 데이터와의 연관성 (Mini StatQuest):** 만약 `favorite_color` 열에 파랑, 초록, 누락된 데이터(0으로 인코딩)가 있다면, 원-핫 인코딩 후 누락된 데이터는 `blue` 열과 `green` 열 모두에서 0이 됩니다. XGBoost는 누락된 데이터가 있는 사람들을 파랑을 좋아하는 사람들과 묶을지, 초록을 좋아하는 사람들과 묶을지 비교하여 최적의 분할을 찾습니다.
    *   **메모리 효율성:** XGBoost는 **희소 행렬(sparse matrices)**을 사용하므로, 0값을 모두 저장하는 대신 1값만 추적하여 메모리를 효율적으로 사용합니다.

### **68.4 XGBoost 모델 구축 및 최적화**

1.  **학습 및 테스트 데이터셋 분할:**
    *   데이터가 **불균형**합니다 (이탈 고객 27%). 따라서 **계층화(stratification)**를 사용하여 학습 및 테스트 데이터셋 모두에서 이탈 고객 비율이 동일하게 유지되도록 `train_test_split` 함수를 사용합니다.
2.  **예비 XGBoost 모델 구축:**
    *   `xgb.XGBClassifier`를 사용하여 모델을 생성합니다. 목표(`objective`)는 분류를 위한 `binary:logistic`으로 설정합니다.
    *   `missing` 매개변수는 `none`으로 설정되며, 이는 기본값으로 0이 누락된 데이터를 나타내는 데 사용됨을 의미합니다.
    *   `fit` 함수를 사용하여 모델을 학습시킵니다. 이때 **조기 종료(early stopping)** 기능을 사용하여 과적합을 방지하고 최적의 트리 개수를 찾습니다. 모델은 AUC(Area Under the Curve)를 평가 기준으로 사용하며, 테스트 데이터셋을 사용하여 조기 종료 여부를 결정합니다.
3.  **예비 모델 평가 (혼동 행렬):**
    *   `plot_confusion_matrix` 함수로 혼동 행렬을 시각화합니다.
    *   예비 모델은 비이탈 고객(상위 행)을 91% 정확도로 잘 분류했지만, **이탈 고객(하위 행)은 51%만 정확하게 분류**했습니다. 이는 데이터 불균형 때문이며, 이탈 고객을 놓치는 것은 회사에 큰 손실을 초래할 수 있습니다.
4.  **매개변수 최적화 (교차 검증 및 그리드 서치):**
    *   **하이퍼파라미터:** `max_depth` (트리 깊이), `learning_rate` (eta), `gamma` (가지치기), `lambda` (릿지 회귀 정규화), `scale_pos_weight` (불균형 데이터 처리) 등을 최적화해야 합니다.
    *   **불균형 데이터 처리:** 불균형 데이터셋에서는 `scale_pos_weight` 매개변수를 사용하여 소수 클래스(이탈 고객)에 대한 오분류에 더 큰 패널티를 부여함으로써 예측 성능을 향상시킬 수 있습니다.
    *   **그리드 서치 (GridSearchCV):** `GridSearchCV`를 사용하여 다양한 하이퍼파라미터 조합을 탐색합니다. 이 튜토리얼에서는 시간이 오래 걸리므로 주석 처리되었지만, 두 단계에 걸쳐 최적화를 진행합니다.
        *   각 트리에 대해 데이터의 무작위 90%와 열의 무작위 50%를 사용하여 과적합을 방지하고 속도를 높입니다.
        *   AUC를 평가 지표로 사용합니다.
    *   **최적화된 매개변수:** 예시 실행 결과, `gamma=0.25`, `learning_rate=0.1`, `max_depth=4`, `regularization_parameter=10`, `scale_pos_weight=3`과 같은 최적의 값을 얻었습니다.
5.  **최적화된 XGBoost 모델 구축 및 평가:**
    *   최적화된 매개변수를 사용하여 최종 XGBoost 모델을 다시 학습시킵니다.
    *   혼동 행렬을 다시 그려보면, **이탈 고객 분류 정확도가 51%에서 84%로 크게 향상**되었습니다. 이는 비이탈 고객 예측의 정확도가 다소 떨어지더라도, 이탈 고객을 미리 파악하여 쿠폰 등을 제공하여 이탈을 방지하는 것이 회사 입장에서 더 큰 이득이 될 수 있기 때문에 바람직한 결과입니다.

### **68.5 모델 해석 (트리 그리기)**

1.  **단일 트리 시각화:** 모델에서 첫 번째 트리를 그려서 시각적으로 해석합니다. 이는 매개변수 최적화를 위한 초기 값 설정에 대한 통찰력을 얻거나, `gain`, `weight`, `cover`와 같은 값들을 이해하는 데 도움이 됩니다.
2.  **트리 해석:** 트리의 각 노드(녹색 상자)에는 열 이름과 분할 임계값이 있습니다.
    *   예를 들어, `contract_month_to_month`가 1 미만인 사람들은 왼쪽으로, 그렇지 않은 사람들은 오른쪽으로 이동합니다.
    *   누락된 데이터는 `yes, missing`으로 표시되어 특정 분기점으로 함께 이동합니다.
    *   XGBoost에서 트리의 **잎(leaves)은 최종 분류를 직접 제공하는 것이 아니라, 최종 예측 확률에 기여하는 작은 증분 확률 조각을 제공**하며, 이들이 모든 트리에 걸쳐 합산되어 최종 확률을 도출합니다.

이 튜토리얼은 파일에서 데이터 로드, 누락된 데이터 처리, 원-핫 인코딩을 사용한 데이터 형식 지정, 예비 XGBoost 분류 모델 구축, 교차 검증 및 그리드 서치를 통한 매개변수 최적화, 그리고 최종 최적화된 XGBoost 모델의 구축, 시각화 및 평가에 이르는 전 과정을 포괄적으로 다룹니다. 

## 69. 코사인 유사도(Cosine Similarity)
텍스트나 다른 데이터 간의 **유사성 또는 차이점을 정량적으로 측정**하는 데 사용되는 매우 유용한 지표인 **코사인 유사도**를 명확하게 설명합니다. 특히 대량의 데이터를 다룰 때 어떤 것들이 서로 유사하고 다른지 파악하기 어려울 때 컴퓨터가 이 작업을 수행하도록 하는 데 코사인 유사도가 유용하게 활용됩니다.

### **69.1 코사인 유사도란 무엇인가요?**
코사인 유사도는 두 사물(예: 두 문장)이 얼마나 유사하거나 다른지 알려줄 수 있는 비교적 계산하기 쉬운 측정 항목입니다.

### **69.2 작동 방식 (간단한 예시: "hello world" vs "hello")**

*   **1단계: 단어 빈도수 표 만들기**
    *   먼저 비교하고자 하는 구문들에 나타나는 단어들을 기준으로 표를 만듭니다.
    *   예를 들어, "hello world"에는 'hello'와 'world'가 각각 한 번씩 나타나고, "hello"에는 'hello'가 한 번 나타나고 'world'는 나타나지 않습니다.

*   **2단계: 2차원 그래프에 점 찍기**
    *   이 단어 빈도수를 사용하여 그래프에 점을 찍습니다.
    *   예시에서는 'hello'의 빈도수를 x축에, 'world'의 빈도수를 y축에 놓습니다.
    *   "hello world"는 (1, 1)에 점이 찍히고, "hello"는 (1, 0)에 점이 찍힙니다.

*   **3단계: 원점(0,0)에서 각 점까지 선 긋기**
    *   그래프의 원점에서 각 점까지 선을 그립니다. 이 선들은 각 구문을 벡터로 표현한 것이라고 볼 수 있습니다.

*   **4단계: 두 선 사이의 각도 계산 및 코사인 값 구하기**
    *   **핵심 개념**: 코사인 유사도는 **두 선 사이의 각도에 의해 전적으로 결정되며, 선의 길이에 의해 결정되지 않습니다**.
    *   "hello world"와 "hello"의 예시에서는 두 선 사이의 각도가 45도이고, 코사인 45도는 **0.71**입니다. 따라서 이 두 구문의 코사인 유사도는 0.71입니다.
    *   **길이의 영향 없음**: 만약 두 번째 구문이 "hello hello hello"였다면, 그래프에서 'hello' 빈도수가 3이 되어 x축 상에서 더 멀리 떨어진 점이 찍히겠지만, 두 선 사이의 각도는 여전히 45도이므로 코사인 유사도는 동일합니다.

### **69.3 코사인 유사도 값의 의미**

*   **정확히 동일한 경우:** 두 구문이 "hello world"처럼 완전히 동일하면, 두 점이 겹쳐지고 두 선 사이의 각도는 0도가 됩니다. 코사인 0도는 **1**이므로, 코사인 유사도가 1이면 두 구문은 정확히 같다는 의미입니다.
*   **아무것도 공통점이 없는 경우:** 두 구문이 "hello"와 "world"처럼 공통된 단어가 전혀 없으면, 두 선 사이의 각도는 90도가 됩니다. 코사인 90도는 **0**이므로, 코사인 유사도가 0이면 두 구문은 아무것도 공통점이 없다는 의미입니다.
*   **일부 중복이 있는 경우:** 두 구문이 완전히 같지는 않지만 일부 겹치는 부분이 있다면 (예: 45도), 코사인 유사도는 **0과 1 사이**의 값을 갖습니다.

### **69.4 컴퓨터를 이용한 계산: 공식**

수동으로 단어를 세고, 점을 찍고, 각도를 계산하는 것은 번거로운 작업입니다. 다행히 단어 빈도수 표에서 코사인 유사도를 직접 계산할 수 있는 비교적 간단한 공식이 있습니다.

*   **공식의 구성 요소:**
    *   `Σ`: 여러 값을 더하라는 의미입니다.
    *   `i`: 인덱스로, 작업 중인 단어를 추적하는 데 사용됩니다.
    *   `N`: 구문에 있는 서로 다른 단어의 총 개수입니다.
    *   `A`와 `B`: 두 개의 구문을 나타냅니다 (예: 'hello world'는 A, 'hello'는 B).
    *   이 공식을 통해 계산하면 위 예시와 동일하게 0.71이라는 값을 얻을 수 있습니다.

### **69.5 고차원 데이터 처리**

*   두 개의 단어만 있는 경우에는 2차원 그래프에 점을 쉽게 그릴 수 있습니다 (예: 'hello'는 X축, 'world'는 Y축).
*   하지만 만약 구문에 세 개 이상의 다른 단어가 있다면, 점을 찍기 위해 세 개 이상의 차원이 필요합니다. 예를 들어, "I love Troll 2"와 "I love Jim Kata"에는 5개의 다른 단어가 있으므로 5차원 그래프가 필요할 것입니다.
*   이러한 고차원 그래프를 그리는 방법을 알 필요는 없습니다. **코사인 유사도 공식을 사용하면 차원 수에 관계없이 단순히 숫자를 공식에 대입하여 유사도 값을 얻을 수 있습니다.**.

코사인 유사도는 데이터 간의 유사성 또는 차이점을 빠르고 쉽게 계산할 수 있는 강력한 측정 지표입니다.이는 특히 텍스트 데이터를 분석하거나 추천 시스템을 구축하는 등 AI 분야에서 폭넓게 활용됩니다.

## 70. Support Vector Machines Part 1/3: 주요 개념

서포트 벡터 머신(SVM)은 분류(classification) 문제에 사용되는 강력한 머신러닝 알고리즘입니다. 이 개념을 이해하기 위해서는 **편향-분산 트레이드오프(bias-variance tradeoff)**와 **교차 검증(cross-validation)**에 대한 사전 지식이 있으면 도움이 됩니다.

### **70.1 초기 분류 및 한계점 (Basic Classification and Limitations)**

*   **임계값(Threshold) 기반 분류**: 가장 기본적인 분류 방법은 데이터의 특정 값을 기준으로 임계값을 설정하여 분류하는 것입니다. 예를 들어, 쥐의 몸무게를 측정하여 비만 여부를 분류할 수 있습니다.
*   **단순 임계값의 문제**: 하지만 이 방법은 새로운 데이터가 임계값 근처에 있을 때 잘못 분류할 가능성이 높으며, 데이터의 분포를 잘 반영하지 못할 수 있습니다.

### **70.2 최대 마진 분류기 (Maximal Margin Classifier)**

*   **개념**: 훈련 데이터에서 각 그룹의 가장자리에 있는 관측치(observations)에 초점을 맞춰, 이들 사이의 중간 지점을 **임계값(threshold)**으로 사용합니다.
*   **마진(Margin)**: 관측치와 임계값 사이의 가장 짧은 거리를 **마진**이라고 합니다. 최대 마진 분류기는 가능한 가장 큰 마진을 제공하는 임계값을 사용합니다.
*   **한계**: 최대 마진 분류기는 **이상치(outliers)**에 매우 민감합니다. 훈련 데이터에 이상치가 있으면 분류기가 이상치에 지나치게 가깝게 설정되어 다른 대부분의 데이터와 멀어지게 되며, 이는 새로운 데이터를 잘못 분류할 가능성을 높입니다.

### **70.3 소프트 마진 분류기 (Soft Margin Classifier) 또는 서포트 벡터 분류기 (Support Vector Classifier)**

*   **개념**: 이상치에 대한 민감성을 줄이기 위해, **오분류(misclassifications)**를 일부 허용하는 임계값을 사용합니다. 이는 머신러닝의 중요한 개념인 **편향-분산 트레이드오프**의 한 예시입니다.
    *   오분류를 허용하지 않는 경우(낮은 편향)는 훈련 데이터에 매우 민감하여 새로운 데이터에서 성능이 좋지 않을 수 있습니다 (높은 분산).
    *   오분류를 허용하는 경우(높은 편향)는 훈련 데이터에 덜 민감하여 새로운 데이터에서 더 나은 성능을 보일 수 있습니다 (낮은 분산).
*   **소프트 마진(Soft Margin)**: 오분류를 허용할 때 관측치와 임계값 사이의 거리를 **소프트 마진**이라고 합니다.
*   **최적의 소프트 마진 찾기**: 어떤 소프트 마진이 더 좋은지는 **교차 검증(cross-validation)**을 사용하여 결정합니다. 이를 통해 허용할 오분류의 수와 소프트 마진 내에 포함될 관측치의 수를 결정하여 최상의 분류 성능을 얻습니다.
*   **서포트 벡터 분류기 (Support Vector Classifier)**: 소프트 마진을 사용하여 임계값의 위치를 결정하는 방법을 **소프트 마진 분류기** 또는 **서포트 벡터 분류기**라고 부릅니다.
*   **서포트 벡터(Support Vectors)**: 소프트 마진의 가장자리에 있거나 마진 내부에 있는 관측치들을 **서포트 벡터**라고 합니다.

### **70.4 데이터 차원과 분류기 형태 (Data Dimensionality and Classifier Shape)**

데이터의 차원에 따라 서포트 벡터 분류기의 형태는 달라집니다:

*   **1차원 데이터**: 서포트 벡터 분류기는 1차원 수직선 상의 한 **점(single point)**입니다.
*   **2차원 데이터**: 서포트 벡터 분류기는 2차원 공간의 **선(line)**입니다. 이 경우 소프트 마진은 두 개의 평행선으로 표현됩니다.
*   **3차원 데이터**: 서포트 벡터 분류기는 3차원 공간의 **평면(plane)**입니다.
*   **4차원 이상 데이터**: 서포트 벡터 분류기는 **초평면(hyperplane)**이라고 불립니다. 수학적으로는 모든 '평평한 어핀 부분공간(flat affine subspaces)'을 초평면이라고 하지만, 일반적으로는 시각화하기 어려운 4차원 이상의 경우에 이 용어를 사용합니다.

### **70.5 서포트 벡터 머신 (Support Vector Machines, SVMs)의 핵심 아이디어**

서포트 벡터 분류기는 이상치와 겹치는 분류를 처리할 수 있지만, 데이터가 크게 겹치거나 선형적으로 분리하기 어려운 경우 (예: 약물 용량이 너무 적거나 많으면 효과가 없고, 적절한 용량에서만 효과가 있는 경우) 잘 작동하지 않을 수 있습니다. SVM은 이러한 문제를 해결하기 위해 다음과 같은 핵심 아이디어를 사용합니다:

1.  **낮은 차원의 데이터로 시작**: 데이터는 원래 낮은 차원에 존재합니다 (예: 1차원 약물 용량).
2.  **데이터를 더 높은 차원으로 이동**: 데이터를 새로운 차원으로 변환하여 더 높은 차원으로 이동시킵니다 (예: 1차원 용량을 제곱하여 2차원 좌표 생성).
3.  **고차원에서 서포트 벡터 분류기 찾기**: 고차원에서 두 그룹을 효과적으로 분리하는 서포트 벡터 분류기(선, 평면, 초평면)를 찾습니다. 이렇게 하면 원래 낮은 차원에서는 선형적으로 분리할 수 없었던 데이터도 고차원에서는 선형적으로 분리할 수 있게 됩니다.

### **70.6 커널 함수 (Kernel Functions)**

*   **데이터 변환 방법**: 데이터를 어떻게 높은 차원으로 변환할지 결정하는 체계적인 방법으로 **커널 함수**가 사용됩니다. 커널 함수는 실제로 데이터를 변환하지 않고도 고차원에서 각 관측치 쌍 간의 관계를 계산합니다.
*   **다항식 커널 (Polynomial Kernel)**:
    *   매개변수 D(차수)를 통해 차원을 체계적으로 증가시킵니다.
    *   D=1일 때는 1차원 관계, D=2일 때는 용량 제곱 기반의 2차원 관계, D=3일 때는 용량 세제곱 기반의 3차원 관계를 계산하여 서포트 벡터 분류기를 찾습니다.
    *   교차 검증을 통해 최적의 D 값을 찾을 수 있습니다.
*   **방사형 커널 (Radial Kernel) / 방사형 기저 함수 커널 (Radial Basis Function Kernel)**:
    *   무한한 차원에서 서포트 벡터 분류기를 찾습니다.
    *   새로운 관측치에 적용될 때, 가장 가까운 관측치(nearest neighbors)들이 분류에 큰 영향을 미치고, 멀리 떨어진 관측치들은 영향이 적은 **가중 최근접 이웃(weighted nearest neighbor)** 모델처럼 작동합니다.
*   **커널 트릭 (Kernel Trick)**:
    *   커널 함수는 데이터를 실제로 고차원으로 변환하지 않고, 마치 고차원에 있는 것처럼 **각 점 사이의 관계만 계산**합니다.
    *   이러한 '커널 트릭' 덕분에 데이터를 낮은 차원에서 높은 차원으로 변환하는 복잡한 수학적 계산을 피할 수 있어 **컴퓨팅 비용이 절감**됩니다. 또한, 방사형 커널이 사용하는 무한 차원에서의 관계 계산을 가능하게 합니다.

서포트 벡터 머신은 데이터를 상대적으로 높은 차원의 공간으로 이동시킨 다음, 그 공간에서 관측치를 효과적으로 분류할 수 있는 고차원 서포트 벡터 분류기(초평면)를 찾아, 선형적으로 분리할 수 없는 데이터를 분류하는 강력한 방법입니다.

## 71. Support Vector Machines Part 2/3: 다항식 커널

다항식 커널은 SVM이 **선형적으로 분리하기 어려운 데이터**를 분류할 수 있도록 돕는 핵심적인 방법 중 하나입니다.

### **71.1 다항식 커널의 필요성: 선형 분리 불가능한 데이터 문제**

*   **약물 용량 예시**: 일부 데이터셋에서는 약물 용량처럼 1차원 데이터만으로는 환자의 치료 여부를 효과적으로 분류하기 어렵습니다. 예를 들어, 약물 용량이 너무 적거나 너무 많으면 효과가 없고, 특정 "적절한" 용량에서만 환자가 치료되는 경우가 있습니다. 이처럼 중첩이 심한 데이터에서는 **만족스러운 서포트 벡터 분류기(Support Vector Classifier, SVC)**를 찾기 어렵습니다.
*   **고차원 변환을 통한 해결**: 이러한 문제를 해결하기 위해, 원래의 용량 측정값을 제곱하여 **Y축 좌표**로 부여함으로써 데이터를 **더 높은 차원**으로 이동시킬 수 있습니다. 이렇게 하면 1차원에서는 중첩되어 있던 두 데이터 범주를 2차원 공간에서는 **직선(line)**으로 분리할 수 있게 됩니다.
*   **다항식 커널의 역할**: 다항식 커널은 이러한 방식으로 **고차원에서의 관측치들 간의 관계를 계산**하여, 고차원에서 좋은 서포트 벡터 분류기를 찾을 수 있도록 돕습니다.

### **71.2 다항식 커널의 매개변수 (Parameters)**

다항식 커널은 두 가지 주요 매개변수를 가집니다:

*   **R (Coefficient)**: 다항식의 **계수(coefficient)**를 결정합니다.
*   **D (Degree)**: 다항식의 **차수(degree)**를 결정합니다. D=2는 용량의 제곱 기반의 2차원 관계를, D=1은 1차원 관계를 나타냅니다.
*   **매개변수 결정**: R과 D의 최적 값은 **교차 검증(cross-validation)**을 통해 결정됩니다.

### **71.3 고차원 관계 계산 방식: 점곱 (Dot Product)과 커널 트릭 (Kernel Trick)**

다항식 커널의 핵심은 **고차원 관계를 효율적으로 계산**하는 방법에 있습니다.

*   **점곱(Dot Product)**: 다항식 커널은 결국 **두 관측치 (A와 B)** 간의 **점곱(dot product)**으로 표현될 수 있습니다.
    *   점곱은 여러 항들을 각각 곱한 후 모두 더하는 연산입니다.
    *   이 점곱은 데이터의 **고차원 좌표**를 제공합니다. 예를 들어, 첫 번째 항은 x축 좌표, 두 번째 항은 y축 좌표, 세 번째 항은 z축 좌표가 될 수 있습니다. 일부 좌표는 모든 점에 대해 상수 값을 가지므로 무시할 수 있습니다.
*   **커널 트릭 (Kernel Trick)**: 다항식 커널은 놀랍게도 **실제로 데이터를 고차원으로 변환하지 않고도** 고차원에서의 관계를 계산할 수 있습니다.
    *   우리가 해야 할 일은 관측치(예: 약물 용량)를 커널 함수에 대입하여 수학적 계산을 수행하는 것뿐입니다. 이렇게 얻은 결과(예: 16000 2.25)가 바로 고차원 관계가 됩니다.
    *   이러한 "커널 트릭" 덕분에 복잡한 고차원 변환 과정 없이도 **컴퓨팅 비용을 절감**하면서 고차원 관계를 활용하여 서포트 벡터 분류기를 만들 수 있습니다.

결론적으로, 다항식 커널은 데이터를 높은 차원으로 이동시키는 전략을 사용하여 선형적으로 분리할 수 없는 데이터를 분류하는 강력한 도구입니다. 이 과정에서 데이터를 실제로 변환하는 대신, 커널 함수를 통해 고차원 관계를 효율적으로 계산하는 '커널 트릭'이 중요한 역할을 합니다.

## 72. Support Vector Machines Part 3/3: The Radial (RBF) Kernel 

SVM의 세 번째 부분으로 **Radial Kernel (RBF)**에 초점을 맞춰, 이 커널의 매개변수, 고차원 관계 계산 방식, 그리고 무한 차원에서의 작동 원리를 설명합니다.

### **72.1 Radial Kernel의 필요성 및 역할**
*   **문제**: 약물 복용량 데이터를 예시로, 치료된 환자(녹색 점)와 치료되지 않은 환자(빨간 점) 사이에 **데이터 겹침이 너무 많아** 만족스러운 Support Vector Classifier를 찾기 어려울 수 있습니다. 즉, 데이터가 선형적으로 분리되지 않는 경우입니다.
*   **해결책**: 이러한 겹치는 데이터를 처리하는 한 가지 방법은 Radial Kernel(일명 Radial Basis Function, RBF)을 사용하는 Support Vector Machine을 이용하는 것입니다.

### **72.2 Radial Kernel의 핵심 작동 원리**
*   **무한 차원에서의 분류**: Radial Kernel은 **무한 차원**에서 Support Vector Classifier를 찾아냅니다. 무한 차원이기 때문에 시각적으로는 그 작동을 파악하기 어렵습니다.
*   **가중치 기반 최근접 이웃 모델**: 그러나 새로운 관측치를 분류할 때 Radial Kernel은 **가중치 기반 최근접 이웃(weighted nearest neighbor) 모델처럼 작동**합니다.
    *   새로운 관측치에 **가장 가까운 관측치들(즉, 최근접 이웃)**은 분류에 많은 영향을 미치고.
    *   멀리 떨어진 관측치들은 분류에 상대적으로 적은 영향을 미칩니다.

### **72.3 영향력 결정 요소: 제곱 거리와 감마(Gamma)**
*   **제곱 거리**: Radial Kernel은 훈련 데이터셋의 각 관측치가 새로운 관측치 분류에 얼마나 많은 영향을 미치는지 결정할 때, 두 관측치(A와 B) 간의 **제곱 거리**를 사용합니다. 즉, 한 관측치가 다른 관측치에 미치는 영향은 제곱 거리의 함수입니다.
*   **감마(Gamma) 파라미터**: 감마는 교차 검증(cross-validation)을 통해 결정되는 매개변수로, **제곱 거리를 스케일링하며, 따라서 영향력을 스케일링합니다**.
    *   예를 들어, 감마가 1일 때 비교적 가까운 두 관측치 간의 관계는 0.11로 계산됩니다.
    *   하지만 감마가 2로 증가하면, 동일한 두 관측치에 대해 관계 값은 0.01로 감소합니다.
    *   이는 **감마 값이 커질수록** 두 지점 사이의 영향력이 더 작아진다는 것을 의미하며, 즉, **거리에 따른 영향력 감소 폭이 더 커져** 더 좁은 범위의 이웃에만 영향을 미치도록 합니다.
    *   두 관측치가 서로 멀리 떨어져 있을수록, 그들의 영향력은 0에 매우 가까운 값이 됩니다. Radial Kernel에 값을 대입하여 얻는 이 값(예: 0.11 또는 0에 가까운 값)은 두 관측치 간의 **고차원적인 관계**를 나타냅니다.

### **72.4 무한 차원 이해를 위한 직관 (다항식 커널 비유)**
*   **다항식 커널 (R=0)**: R=0으로 설정된 다항식 커널은 D 값에 따라 데이터를 원본 축 상에서 이동시키는 역할을 합니다. 예를 들어 D=2는 값을 제곱하여 이동시키고, D=3은 값을 세제곱하여 더 멀리 이동시킵니다. D=1일 때는 데이터가 원래 위치에 그대로 있습니다.
*   **차원 추가**: R=0인 다항식 커널들을 결합하면 새로운 차원을 만들 수 있습니다.
    *   R=0, D=1인 다항식 커널과 R=0, D=2인 다항식 커널을 더하면, 원본 복용량과 복용량의 제곱이라는 두 차원을 위한 좌표를 갖는 내적(dot product)을 얻습니다.
    *   이를 XY 축에 플로팅하여 분류기를 찾을 수 있습니다.
    *   R=0, D=3인 커널을 추가하면 세 차원(XYZ 축)을 얻을 수 있습니다.
*   **Radial Kernel의 무한 차원**: 만약 R=0인 다항식 커널들을 **D가 무한대에 이를 때까지 계속 추가한다면, 무한한 수의 차원을 위한 좌표를 갖는 내적을 얻을 수 있을 것입니다.** **이것이 바로 Radial Kernel이 하는 일입니다**.

### **72.5 수학적 배경 (테일러 급수 확장)**
*   Radial Kernel의 특정 부분을 **테일러 급수(Taylor series expansion)**를 이용하여 무한 합으로 확장할 수 있습니다. 테일러 급수는 함수를 무한한 항의 합으로 나타내는 방법입니다.
*   Radial Kernel의 일부 항을 조작하고 감마를 1/2로 설정한 다음, 특정 항(`e^(AB)`)을 테일러 급수로 확장하면 `1 + AB + (AB)^2/2! + (AB)^3/3! + ...`와 같은 형태로 표현됩니다.
*   이러한 테일러 급수 확장의 각 항은 R=0인 다항식 커널의 형태를 포함하고 있으며, 이 무한한 합은 결국 **무한한 수의 차원을 갖는 내적**으로 표현될 수 있습니다.
*   최종적으로, Radial Kernel은 **무한 차원을 갖는 내적**과 동일하다는 것이 수학적으로 증명됩니다.

Radial Kernel은 데이터를 직접 무한 차원으로 변환하지 않고 (실제로는 내적만 계산하여 고차원 관계를 얻습니다), 두 데이터 포인트 간의 **무한 차원 관계를 효율적으로 계산**하여 복잡한 분류 문제를 해결합니다.

## 73. Python에서 Support Vector Machine(SVM)을 구축
**scikit-learn** 라이브러리와 **방사형 기저 함수(Radial Basis Function, RBF) 커널**을 사용하여 **분류(classification)를 위한 Support Vector Machine(SVM)을 구축하는 실용적인 방법**을 안내합니다. UCI 머신러닝 저장소의 신용카드 채무 불이행 데이터셋을 활용하여 한 개인이 신용카드를 연체할지 여부를 예측하는 모델을 만듭니다.

### **73.1 SVM을 사용하는 이유 및 기본 특성**
*   **정확한 결과가 중요한 경우 최적의 알고리즘 중 하나**입니다. 결과의 원리를 이해하는 것보다 정확한 예측이 우선일 때 특히 유용합니다.
*   비교적 **작은 데이터셋에서 매우 잘 작동**하며, 특별한 **최적화 없이도 초기 성능이 좋은 경향**이 있습니다 (즉, "out of the box" 성능이 좋습니다).

### **73.2 전제 지식 및 학습 방법**
*   이 튜토리얼은 기본적인 **Python 지식**과 **Support Vector Machine 이론, Radial Basis Function, 정규화(regularization), 교차 검증(cross-validation), 혼동 행렬(confusion matrices)**에 대한 이해를 전제로 합니다.
*   제공된 코드를 **직접 실행하고 변경해보는 것이 학습에 가장 좋은 방법**임을 강조합니다.

### **73.3 데이터 준비 및 전처리 과정 (Jupyter Notebook 사용)**
Python 3 환경의 **Jupyter Notebook**을 사용하며, 데이터 처리 및 모델링을 위해 **pandas, numpy, matplotlib, scikit-learn** 등의 모듈을 임포트합니다.

*   **데이터 가져오기:** UCI 머신러닝 저장소에서 신용카드 채무 불이행 데이터셋을 로드합니다. 데이터는 **데이터프레임(DataFrame)** 형태로 구성되며, 각 행은 신용카드 사용자, 각 열은 해당 사용자의 특성을 나타냅니다.
*   **초기 데이터 정리:**
    *   타겟 변수인 'default payment next month'의 이름을 **'default'**로 변경하여 간결하게 만듭니다.
    *   정보를 제공하지 않는 'ID' 열은 분석에서 제외하기 위해 **삭제**합니다.
*   **누락된 데이터 처리:**
    *   데이터 분석에서 가장 중요한 부분 중 하나입니다. 데이터셋에서 '0' 값을 가지고 있는 'education' 및 'marriage' 열의 **누락된 값(missing values)**을 식별합니다.
    *   SVM은 누락된 값을 지원하지 않으므로, 전체 30,000개 행 중 68개 행(1% 미만)에 해당하는 누락된 값이 있는 행들을 **데이터셋에서 제거**합니다.
*   **데이터 다운샘플링:**
    *   SVM은 대규모 데이터셋에서 계산 시간이 오래 걸릴 수 있으므로, '채무 불이행' 및 '채무 불이행 아님' 각각의 범주에서 **1,000개씩, 총 2,000개의 샘플로 다운샘플링**합니다.
*   **데이터 포맷팅 (SVM 적용 준비):**
    *   **특성(features)을 나타내는 X 변수**와 **예측하려는 목표(target)를 나타내는 Y 변수**로 데이터를 분리합니다.
    *   **원-핫 인코딩(One-Hot Encoding):** 'Sex', 'Education', 'Marriage', 'Pay'와 같은 **범주형 데이터(categorical data)**는 SVM이 기본적으로 지원하지 않습니다. 범주형 데이터를 연속형 데이터처럼 처리하면 잘못된 유사성을 가정할 수 있으므로, 각 범주를 독립적인 이진(binary) 열로 변환하는 원-핫 인코딩을 수행합니다 (`pandas.get_dummies` 함수 사용).
    *   **데이터 중앙화(Centering) 및 스케일링(Scaling):** RBF 커널을 사용하는 SVM은 데이터가 중앙에 위치하고(평균 0) 스케일링되어(표준편차 1) 있다고 가정합니다. 이를 위해 **훈련(training) 및 테스트(test) 데이터셋으로 분할**한 후, **데이터를 스케일링**합니다.

### **73.4 SVM 모델 구축 및 최적화**
*   **예비 SVM 구축:** `SVC` 함수를 사용하여 SVM 분류기 객체를 생성하고, 훈련 데이터로 **모델을 훈련(fit)**시킵니다.
*   **예비 SVM 평가:** **혼동 행렬**을 사용하여 테스트 데이터셋에서 모델의 성능을 평가합니다. 초기 모델은 특정 범주에서 만족스럽지 못한 성능을 보였습니다 (예: 채무 불이행 아닌 경우 79%, 채무 불이행인 경우 61% 정확도).
*   **SVM 최적화 (교차 검증):**
    *   성능 개선을 위해 **그리드 서치 교차 검증(Grid Search CV)**을 사용하여 **감마(gamma)** 및 **정규화 매개변수 C(regularization parameter C)**와 같은 최적의 하이퍼파라미터를 찾습니다.
    *   최적화된 매개변수 (예: C=100, gamma=0.001)를 사용하여 새로운 SVM 모델을 구축하고 다시 평가합니다.
    *   동영상에서는 SVM이 초기 성능이 좋기 때문에, **최적화 후의 성능 향상은 크지 않음**을 보여줍니다.

### **73.5 결정 경계(Decision Boundary) 시각화**
*   모델의 **결정 경계**를 시각화하기 위해, 24개의 데이터 열을 **주성분 분석(Principal Component Analysis, PCA)**을 사용하여 2차원으로 축소합니다.
*   2차원으로 축소된 데이터로 SVM을 재훈련 및 최적화하여 결정 경계를 그립니다.
*   시각화된 그래프에서 **핑크색 영역은 '채무 불이행 아님'으로 분류되는 결정 영역**을, **노란색 영역은 '채무 불이행'으로 분류되는 결정 영역**을 나타냅니다. 훈련 데이터의 점들은 빨간색과 초록색으로 표시됩니다.
*   다차원 데이터를 2차원으로 축소한 결과이므로, 시각화된 결정 경계는 **실제 분류를 근사한 결과**이며, 데이터의 모든 변동성을 정확히 반영하지 못할 수 있음을 언급합니다.
