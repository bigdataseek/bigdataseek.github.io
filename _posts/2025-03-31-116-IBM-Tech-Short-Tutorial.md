---
title: 25차시 15:IBM TECH(종합 내용)
layout: single
classes: wide
categories:
  - IBM TECH(종합 내용)
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 141. AI 모델 수명 주기
- 출처: [AI Model Life Cycle: From Planning to Deployment to Retirement](https://www.youtube.com/watch?v=-x9bVcEmkUk)

### 141.1   **AI 모델 수명 주기의 중요성**: 
AI가 일상 업무에 널리 사용되면서, AI 모델을 안전하게 구축하고 사용하는 방법을 배우는 것이 중요해졌습니다. 이 영상은 모델의 '탄생'부터 '은퇴'까지의 모든 단계를 다룹니다.

*   **1단계: 계획 수립 (Planning)**
    *   **목표 설정**: 모델이 무엇을 할 것인지, 어떤 종류의 대화를 나눌 것인지, 누가 사용자가 될 것인지 등 모델의 목적을 명확히 정의합니다. 예를 들어, 사용자가 요리 레시피를 만드는 것을 돕는 모델을 설계하는 것을 가정합니다.

*   **2단계: 데이터 수집 및 준비 (Data Collection & Preparation)**
    *   **데이터 수집**: 사용 사례에 맞춰진(tailored), 윤리적이고 신뢰할 수 있는(trustworthiness) 훈련 데이터를 수집하는 것이 중요합니다. 좋은 AI는 좋은 데이터에서 시작됩니다. 대화 데이터, 신뢰할 수 있는 출처의 레시피, 견고한 요리 기술 관련 데이터 등이 포함될 수 있습니다.
    *   **다양성 및 추적성**: 다양한 배경과 관점에서 데이터를 수집하고, 모든 데이터를 원본까지 추적하여 신뢰성을 확보해야 합니다.
    *   **데이터 정제**: 개인 식별 정보(PII) 제거, 중복 제거(deduplicating), 누락된 값 대체, 형식 표준화 등을 통해 데이터를 정제합니다.
    *   **편향성 확인**: 데이터의 편향성을 확인하고, 불균형이 발견되면 합성 데이터를 생성하여 균형을 맞출 수 있습니다.

*   **3단계: 모델 개발 (Model Development)**
    *   **아키텍처 및 방법 선택**: 다양한 알고리즘, 방법 및 아키텍처를 사용하여 AI 모델을 개발할 수 있습니다.
    *   **예시**: 대화형 및 지시형 모델의 경우 **트랜스포머 아키텍처(Transformer architecture)**가 텍스트 처리 및 생성에 적합하며, **전문가 혼합(Mixture of Experts) 아키텍처**를 사용한 소규모 전문 모델의 조합은 성능을 향상시키면서 계산 및 환경 비용을 줄일 수 있습니다.

*   **4단계: 평가 및 검증 (Evaluation & Validation)**
    *   **AI 거버넌스 검토 위원회**: 모델이 EU AI Act와 같은 규정을 준수하는지 확인하기 위해 AI 거버넌스 검토 위원회를 구축합니다.
    *   **성능 측정**: 정확성(accuracy), 공정성(fairness), 편향성(bias)을 확인하기 위해 인구 통계 그룹 전반의 성능을 측정하고, 출력의 다양성을 확인합니다.
    *   **엣지 케이스 테스트**: 예상치 못한 모든 가능성을 테스트하고 엣지 케이스를 브레인스토밍합니다.
    *   **조정**: 불일치가 발견되면 알고리즘을 조정하거나 합성 데이터를 추가하여 데이터를 보강할 수 있습니다.

*   **5단계: 배포 (Deployment)**
    *   **자동화 및 보안**: 배포 프로세스는 반복 가능하고(repeatable), 자동화되어 있으며(automated), 보안이 유지되어야 합니다.
    *   **클라우드 플랫폼 활용**: 클라우드 플랫폼을 사용하여 스토리지, 컴퓨팅, 네트워킹을 설정한 다음, 모델을 컨테이너화하여 배포합니다.

*   **6단계: 모니터링 및 재훈련 (Monitoring & Retraining)**
    *   **지속적인 관리**: 모델이 프로덕션 환경에 배포되면 지속적인 모니터링(ongoing monitoring), 버전 제어(version control), 재훈련(retraining)을 통해 모델을 건강하고 신뢰할 수 있게 유지합니다.
    *   **드리프트(Drift) 모니터링**: 모델이 이전에 작동하던 방식대로 작동하지 않는 '드리프트' 현상을 모니터링하여 모델의 공정성과 편향되지 않음을 지속적으로 확인합니다.
    *   **성능 지표 모니터링**: 처리량(throughput), 지연 시간(latency), 오류율(error rates)과 같은 성능 지표를 모니터링합니다.
    *   **주기적 재훈련**: 자동화된 알림 및 파이프라인을 설정하여 주기적인 재훈련을 계획해야 합니다.

*   **7단계: 모델 은퇴 (Model Retirement)**
    *   **아카이빙**: 모델이 더 이상 필요하지 않을 경우, 나중에 활용할 수 있도록 보관(archive)합니다.

이러한 단계를 통해, 신중한 계획과 개발을 통해 사용자 요구를 충족하고 편향성과 드리프트를 방지하며 투명성과 신뢰를 보장하는 AI 모델을 구축할 수 있습니다.

## 142. AI, 필요할 때마다 새로운 능력을 쏙쏙! (Hot Swapping)
- 출처: [Hot Swapping AI Skills: Attention Mechanisms & ALoRA Explained](https://www.youtube.com/watch?v=qmUWsFCnsz4)

AI 모델이 마치 게임 콘솔에 새로운 게임 카트리지를 끼우듯, 필요에 따라 특정 기능을 즉시 바꿔가며 사용할 수 있다면 어떨까요? 바로 이 아이디어가 **AI 기술 핫스왑핑(Hot Swapping)**의 핵심입니다.

### 142. 핫스왑핑이 뭔가요?

일반적인 AI 모델은 한 번 학습되면 특정 기능에 특화되어 있습니다. 하지만 핫스왑핑 기술을 사용하면 모델을 처음부터 다시 학습시키거나, 통째로 메모리에 다시 로드할 필요 없이 특정 기능을 **실시간으로 바꿔가며 사용**할 수 있습니다. 예를 들어, 한 AI 모델이 문서 요약도 하고, 논평도 작성하고, 과학 노트도 쓸 수 있도록 필요에 따라 능력을 전환하는 거죠.

이 기술은 AI 모델의 **주의(Attention) 메커니즘**과 **ALoRA(Activated Low-Rank Adaptation Adapters)**라는 기술을 활용합니다. 마치 게임 콘솔이 켜진 상태에서 새 게임을 바로 넣는 것처럼, AI 모델의 핵심은 그대로 유지하면서 새로운 기능을 동적으로 추가하고 활성화할 수 있습니다.

*   **이점:**
    * **실시간 전문성 주입:** 새로운 비즈니스 요구에 맞춰 AI 모델의 전문성을 즉시 주입하거나 업데이트할 수 있습니다.
    * **효율성:** 모델을 재훈련하거나 다시 로드할 필요가 없어 시간과 비용을 절약합니다.
    * **확장성:** 다양한 작업을 수행하는 데 필요한 유연성을 제공하여 AI의 활용 범위를 넓힐 수 있습니다.


### 142.3 AI의 '집중력' 비결: 주의(Attention) 메커니즘

요즘 AI, 특히 챗GPT 같은 **대규모 언어 모델(LLM)**은 텍스트, 이미지, 소리 등 엄청나게 다양한 정보를 처리합니다. 이 많은 정보 속에서 AI가 정말 중요한 부분에 '집중'하도록 돕는 것이 바로 **주의(Attention) 메커니즘**입니다.

1.Attention 메커니즘은 어떻게 작동하나요?

간단히 말해, 주의 메커니즘은 AI 모델이 어떤 단어나 정보에 더 중요하게 생각해야 할지 알려주는 역할을 합니다. 문장을 예로 들면, AI는 각 단어를 다른 단어와 비교하여 얼마나 관련이 있는지 점수를 매깁니다. 이 점수를 바탕으로 중요하다고 판단되는 단어에는 더 높은 **가중치**를 부여하여 해당 정보에 더 집중합니다.

* **셀프-어텐션 (Self-Attention):** AI가 문장 내에서 자기 자신(각 단어)과 다른 모든 단어들 사이의 관계를 파악합니다.
    * 각 단어는 **질문(Query, Q)**, **열쇠(Key, K)**, **가치(Value, V)** 세 가지 형태로 변환됩니다.
    * AI는 어떤 단어의 '질문'을 다른 모든 단어의 '열쇠'와 비교하여 얼마나 중요한지 점수를 매깁니다.
    * 이 점수를 확률로 변환하고, 각 단어의 '가치'에 이 확률을 곱해서 최종적인 출력값을 만듭니다. 이렇게 하면 AI는 문맥에 따라 어떤 단어에 집중해야 할지 알게 됩니다.
* **멀티-헤드 어텐션 (Multi-Head Attention):** 이 '집중' 과정은 한 번만 일어나는 것이 아니라 여러 번 동시에 일어납니다. 각 '헤드'는 문법, 대명사 관계, 고유 명사 등 문장의 다양한 측면에 집중하도록 학습되어 AI가 더욱 복잡한 문맥을 이해할 수 있게 돕습니다.

2.Attention 메커니즘의 한계와 해결책

주의 메커니즘은 AI의 성능을 크게 향상시켰지만, 몇 가지 문제점도 있습니다.

* **느려질 수 있음:** 특히 긴 문장을 처리할 때, 각 단어를 다른 모든 단어와 비교해야 하므로 계산량이 **기하급수적으로 증가**하여 AI 모델의 속도를 늦출 수 있습니다.
* **메모리 사용량 증가:** 긴 텍스트를 처리하거나 한 번에 많은 정보를 처리할수록 더 많은 메모리가 필요합니다.

*   **성능 최적화 전략:**
    * **키-값 캐싱 (Key-Value Caching, KV Cache):** 이전에 계산했던 정보를 기억해두고 필요할 때 다시 사용함으로써 긴 대화에서 계산량을 크게 줄일 수 있습니다. 마치 이미 본 내용을 다시 읽지 않고 기억하는 것과 같습니다.
    * **플래시 어텐션 (Flash Attention):** GPU(그래픽 처리 장치)에서 주의 계산을 더욱 효율적으로 처리하는 방법입니다. 덕분에 긴 문장도 빠르게 처리할 수 있습니다.
    * **희소(Sparse) 및 선형(Linear) 어텐션:** 모든 단어 간의 관계를 계산하는 대신, 일부 단어 간의 관계만 계산하여 계산량을 줄이는 방법입니다. 매우 긴 문맥을 처리하는 데 유용합니다.
    * **모델 압축 (Model Compression):** AI 모델의 크기를 줄여 메모리 사용량을 줄이고 계산 속도를 높입니다. 마치 큰 파일을 작게 압축하는 것과 비슷합니다.

### 142.4 AI의 '카트리지' ALoRA: 실시간 전문성 주입

앞서 설명한 핫스왑핑 기술의 핵심 중 하나가 바로 **ALoRA(Activated Low-Rank Adaptation Adapters)**입니다. ALoRA는 AI 모델의 능력을 실시간으로 바꾸는 '게임 카트리지'와 같은 역할을 합니다.

1.ALoRA는 무엇인가요?

ALoRA는 **저랭크 적응(Low-Rank Adaptation)**이라는 기술의 한 종류입니다. 기존 AI 모델의 거의 모든 부분(예: 99.99%)은 그대로 두고, 아주 작은 일부(예: 0.01%)만 업데이트하여 특정 작업에 맞게 미세 조정하는 방법입니다. 이렇게 수정된 작은 부분이 바로 '게임 카트리지' 또는 '어댑터'가 되어 AI 모델에 새로운 전문성을 부여합니다.

2."Activated"의 중요성

ALoRA의 "활성화(Activated)" 부분은 AI가 이미 수행한 계산(KV 캐시)을 재사용한다는 의미입니다. 덕분에 AI 모델이 특정 전문 분야로 전환할 때, 전체 과정을 다시 계산할 필요 없이 **거의 실시간으로** 전환이 가능합니다.

ALoRA는 특히 AI 모델이 '집중'할 대상을 결정하는 주의 메커니즘의 중요한 부분(투영 계층)을 대상으로 합니다. 이 부분에 작은 '어댑터'를 주입하여 AI가 특정 정보를 더 효과적으로 처리하고 새로운 기능을 수행할 수 있도록 돕습니다.

ALoRA를 통해 의료 관련 질문 답변, 코드 생성, 법률 분석 등 다양한 전문 분야에 AI 모델을 효과적으로 특화시킬 수 있습니다. 그리고 이 모든 과정은 모델을 다시 학습시키거나 통째로 로드할 필요 없이 빠르게 이루어집니다.

**결론적으로,** 주의 메커니즘은 AI가 방대한 정보 속에서 중요한 것에 집중하도록 돕는 역할을 하며, ALoRA는 이러한 주의 메커니즘에 가볍고 작은 '어댑터'를 주입하여 AI 모델이 마치 게임 카트리지를 바꾸듯 **새로운 전문 기술을 즉시 교환**할 수 있게 합니다. 이는 기존에 계산된 정보를 재사용(키-값 캐싱)함으로써 빠르고 효율적인 실시간 AI 전문화를 가능하게 합니다.

