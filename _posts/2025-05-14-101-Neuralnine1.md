---
title: 8차시 1:Neuralnine 1

layout: single
classes: wide
categories:
  - NeuralNine
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 1. Wikipedia 기반 RAG (Retrieval Augmented Generation) 시스템 구축
- 출처: [Wikipedia RAG System in Python - Beginner Tutorial with LlamaIndex](https://www.youtube.com/watch?v=M9GtHb32F8w)

### 1. 개요

  * **목표:** 방대한 정보를 담고 있는 Wikipedia를 기반으로 사용자의 질문에 정확하고 관련성 높은 답변을 제공하는 RAG (Retrieval Augmented Generation) 시스템을 구축합니다. 이 시스템은 특정 주제에 대한 심층적인 이해를 돕고, 정보 검색 시간을 단축하며, 다양한 관점을 제공할 수 있습니다.
  * **기술 스택:** 시스템 개발에는 다음과 같은 강력한 기술들이 활용됩니다.
      * **Python:** 높은 생산성과 다양한 라이브러리 지원으로 데이터 처리, 모델 통합, 웹 애플리케이션 개발 등 전반적인 시스템 구축에 사용됩니다.
      * **Llama Index:** RAG 파이프라인의 핵심 엔진 역할을 수행하며, 데이터 로딩, 인덱싱, 검색, 답변 생성 등 복잡한 과정을 추상화하여 개발자가 비즈니스 로직에 집중할 수 있도록 지원합니다.
      * **Streamlit:** 개발된 RAG 시스템을 사용자가 웹 브라우저를 통해 직관적으로 상호작용할 수 있는 사용자 친화적인 인터페이스를 쉽고 빠르게 구축할 수 있도록 돕습니다.
  * **핵심 컨셉:** 이 시스템은 다음의 핵심 컨셉들을 기반으로 작동합니다.
      * **RAG (Retrieval Augmented Generation):** 
        *   질문에 대한 답변을 생성할 때, 먼저 Wikipedia에서 질문과 관련된 정보를 효율적으로 검색(Retrieval)하고, 검색된 정보를 기반으로 답변을 생성(Generation)하는 방식입니다. 이는 LLM(Large Language Model)이 가진 지식의 한계를 보완하고, 최신 정보나 특정 도메인에 특화된 답변을 제공할 수 있도록 합니다.
      * **Llama Index:** 
        *   다양한 데이터 소스에 대한 연결, 텍스트 분할, 벡터 임베딩 생성, 검색 엔진 구축 등 복잡한 RAG 파이프라인 구축 과정을 단순화하고 자동화합니다. 이를 통해 개발자는 몇 줄의 코드로 강력한 검색 및 답변 기능을 구현할 수 있습니다.
      * **Streamlit:** 
        *   Python 스크립트만으로 데이터 시각화, 모델 데모, 간단한 웹 애플리케이션 등을 빠르게 개발하고 배포할 수 있도록 지원하는 라이브러리입니다. 이를 통해 개발자는 RAG 시스템의 기능을 사용자가 쉽게 경험하고 활용할 수 있는 인터페이스를 구축할 수 있습니다.

### 2. 개발 과정

1.  **필요 패키지 설치:** 시스템 구축에 필요한 핵심 라이브러리들을 `pip` 명령어를 사용하여 설치합니다.
      * `streamlit`: 
        *   웹 인터페이스 구축을 위한 필수 라이브러리입니다.
      * `llama-index`: 
        *   RAG 시스템의 핵심 기능을 제공합니다.
      * `python-dotenv` (선택 사항이지만 권장): 
        *   중요한 API 키와 같은 환경 변수를 코드 외부의 `.env` 파일에서 안전하게 관리할 수 있도록 돕습니다.
      * `llama-index-embeddings-openai`: 
        *   OpenAI의 강력한 임베딩 모델을 Llama Index에서 쉽게 사용할 수 있도록 지원합니다. 텍스트 데이터를 의미론적 벡터 공간으로 변환하여 검색의 정확도를 높입니다.
      * `llama-index-llms-openai`: 
        *   OpenAI의 다양한 LLM (GPT-4, gpt-3.5-turbo 등)을 Llama Index와 통합하여 답변 생성 기능을 구현합니다.
      * `llama-index-readers-wikipedia`: 
        *   Wikipedia API를 통해 특정 문서의 내용을 Llama Index로 쉽게 불러올 수 있도록 지원합니다.
2.  **OpenAI API 키 설정:** OpenAI의 LLM과 임베딩 모델을 사용하기 위해서는 API 키가 필요합니다.
      * [OpenAI 개발자 플랫폼](https://platform.openai.com/)에서 계정을 생성하고 API 키를 발급받습니다.
      * 발급받은 API 키를 코드에 직접 문자열 형태로 입력할 수도 있지만, 보안을 위해 `.env` 파일을 생성하여 API 키를 저장하고 `python-dotenv` 라이브러리를 사용하여 로드하는 방식을 권장합니다. `.env` 파일은 Git과 같은 버전 관리 시스템에서 제외하여 API 키가 외부에 노출되지 않도록 주의해야 합니다.
3.  **Wikipedia 문서 선택:** RAG 시스템의 지식 기반으로 활용할 특정 Wikipedia 문서의 제목 리스트를 준비합니다. 
    *   예를 들어, '인공지능', '머신러닝', '자연어 처리' 등 관심 있는 분야의 문서를 선택할 수 있습니다.
4.  **코드 구현 (main.py):** RAG 시스템의 핵심 로직과 Streamlit 웹 인터페이스를 구현하는 Python 스크립트(`main.py`)를 작성합니다.
      * 필요한 라이브러리들을 import 합니다: 
        *   `os`, `streamlit`, `load_dotenv` (선택 사항), `OpenAI`, `OpenAIEmbedding`, `WikipediaReader`, `VectorStoreIndex`, `StorageContext`, `load_index_from_storage`.
      * `.env` 파일이 존재한다면 `load_dotenv()` 함수를 호출하여 환경 변수를 로드합니다.
      * Vector Index를 저장할 디렉토리 경로를 정의합니다. 이는 생성된 Index를 재사용하여 시스템의 효율성을 높이는 데 사용됩니다.
      * **`get_index()` 함수:** 이 함수는 Wikipedia 문서를 기반으로 Vector Store Index를 생성하거나, 이미 생성된 Index가 있다면 로컬 저장소에서 불러오는 역할을 합니다.
          * 정의된 Index 저장 디렉토리가 존재하는지 확인합니다.
          * 디렉토리가 존재하면 `load_index_from_storage` 함수를 사용하여 저장된 Index를 불러옵니다. 이를 통해 매번 새로운 질문마다 Wikipedia 문서를 다시 로드하고 인덱싱하는 비효율성을 방지합니다.
          * 디렉토리가 존재하지 않으면, `WikipediaReader`를 사용하여 지정된 Wikipedia 문서들을 로드합니다.
          * `OpenAIEmbedding` 모델을 사용하여 로드된 각 문서의 내용을 벡터 임베딩으로 변환합니다. 임베딩은 텍스트의 의미를 수치화한 표현으로, 의미적으로 유사한 텍스트는 벡터 공간에서 가까운 거리에 위치하게 됩니다.
          * `VectorStoreIndex.from_documents()` 함수를 사용하여 문서와 해당 임베딩을 기반으로 Vector Store Index를 생성합니다. 이 Index는 질문과 관련된 문서를 효율적으로 검색하는 데 사용됩니다.
          * 생성된 Index를 정의된 디렉토리에 저장하여 이후 재사용할 수 있도록 합니다.
          * 생성 또는 로드된 Index 객체를 반환합니다.
      * **`get_query_engine()` 함수:** 
          *   이 함수는 생성된 Vector Store Index를 기반으로 질문에 답변할 수 있는 Query Engine을 생성.
          * `OpenAI` LLM 모델 (예: `gpt-3.5-turbo`, `gpt-4`)을 지정하고, 답변의 창의성을 조절하는 `temperature` 파라미터를 설정합니다 (0에 가까울수록 결정적인 답변을 생성).
          * Index 객체의 `as_query_engine()` 메서드를 호출하여 Query Engine을 생성합니다. 이때, 사용할 LLM 모델과 검색할 관련 문서의 개수(`similarity_top_k`) 등을 설정할 수 있습니다. Llama Index는 내부적으로 질문과 Index 내의 문서 임베딩 간의 유사도를 계산하여 가장 관련 있는 문서를 찾고, 이를 LLM에 전달하여 답변을 생성하는 과정을 자동화합니다.
          * 생성된 Query Engine 객체를 반환합니다.
      * **`main()` 함수:** 
          * 이 함수는 Streamlit 웹 인터페이스를 구축하고 사용자 입력을 처리하며 답변을 화면에 표시하는 역할을 합니다.
          * `streamlit.title()` 함수를 사용하여 웹 애플리케이션의 제목을 표시합니다.
          * `streamlit.text_input()` 함수를 사용하여 사용자로부터 질문을 입력받을 수 있는 텍스트 입력 상자를 만듭니다.
          * `streamlit.button()` 함수를 사용하여 질문 제출 버튼을 생성합니다.
          * 버튼이 클릭되면, `get_query_engine()` 함수를 호출하여 Query Engine을 얻고, 사용자가 입력한 질문에 대해 `query()` 메서드를 사용하여 답변을 생성합니다.
          * 생성된 답변을 `streamlit.write()` 함수를 사용하여 화면에 표시합니다.
          * 답변과 함께, 답변 생성의 근거가 된 검색된 Wikipedia 문서의 내용 (context)을 함께 표시하여 답변의 신뢰도를 높이고 사용자의 이해를 돕습니다.
5.  **실행:** 개발된 Streamlit 웹 애플리케이션을 실행합니다.
      * 터미널을 열고 `main.py` 파일이 있는 디렉토리로 이동합니다.
      * `streamlit run main.py` 명령어를 실행합니다.
      * 명령어가 성공적으로 실행되면, 웹 브라우저가 자동으로 열리면서 개발된 RAG 시스템의 인터페이스를 확인할 수 있습니다.
      * 웹 인터페이스의 질문 입력 창에 질문을 입력하고 제출 버튼을 클릭하여 답변을 확인합니다.

### 3. 핵심 코드 설명

  * **Wikipedia 문서 로드:** 
    *   지정된 Wikipedia 페이지 제목 리스트를 기반으로 해당 문서의 내용을 불러옵니다. `auto_suggest=False` 옵션은 정확한 페이지 제목만 로드하도록 설정합니다.

    ```python
    reader = WikipediaReader()
    documents = reader.load_data(pages=pages, auto_suggest=False)
    ```

  * **Vector Store Index 생성:** 
    *   로드된 Wikipedia 문서들을 OpenAI 임베딩 모델을 사용하여 벡터화하고, 이를 기반으로 Vector Store Index를 구축합니다. 이 Index는 의미 기반 검색을 가능하게 합니다.

    ```python
    embedding_model = OpenAIEmbedding(model_name="text-embedding-3-small")
    index = VectorStoreIndex.from_documents(documents, embedding_model=embedding_model)
    ```

  * **Query Engine 생성:** 
    *   생성된 Vector Store Index와 OpenAI LLM을 연결하여 질문에 답변할 수 있는 Query Engine을 만듭니다. `similarity_top_k=3`은 답변 생성 시 가장 관련 있는 상위 3개의 문서를 참고하도록 설정합니다.

    ```python
    llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
    query_engine = index.as_query_engine(llm=llm, similarity_top_k=3)
    ```

  * **질문 및 답변:** 
    *   사용자가 입력한 질문을 Query Engine에 전달하고, 생성된 답변을 `response` 변수에 저장합니다.

    ```python
    response = query_engine.query(question)
    ```

### 4. 장점

  * **간편한 RAG 시스템 구축:** 
    *   Llama Index는 복잡한 RAG 파이프라인을 추상화하여 몇 줄의 코드로 강력한 기능을 구현할 수 있도록 지원합니다. 데이터 로딩부터 검색, 답변 생성까지의 과정을 효율적으로 관리할 수 있습니다.
  * **쉬운 웹 인터페이스 개발:** 
    *   Streamlit은 Python 스크립트만으로 대화형 웹 애플리케이션을 빠르게 구축할 수 있도록 해줍니다. 이를 통해 개발자는 RAG 시스템의 기능을 사용자가 쉽게 접근하고 사용할 수 있는 인터페이스를 직관적으로 만들 수 있습니다.
  * **풍부한 지식 기반 활용:** 
    *   세계 최대의 온라인 백과사전인 Wikipedia를 지식 베이스로 활용하여 다양한 분야의 질문에 대해 폭넓고 깊이 있는 답변을 제공할 수 있습니다.
  * **최신 정보 접근 용이:** 
    *   Wikipedia는 지속적으로 업데이트되므로, 시스템은 비교적 최신 정보를 기반으로 답변을 생성할 수 있습니다.

### 5. 개선 방향

  * **임베딩 모델 성능 향상:** 
    *   더 크고 성능이 우수한 임베딩 모델 (예: `text-embedding-ada-002`, Cohere Embed)을 사용하여 검색의 정확도와 답변의 관련성을 높일 수 있습니다.
  * **다양한 데이터 소스 통합:** 
    *   Wikipedia 외에도 다양한 형태의 데이터 소스 (예: 내부 문서, 웹사이트, 데이터베이스)를 통합하여 답변의 범위를 넓히고 특정 요구사항에 맞는 정보를 제공할 수 있도록 확장할 수 있습니다. 
    *   Llama Index는 다양한 데이터 로더를 지원합니다.
  * **사용자 인터페이스 개선:** 
    *   Streamlit의 다양한 기능을 활용하여 더욱 직관적이고 사용하기 쉬운 웹 인터페이스를 구축할 수 있습니다. 예를 들어, 검색 결과 미리보기, 답변의 출처 표시, 사용자 피드백 반영 기능 등을 추가할 수 있습니다.
  * **답변 정확도 및 관련성 향상:** 
    *   프롬프트 엔지니어링 기법을 적용하여 LLM이 더욱 정확하고 맥락에 맞는 답변을 생성하도록 유도할 수 있습니다. 또한, 특정 도메인에 대한 파인튜닝을 통해 답변의 전문성을 높일 수 있습니다.
  * **검색 알고리즘 최적화:** 
    *   Llama Index가 제공하는 다양한 검색 알고리즘 및 파라미터 조정을 통해 검색 성능을 최적화하고, 사용자의 질문 의도에 더욱 부합하는 정보를 찾을 수 있도록 개선할 수 있습니다.
  * **캐싱 및 성능 최적화:** 
    *   대규모 데이터에 대한 인덱싱 및 검색 성능을 최적화하기 위해 캐싱 전략을 적용하고, 필요에 따라 더 강력한 컴퓨팅 자원을 활용할 수 있습니다.



## 2. AWS Bedrock & Python을 활용한 생성형 AI 애플리케이션 개발
- 출처: [Deploy Generative AI Models with Amazon Bedrock & Python](https://www.youtube.com/watch?v=wLNBsr_JKuc&t=229s)

### 1. **AWS Bedrock** 
자체 AI 모델 관리 부담 없이 foundation model을 활용하여 생성형 AI 애플리케이션을 쉽게 구축하고 확장할 수 있는 완전 관리형 서비스.

*    AWS Bedrock은 대규모 언어 모델(LLM)과 같은 foundation model을 API 형태로 제공하여, 사용자가 모델 훈련, 서버 관리, 인프라 설정 등의 복잡한 작업 없이 생성형 AI 애플리케이션을 빠르게 개발할 수 있도록 지원합니다. 예를 들어, 챗봇, 텍스트 생성, 이미지 생성 등 다양한 AI 애플리케이션을 간단히 구현할 수 있습니다.
*   **로드 밸런싱, 확장 등의 관리 필요 없이 사용량 기반 과금 모델 적용:** Bedrock은 서버리스 아키텍처를 기반으로 하여, 사용자가 인프라 관리(예: 서버 확장, 부하 분산)를 신경 쓸 필요 없이 요청한 만큼만 비용을 지불하는 구조입니다. 이는 소규모 스타트업부터 대기업까지 비용 효율적으로 AI를 도입할 수 있게 합니다.
*   **Llama, Claude, Mistral 등 다양한 모델 지원:** Meta AI의 Llama, Anthropic의 Claude, Mistral AI의 Mistral 등 최신 foundation model을 지원하며, 각 모델은 텍스트 생성, 요약, 번역 등 특정 작업에 최적화되어 있습니다. 사용자는 애플리케이션 요구사항에 따라 적합한 모델을 선택할 수 있습니다.

### 2.   **사용 이유** 
AI 모델 직접 관리의 어려움 해소, On-Demand 방식의 편리한 사용, 합리적인 가격.

*    AI 모델을 직접 관리하려면 GPU 서버 설정, 모델 최적화, 지속적인 업데이트 등 상당한 자원과 전문 지식이 필요합니다. Bedrock은 이러한 복잡성을 제거하고, API 호출로 즉시 사용 가능한(On-Demand) 환경을 제공합니다. 또한, 사용량 기반 과금 방식은 초기 투자 비용을 줄이고, 소규모 프로젝트에서도 AI를 쉽게 도입할 수 있도록 합니다. 예를 들어, 테스트 단계에서는 소량의 API 호출로 저렴하게 시작할 수 있습니다.

### 3.   **모델 선택** 
제공 모델 및 지원 지역 확인 필요 (특히, 유럽의 경우 GDPR 등 규제 준수 중요).

*    AWS Bedrock은 지역별로 지원되는 모델이 다를 수 있으므로, AWS Management Console 또는 공식 문서에서 사용 가능한 모델과 지역을 확인해야 합니다. 특히 유럽 지역에서는 GDPR(일반 데이터 보호 규정) 준수를 위해 데이터 처리 및 저장 위치를 신중히 고려해야 합니다. 예를 들어, Claude 모델은 특정 지역에서만 사용 가능하며, 데이터 주권 요구사항을 충족하는 지역을 선택해야 할 수 있습니다.

### 4.   **Python 연동 방법**
1.  **AWS CLI 설치:** [설치 링크](https://aws.amazon.com/cli/) 참조.
    *    AWS CLI(Command Line Interface)는 AWS 서비스를 터미널에서 관리할 수 있는 도구입니다. Bedrock API 호출을 위해 Python에서 AWS SDK를 사용하기 전에, CLI를 설치하여 계정 인증 및 설정을 간편히 수행할 수 있습니다. 설치 후 `aws --version` 명령어로 제대로 설치되었는지 확인.
2.  **AWS 계정 설정:** 액세스 키 ID 및 액세스 키 필요 (보안 자격 증명에서 생성).
    *    AWS Management Console에서 IAM(Identity and Access Management) 메뉴로 이동해 액세스 키를 생성합니다. 이 키는 Python 코드에서 Bedrock API를 호출할 때 인증 정보로 사용됩니다. 보안상, 액세스 키는 안전한 곳에 저장하고 외부에 노출되지 않도록 주의해야 합니다.
3.  **터미널 설정:** `aws configure` 명령어 실행 후 액세스 키 ID, 액세스 키, 지역 이름, 출력 형식 입력.
    *    터미널에서 `aws configure`를 실행하면, AWS CLI에 자격 증명을 설정할 수 있는 대화형 인터페이스가 열립니다. 예를 들어, `us-east-1` 또는 `eu-west-1`과 같은 지역 이름을 입력하고, 출력 형식을 `json`으로 설정하면 Bedrock API 호출 결과가 JSON 형식으로 반환됩니다.
4.  **Python 패키지 설치:**
    *    Python 환경에서 Bedrock과 상호작용하려면 몇 가지 필수 및 선택 패키지를 설치해야 합니다.
    *   **`boto3`:** AWS SDK for Python으로, Bedrock API 호출에 필수입니다. `pip install boto3`로 설치합니다.
    *   **`pydantic`, `instructor`:** (선택 사항) 구조화된 데이터를 처리하고자 할 때 유용합니다. 예를 들어, `pydantic`은 데이터 유효성 검사와 직렬화를 지원하며, `instructor`는 모델 출력에서 원하는 데이터 구조를 추출하는 데 도움을 줍니다. 설치 명령어는 `pip install pydantic instructor`입니다.
5.  **Python 코드 작성:**
    *    Python 코드를 통해 Bedrock API를 호출하는 과정은 다음과 같습니다.
    *   **`boto3`를 사용하여 Bedrock 런타임 클라이언트 생성:** 예를 들어, `boto3.client('bedrock-runtime', region_name='us-east-1')`로 클라이언트를 초기화합니다.
    *   **모델에 맞는 API 요청 파라미터 설정:** 각 모델(Llama, Claude 등)은 고유한 파라미터 형식을 요구합니다. AWS Bedrock 문서에서 해당 모델의 API 요청 형식을 확인하세요. 예: Claude 모델은 `max_tokens`와 `temperature` 같은 파라미터를 설정할 수 있습니다.
    *   **`invoke_model` 메서드를 사용하여 모델 호출 및 결과 스트림 처리:** `invoke_model`은 요청한 모델에 프롬프트를 보내고 결과를 반환합니다. 예를 들어, 텍스트 생성 요청 후 JSON 응답을 파싱하여 원하는 데이터를 추출할 수 있습니다.
6.  **구조화된 출력 활용 (선택 사항):**
    *    구조화된 출력은 AI 모델의 출력을 특정 데이터 형식(예: JSON 객체)으로 정리하여 애플리케이션에서 쉽게 활용할 수 있도록 합니다.
    *   **`instructor` 패키지를 사용하여 Bedrock 런타임 클라이언트 초기화:** `instructor`는 Bedrock 클라이언트와 통합되어 모델 출력을 구조화된 형태로 변환합니다.
    *   **`pydantic`의 `BaseModel`을 상속받아 원하는 데이터 구조 정의:** 예를 들어, 챗봇 응답에서 사용자 이름과 메시지만 추출하도록 데이터 클래스를 정의할 수 있습니다.
    *   **`client.completions.create` 메서드에 모델 ID, 메시지, 응답 모델 정보 전달:** `instructor`를 사용하면 모델 ID(예: `anthropic.claude-v2`)와 프롬프트를 전달하여 원하는 출력 형식을 보장할 수 있습니다.

### 5. **요약**
*   AWS Bedrock은 AI 모델 관리에 대한 복잡성을 줄이고, Python을 통해 생성형 AI 애플리케이션을 효율적으로 개발할 수 있도록 돕는 강력한 서비스입니다. AWS CLI를 설정하고, `boto3`와 같은 Python 패키지를 설치한 후, 모델별 API 요청 파라미터를 적절히 설정하여 모델을 호출하면 됩니다. 추가로 `instructor`와 `pydantic` 패키지를 사용하면 모델 출력을 구조화하여 더 정교한 애플리케이션을 구현할 수 있습니다. 초보자는 간단한 텍스트 생성부터 시작해, 점차 복잡한 워크플로우로 확장해 나갈 수 있습니다.

**추가 학습 제안**
*   **SageMaker, Google Vertex AI 등 다른 플랫폼에 대한 정보:** AWS SageMaker는 모델 훈련과 배포에 중점을 둔 반면, Google Vertex AI는 Google Cloud 환경에서 유사한 생성형 AI 기능을 제공합니다. 두 플랫폼을 비교하여 프로젝트 요구사항에 맞는 최적의 선택을 고려하세요.
*   **AWS Bedrock 심화 기능 (커스텀 모델 등):** Bedrock은 커스텀 모델 파인튜닝 및 임베딩 생성과 같은 고급 기능을 제공합니다. 예를 들어, 특정 도메인에 특화된 데이터를 사용하여 모델을 미세 조정할 수 있습니다.

**참고:** UV 패키지 매니저는 pip의 대안으로, 더 빠른 의존성 관리와 프로젝트 환경 설정을 제공합니다. 하지만 초보자는 익숙한 `pip`를 사용해도 충분하며, `pip install boto3 pydantic instructor`로 필요한 패키지를 설치할 수 있습니다.


## 3. 파이썬 기반 로컬 AI 에이전트 구축 가이드
- 출처: [Coding Local AI Agent in Python (Ollama + LangGraph)](https://www.youtube.com/watch?v=9mLzD997JsU)

### 1. 목표
- **목적:** 랭체인(Langchain), 랭그래프(Langraph), 올라마(Ollama)를 활용하여 파이썬으로 로컬에서 실행 가능한 AI 에이전트를 구축합니다. 이 에이전트는 로컬 환경에서 독립적으로 작동하며, 복잡한 작업을 단계적으로 처리할 수 있는 자율적 시스템을 목표로 합니다.
- **기능:** 다단계 작업을 자율적으로 수행하는 시스템을 구현합니다. 예를 들어, 이메일 읽기, 내용 요약, 사용자 요청에 따른 응답 생성 등을 자동화하여 생산성을 높입니다.
- **장점:** 클라우드 서비스에 의존하지 않고 로컬에서 실행되므로 데이터 프라이버시를 강화하고, 인터넷 연결 없이도 동작 가능합니다.

### 2. 필요 조건
- **GPU:** 최소 8GB VRAM을 갖춘 GPU를 권장합니다. 이는 대형 언어 모델(LLM)을 효율적으로 실행하기 위한 필수 하드웨어입니다. 예를 들어, NVIDIA의 RTX 시리즈 GPU가 적합합니다.
- **올라마(Ollama) 설치 및 모델 다운로드:** Ollama는 로컬에서 LLM을 실행하기 위한 경량화된 도구입니다. Quen 3 (8B 파라미터 모델)를 권장하며, 이는 성능과 메모리 사용량 간 균형이 잘 맞는 모델입니다. 설치 후 `ollama pull quen3` 명령어로 모델을 다운로드할 수 있습니다.
- **이메일 계정 정보:** IMAP 프로토콜을 지원하는 이메일 계정 정보가 필요합니다. IMAP 호스트(예: `imap.gmail.com`), 사용자 이름, 비밀번호(또는 앱 전용 비밀번호), 그리고 작업 대상 IMAP 폴더(예: `INBOX`)를 준비해야 합니다.

### 3. 구현 내용
1. **환경 설정:**
   - **패키지 설치:** `langchain`, `langchain-ollama`, `langraph`, `python-dotenv`, `imap-tools` 패키지를 설치합니다. `pip install langchain langchain-ollama langraph python-dotenv imap-tools` 명령어를 사용하세요. 이 패키지들은 LLM 워크플로우 관리, 환경 변수 처리, 이메일 연결 등을 지원합니다.
   - **환경 변수 설정:** `.env` 파일을 프로젝트 루트 디렉토리에 생성하여 이메일 계정 정보(IMAP_HOST, EMAIL_USER, EMAIL_PASS 등)를 저장합니다. 이는 보안을 강화하고 코드 내에서 민감한 정보를 직접 노출하지 않도록 합니다.

2. **필요 모듈 import:**
   - **기본 모듈:** `os` (환경 변수 접근), `json` (데이터 직렬화), `typing` (타입 힌트 지원).
   - **Langchain 관련 모듈:** `langchain`과 `langchain_core`를 통해 LLM과 도구를 관리합니다.
   - **Langraph:** 워크플로우 그래프를 정의하고 실행하기 위해 `langraph` 모듈을 사용합니다.

3. **변수 설정:**
   - **환경 변수 로드:** `python-dotenv`를 사용하여 `.env` 파일에서 이메일 계정 정보와 기타 설정을 불러옵니다. 예: `load_dotenv()` 후 `os.getenv("IMAP_HOST")`.
   - **LLM 모델 설정:** Ollama에서 제공하는 Quen 3 모델을 기본으로 설정합니다. 필요 시 다른 모델(예: LLaMA, Mistral)로 변경 가능합니다.

4. **상태 정의:**
   - **ChatState 정의:** `typing.TypedDict`를 사용하여 `ChatState` 클래스를 정의합니다. 이 클래스는 에이전트의 상태(예: 메시지 기록, 작업 상태)를 저장하며, 워크플로우 진행 중 데이터의 일관성을 유지합니다.

5. **메일 연결 함수 정의:**
   - **IMAP 연결:** `imap-tools`를 사용하여 IMAP 서버에 연결하는 `connect` 함수를 구현합니다. 이 함수는 이메일 서버에 인증하고, 지정된 폴더(예: INBOX)에 접근합니다. 예: `MailBox(host).login(email, password)`.

6. **도구(Tool) 정의:**
   - **`list_unread_emails`:** 읽지 않은 이메일 목록을 가져오는 함수입니다. 각 이메일의 제목, 날짜, 발신자, UID(고유 식별자)를 포함한 리스트를 반환합니다. 이를 통해 에이전트가 최신 이메일을 식별할 수 있습니다.
   - **`summarize_email`:** 특정 UID를 가진 이메일의 본문을 읽고, LLM을 활용해 내용을 요약합니다. 요약은 간결하면서도 핵심 정보를 포함해야 합니다.

7. **LLM (Large Language Model) 설정:**
   - **LLM 초기화:** `init_chat_model` 함수를 통해 Ollama 기반 LLM 인스턴스를 생성합니다. 예: `Ollama(model="quen3")`.
   - **도구 바인딩:** `bind_tools`를 사용하여 LLM이 `list_unread_emails`, `summarize_email` 같은 도구를 호출할 수 있도록 설정합니다.
   - **Raw LLM:** 도구 사용 권한이 없는 별도의 LLM 인스턴스를 생성하여 이메일 요약, 텍스트 생성 등 일반적인 작업에 활용합니다.

8. **랭그래프(Langraph) 그래프 구축:**
   - **노드(Node) 정의:**
     - `llm_node`: LLM을 호출하여 사용자 요청에 대한 응답을 생성하거나 도구 호출을 결정합니다.
     - `tools_node`: LLM이 호출한 도구를 실행하고 결과를 반환합니다.
   - **라우터(Router) 정의:** LLM 응답을 분석하여 도구 호출이 필요한지, 작업을 종료할지 결정합니다. 예: `if tools_called: goto tools_node else: end`.
   - **엣지(Edge) 정의:** 워크플로우의 흐름을 정의합니다. 예: 시작 → LLM → 라우터 → (도구 호출 시) 도구 → LLM, 또는 (종료 시) 종료.
   - **그래프 컴파일:** 정의된 노드와 엣지를 `StateGraph` 객체에 추가하고 컴파일하여 실행 가능한 워크플로우를 생성합니다.

9. **에이전트 실행 루프:**
   - **입력 처리:** 사용자가 입력한 요청(예: "최신 이메일 요약해줘")을 받아 `ChatState`에 추가합니다.
   - **그래프 실행:** 컴파일된 그래프를 통해 상태를 업데이트하며 작업을 진행합니다.
   - **결과 출력:** 에이전트가 생성한 응답(예: 이메일 요약)을 사용자에게 출력합니다.

### 4. 핵심 개념
- **AI 에이전트:** 사용자의 요청을 이해하고, 다단계 작업(예: 이메일 확인 → 요약 → 응답)을 자율적으로 수행하는 지능형 시스템입니다.
- **랭체인(Langchain):** LLM을 활용한 애플리케이션 개발을 간소화하는 프레임워크로, 도구 호출, 메모리 관리, 체인 구성 등을 지원합니다.
- **랭그래프(Langraph):** 복잡한 워크플로우를 그래프 형태로 시각화하고 관리하는 라이브러리입니다. 노드와 엣지를 통해 작업 흐름을 명확히 정의할 수 있습니다.
- **올라마(Ollama):** 로컬 환경에서 LLM을 실행하기 위한 도구로, GPU를 활용해 효율적으로 모델을 구동합니다.
- **도구(Tool):** LLM이 외부 시스템(예: 이메일 서버)과 상호작용할 수 있도록 정의된 함수입니다.
- **노드(Node):** 그래프의 각 작업 단계를 나타내며, 특정 작업(예: LLM 호출, 도구 실행)을 처리합니다.
- **엣지(Edge):** 노드 간의 연결로, 작업의 진행 순서를 정의합니다.
- **라우터(Router):** 조건에 따라 적절한 노드로 작업 흐름을 전환하는 논리적 요소입니다.

### 5. 추가 개선 사항
- **더 강력한 LLM 모델 사용:** Quen 3보다 파라미터가 큰 모델(예: 70B 모델)을 사용하여 더 높은 성능과 정확도를 얻을 수 있습니다. 단, 더 많은 메모리와 GPU 성능이 필요합니다.
- **도구 확장:** 이메일 외에도 PDF 문서 생성, 캘린더 일정 관리, 외부 API 호출 등 다양한 도구를 추가하여 에이전트의 활용성을 높일 수 있습니다.
- **복잡한 그래프 구조:** 다중 에이전트 협업, 병렬 작업 처리, 조건부 분기 등을 포함한 복잡한 워크플로우를 설계하여 더 정교한 작업을 수행할 수 있습니다.


## 4. Hugging Face Inference Endpoints를 이용한 GPU 서버에 오픈 소스 모델 배포하기
- 출처: [The Easiest Way To Deploy Open Source Models...](https://www.youtube.com/watch?v=kQYzz4GnkIU)

### 1. **목표** 
GPU 서버에 오픈 소스 모델을 배포하고, 파이썬을 이용하여 모델을 효과적으로 사용하는 방법을 배우는 것. 이 가이드는 초보자부터 숙련된 개발자까지 Hugging Face의 Inference Endpoints를 활용하여 모델 배포와 활용 과정을 쉽게 이해하고 실행할 수 있도록 돕습니다.

### 2. **핵심 내용**

**1.Hugging Face Inference Endpoints 활용:**
*	**배경 설명:** 
	*	Hugging Face는 자연어 처리(NLP), 컴퓨터 비전, 오디오 처리 등 다양한 오픈 소스 모델을 제공하는 플랫폼으로, Inference Endpoints는 이러한 모델을 클라우드 기반 GPU 서버에 배포할 수 있는 서비스입니다. 로컬에 고성능 GPU가 없더라도, 이 서비스를 통해 강력한 하드웨어를 임대하여 모델을 실행할 수 있습니다.
- **장점:** 
	*	자체 GPU를 구매하거나 복잡한 서버 설정 없이, 몇 번의 클릭으로 모델을 배포하고 API 호출을 통해 즉시 사용할 수 있습니다. 또한, 다양한 하드웨어 옵션과 유연한 스케일링 설정을 제공하여 비용 효율성을 높일 수 있습니다.
- **간단한 절차:** 
	*	모델 선택 → 하드웨어 선택 → 결제 수단 연결 → 배포. 파이썬으로 작성된 간단한 코드로 모델을 호출할 수 있어, 개발자 친화적입니다.
- **추가 팁:** 
	*	Hugging Face는 무료 체험 크레딧을 제공할 수 있으므로, 처음 시작하는 사용자는 이를 활용하여 비용 부담 없이 테스트해볼 수 있습니다.

**2.배포 과정:**
- **Hugging Face Inference Endpoints 페이지 접속:** 
	*	Hugging Face 웹사이트에서 Inference Endpoints 섹션으로 이동한 후, "Deploy your first model" 버튼을 클릭합니다. 이 버튼은 직관적인 UI로 사용자를 배포 워크플로로 안내합니다.
- **계정 생성 및 결제:** 
	*	Hugging Face 계정이 없으면 계정을 생성하고, 결제 수단(신용카드 등)을 연결해야 합니다. 결제 수단은 배포된 엔드포인트의 사용 요금 청구를 위해 필요합니다.
- **모델 선택:** 
	*	Hugging Face Hub에서 제공하는 수천 개의 오픈 소스 모델 중 원하는 모델을 선택합니다. 예를 들어, 텍스트 생성 모델(예: LLaMA, Mistral)이나 이미지 생성 모델(예: Stable Diffusion)을 선택할 수 있습니다. 카테고리 필터링(예: NLP, Vision) 또는 검색 기능을 사용해 원하는 모델을 빠르게 찾을 수 있습니다.
- **플랫폼 선택:** 
	*	AWS, Azure, Google Cloud 중 원하는 클라우드 제공자를 선택합니다. 각 플랫폼은 성능과 가격이 약간 다를 수 있으므로, 요구 사항에 맞게 선택하세요.
- **EU 지역 선택:** 
	*	데이터 규제(예: GDPR)를 준수해야 하는 경우, EU 지역의 서버를 선택할 수 있습니다. 이는 특히 유럽 내 사용자에게 유용합니다.
- **GPU 서버 선택:** 
	*	모델의 메모리 요구 사항을 확인한 후 적절한 GPU를 선택합니다(예: NVIDIA A100, T4). 각 GPU 옵션의 가격과 성능을 비교하여 예산과 필요에 맞는 선택을 할 수 있습니다.
- **자동 스케일링 설정:** 
	*	엔드포인트가 사용되지 않을 때(idle 상태) 자동으로 서버를 종료하거나 일시 중지하도록 설정하여 비용을 절감할 수 있습니다. 예를 들어, 테스트 환경에서는 이 설정을 활성화하는 것이 좋습니다.
- **모델 저장소에서 직접 배포:** 
	*	Hugging Face Hub의 모델 페이지에서 "Deploy" 버튼을 클릭해 바로 배포를 시작할 수도 있습니다.
- **엔드포인트 이름 설정:** 
	*	배포된 엔드포인트에 고유한 이름을 지정하여 관리와 호출이 편리하도록 설정합니다. 이후 "Create" 버튼을 클릭해 배포를 완료합니다.

**3.모델 사용 준비:**
- **엔드포인트 초기화:** 
	*	배포가 완료되면 엔드포인트가 초기화됩니다(보통 몇 분 소요). 초기화가 완료되면 Hugging Face에서 제공하는 API URL을 통해 모델을 호출할 수 있습니다.
- **Access Token 생성:** 
	*	Hugging Face 계정의 설정(Settings)에서 API 토큰을 생성합니다. 보안을 위해, 토큰에 "Inference Endpoint 호출" 권한만 부여하는 것이 좋습니다. 생성된 토큰은 안전하게 저장하세요.
- **환경 변수 설정:** 
	*	`.env` 파일을 프로젝트 디렉토리에 생성하고, `HF_TOKEN=your_token` 형식으로 토큰을 저장합니다. 이는 코드에서 민감한 정보를 하드코딩하지 않도록 도와줍니다.
- **추가 팁:** 
	*	`.env` 파일을 사용할 때는 `.gitignore`에 추가하여 소스 코드 저장소에 업로드되지 않도록 주의하세요.

**4.파이썬 코드 작성:**
- **필요 패키지 설치:** 
	*	모델 호출과 데이터 처리를 위해 다음 패키지를 설치
     ```bash
     pip install pydantic huggingface_hub langchain-openai python-dotenv
     ```
     여기서 `langchain-openai`는 선택 사항이며, 특정 작업 흐름(예: 챗봇 구현)에 필요할 수 있습니다.
- **Inference Client 생성:** 
	*	Hugging Face의 `InferenceClient`를 사용하여 엔드포인트를 호출합니다. 아래는 기본 설정 코드

     ```python
     from huggingface_hub import InferenceClient
     import os
     from dotenv import load_dotenv

     load_dotenv()

     HF_TOKEN = os.getenv("HF_TOKEN")
     ENDPOINT_URL = "your_endpoint_url"  # Hugging Face에서 제공한 엔드포인트 URL로 변경

     client = InferenceClient(endpoint=ENDPOINT_URL, token=HF_TOKEN)
     ```
	- **설명:** 
		*	`ENDPOINT_URL`은 배포 완료 후 Hugging Face에서 제공하는 고유 URL입니다. `.env` 파일에서 `HF_TOKEN`을 안전하게 가져옵니다.
- **모델 호출:** 
	*	예를 들어, 사용자 질문을 모델에 전달하고 응답을 받는 코드는 다음과 같다
     ```python
     messages = [{"role": "user", "content": "What is machine learning?"}]
     response = client.chat.completions.create(
         model="your_model_name",  # 배포된 모델 이름(엔드포인트 설정에 따라 무시될 수 있음)
         messages=messages,
         temperature=0.3,  # 출력의 창의성 조절(0.0~1.0, 낮을수록 보수적)
         max_tokens=256  # 최대 출력 토큰 수
     )
     print(response.choices[0].message.content)
     ```
	- **설명:** 
		*	`temperature`는 모델의 창의성을 조절하며, 낮은 값(예: 0.3)은 더 예측 가능한 응답을 생성합니다. `max_tokens`는 응답의 최대 길이를 제한합니다.

**5.Structured Output (구조화된 출력):**
- **구조화된 출력의 필요성:** 
	*	모델의 출력을 특정 형식(예: JSON)으로 강제하여 데이터 처리를 간소화하고, 일관된 결과를 얻고자 할 때 유용합니다.
- **Pydantic 모델 정의:** 
	*	`pydantic`를 사용해 출력 형식을 정의합니다. 예를 들어, 사람 정보를 구조화하려면:

     ```python
     from pydantic import BaseModel

     class Person(BaseModel):
         name: str
         age: int
         job: str
     ```
    - **설명:** 
		*	`Person` 클래스는 이름, 나이, 직업 필드를 가진 데이터 구조를 정의합니다. `pydantic`은 데이터 유효성 검사를 자동으로 수행합니다.
- **JSON 스키마 생성:** 
	*	모델의 JSON 스키마를 생성하여 모델에 전달

     ```python
     person_schema = Person.model_json_schema()
     ```
- **응답 형식 지정:** 
	*	모델 호출 시 JSON 형식 출력을 요청

     ```python
     response_format = {"type": "json_object", "value": person_schema}
     ```
- **모델 호출 및 출력 처리:** 
	*	모델의 응답을 JSON으로 파싱하고, Pydantic 모델로 검증

     ```python
     json_str = response.choices[0].message.content  # 모델의 JSON 문자열 응답
     instance = Person.model_validate_json(json_str)  # JSON을 Pydantic 객체로 변환
     print(instance)  # 예: Person(name="John", age=30, job="Engineer")
     ```
     - **추가 팁:** JSON 파싱 오류가 발생할 경우, `try-except` 블록을 사용하여 예외 처리를 추가.

**6.엔드포인트 관리:**
- **하드웨어 업그레이드:** 
	*	모델 성능이 부족하거나 더 빠른 처리가 필요할 경우, Hugging Face 대시보드에서 GPU 사양을 업그레이드할 수 있습니다.
- **자동 스케일링 설정:** 
	*	사용하지 않을 때 엔드포인트를 자동으로 일시 중지하거나 종료하도록 설정하여 비용을 절감합니다. 예를 들어, 테스트 중에는 10분 idle 후 자동 종료를 설정할 수 있습니다.
- **사용량/비용 확인:** 
	*	대시보드에서 실시간 사용량과 비용을 모니터링할 수 있습니다. 이를 통해 예산을 관리하고 예상치 못한 과금을 방지할 수 있습니다.
- **로그 확인:** 
	*	엔드포인트의 호출 로그를 확인하여 디버깅하거나 성능을 분석할 수 있습니다.
- **엔드포인트 삭제:** 
	*	더 이상 필요하지 않은 엔드포인트는 대시보드에서 삭제하여 추가 비용 발생을 방지합니다.
- **추가 팁:** 
	*	정기적으로 대시보드를 확인하여 사용하지 않는 엔드포인트가 실행 중인지 점검하세요.

### 3 **요약** 
Hugging Face Inference Endpoints는 오픈 소스 모델을 GPU 서버에 배포하고, 파이썬을 통해 쉽게 호출할 수 있는 강력하고 사용자 친화적인 도구입니다. Pydantic을 활용하면 모델 출력을 구조화하여 데이터 처리의 효율성을 높일 수 있습니다. 자동 스케일링과 비용 관리 기능을 통해 예산 내에서 효율적으로 모델을 운영할 수 있으며, 초보자부터 전문가까지 다양한 사용자에게 적합한 솔루션입니다.

## 5. 실시간 e-커머스 AI 에이전트 개발 (Python)
- 출처: [E-Commerce AI Agent in Python - Full Tutorial](https://www.youtube.com/watch?v=Ij8lhJz5flU)

### 1. 개요

*   **목표:** 웹의 실시간 정보를 활용하여 특정 제품을 검색하는 AI 에이전트 구축  
    * 이 프로젝트는 사용자가 원하는 특정 조건(예: 가격, 평점, 특정 속성)에 맞는 제품을 실시간으로 검색할 수 있는 AI 기반 도구를 개발하는 것을 목표로 합니다. 이를 통해 사용자들은 최신 시장 정보를 기반으로 빠르고 정확한 제품 정보를 얻을 수 있습니다. 기존의 정적 데이터셋 기반 검색과 달리, 실시간 데이터를 활용함으로써 최신 트렌드와 가격 변동을 반영한 결과를 제공합니다.

*   **특징:** 과거 데이터나 Kaggle 데이터셋이 아닌, 현재 온라인에서 이용 가능한 최신 정보 활용  
    * 이 에이전트는 Kaggle과 같은 정적 데이터셋이나 사전 수집된 데이터에 의존하지 않고, 웹 스크래핑 및 API 호출을 통해 Amazon, eBay 등의 플랫폼에서 실시간으로 데이터를 수집합니다. 이를 통해 최신 재고 상태, 할인 정보, 사용자 리뷰 등을 반영한 동적인 검색 결과를 제공합니다.

*   **기능:**  
    *   사용자 프롬프트에 따라 특정 제품 검색 (예: 특정 가격 이하의 모니터, 특정 VRAM 이상의 GPU)  
        * 사용자는 자연어로 검색 조건을 입력할 수 있으며, 예를 들어 “500달러 이하의 27인치 모니터” 또는 “8GB 이상 VRAM의 그래픽카드”와 같은 구체적인 요구사항을 지정할 수 있습니다.  
    *   다양한 플랫폼 지정 가능 (예: Amazon, eBay, Walmart)  
        * 사용자는 검색하고 싶은 플랫폼을 선택할 수 있으며, 에이전트는 지정된 플랫폼에서 데이터를 수집하여 결과를 통합적으로 제공합니다.  
    *   지정된 플랫폼에서 실시간 정보 (평점, 가격 등)를 수집하여 검색 결과 제공  
        * 평점, 가격, 배송 정보, 재고 상태 등 다양한 메타데이터를 수집하며, 이를 기반으로 사용자가 원하는 조건에 가장 적합한 제품을 필터링하여 제공합니다.

### 2. 최종 결과

*   Flask 애플리케이션 형태의 간단한 UI 제공  
    * Flask는 경량 웹 프레임워크로, 빠르게 프로토타입을 구현하기에 적합합니다. 이 프로젝트에서는 Flask를 사용해 간단하면서도 직관적인 웹 인터페이스를 제공합니다.

*   사용자는 프롬프트 (검색 조건)와 플랫폼을 선택하여 에이전트 실행  
    * 사용자는 텍스트 입력창에 검색 조건을 입력하고, 체크박스나 드롭다운 메뉴를 통해 검색 대상 플랫폼을 선택할 수 있습니다. 이를 통해 사용자 경험을 단순화하고 맞춤형 검색을 가능하게 합니다.

*   에이전트는 실시간 웹 데이터를 기반으로 검색 조건을 충족하는 제품 목록 제공  
    * 에이전트는 실시간으로 웹을 탐색하여 최신 데이터를 수집하고, 사용자가 지정한 조건에 부합하는 제품 목록을 생성합니다. 이는 동적인 시장 상황을 반영하는 핵심 기능입니다.

*   각 제품에 대한 URL, 제목, 평점 정보 표시  
    * 검색 결과는 각 제품의 세부 정보(제품명, URL 링크, 사용자 평점 등)를 포함하며, 사용자가 바로 해당 제품 페이지로 이동할 수 있도록 설계됩니다.

*   플랫폼별 검색 결과 제공 및 제품 비교 가능 (선택 사항)  
    * 사용자는 Amazon, eBay 등 여러 플랫폼의 결과를 한눈에 비교할 수 있으며, 추가적으로 가격, 평점, 배송 시간 등을 기준으로 정렬하거나 필터링할 수 있는 기능을 선택적으로 구현할 수 있습니다.

*   UI는 사용자 정의 가능 (가격 범위 슬라이더, 추가 플랫폼 등)  
    * UI는 유연하게 설계되어, 사용자가 가격 범위를 슬라이더로 조정하거나 추가 플랫폼을 선택할 수 있는 기능을 지원합니다. 이는 사용자 친화성을 높이고 검색의 유연성을 강화합니다.

### 3. 아키텍처

1.  사용자 요청을 AI 에이전트에게 전달  
    * 사용자가 웹 인터페이스를 통해 입력한 검색 조건과 플랫폼 선택 정보는 Flask 서버를 통해 AI 에이전트로 전달됩니다.

2.  AI 에이전트는 GPT-4o를 기반으로 작동하며, 웹 검색 및 스크래핑 도구 활용  
    * GPT-4o는 강력한 언어 모델로, 자연어 처리와 도구 호출을 통해 복잡한 작업을 수행합니다. 웹 검색 및 스크래핑 도구는 실시간 데이터 수집을 가능하게 합니다.

3.  특정 플랫폼 (예: Amazon)에 대한 전용 도구 사용 가능  
    * Amazon과 같은 특정 플랫폼의 경우, 전용 API나 스크래핑 도구를 활용하여 더 정확하고 빠른 데이터 수집이 가능합니다.

4.  에이전트는 여러 도구를 호출하여 정보를 수집하고, 언어 모델과 정보를 교환  
    * 에이전트는 검색 엔진, 웹 스크래퍼, API 호출 도구 등을 조합하여 데이터를 수집하고, 이를 GPT-4o와 통합하여 최적의 결과를 도출합니다.

5.  최종적으로 구조화된 JSON 형태의 결과물을 웹 페이지에 표시  
    * 수집된 데이터는 JSON 형식으로 구조화되어 Flask 애플리케이션을 통해 웹 페이지에 렌더링됩니다. 이는 데이터의 가독성과 재사용성을 높입니다.

### 4. 개발 준비

1.  **필요 패키지 설치:**  
    *   `python-dotenv`: 환경 변수 로드  
        * `.env` 파일을 통해 API 키와 같은 민감한 정보를 안전하게 관리합니다.  
    *   `flask`: 웹 애플리케이션 구축  
        * Flask는 간단한 웹 서버를 구축하기 위한 파이썬 프레임워크로, 이 프로젝트의 UI를 구현하는 데 사용됩니다.  
    *   `mcp`: 모델 컨텍스트 프로토콜 (Bright Data 도구 연동)  
        * Bright Data의 MCP는 웹 스크래핑 및 데이터 수집을 위한 도구와의 연동을 지원합니다.  
    *   `langchain`: 언어 모델 기반 에이전트 구축  
        * Langchain은 언어 모델과 외부 도구를 통합하여 복잡한 워크플로우를 구현하는 데 사용됩니다.  
    *   `langchain-mcp-adapters`: Langchain과 MCP 연동  
        * Langchain과 Bright Data의 MCP를 연결하여 스크래핑 도구를 효과적으로 활용합니다.  
    *   `langraph`: 에이전트 워크플로우 관리  
        * Langraph는 에이전트의 작업 흐름을 체계적으로 관리하기 위한 도구입니다.  
    *   `langchain-openai`: OpenAI 모델 사용  
        * OpenAI의 GPT-4o 모델을 호출하기 위한 Langchain 모듈입니다.  
    *   `pydantic`: 구조화된 출력 정의  
        * Pydantic은 데이터 검증과 구조화를 위한 파이썬 라이브러리로, 검색 결과를 일관된 형식으로 관리합니다.

2.  **환경 변수 설정 (`.env` 파일):**  
    *   `OPENAI_API_KEY`: OpenAI API 키  
        * GPT-4o 모델을 호출하기 위해 필요한 인증 키입니다.  
    *   `WEB_UNLOCKER_ZONE`: Bright Data Web Unlocker Zone ID  
        * Bright Data의 Web Unlocker를 통해 차단된 웹사이트에 접근할 수 있습니다.  
    *   `BROWSER_O_ZONE`: Bright Data Scraping Browser Zone ID  
        * 동적 웹사이트 스크래핑을 위한 브라우저 존 설정입니다.  
    *   `API_TOKEN`: Bright Data API 키  
        * Bright Data의 API 호출을 인증하기 위한 키입니다.

### 5. 코드 구현

1.  **필요 모듈 임포트:** 필요한 파이썬 패키지들을 임포트합니다.  
    * `flask`, `langchain`, `pydantic` 등 프로젝트에 필요한 모든 모듈을 한 번에 임포트하여 코드의 구조를 명확히 합니다.

2.  **환경 변수 로드:** `load_dotenv()` 함수를 사용하여 `.env` 파일에서 환경 변수를 로드합니다.  
    * 환경 변수를 로드함으로써 민감한 정보를 코드에 하드코딩하지 않고 안전하게 관리할 수 있습니다.

3.  **GPT-4o 모델 정의:** `ChatOpenAI` 클래스를 사용하여 GPT-4o 모델을 정의합니다.  
    * `ChatOpenAI`는 Langchain에서 제공하는 OpenAI 모델 인터페이스로, GPT-4o를 에이전트의 핵심 언어 모델로 설정합니다.

4.  **Bright Data MCP 서버 연결:** Bright Data에서 제공하는 MCP 서버에 연결하기 위한 설정을 구성합니다. API 토큰, 웹 언락커 존, 브라우저 존 등의 정보를 포함합니다.  
    * Bright Data의 MCP 서버는 웹 스크래핑을 위한 인프라를 제공하며, 이를 통해 차단 방지 및 효율적인 데이터 수집이 가능합니다.

5.  **시스템 프롬프트 정의:** 에이전트의 행동 방식을 지정하는 시스템 프롬프트를 정의합니다. 예: 제품 검색 시 검색 엔진 도구 사용, 특정 제품 정보 획득 시 웹 데이터 도구 사용 등  
    * 시스템 프롬프트는 에이전트가 어떤 도구를 언제 사용해야 하는지, 그리고 어떤 방식으로 응답해야 하는지를 명확히 정의합니다.

6.  **사용자 플랫폼 목록 정의:** 사용자에게 제공할 플랫폼 목록을 정의합니다 (Amazon, eBay, Walmart 등).  
    * 플랫폼 목록은 사용자가 선택할 수 있는 옵션으로, 동적으로 확장 가능하도록 설계됩니다.

7.  **데이터 모델 정의 (Pydantic):** 검색 결과의 구조를 정의하는 Pydantic 모델을 생성합니다.  
    *   `Hit`: 제품 URL, 제목, 평점 정보를 담는 모델  
        * 각 제품의 세부 정보를 구조화하여 일관된 데이터 형식을 유지합니다.  
    *   `PlatformBlock`: 플랫폼 이름과 해당 플랫폼의 제품 목록 (`Hit` 리스트)을 담는 모델  
        * 플랫폼별로 검색 결과를 그룹화하여 표시합니다.  
    *   `ProductSearchResponse`: 전체 플랫폼의 검색 결과 (`PlatformBlock` 리스트)를 담는 모델  
        * 모든 검색 결과를 통합하여 JSON 형식으로 반환합니다.

8.  **Flask 애플리케이션 초기화:** Flask 애플리케이션을 초기화하고, Flash 메시지 사용을 위한 secret key 설정  
    * Flash 메시지는 사용자에게 오류나 성공 메시지를 전달하는 데 사용됩니다.

9.  **`run_agent` 함수 정의 (비동기):** 에이전트를 실행하는 비동기 함수를 정의합니다.  
    *   MCP 서버에 연결하고 필요한 도구를 로드합니다.  
        * 비동기 처리를 통해 대량의 웹 요청을 효율적으로 처리합니다.  
    *   `create_react_agent` 함수를 사용하여 에이전트를 생성합니다 (GPT-4o 모델, 도구, 구조화된 출력 형식 지정).  
        * `create_react_agent`는 Langchain에서 제공하는 ReAct 에이전트 생성 함수로, 도구 호출과 추론을 조합합니다.  
    *   사용자 쿼리와 플랫폼 정보를 결합하여 프롬프트 생성  
        * 사용자 입력과 플랫폼 정보를 결합하여 에이전트가 이해할 수 있는 단일 프롬프트를 생성합니다.  
    *   생성된 프롬프트를 에이전트에 전달하고, 결과를 구조화된 JSON 형태로 반환  
        * 결과는 Pydantic 모델을 통해 구조화되어 일관된 출력 형식을 보장합니다.

10. **Flask 라우트 정의 (`/`):**  
    *   GET 요청: 빈 쿼리, 선택된 플랫폼 없음, 응답 없음 상태로 `index.html` 렌더링  
        * 초기 페이지 로드 시 빈 검색 상태를 표시합니다.  
    *   POST 요청:  
        *   폼에서 쿼리 및 선택된 플랫폼 목록 가져오기  
            * HTML 폼에서 사용자가 입력한 데이터를 수집합니다.  
        *   쿼리 또는 플랫폼이 누락된 경우 오류 메시지와 함께 동일 엔드포인트로 리디렉션  
            * 입력 검증을 통해 잘못된 요청을 방지합니다.  
        *   `run_agent` 함수를 실행하여 에이전트 실행  
            * 에이전트가 실시간 데이터를 수집하고 결과를 생성합니다.  
        *   결과를 `index.html`에 전달하여 렌더링  
            * 검색 결과는 Jinja 템플릿을 통해 동적으로 렌더링됩니다.

### 6. UI (HTML) 구현

*   `templates/base.html`: 기본 레이아웃 (헤더, 네비게이션, 스타일링) 정의  
    * 공통 레이아웃을 정의하여 모든 페이지에서 일관된 디자인을 유지합니다.

*   `templates/index.html`:  
    *   검색 쿼리 입력 텍스트 박스 및 플랫폼 선택 체크박스 제공  
        * 사용자가 검색 조건을 입력하고 플랫폼을 선택할 수 있는 직관적인 인터페이스를 제공합니다.  
    *   선택된 플랫폼 및 이전 쿼리 내용 유지  
        * 이전 검색 상태를 유지하여 사용자 경험을 개선합니다.  
    *   Jinja 템플릿 엔진을 사용하여 검색 결과를 반복하여 표시 (플랫폼별로 그룹화)  
        * Jinja를 활용해 동적으로 플랫폼별 결과를 테이블이나 리스트 형태로 표시합니다.

### 7. 실행

1.  터미널에서 `uv run app.py` 또는 `python app.py` 실행  
    * `uv`는 빠른 파이썬 패키지 관리 도구로, 의존성 설치와 실행을 간소화합니다.

2.  웹 브라우저에서 `localhost:8000` 접속  
    * 로컬 서버에 접속하여 웹 인터페이스를 확인할 수 있습니다.

### 8. 결론

Bright Data, Langchain, Python, GPT-4o를 활용하여 실시간 정보를 기반으로 작동하는 e-커머스 AI 에이전트를 구축하는 방법 설명.  
* 이 프로젝트는 최신 기술 스택을 활용하여 동적인 e-커머스 검색 도구를 구현하는 방법을 제시합니다. Bright Data의 강력한 스크래핑 기능, Langchain의 에이전트 워크플로우, GPT-4o의 자연어 처리 능력을 결합하여 사용자 중심의 실시간 검색 경험을 제공합니다.

## 6. `uv`를 Docker를 이용해 프로덕션에 배포하는 방법
- 출처: [How To Use uv in Production - Simple Docker Setup](https://www.youtube.com/watch?v=45bAPTZW16o)


최근 Python 패키지 관리 분야에서 **Rust 기반의 패키지 관리자인 `uv`가 `pip`나 `Poetry`의 대안으로 크게 주목받고 있습니다**. `uv`는 `Poetry`, `pipx`, 일반 `pip`는 물론 가상 환경 기능까지 대체하며 매우 빠른 속도를 자랑합니다. 이 가이드는 `uv`를 Docker와 통합하여 프로덕션 환경에 배포하는 구체적인 방법을 다룹니다.

### **1. `uv` 로컬 프로젝트 설정 (선행 작업)**
프로덕션 배포를 위한 Docker 설정을 하기 전에, 먼저 로컬에서 `uv` 프로젝트를 설정하는 것이 중요합니다:
*   **`uv` 설치**: `pip install uv` (또는 `cargo` 사용) 명령어를 통해 시스템에 `uv`를 설치합니다.
*   **`uv` 프로젝트 초기화**: 작업 디렉토리에서 `uv init`을 실행하여 `uv` 프로젝트를 생성합니다.
*   **패키지 추가**: `uv add fastai yfinance uvicorn`과 같이 필요한 패키지를 추가하여 `uv`가 종속성을 관리하고 `UV.lock` 파일을 생성하도록 합니다. `UV.lock` 파일은 `requirements.txt` 파일과 유사하게 패키지 버전을 고정(lock)하는 역할을 합니다.
*   **애플리케이션 개발**: Fast API와 같은 프레임워크를 사용하여 간단한 애플리케이션을 작성합니다. 예를 들어, `uv run uvicorn main:app` 명령어를 사용하여 로컬에서 애플리케이션이 잘 실행되는지 확인할 수 있습니다.

### **2. Docker 파일을 통한 `uv` 통합 (핵심 단계)**
애플리케이션을 컨테이너화하고 `uv`를 사용하려면 `Dockerfile`을 생성해야 합니다.

*   **기본 이미지 선택**: `Python 3.13 Slim Bookworm`과 같은 파이썬 기본 이미지를 사용합니다.
*   **`uv` 바이너리 설치**:
    *   `uv`는 파이썬 기본 이미지에 포함되어 있지 않으므로 수동으로 설치해야 합니다.
    *   가장 권장되는 방법은 `uv`의 공식 문서에서 제공하는 명령어를 사용하여 온라인 소스에서 `uv` 바이너리를 복사하는 것입니다.
    *   **프로덕션 환경에서는 `uv` 버전을 고정(pin)하는 것이 가장 좋은 습관입니다**. `ghcr.io/astral-sh/uv`와 같은 소스에서 특정 `uv` 버전을 지정하여 복사해야 합니다 (예: `0.6.6` 대신 `latest` 사용 지양).

    ```dockerfile
    COPY --from=ghcr.io/astral-sh/uv:0.6.6 /usr/bin/uv /usr/bin/uv
    COPY --from=ghcr.io/astral-sh/uv:0.6.6 /usr/bin/uvx /usr/bin/uvx
    # 또는 단일 라인으로 복사할 수 있습니다.
    # COPY --from=ghcr.io/astral-sh/uv:0.6.6 /usr/bin/uv* /usr/bin/
    ```
    *   `apt-get` 등을 통해 `uv` 인스톨러 스크립트를 사용하는 방법도 있지만, 위의 바이너리 복사 방법이 Docker 파일에 단일 라인으로 추가하기에 더 편리할 수 있습니다.
*   **애플리케이션 코드 복사**: 로컬 디렉토리의 내용을 `/app` 디렉토리로 복사합니다.
    ```dockerfile
    ADD . /app
    ```
*   **작업 디렉토리 설정**: 컨테이너의 작업 디렉토리를 `/app`으로 설정합니다.
    ```dockerfile
    WORKDIR /app
    ```
*   **종속성 설치 (`uv sync --locked`)**:
    *   **`pip install -r requirements.txt`의 대안으로 `uv sync --locked` 명령어를 사용합니다**. 이는 `UV.lock` 파일에 명시된 종속성을 설치하며, 모든 패키지 버전이 `UV.lock` 파일에 고정(lock)되어 있어 `requirements.txt`와 동일한 역할을 합니다.
    ```dockerfile
    RUN uv sync --locked
    ```
*   **포트 노출**: 애플리케이션이 사용할 포트(예: 8000)를 노출합니다.
    ```dockerfile
    EXPOSE 8000
    ```
*   **애플리케이션 실행 명령어**: `uv run` 명령어를 사용하여 `uvicorn` 또는 다른 웹 서버로 애플리케이션을 실행합니다.
    ```dockerfile
    CMD uv run uvicorn main:app --host 0.0.0.0 --port 8000
    ```

### **3. Docker 이미지 빌드 및 컨테이너 실행**

*   **Docker 이미지 빌드**: Docker 파일을 사용하여 이미지를 빌드합니다. `uv-production`과 같이 태그를 지정할 수 있습니다.
    ```bash
    docker build -t uv-production .
    ```
*   **Docker 컨테이너 실행**: 빌드된 이미지를 사용하여 컨테이너를 실행하고, 포트 매핑을 설정합니다.
    ```bash
    docker run -p 8000:8000 --name uv-prod-container uv-production
    ```
이제 `uv`가 Docker 컨테이너 내에서 애플리케이션의 패키지 관리 및 실행을 담당하게 됩니다. 이 설정을 통해 `uv`의 빠른 속도와 효율성을 활용하면서, Docker를 통한 일관된 배포 환경을 구축할 수 있습니다.

## 7. 로지스틱 회귀를 바닥부터 이해하고 구현하기: 개념부터 코드까지
- 출처: [Logistic Regression From Scratch in Python (Mathematical)](https://www.youtube.com/watch?v=lfTiZS6wI4g)

기계 학습의 중요한 알고리즘 중 하나인 **로지스틱 회귀(Logistic Regression)**에 대해 깊이 있게 다뤄보겠습니다. 이름에 '회귀'가 들어가지만 실제로는 **분류(Classification) 알고리즘**인 로지스틱 회귀가 어떻게 작동하는지, 그 배경에 있는 수학적 원리는 무엇이며, 이를 Python에서 처음부터 어떻게 구현할 수 있는지 자세히 살펴보겠습니다.

### 1. 로지스틱 회귀란 무엇인가?

로지스틱 회귀는 이진 분류(binary classification) 문제에 주로 사용되는 알고리즘입니다. 예를 들어, 이미지가 고양이인지 아닌지(1 또는 0), 학생이 시험에 합격할지 불합격할지(1 또는 0) 등을 예측하는 데 활용됩니다.

가장 중요한 특징은 다음과 같습니다:
*   **분류 알고리즘**: 이름과는 달리, 로지스틱 회귀는 주식 가격이나 온도와 같은 실제 값을 예측하는 것이 아니라 **클래스를 예측**합니다.
*   **확률 출력**: 로지스틱 회귀는 직접적인 클래스 레이블(0 또는 1)을 출력하는 대신, 출력이 특정 클래스(예: 1 또는 '예')일 **확률(0과 1 사이의 값)**을 출력합니다. 예를 들어, 0.768이라는 값은 이미지가 고양이일 확률이 76.8%임을 나타낼 수 있습니다.
*   **'회귀'라는 이름의 유래**: 연속적인 척도에서 확률이라는 수치 값을 예측하기 때문에 '회귀'라는 이름이 붙었으며, 최종 분류는 이 확률 값에 임계값(일반적으로 0.5)을 적용하여 이루어집니다.

### 2. 로지스틱 회귀의 수학적 원리

로지스틱 회귀의 핵심은 일반적인 선형 회귀의 결과에 특정 함수를 적용하여 값을 0과 1 사이로 강제하는 것입니다.

*   **선형 결합 (Logit)**:
    *   먼저 입력 특징(X)에 일련의 **파라미터(theta)**를 곱하여 선형 결합을 수행합니다. 이는 일반적인 선형 회귀와 유사합니다. 각 특징에 대한 가중치와 **바이어스(절편)** 항이 포함됩니다.
    *   이 선형 결합의 결과는 **로짓(Logit, Z)**이라고 불리며, 이 값은 어떤 숫자도 될 수 있습니다.

*   **시그모이드 함수 (Sigmoid Function)**:
    *   로짓(Z) 값을 **시그모이드 함수(Sigmoid Function)**에 입력하여 0과 1 사이의 값, 즉 예측된 확률(H)로 변환합니다.
    *   시그모이드 함수의 정의는 다음과 같습니다: `σ(z) = 1 / (1 + e^(-z))`.
    *   이 함수는 입력 Z가 음의 무한대로 가면 0에 수렴하고, 양의 무한대로 가면 1에 수렴하여 어떤 입력값에 대해서도 항상 0과 1 사이의 부드러운 출력을 제공합니다.

*   **최적 파라미터 찾기 (경사 하강법)**:
    *   모델의 목표는 **최적의 파라미터(theta)**를 찾는 것입니다. 이를 위해 **손실 함수(Loss Function)** 또는 **비용 함수(Cost Function)**를 정의하고 이를 최소화하는 방식으로 파라미터를 조정합니다. 이 최적화 과정은 **경사 하강법(Gradient Descent)**을 통해 이루어집니다.

*   **크로스 엔트로피 손실 함수 (Cross-Entropy Loss Function)**:
    *   초기에는 예측이 실제 값과 얼마나 일치하는지를 나타내는 **우도 함수(Likelihood Function)**를 사용합니다.
    *   이 우도 함수는 최적화를 위해 다음과 같은 변환을 거쳐 **크로스 엔트로피 손실 함수(Cross-Entropy Loss Function)**가 됩니다:
        1.  **부정**: 우도 함수는 최댓값을 찾는 반면, 경사 하강법은 최솟값을 찾으므로 우도 함수에 -1을 곱하여 손실 함수로 만듭니다.
        2.  **로그 변환**: 수치적 안정성을 위해 **자연로그(ln)**를 적용합니다. 로그는 곱셈을 덧셈으로 변환하여 계산을 용이하게 합니다.
        3.  **스케일링**: 인스턴스 수(M)로 나누어 스케일 불변성을 확보합니다.
    *   이렇게 변환된 함수는 **음의 평균 로그 우도(Negative Average Log Likelihood)**라고도 불리며, 신경망 훈련에서 자주 사용되는 **크로스 엔트로피 손실 함수**와 동일합니다.

*   **경사 (Gradient) 계산**:
    *   손실 함수를 최소화하기 위해 각 파라미터(`theta_j`)에 대한 손실 함수의 **편도함수(Partial Derivative)**, 즉 **경사(Gradient)**를 계산합니다.
    *   경사는 **연쇄 법칙(Chain Rule of Calculus)**을 사용하여 계산되며, 이는 손실 함수가 예측된 확률(H)에 의존하고, H는 로짓(Z)에 의존하며, Z는 파라미터(theta)에 의존하기 때문입니다.
    *   최종적인 경사 공식은 단일 인스턴스에 대해 `(1/m) * (y - h) * x_j`로 간결하게 표현될 수 있으며, 벡터/행렬 형태로 모든 인스턴스에 대해 `(1/m) * X^T * (Y - H)`가 됩니다.

*   **파라미터 업데이트**:
    *   계산된 경사는 손실 함수가 가장 가파르게 증가하는 방향을 나타냅니다.
    *   경사 하강법에서는 이 경사 방향의 반대(감소) 방향으로 **학습률(Learning Rate, alpha)**만큼 이동하여 파라미터를 업데이트합니다: `theta = theta - alpha * gradient`. 이 과정을 여러 번 반복하여 최적의 파라미터를 찾습니다.

### 3. Python을 이용한 로지스틱 회귀 구현 (From Scratch)

위에서 설명한 수학적 개념들은 Python 코드로 구현될 수 있습니다. 여기서는 **NumPy**만 외부 패키지로 사용하여 효율적인 벡터 및 행렬 연산을 수행합니다.

*   **`sigmoid(z)` 함수**:
    ```python
    import numpy as np

    def sigmoid(z):
        return 1.0 / (1 + np.exp(-z))
    ```
   

*   **`calculate_gradient(theta, X, y)` 함수**:
    *   주어진 파라미터(`theta`), 특징 행렬(`X`), 실제 결과 벡터(`y`)를 사용하여 경사를 계산합니다.
    *   수학적으로 유도된 `(1/m) * X^T * (Y - H)` 공식을 직접 적용합니다. 여기서 `H`는 `sigmoid(X @ theta)`입니다.

*   **`gradient_descent(X, y, learning_rate, num_iterations, tolerance)` 함수**:
    *   **바이어스 추가**: 입력 특징 `X`에 편향(bias) 항을 위해 각 인스턴스 앞에 1을 추가하는 차원을 붙입니다.
    *   파라미터 `theta`는 초기에 0으로 설정됩니다.
    *   지정된 반복 횟수(`num_iterations`) 동안 `calculate_gradient`를 호출하고 `theta`를 업데이트하는 과정을 반복합니다.
    *   경사의 크기가 `tolerance`보다 작으면 조기 종료(early stopping)할 수 있습니다.

*   **`predict_proba(x, theta)` 함수**:
    *   입력 `x`와 학습된 `theta`를 사용하여 예측 확률(0과 1 사이의 값)을 반환합니다. 이는 `sigmoid(X_B @ theta)`를 계산하는 것입니다.

*   **`predict(x, theta, threshold=0.5)` 함수**:
    *   `predict_proba`로 얻은 확률 값을 입력받아, 주어진 `threshold` (기본값 0.5) 이상이면 1, 그렇지 않으면 0을 반환하여 최종 클래스를 예측합니다.

### 4. 구현된 모델 평가

구현된 로지스틱 회귀 모델의 성능을 검증하기 위해 **scikit-learn** 라이브러리의 유방암 데이터셋을 사용합니다. 여기서 scikit-learn은 데이터셋 로드, 데이터 전처리(StandardScaler), 데이터 분할(train_test_split), 정확도 평가(accuracy_score)에만 사용되며, 로지스틱 회귀 자체의 구현에는 전혀 사용되지 않습니다.

이 예시에서는 98.25%의 훈련 정확도와 97.36%의 테스트 정확도를 달성하여, 우리가 직접 구현한 로지스틱 회귀 모델이 실제 분류 문제에 효과적으로 작동함을 보여줍니다.

### 5. 결론

이 포스팅을 통해 로지스틱 회귀가 단순히 블랙박스처럼 작동하는 것이 아니라, 명확한 수학적 원리와 그에 따른 코딩 논리를 가지고 있음을 이해하셨기를 바랍니다. 이처럼 밑바닥부터 알고리즘을 구현하는 것은 기계 학습에 대한 깊은 통찰력을 제공하며, 더 복잡한 모델을 이해하는 데 큰 도움이 됩니다.

