---
title: 3ì°¨ì‹œ 4:NLP Basic
layout: single
classes: wide
categories:
  - NLP
toc: true # ì´ í¬ìŠ¤íŠ¸ì—ì„œ ëª©ì°¨ë¥¼ í™œì„±í™”
toc_sticky: true # ëª©ì°¨ë¥¼ ê³ ì •í• ì§€ ì—¬ë¶€ (ì„ íƒ ì‚¬í•­)
---

# 1. NLP

## 1. **NLPì˜ ê¸°ë³¸ê°œë…**

### 1.1 **NLPë€**
NLPëŠ” **í…ìŠ¤íŠ¸ ë°ì´í„°** ë˜ëŠ” **ìŒì„± ë°ì´í„°**ë¥¼ ë¶„ì„í•˜ê³  ì´í•´í•˜ì—¬, ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ì™€ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì£¼ìš” ì‘ì—…ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì´ í¬í•¨ë©ë‹ˆë‹¤:
- **í…ìŠ¤íŠ¸ ë¶„ë¥˜**: í…ìŠ¤íŠ¸ë¥¼ íŠ¹ì • ë²”ì£¼ë¡œ ë¶„ë¥˜ (ì˜ˆ: ìŠ¤íŒ¸ ë©”ì¼ í•„í„°ë§)
- **ê°ì • ë¶„ì„**: í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„ (ì˜ˆ: ê¸ì •/ë¶€ì • íŒë‹¨)
- **ê¸°ê³„ ë²ˆì—­**: í•œ ì–¸ì–´ì—ì„œ ë‹¤ë¥¸ ì–¸ì–´ë¡œ í…ìŠ¤íŠ¸ ë²ˆì—­ (ì˜ˆ: Google ë²ˆì—­)
- **ì§ˆë¬¸ ì‘ë‹µ**: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ì˜ˆ: ì±—ë´‡)
- **í…ìŠ¤íŠ¸ ìš”ì•½**: ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì§§ê²Œ ìš”ì•½
- **ê°œì²´ëª… ì¸ì‹**: í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ëŒ, ì¥ì†Œ, ì¡°ì§ ë“±ì„ ì‹ë³„
- **í…ìŠ¤íŠ¸ ìƒì„±**: ì£¼ì–´ì§„ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ìƒì„± (ì˜ˆ: GPT ëª¨ë¸)

### 1.2 NLPì˜ í•„ìš”ì„±

- **ë°ì´í„°ì˜ í­ë°œì  ì¦ê°€**
    - ì¸í„°ë„·, ì†Œì…œ ë¯¸ë””ì–´, ë©”ì‹œì§€ í”Œë«í¼ ë“±ì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ê¸‰ì¦í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    - ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  í™œìš©í•˜ë ¤ë©´ NLP ê¸°ìˆ ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.

- **ìë™í™”ì™€ íš¨ìœ¨ì„±**
    - NLPë¥¼ í†µí•´ ë°˜ë³µì ì´ê³  ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ëŠ” ì‘ì—…ì„ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ì˜ˆ: ê³ ê° ë¬¸ì˜ ìë™ ì‘ë‹µ, ë¬¸ì„œ ìš”ì•½, ë¦¬í¬íŠ¸ ìƒì„± ë“±

- **ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ**
    - ì±—ë´‡, ìŒì„± ë³´ì¡°, ì¶”ì²œ ì‹œìŠ¤í…œ ë“±ì€ NLPë¥¼ í†µí•´ ë” ìì—°ìŠ¤ëŸ½ê³  ì§ê´€ì ì¸ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤.
    - ì˜ˆ: Amazon Alexa, Google Assistant

- **ì˜ì‚¬ê²°ì • ì§€ì›**
    - í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¹„ì¦ˆë‹ˆìŠ¤, ì˜ë£Œ, ê¸ˆìœµ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì˜ì‚¬ê²°ì •ì„ ì§€ì›í•©ë‹ˆë‹¤.
    - ì˜ˆ: ì¦ê¶Œ ë¦¬í¬íŠ¸ ë¶„ì„, í™˜ì ê¸°ë¡ ë¶„ì„

- **ì–¸ì–´ ì¥ë²½ í•´ì†Œ**
    - ê¸°ê³„ ë²ˆì—­ ê¸°ìˆ ì€ ì–¸ì–´ ê°„ ì†Œí†µì˜ ì¥ë²½ì„ ë‚®ì¶”ê³ , ê¸€ë¡œë²Œ í˜‘ë ¥ì„ ì´‰ì§„í•©ë‹ˆë‹¤.
    - ì˜ˆ: Google ë²ˆì—­, DeepL

### 1.3 NLPì˜ í™œìš© ë¶„ì•¼
- **ê³ ê° ì„œë¹„ìŠ¤**: ì±—ë´‡, ìë™ ì‘ë‹µ ì‹œìŠ¤í…œ.
- **ì˜ë£Œ**: í™˜ì ê¸°ë¡ ë¶„ì„, ì§ˆë³‘ ì§„ë‹¨ ì§€ì›.
- **ê¸ˆìœµ**: ì‹œì¥ ë¶„ì„, ê°ì • ë¶„ì„ì„ í†µí•œ ì£¼ê°€ ì˜ˆì¸¡.
- **êµìœ¡**: ìë™ ì±„ì , ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ì œê³µ.
- **ì—”í„°í…Œì¸ë¨¼íŠ¸**: ì˜í™” ì¶”ì²œ, ê²Œì„ ë‚´ ëŒ€í™” ì‹œìŠ¤í…œ.

### 1.4 NLPì˜ ì£¼ìš” ê°œë…
- **í† í°í™”(Tokenization)**: í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ë˜ëŠ” ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬.
- **ì„ë² ë”©(Embedding)**: ë‹¨ì–´ë¥¼ ë²¡í„° í˜•íƒœë¡œ ë³€í™˜ (ì˜ˆ: Word2Vec, GloVe).
- **ì‹œí€€ìŠ¤ ëª¨ë¸ë§**: RNN, LSTM, GRU ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬.
- **íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)**: Self-Attention ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•œ ê³ ê¸‰ ëª¨ë¸ (ì˜ˆ: BERT, GPT).
- **ì „ì²˜ë¦¬(Preprocessing)**: í…ìŠ¤íŠ¸ ì •ê·œí™”, ë¶ˆìš©ì–´ ì œê±°, ì–´ê°„ ì¶”ì¶œ ë“±.

### 1.5 **NLPì˜ ì£¼ìš” íë¦„**
- **í…ìŠ¤íŠ¸ ìˆ˜ì§‘**: ì›¹ í¬ë¡¤ë§, API, ë°ì´í„°ë² ì´ìŠ¤ ë“±ì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘
- **í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**: í† í°í™”, ì •ì œ, ì •ê·œí™”, ë¶ˆìš©ì–´ ì œê±°
- **íŠ¹ì„± ì¶”ì¶œ**: Bag of Words, TF-IDF, Word Embedding ë“±ì„ ì´ìš©í•´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜ ë°ì´í„°ë¡œ ë³€í™˜
- **ëª¨ë¸ êµ¬ì¶•**: ë¶„ë¥˜, êµ°ì§‘í™”, ê°ì„± ë¶„ì„, ê°œì²´ëª… ì¸ì‹ ë“± ëª©ì ì— ë§ëŠ” ëª¨ë¸ êµ¬ì¶•
- **ëª¨ë¸ í‰ê°€ ë° ê°œì„ **: ì •í™•ë„, ì¬í˜„ìœ¨, F1 ìŠ¤ì½”ì–´ ë“±ì„ í†µí•œ ëª¨ë¸ í‰ê°€ ë° ê°œì„ 

## 2. NLP ì£¼ìš”ê¸°ìˆ 
### 2.1 **í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**
1. **í† í°í™”(Tokenization)**: í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´, ë¬¸ì¥ ë“±ì˜ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„ë¦¬

    ```python
    import nltk
    from nltk.tokenize import word_tokenize, sent_tokenize

    nltk.download('punkt')  # í† í°í™”ë¥¼ ìœ„í•œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    nltk.download('punkt_tab')

    text = "ChatGPT is amazing! It helps with natural language processing."

    # ë‹¨ì–´ í† í°í™”
    word_tokens = word_tokenize(text)
    print("Word Tokenization:", word_tokens)

    # ë¬¸ì¥ í† í°í™”
    sent_tokens = sent_tokenize(text)
    print("Sentence Tokenization:", sent_tokens)
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    Word Tokenization: ['ChatGPT', 'is', 'amazing', '!', 'It', 'helps', 'with', 'natural', 'language', 'processing', '.']
    Sentence Tokenization: ['ChatGPT is amazing!', 'It helps with natural language processing.']
    ```


2. **ì •ì œ(Cleaning)**: ë¶ˆí•„ìš”í•œ ë¬¸ì, ê¸°í˜¸ ì œê±°

    ```python
    import re

    text = "This is an Example! NLP is AWESOME!!!"
    lower_text = text.lower()

    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    # r'[^a-zA-Z0-9\s]': ì•ŒíŒŒë²³, ìˆ«ì, ê³µë°±ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì
    clean_text = re.sub(r'[^a-zA-Z0-9\s]', '', lower_text)
    print("Cleaned Text:", clean_text)
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    Cleaned Text: this is an example nlp is awesome
    ```



3. **ì •ê·œí™”(Normalization)**: ëŒ€ì†Œë¬¸ì í†µì¼, ì–´ê·¼ ì¶”ì¶œ(Stemming), í‘œì œì–´ ì¶”ì¶œ(Lemmatization)

    - ì–´ê·¼ ì¶”ì¶œ(Stemming) 
        - ì–´ê·¼ ì¶”ì¶œì€ ë‹¨ì–´ì˜ ì–´ê°„(Stem)ë§Œ ë‚¨ê¸°ê³  ì ‘ì‚¬(Suffix)ë¥¼ ì œê±°í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.
        - ëŒ€í‘œì ì¸ ë°©ì‹ìœ¼ë¡œ Porter Stemmerì™€ Lancaster Stemmer

        ```python
        from nltk.stem import PorterStemmer, LancasterStemmer

        stemmer1 = PorterStemmer()
        stemmer2 = LancasterStemmer()

        words = ["running", "flies", "happily", "studies", "better"]

        print("Porter Stemmer ê²°ê³¼:")
        print([stemmer1.stem(word) for word in words])

        print("\nLancaster Stemmer ê²°ê³¼:")
        print([stemmer2.stem(word) for word in words])

        ```
    - í‘œì œì–´ ì¶”ì¶œ(Lemmatization):
        - í‘œì œì–´ ì¶”ì¶œì€ ë‹¨ì–´ì˜ ì›í˜•(ê¸°ë³¸ ì‚¬ì „í˜•, Lemma) ì„ ì°¾ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
        - ì´ ë°©ì‹ì€ ë¬¸ë§¥ê³¼ í’ˆì‚¬ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì œê³µ

        ```python
        import spacy

        nlp = spacy.load("en_core_web_sm")

        text = "running flies happily studies better"
        doc = nlp(text)

        print("í‘œì œì–´ ì¶”ì¶œ ê²°ê³¼:")
        for token in doc:
            print(token.text, "â†’", token.lemma_)
        ```

    - í•œêµ­ì–´: ì–´ê·¼ ì¶”ì¶œë³´ë‹¤ í˜•íƒœì†Œ ë¶„ì„ì„ í™œìš©í•œ ì›í˜• ë³µì›ì´ ë” íš¨ê³¼ì 

        ```python
        from konlpy.tag import Okt

        okt = Okt()
        text = "ë‹¬ë ¤ê°€ëŠ” ê°•ì•„ì§€ê°€ ê·€ì—½ìŠµë‹ˆë‹¤."

        # í˜•íƒœì†Œ ë¶„ì„ í›„ ì›í˜• ì¶œë ¥
        morphs = okt.pos(text, stem=True)
        print(morphs)

        ```

    - `konlpy.tag`ì˜ ì£¼ìš” í˜•íƒœì†Œ ë¶„ì„ê¸° ì •ë¦¬

        | ë¶„ì„ê¸° | ì„¤ëª… | ì¥ì  | ë‹¨ì  | ì¶”ì²œ ìš©ë„ |
        |--------|------|------|------|-----------|
        | **Okt (Twitter)** | íŠ¸ìœ„í„°ì—ì„œ ê°œë°œëœ ë¶„ì„ê¸°. `konlpy`ì˜ ê¸°ë³¸ ë¶„ì„ê¸° | ë¹ ë¥´ê³ , ì‹ ì¡°ì–´/ì¸í„°ë„· ì–´íœ˜ì— ê°•í•¨ | ì •í™•ë„ëŠ” ì¤‘ê°„ ìˆ˜ì¤€, ë³µí•©ì–´ ë¶„ì„ ì•½í•¨ | ê°„ë‹¨í•œ í† í°í™”, íŠ¸ìœ„í„°/ëŒ“ê¸€ ë¶„ì„ |
        | **Kkma (ê¼¬ê¼¬ë§ˆ)** | ì—°ì„¸ëŒ€í•™êµì—ì„œ ê°œë°œí•œ ë¶„ì„ê¸° | í’ˆì‚¬ íƒœê¹…ê³¼ ë¬¸ì¥ ë¶„í•  ê¸°ëŠ¥ ê°•ë ¥ | ëŠë¦¬ê³ , ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„± ë†’ìŒ | ë¬¸ì¥ ë¶„í• , í’ˆì‚¬ íƒœê¹… ì¤‘ì‹¬ ì‘ì—… |
        | **Komoran** | ì´ì§€íŠ¸ë¡ ì—ì„œ ê°œë°œí•œ ë¶„ì„ê¸° | ì •í™•ë„ì™€ ì†ë„ ê· í˜• ì¡í˜, ë³µí•©ëª…ì‚¬ ë¶„ì„ ê°•ì  | ì„¤ì¹˜ ì‹œ ì¶”ê°€ ì„¤ì • í•„ìš” | ì¼ë°˜ NLP, ëª…ì‚¬ ì¶”ì¶œ, ê°ì„± ë¶„ì„ |
        | **Hannanum** | í•œë‚˜ëˆ” í”„ë¡œì íŠ¸ (í•œêµ­ì „ìí†µì‹ ì—°êµ¬ì›) | ëª…ì‚¬, ë™ì‚¬ ë¶„ì„ ì •í™•ë„ ë†’ìŒ | ì „ì²˜ë¦¬ í•„ìš”, ëŠë¦¼ | í•™ìˆ ì  ì—°êµ¬, ê³ ì •ë°€ ë¶„ì„ |
        | **Mecab-KO** *(konlpy ì™¸ë¶€)* | ì¼ë³¸ Mecab ê¸°ë°˜ í•œêµ­ì–´ ë²„ì „ | ë§¤ìš° ë¹ ë¥´ê³  ì •í™•, ë³µí•©ì–´ ë¶„ì„ íƒì›” | konlpy ê¸°ë³¸ íŒ¨í‚¤ì§€ ì•„ë‹˜, ë³„ë„ ì„¤ì¹˜ | ëŒ€ìš©ëŸ‰ ì²˜ë¦¬, ìƒì—…ìš© ì‹œìŠ¤í…œ |


4. **ë¶ˆìš©ì–´ ì œê±°(Stopwords Removal)**: 'the', 'a', 'is'ì™€ ê°™ì€ ë¶„ì„ì— í° ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ ì œê±°

- ë¶ˆí•„ìš”í•œ ë‹¨ì–´(ì˜ˆ: "is", "an", "the")ë¥¼ ì œê±°í•˜ì—¬ í•µì‹¬ ë‹¨ì–´ë§Œ ë‚¨ê¹ë‹ˆë‹¤.  
    ```python
    from nltk.corpus import stopwords

    nltk.download('stopwords')

    words = ["this", "is", "an", "example", "of", "stopword", "removal"]
    filtered_words = [word for word in words if word not in stopwords.words('english')]

    print("Filtered Words:", filtered_words)
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    Filtered Words: ['example', 'stopword', 'removal']
    ```

### 2.2 **í˜•íƒœì†Œ ë¶„ì„**

1. **í˜•íƒœì†Œ ë¶„ì„(Morphological Analysis) ì‹¤ìŠµ**  
- í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•´ ë‹¨ì–´ì˜ ì–´ê·¼, í’ˆì‚¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.  

    ```python
    import spacy

    nlp = spacy.load("en_core_web_sm")
    text = "Running faster is good for your health."

    # í˜•íƒœì†Œ ë¶„ì„ ìˆ˜í–‰
    doc = nlp(text)
    for token in doc:
        print(token.text, "â†’", token.lemma_, "/", token.pos_)
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    Running â†’ run / VERB
    faster â†’ fast / ADV
    is â†’ be / AUX
    good â†’ good / ADJ
    for â†’ for / ADP
    your â†’ your / PRON
    health â†’ health / NOUN
    . â†’ . / PUNCT
    ```

2. **í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ (KoNLPy í™œìš©)**

    ```python
    from konlpy.tag import Okt

    okt = Okt()
    text = "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¬ë¯¸ìˆìŠµë‹ˆë‹¤."

    # í˜•íƒœì†Œ ë¶„ì„
    morphs = okt.morphs(text)
    print("í˜•íƒœì†Œ:", morphs)

    # í’ˆì‚¬ íƒœê¹…
    pos_tags = okt.pos(text)
    print("í’ˆì‚¬ íƒœê¹…:", pos_tags)
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    í˜•íƒœì†Œ: ['ìì—°ì–´', 'ì²˜ë¦¬', 'ëŠ”', 'ì¬ë¯¸ìˆ', 'ìŠµë‹ˆë‹¤', '.']
    í’ˆì‚¬ íƒœê¹…: [('ìì—°ì–´', 'Noun'), ('ì²˜ë¦¬', 'Noun'), ('ëŠ”', 'Josa'), ('ì¬ë¯¸ìˆ', 'Adjective'), ('ìŠµë‹ˆë‹¤', 'Eomi'), ('.', 'Punctuation')]
    ```


### 2.3 í…ìŠ¤íŠ¸ í‘œí˜„
1. **Bag of Words**: ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ë¥¼ ë²¡í„°ë¡œ í‘œí˜„
    ```python
    from sklearn.feature_extraction.text import CountVectorizer

    # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    documents = [
        "I love NLP",
        "NLP is fascinating",
        "I enjoy learning NLP"
    ]

    # CountVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ Bag of Words ìƒì„±
    vectorizer = CountVectorizer()
    bow_matrix = vectorizer.fit_transform(documents)

    # ê²°ê³¼ ì¶œë ¥
    print("ë‹¨ì–´ ëª©ë¡:", vectorizer.get_feature_names_out())
    print("Bag of Words í–‰ë ¬:\n", bow_matrix.toarray())
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    ë‹¨ì–´ ëª©ë¡: ['enjoy' 'fascinating' 'is' 'learning' 'love' 'nlp']
    Bag of Words í–‰ë ¬:
    [[0 0 0 0 1 1]
    [0 1 1 0 0 1]
    [1 0 0 1 0 1]]
    ```


2. **TF-IDF**: ë‹¨ì–´ ë¹ˆë„ì™€ ë¬¸ì„œ ë¹ˆë„ì˜ ì—­ìˆ˜ë¥¼ ê³±í•œ ê°’ìœ¼ë¡œ ì¤‘ìš”ë„ í‘œí˜„

    ```python
    from sklearn.feature_extraction.text import TfidfVectorizer

    # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    documents = [
        "I love NLP",
        "NLP is fascinating",
        "I enjoy learning NLP"
    ]

    # TfidfVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ TF-IDF í–‰ë ¬ ìƒì„±
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(documents)

    # ê²°ê³¼ ì¶œë ¥
    print("ë‹¨ì–´ ëª©ë¡:", vectorizer.get_feature_names_out())
    print("TF-IDF í–‰ë ¬:\n", tfidf_matrix.toarray())
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:** 
    ```
    ë‹¨ì–´ ëª©ë¡: ['enjoy' 'fascinating' 'is' 'learning' 'love' 'nlp']
    TF-IDF í–‰ë ¬:
    [[0.         0.         0.         0.         0.70710678 0.70710678]
    [0.         0.70710678 0.70710678 0.         0.         0.70710678]
    [0.57735027 0.         0.         0.57735027 0.         0.57735027]]
    ```

3. **Word Embedding**: Word2Vec, GloVe, FastText ë“±ì„ ì´ìš©í•œ ë‹¨ì–´ì˜ ì˜ë¯¸ì  í‘œí˜„
    - ë‹¨ì–´ë¥¼ ë²¡í„° ê³µê°„ì— í‘œí˜„í•˜ì—¬ ë‹¨ì–´ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    - Word2Vec ì‚¬ìš©
    
    ```python
    from gensim.models import Word2Vec

    # ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    sentences = [
        ["I", "love", "NLP"],
        ["NLP", "is", "fascinating"],
        ["I", "enjoy", "learning", "NLP"]
    ]

    # Word2Vec ëª¨ë¸ í•™ìŠµ
    model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=1)

    # ë‹¨ì–´ "NLP"ì˜ ë²¡í„° í‘œí˜„
    nlp_vector = model.wv["NLP"]
    print("'NLP'ì˜ ë²¡í„° í‘œí˜„:", nlp_vector)

    # ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°
    # ë‹¨ì–´ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•  ë•Œ **ì½”ì‚¬ì¸ ìœ ì‚¬ë„(Cosine Similarity(-1~1)
    similar_words = model.wv.most_similar("NLP")
    print("'NLP'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´:", similar_words)
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:** 
    ```
    'NLP'ì˜ ë²¡í„° í‘œí˜„: [ 0.12345678 -0.23456789  0.34567891 ... ]
    'NLP'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´: [('love', 0.98765432), ('fascinating', 0.87654321), ...]
    ```

## 3. ì–¸ì–´ ëª¨ë¸
### 3.1 **í†µê³„ì  ì–¸ì–´ ëª¨ë¸**: N-gram ëª¨ë¸
- N-gram ëª¨ë¸ì€ ì´ì „ N-1ê°œì˜ ë‹¨ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í†µê³„ì  ëª¨ë¸ì…ë‹ˆë‹¤.

    ```python
    from nltk import ngrams
    from collections import defaultdict, Counter

    # ìƒ˜í”Œ ë¬¸ì¥
    sentence = "I love natural language processing"

    # 2-gram ëª¨ë¸ ìƒì„±
    n = 2
    bigrams = list(ngrams(sentence.split(), n))

    # ë¹ˆë„ìˆ˜ ê³„ì‚°
    bigram_freq = Counter(bigrams)

    # ê²°ê³¼ ì¶œë ¥
    for bigram, freq in bigram_freq.items():
        print(f"{bigram}: {freq}")
    ```

    ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
    ```
    ('I', 'love'): 1
    ('love', 'natural'): 1
    ('natural', 'language'): 1
    ('language', 'processing'): 1
    ```

### 3.2 **ì‹ ê²½ë§ ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸**: RNN, LSTM, GRU
- RNN, LSTM, GRUëŠ” ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì‹ ê²½ë§ ëª¨ë¸ì…ë‹ˆë‹¤.

    ```python
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    import numpy as np
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Embedding, SimpleRNN, Dense

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë°ì´í„°
    texts = [
        "I love natural language processing",
        "NLP is a fascinating field",
        "I enjoy learning new things"
    ]

    # í† í¬ë‚˜ì´ì € ê°ì²´ ìƒì„± ë° ë‹¨ì–´ ì‚¬ì „ êµ¬ì¶•
    tokenizer = Tokenizer(num_words=10000)
    tokenizer.fit_on_texts(texts)

    # í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
    sequences = tokenizer.texts_to_sequences(texts)

    # ì…ë ¥ ë°ì´í„° (ë‹¨ì–´ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤)
    input_sequences = np.array(sequences)

    # íƒ€ê²Ÿ ë°ì´í„° (ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ìœ„í•œ ë ˆì´ë¸”)
    target_words = np.array([5, 10, 14])  # ê° ì‹œí€€ìŠ¤ì˜ ë‹¤ìŒ ë‹¨ì–´

    # RNN ëª¨ë¸ ì •ì˜
    vocab_size = 10000
    embedding_dim = 128
    rnn_units = 64

    model = Sequential([
        Embedding(vocab_size, embedding_dim),
        SimpleRNN(rnn_units),
        Dense(vocab_size, activation='softmax')
    ])

    # ëª¨ë¸ ì»´íŒŒì¼
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

    # ëª¨ë¸ í•™ìŠµ
    model.fit(input_sequences, target_words, epochs=10, batch_size=1)

    # ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ë°ì´í„°
    new_text = "I enjoy learning"

    # ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
    new_sequence = tokenizer.texts_to_sequences([new_text])

    # íŒ¨ë”© ì¶”ê°€ (ëª¨ë¸ ì…ë ¥ ê¸¸ì´ì— ë§ì¶”ê¸°)
    new_sequence = pad_sequences(new_sequence, maxlen=5)

    # ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡
    predictions = model.predict(new_sequence)

    # ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ë‹¨ì–´ ì¸ë±ìŠ¤ ì°¾ê¸°
    predicted_word_index = np.argmax(predictions, axis=-1)

    # ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜
    predicted_word = tokenizer.index_word.get(predicted_word_index[0], "UNK")

    print("ìƒˆë¡œìš´ ì…ë ¥ ì‹œí€€ìŠ¤:", new_sequence)
    print("ì˜ˆì¸¡ëœ ë‹¤ìŒ ë‹¨ì–´:", predicted_word)
    ```

### 3.3 **íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸**: BERT, GPT, T5
- íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì€ self-attention ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

1. **BERTë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜**

    ```python
    
    from transformers import BertTokenizer, TFBertForSequenceClassification
    import tensorflow as tf

    # BERT í† í¬ë‚˜ì´ì € ë¡œë“œ
    # bert-base-uncasedëŠ” ì˜ì–´ í…ìŠ¤íŠ¸ì— íŠ¹í™”ëœ ëª¨ë¸ì´ë¯€ë¡œ, 
    # í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ë ¤ë©´ bert-base-multilingual-uncased ë˜ëŠ” í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸(ì˜ˆ: klue/bert-base)ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸
    text = "I love natural language processing"

    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(text, return_tensors='tf', truncation=True, padding=True)

    # BERT ëª¨ë¸ ë¡œë“œ
    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')

    # ì˜ˆì¸¡
    outputs = model(inputs)
    logits = outputs.logits

    # ê²°ê³¼ ì¶œë ¥
    print(logits)
    ```

2. **GPTë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„±**
    ```python
    from transformers import GPT2Tokenizer, TFGPT2LMHeadModel
    import tensorflow as tf

    # GPT-2 í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

    # GPT-2ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ pad_tokenì´ ì—†ìœ¼ë¯€ë¡œ eos_tokenì„ pad_tokenìœ¼ë¡œ ì„¤ì •
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

    # GPT-2 ëª¨ë¸ ë¡œë“œ
    model = TFGPT2LMHeadModel.from_pretrained('gpt2')

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸
    text = "Once upon a time"

    # í† í¬ë‚˜ì´ì§• (attention_mask í¬í•¨)
    inputs = tokenizer(text, return_tensors='tf', return_attention_mask=True, padding=True)

    # í…ìŠ¤íŠ¸ ìƒì„±
    outputs = model.generate(
        input_ids=inputs['input_ids'],
        attention_mask=inputs['attention_mask'],
        max_length=50,
        num_return_sequences=1,
        do_sample=True,  # ìƒ˜í”Œë§ í™œì„±í™”
        top_k=50,        # ìƒìœ„ 50ê°œ í† í°ì—ì„œ ìƒ˜í”Œë§
        top_p=0.95,      # ëˆ„ì  í™•ë¥  95% ë‚´ì—ì„œ ìƒ˜í”Œë§
        no_repeat_ngram_size=2  # 2-gram ë°˜ë³µ ë°©ì§€
    )

    # ê²°ê³¼ ì¶œë ¥
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(generated_text)
    ```

3. **T5ë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìš”ì•½**
    ```python
    from transformers import T5Tokenizer, TFT5ForConditionalGeneration

    # T5 í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = T5Tokenizer.from_pretrained('t5-small')

    # T5 ëª¨ë¸ ë¡œë“œ
    model = TFT5ForConditionalGeneration.from_pretrained('t5-small')

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸
    text = "Natural language processing is a field of artificial intelligence."

    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer.encode("summarize: " + text, return_tensors='tf', max_length=512, truncation=True)

    # ìš”ì•½ ìƒì„±
    outputs = model.generate(inputs, max_length=50, num_beams=4, early_stopping=True)

    # ê²°ê³¼ ì¶œë ¥
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(summary)
    ```

---

# 2. ì‹¤ìŠµ ì˜ˆì œ

## 1. WordCloud
- **WordCloud**ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ë‹¨ì–´ë¥¼ í¬ê²Œ, ë¹ˆë„ìˆ˜ê°€ ë‚®ì€ ë‹¨ì–´ë¥¼ ì‘ê²Œ í‘œí˜„í•œ ì‹œê°í™” ë„êµ¬ì…ë‹ˆë‹¤.
- í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ì£¼ìš” í‚¤ì›Œë“œë¥¼ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆì–´, ë°ì´í„° ë¶„ì„ì˜ ì²« ë‹¨ê³„ë¡œ ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.

```python

from wordcloud import WordCloud
from konlpy.tag import Okt
from collections import Counter
import matplotlib.pyplot as plt

# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°ì´í„°
text = """
ìì—°ì–´ ì²˜ë¦¬(NLP)ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ, ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.
NLPëŠ” í…ìŠ¤íŠ¸ ë¶„ì„, ê°ì • ë¶„ì„, ê¸°ê³„ ë²ˆì—­, ì±—ë´‡ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤.
í•œêµ­ì–´ëŠ” ë„ì–´ì“°ê¸°ì™€ ì¡°ì‚¬ ì²˜ë¦¬ì— ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
"""

# í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
okt = Okt()

# ëª…ì‚¬ ì¶”ì¶œ
nouns = okt.nouns(text)

# ë¶ˆìš©ì–´ ì œê±°
stopwords = ["ëŠ”", "ì„", "ë¥¼", "ì´", "ê°€", "ì˜", "ì—", "ì™€", "ê³¼", "ì…ë‹ˆë‹¤", "í•©ë‹ˆë‹¤"]
words = [word for word in nouns if word not in stopwords]

# ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°
word_count = Counter(words)

# WordCloud ê°ì²´ ìƒì„±
# !apt install fonts-nanum
wordcloud = WordCloud(
    font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',  # í•œêµ­ì–´ í°íŠ¸ ê²½ë¡œ
    width=800,
    height=400,
    background_color='white'
).generate_from_frequencies(word_count)

# WordCloud ì‹œê°í™”
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
```

ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
![wordcloud](/assets/images/wordcloud_image.png)


## 2. **ê°ì„±ë¶„ì„**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” íŒŒì¼ì´ë‚˜ APIì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ)
data = {
    'text': [
        'ì´ ì˜í™”ëŠ” ì •ë§ ì¬ë¯¸ìˆì—ˆì–´ìš”!', 
        'ì—°ê¸°ê°€ ë„ˆë¬´ ì¢‹ì•˜ìŠµë‹ˆë‹¤.', 
        'ìŠ¤í† ë¦¬ê°€ í¥ë¯¸ë¡­ê³  ê°ë™ì ì´ì—ˆì–´ìš”.',
        'ì‹œê°„ ë‚­ë¹„ì˜€ìŠµë‹ˆë‹¤. ì •ë§ ë³„ë¡œì˜€ì–´ìš”.',
        'ì—°ê¸°ë„ ìŠ¤í† ë¦¬ë„ ëª¨ë‘ ì‹¤ë§ìŠ¤ëŸ¬ì› ìŠµë‹ˆë‹¤.',
        'ëˆê³¼ ì‹œê°„ì´ ì•„ê¹Œì› ì–´ìš”.',
        'ë°°ìš°ë“¤ì˜ ì—°ê¸°ê°€ ì¸ìƒì ì´ì—ˆìŠµë‹ˆë‹¤.',
        'ë‹¤ì‹œëŠ” ë³´ê³  ì‹¶ì§€ ì•Šì€ ì˜í™”ì…ë‹ˆë‹¤.',
        'ìŒì•…ê³¼ ì˜ìƒë¯¸ê°€ ì•„ë¦„ë‹¤ì› ì–´ìš”.',
        'ê¸°ëŒ€í–ˆë˜ ê²ƒë³´ë‹¤ í›¨ì”¬ ì¬ë¯¸ì—†ì—ˆìŠµë‹ˆë‹¤.'
    ],
    'sentiment': [1, 1, 1, 0, 0, 0, 1, 0, 1, 0]  # 1: ê¸ì •, 0: ë¶€ì •
}

df = pd.DataFrame(data)
print("ë°ì´í„° ìƒ˜í”Œ:")
print(df.head())

# 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_text(text):
    # ì†Œë¬¸ì ë³€í™˜ (ì˜ì–´ì˜ ê²½ìš°)
    text = text.lower()
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'[^\w\s]', '', text)
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# ë°ì´í„°ì— ì „ì²˜ë¦¬ ì ìš©
df['clean_text'] = df['text'].apply(preprocess_text)
print("\nì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ:")
print(df[['text', 'clean_text']].head())

# 3. ë°ì´í„° ë¶„í•  (í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ìš©)
X_train, X_test, y_train, y_test = train_test_split(
    df['clean_text'], df['sentiment'], test_size=0.3, random_state=42)

print(f"\ní•™ìŠµ ë°ì´í„° í¬ê¸°: {len(X_train)}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {len(X_test)}")

# 4. íŠ¹ì„± ì¶”ì¶œ: TF-IDF ë²¡í„°í™”
tfidf_vectorizer = TfidfVectorizer(min_df=2)  # ìµœì†Œ 2ê°œ ë¬¸ì„œì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë§Œ í¬í•¨
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# ì£¼ìš” íŠ¹ì„±(ë‹¨ì–´) ì‹œê°í™”
feature_names = tfidf_vectorizer.get_feature_names_out()
print(f"\nì¶”ì¶œëœ íŠ¹ì„±(ë‹¨ì–´) ìˆ˜: {len(feature_names)}")
print(f"ì£¼ìš” íŠ¹ì„±(ë‹¨ì–´): {', '.join(feature_names[:10])}")

# 5. ëª¨ë¸ í•™ìŠµ: ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜ê¸°
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train_tfidf, y_train)

# 6. ëª¨ë¸ í‰ê°€
y_pred = nb_classifier.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
print(f"\nëª¨ë¸ ì •í™•ë„: {accuracy:.4f}")
print("\në¶„ë¥˜ ë³´ê³ ì„œ:")
print(classification_report(y_test, y_pred, target_names=['ë¶€ì •', 'ê¸ì •']))

# 7. ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡
def predict_sentiment(text):
    # ì „ì²˜ë¦¬
    clean_text = preprocess_text(text)
    # ë²¡í„°í™”
    text_tfidf = tfidf_vectorizer.transform([clean_text])
    # ì˜ˆì¸¡
    prediction = nb_classifier.predict(text_tfidf)[0]
    prob = nb_classifier.predict_proba(text_tfidf)[0]
    
    sentiment = "ê¸ì •" if prediction == 1 else "ë¶€ì •"
    confidence = prob[prediction]
    
    return sentiment, confidence

# ì˜ˆì œ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
test_texts = [
    "ì´ ì œí’ˆì€ ê°€ê²© ëŒ€ë¹„ ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ìŠµë‹ˆë‹¤.",
    "ë°°ì†¡ì´ ë„ˆë¬´ ëŠ¦ê³  ì„œë¹„ìŠ¤ê°€ ë¶ˆì¹œì ˆí–ˆì–´ìš”.",
    "ë””ìì¸ì€ ê´œì°®ì§€ë§Œ ì„±ëŠ¥ì´ ê¸°ëŒ€ì— ë¯¸ì¹˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
]

print("\nìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê°ì„± ì˜ˆì¸¡:")
for text in test_texts:
    sentiment, confidence = predict_sentiment(text)
    print(f"í…ìŠ¤íŠ¸: '{text}'")
    print(f"ì˜ˆì¸¡ ê°ì„±: {sentiment} (í™•ë¥ : {confidence:.4f})")
    print("-" * 50)
```

## 3. **ë‹¨ì–´ ì„ë² ë”© ë° í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¶„ì„**

```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ìµœì´ˆ ì‹¤í–‰ ì‹œ)
# pip install gensim numpy scikit-learn

from gensim.models import Word2Vec
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# 1. ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
# ê°„ë‹¨í•œ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¡œ ë‹¨ì–´ ì„ë² ë”© í•™ìŠµ
sentences = [
    ["ê³ ì–‘ì´", "ê°•ì•„ì§€", "ì¢‹ì•„í•´"],
    ["ê³ ì–‘ì´", "ê·€ì—¬ì›Œ"],
    ["ê°•ì•„ì§€", "ì¶©ì„±ìŠ¤ëŸ½ë‹¤"],
    ["ê³ ì–‘ì´", "ë…ë¦½ì ì´ë‹¤"],
    ["ê°•ì•„ì§€", "ì¹œêµ¬"]
]

# 2. Word2Vec ëª¨ë¸ í•™ìŠµ
# ë‹¨ì–´ ì„ë² ë”© ìƒì„± (ë²¡í„° í¬ê¸°: 10, ìœˆë„ìš°: 2, ìµœì†Œ ë¹ˆë„: 1)
model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=0)

# 3. ë‹¨ì–´ ë²¡í„° í™•ì¸
word1 = "ê³ ì–‘ì´"
word2 = "ê°•ì•„ì§€"
vector1 = model.wv[word1]  # "ê³ ì–‘ì´"ì˜ ë²¡í„°
vector2 = model.wv[word2]  # "ê°•ì•„ì§€"ì˜ ë²¡í„°

print(f"'{word1}'ì˜ ë²¡í„°: {vector1}")
print(f"'{word2}'ì˜ ë²¡í„°: {vector2}")

# 4. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
# ë²¡í„°ë¥¼ 2D ë°°ì—´ë¡œ reshaping
vector1_reshaped = vector1.reshape(1, -1)
vector2_reshaped = vector2.reshape(1, -1)
similarity = cosine_similarity(vector1_reshaped, vector2_reshaped)[0][0]

print(f"'{word1}'ì™€ '{word2}'ì˜ ìœ ì‚¬ë„: {similarity:.4f}")

# 5. ëª¨ë¸ì—ì„œ ë¹„ìŠ·í•œ ë‹¨ì–´ ì°¾ê¸°
similar_words = model.wv.most_similar(word1, topn=3)
print(f"'{word1}'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´: {similar_words}")
```

ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
```
'ê³ ì–‘ì´'ì˜ ë²¡í„°: [ 0.07379206 -0.01533812 -0.04534608  0.06552739 -0.0486109  -0.01816626
  0.02878772  0.00990492 -0.08285812 -0.09450678]
'ê°•ì•„ì§€'ì˜ ë²¡í„°: [-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809
  0.06458873  0.08972988 -0.05015428 -0.03763372]
'ê³ ì–‘ì´'ì™€ 'ê°•ì•„ì§€'ì˜ ìœ ì‚¬ë„: 0.5437
'ê³ ì–‘ì´'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´: [('ê°•ì•„ì§€', 0.5436561107635498), ('ì¹œêµ¬', 0.3293631672859192), ('ì¢‹ì•„í•´', -0.18002544343471527)]
```

## 4. **í† í”½ ëª¨ë¸ë§**
- LDAë¥¼ í™œìš©í•œ í† í”½ ëª¨ë¸ë§

```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ìµœì´ˆ ì‹¤í–‰ ì‹œ)
# pip install gensim nltk

import gensim
from gensim import corpora
from nltk.tokenize import word_tokenize
import nltk

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œ)
nltk.download('punkt')

# 1. ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
# ê°„ë‹¨í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (í•œêµ­ì–´ë¡œ ì˜ˆì‹œ)
documents = [
    "ê³ ì–‘ì´ê°€ ê·€ì—¬ì›Œì„œ ì¢‹ì•„í•´ìš” ë§¤ì¼ ê³ ì–‘ì´ì™€ ë†€ì•„ìš”",
    "ê°•ì•„ì§€ëŠ” ì¶©ì„±ìŠ¤ëŸ½ê³  ì¹œêµ¬ ê°™ì€ ë™ë¬¼ì´ì—ìš”",
    "ê³ ì–‘ì´ëŠ” ë…ë¦½ì ì¸ ì„±ê²©ì„ ê°€ì§€ê³  ìˆì–´ìš”",
    "ê°•ì•„ì§€ì™€ ì‚°ì±…í•˜ëŠ” ê²Œ ì •ë§ ì¬ë°Œì–´ìš”",
    "ê³ ì–‘ì´ì™€ ê°•ì•„ì§€ ë‘˜ ë‹¤ ì‚¬ë‘ìŠ¤ëŸ¬ì›Œìš”"
]

# 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
# í† í°í™” ë° ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±° (ê°„ë‹¨íˆ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬)
tokenized_docs = [word_tokenize(doc) for doc in documents]

# 3. ì‚¬ì „(Dictionary) ìƒì„±
dictionary = corpora.Dictionary(tokenized_docs)

# 4. ë¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬(Bag of Words) ìƒì„±
corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]

# 5. LDA ëª¨ë¸ í•™ìŠµ
# í† í”½ ìˆ˜: 2, ë°˜ë³µ íšŸìˆ˜: 10
lda_model = gensim.models.LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=2,
    passes=10,
    random_state=42
)

# 6. í† í”½ ì¶œë ¥
topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(f"í† í”½ {topic[0] + 1}: {topic[1]}")

# 7. ê° ë¬¸ì„œì˜ í† í”½ ë¶„í¬ í™•ì¸
print("\në¬¸ì„œë³„ í† í”½ ë¶„í¬:")
for i, doc_bow in enumerate(corpus):
    doc_topics = lda_model.get_document_topics(doc_bow)
    print(f"ë¬¸ì„œ {i + 1}: {doc_topics}")
```

ğŸ”¹ **ì¶œë ¥ ì˜ˆì‹œ:**  
```
í† í”½ 1: 0.065*"ê³ ì–‘ì´ì™€" + 0.063*"ë§¤ì¼" + 0.063*"ì¢‹ì•„í•´ìš”" + 0.063*"ê³ ì–‘ì´ê°€" + 0.063*"ë†€ì•„ìš”"
í† í”½ 2: 0.054*"ì¶©ì„±ìŠ¤ëŸ½ê³ " + 0.054*"ì¹œêµ¬" + 0.054*"ê°•ì•„ì§€ì™€" + 0.054*"ì¬ë°Œì–´ìš”" + 0.054*"ì •ë§"

ë¬¸ì„œë³„ í† í”½ ë¶„í¬:
ë¬¸ì„œ 1: [(0, 0.9229462), (1, 0.077053785)]
ë¬¸ì„œ 2: [(0, 0.087666884), (1, 0.9123331)]
ë¬¸ì„œ 3: [(0, 0.9136158), (1, 0.0863842)]
ë¬¸ì„œ 4: [(0, 0.08766774), (1, 0.9123323)]
ë¬¸ì„œ 5: [(0, 0.09521585), (1, 0.90478414)]
```
