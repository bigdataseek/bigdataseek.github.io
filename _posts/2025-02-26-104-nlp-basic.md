---
title: 3차시 4:NLP Basic
layout: single
classes: wide
categories:
  - NLP
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

# 1. NLP

## 1. **NLP의 기본개념**

### 1.1 **NLP란**
NLP는 **텍스트 데이터** 또는 **음성 데이터**를 분석하고 이해하여, 컴퓨터가 인간의 언어와 상호작용할 수 있도록 하는 기술입니다. 주요 작업에는 다음과 같은 것들이 포함됩니다:
- **텍스트 분류**: 텍스트를 특정 범주로 분류 (예: 스팸 메일 필터링)
- **감정 분석**: 텍스트의 감정을 분석 (예: 긍정/부정 판단)
- **기계 번역**: 한 언어에서 다른 언어로 텍스트 번역 (예: Google 번역)
- **질문 응답**: 질문에 대한 답변 생성 (예: 챗봇)
- **텍스트 요약**: 긴 텍스트를 짧게 요약
- **개체명 인식**: 텍스트에서 사람, 장소, 조직 등을 식별
- **텍스트 생성**: 주어진 입력을 기반으로 새로운 텍스트 생성 (예: GPT 모델)

### 1.2 NLP의 필요성

- **데이터의 폭발적 증가**
    - 인터넷, 소셜 미디어, 메시지 플랫폼 등에서 텍스트 데이터가 급증하고 있습니다.
    - 이러한 데이터를 효과적으로 분석하고 활용하려면 NLP 기술이 필수적입니다.

- **자동화와 효율성**
    - NLP를 통해 반복적이고 시간이 많이 걸리는 작업을 자동화할 수 있습니다.
    - 예: 고객 문의 자동 응답, 문서 요약, 리포트 생성 등

- **사용자 경험 향상**
    - 챗봇, 음성 보조, 추천 시스템 등은 NLP를 통해 더 자연스럽고 직관적인 사용자 경험을 제공합니다.
    - 예: Amazon Alexa, Google Assistant

- **의사결정 지원**
    - 텍스트 데이터에서 유용한 정보를 추출하여 비즈니스, 의료, 금융 등 다양한 분야에서 의사결정을 지원합니다.
    - 예: 증권 리포트 분석, 환자 기록 분석

- **언어 장벽 해소**
    - 기계 번역 기술은 언어 간 소통의 장벽을 낮추고, 글로벌 협력을 촉진합니다.
    - 예: Google 번역, DeepL

### 1.3 NLP의 활용 분야
- **고객 서비스**: 챗봇, 자동 응답 시스템.
- **의료**: 환자 기록 분석, 질병 진단 지원.
- **금융**: 시장 분석, 감정 분석을 통한 주가 예측.
- **교육**: 자동 채점, 맞춤형 학습 콘텐츠 제공.
- **엔터테인먼트**: 영화 추천, 게임 내 대화 시스템.

### 1.4 NLP의 주요 개념
- **토큰화(Tokenization)**: 텍스트를 단어 또는 서브워드 단위로 분리.
- **임베딩(Embedding)**: 단어를 벡터 형태로 변환 (예: Word2Vec, GloVe).
- **시퀀스 모델링**: RNN, LSTM, GRU 등을 사용하여 시퀀스 데이터 처리.
- **트랜스포머(Transformer)**: Self-Attention 메커니즘을 사용한 고급 모델 (예: BERT, GPT).
- **전처리(Preprocessing)**: 텍스트 정규화, 불용어 제거, 어간 추출 등.

### 1.5 **NLP의 주요 흐름**
- **텍스트 수집**: 웹 크롤링, API, 데이터베이스 등에서 텍스트 데이터 수집
- **텍스트 전처리**: 토큰화, 정제, 정규화, 불용어 제거
- **특성 추출**: Bag of Words, TF-IDF, Word Embedding 등을 이용해 텍스트 데이터를 수치 데이터로 변환
- **모델 구축**: 분류, 군집화, 감성 분석, 개체명 인식 등 목적에 맞는 모델 구축
- **모델 평가 및 개선**: 정확도, 재현율, F1 스코어 등을 통한 모델 평가 및 개선

## 2. NLP 주요기술
### 2.1 **텍스트 전처리**
1. **토큰화(Tokenization)**: 텍스트를 단어, 문장 등의 의미 있는 단위로 분리

    ```python
    import nltk
    from nltk.tokenize import word_tokenize, sent_tokenize

    nltk.download('punkt')  # 토큰화를 위한 데이터 다운로드
    nltk.download('punkt_tab')

    text = "ChatGPT is amazing! It helps with natural language processing."

    # 단어 토큰화
    word_tokens = word_tokenize(text)
    print("Word Tokenization:", word_tokens)

    # 문장 토큰화
    sent_tokens = sent_tokenize(text)
    print("Sentence Tokenization:", sent_tokens)
    ```

    🔹 **출력 예시:**  
    ```
    Word Tokenization: ['ChatGPT', 'is', 'amazing', '!', 'It', 'helps', 'with', 'natural', 'language', 'processing', '.']
    Sentence Tokenization: ['ChatGPT is amazing!', 'It helps with natural language processing.']
    ```


2. **정제(Cleaning)**: 불필요한 문자, 기호 제거

    ```python
    import re

    text = "This is an Example! NLP is AWESOME!!!"
    lower_text = text.lower()

    # 특수문자 제거
    # r'[^a-zA-Z0-9\s]': 알파벳, 숫자, 공백을 제외한 모든 문자
    clean_text = re.sub(r'[^a-zA-Z0-9\s]', '', lower_text)
    print("Cleaned Text:", clean_text)
    ```

    🔹 **출력 예시:**  
    ```
    Cleaned Text: this is an example nlp is awesome
    ```



3. **정규화(Normalization)**: 대소문자 통일, 어근 추출(Stemming), 표제어 추출(Lemmatization)

    - 어근 추출(Stemming) 
        - 어근 추출은 단어의 어간(Stem)만 남기고 접사(Suffix)를 제거하는 기법입니다.
        - 대표적인 방식으로 Porter Stemmer와 Lancaster Stemmer

        ```python
        from nltk.stem import PorterStemmer, LancasterStemmer

        stemmer1 = PorterStemmer()
        stemmer2 = LancasterStemmer()

        words = ["running", "flies", "happily", "studies", "better"]

        print("Porter Stemmer 결과:")
        print([stemmer1.stem(word) for word in words])

        print("\nLancaster Stemmer 결과:")
        print([stemmer2.stem(word) for word in words])

        ```
    - 표제어 추출(Lemmatization):
        - 표제어 추출은 단어의 원형(기본 사전형, Lemma) 을 찾는 과정입니다.
        - 이 방식은 문맥과 품사 정보를 반영하여 더 정확한 결과를 제공

        ```python
        import spacy

        nlp = spacy.load("en_core_web_sm")

        text = "running flies happily studies better"
        doc = nlp(text)

        print("표제어 추출 결과:")
        for token in doc:
            print(token.text, "→", token.lemma_)
        ```

    - 한국어: 어근 추출보다 형태소 분석을 활용한 원형 복원이 더 효과적

        ```python
        from konlpy.tag import Okt

        okt = Okt()
        text = "달려가는 강아지가 귀엽습니다."

        # 형태소 분석 후 원형 출력
        morphs = okt.pos(text, stem=True)
        print(morphs)

        ```

    - `konlpy.tag`의 주요 형태소 분석기 정리

        | 분석기 | 설명 | 장점 | 단점 | 추천 용도 |
        |--------|------|------|------|-----------|
        | **Okt (Twitter)** | 트위터에서 개발된 분석기. `konlpy`의 기본 분석기 | 빠르고, 신조어/인터넷 어휘에 강함 | 정확도는 중간 수준, 복합어 분석 약함 | 간단한 토큰화, 트위터/댓글 분석 |
        | **Kkma (꼬꼬마)** | 연세대학교에서 개발한 분석기 | 품사 태깅과 문장 분할 기능 강력 | 느리고, 오류 발생 가능성 높음 | 문장 분할, 품사 태깅 중심 작업 |
        | **Komoran** | 이지트론에서 개발한 분석기 | 정확도와 속도 균형 잡힘, 복합명사 분석 강점 | 설치 시 추가 설정 필요 | 일반 NLP, 명사 추출, 감성 분석 |
        | **Hannanum** | 한나눔 프로젝트 (한국전자통신연구원) | 명사, 동사 분석 정확도 높음 | 전처리 필요, 느림 | 학술적 연구, 고정밀 분석 |
        | **Mecab-KO** *(konlpy 외부)* | 일본 Mecab 기반 한국어 버전 | 매우 빠르고 정확, 복합어 분석 탁월 | konlpy 기본 패키지 아님, 별도 설치 | 대용량 처리, 상업용 시스템 |


4. **불용어 제거(Stopwords Removal)**: 'the', 'a', 'is'와 같은 분석에 큰 의미가 없는 단어 제거

- 불필요한 단어(예: "is", "an", "the")를 제거하여 핵심 단어만 남깁니다.  
    ```python
    from nltk.corpus import stopwords

    nltk.download('stopwords')

    words = ["this", "is", "an", "example", "of", "stopword", "removal"]
    filtered_words = [word for word in words if word not in stopwords.words('english')]

    print("Filtered Words:", filtered_words)
    ```

    🔹 **출력 예시:**  
    ```
    Filtered Words: ['example', 'stopword', 'removal']
    ```

### 2.2 **형태소 분석**

1. **형태소 분석(Morphological Analysis) 실습**  
- 형태소 분석을 통해 단어의 어근, 품사를 확인합니다.  

    ```python
    import spacy

    nlp = spacy.load("en_core_web_sm")
    text = "Running faster is good for your health."

    # 형태소 분석 수행
    doc = nlp(text)
    for token in doc:
        print(token.text, "→", token.lemma_, "/", token.pos_)
    ```

    🔹 **출력 예시:**  
    ```
    Running → run / VERB
    faster → fast / ADV
    is → be / AUX
    good → good / ADJ
    for → for / ADP
    your → your / PRON
    health → health / NOUN
    . → . / PUNCT
    ```

2. **한국어 형태소 분석 (KoNLPy 활용)**

    ```python
    from konlpy.tag import Okt

    okt = Okt()
    text = "자연어 처리는 재미있습니다."

    # 형태소 분석
    morphs = okt.morphs(text)
    print("형태소:", morphs)

    # 품사 태깅
    pos_tags = okt.pos(text)
    print("품사 태깅:", pos_tags)
    ```

    🔹 **출력 예시:**  
    ```
    형태소: ['자연어', '처리', '는', '재미있', '습니다', '.']
    품사 태깅: [('자연어', 'Noun'), ('처리', 'Noun'), ('는', 'Josa'), ('재미있', 'Adjective'), ('습니다', 'Eomi'), ('.', 'Punctuation')]
    ```


### 2.3 텍스트 표현
1. **Bag of Words**: 단어의 등장 빈도를 벡터로 표현
    ```python
    from sklearn.feature_extraction.text import CountVectorizer

    # 문서 리스트
    documents = [
        "I love NLP",
        "NLP is fascinating",
        "I enjoy learning NLP"
    ]

    # CountVectorizer를 사용하여 Bag of Words 생성
    vectorizer = CountVectorizer()
    bow_matrix = vectorizer.fit_transform(documents)

    # 결과 출력
    print("단어 목록:", vectorizer.get_feature_names_out())
    print("Bag of Words 행렬:\n", bow_matrix.toarray())
    ```

    🔹 **출력 예시:**  
    ```
    단어 목록: ['enjoy' 'fascinating' 'is' 'learning' 'love' 'nlp']
    Bag of Words 행렬:
    [[0 0 0 0 1 1]
    [0 1 1 0 0 1]
    [1 0 0 1 0 1]]
    ```


2. **TF-IDF**: 단어 빈도와 문서 빈도의 역수를 곱한 값으로 중요도 표현

    ```python
    from sklearn.feature_extraction.text import TfidfVectorizer

    # 문서 리스트
    documents = [
        "I love NLP",
        "NLP is fascinating",
        "I enjoy learning NLP"
    ]

    # TfidfVectorizer를 사용하여 TF-IDF 행렬 생성
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(documents)

    # 결과 출력
    print("단어 목록:", vectorizer.get_feature_names_out())
    print("TF-IDF 행렬:\n", tfidf_matrix.toarray())
    ```

    🔹 **출력 예시:** 
    ```
    단어 목록: ['enjoy' 'fascinating' 'is' 'learning' 'love' 'nlp']
    TF-IDF 행렬:
    [[0.         0.         0.         0.         0.70710678 0.70710678]
    [0.         0.70710678 0.70710678 0.         0.         0.70710678]
    [0.57735027 0.         0.         0.57735027 0.         0.57735027]]
    ```

3. **Word Embedding**: Word2Vec, GloVe, FastText 등을 이용한 단어의 의미적 표현
    - 단어를 벡터 공간에 표현하여 단어의 의미적 관계를 파악할 수 있게 합니다.
    - Word2Vec 사용
    
    ```python
    from gensim.models import Word2Vec

    # 문장 리스트
    sentences = [
        ["I", "love", "NLP"],
        ["NLP", "is", "fascinating"],
        ["I", "enjoy", "learning", "NLP"]
    ]

    # Word2Vec 모델 학습
    model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=1)

    # 단어 "NLP"의 벡터 표현
    nlp_vector = model.wv["NLP"]
    print("'NLP'의 벡터 표현:", nlp_vector)

    # 가장 유사한 단어 찾기
    # 단어 간의 유사도를 측정할 때 **코사인 유사도(Cosine Similarity(-1~1)
    similar_words = model.wv.most_similar("NLP")
    print("'NLP'와 가장 유사한 단어:", similar_words)
    ```

    🔹 **출력 예시:** 
    ```
    'NLP'의 벡터 표현: [ 0.12345678 -0.23456789  0.34567891 ... ]
    'NLP'와 가장 유사한 단어: [('love', 0.98765432), ('fascinating', 0.87654321), ...]
    ```

## 3. 언어 모델
### 3.1 **통계적 언어 모델**: N-gram 모델
- N-gram 모델은 이전 N-1개의 단어를 기반으로 다음 단어를 예측하는 통계적 모델입니다.

    ```python
    from nltk import ngrams
    from collections import defaultdict, Counter

    # 샘플 문장
    sentence = "I love natural language processing"

    # 2-gram 모델 생성
    n = 2
    bigrams = list(ngrams(sentence.split(), n))

    # 빈도수 계산
    bigram_freq = Counter(bigrams)

    # 결과 출력
    for bigram, freq in bigram_freq.items():
        print(f"{bigram}: {freq}")
    ```

    🔹 **출력 예시:**  
    ```
    ('I', 'love'): 1
    ('love', 'natural'): 1
    ('natural', 'language'): 1
    ('language', 'processing'): 1
    ```

### 3.2 **신경망 기반 언어 모델**: RNN, LSTM, GRU
- RNN, LSTM, GRU는 시퀀스 데이터를 처리하는 데 사용되는 신경망 모델입니다.

    ```python
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    import numpy as np
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Embedding, SimpleRNN, Dense

    # 샘플 텍스트 데이터
    texts = [
        "I love natural language processing",
        "NLP is a fascinating field",
        "I enjoy learning new things"
    ]

    # 토크나이저 객체 생성 및 단어 사전 구축
    tokenizer = Tokenizer(num_words=10000)
    tokenizer.fit_on_texts(texts)

    # 텍스트를 단어 인덱스 시퀀스로 변환
    sequences = tokenizer.texts_to_sequences(texts)

    # 입력 데이터 (단어 인덱스 시퀀스)
    input_sequences = np.array(sequences)

    # 타겟 데이터 (다음 단어 예측을 위한 레이블)
    target_words = np.array([5, 10, 14])  # 각 시퀀스의 다음 단어

    # RNN 모델 정의
    vocab_size = 10000
    embedding_dim = 128
    rnn_units = 64

    model = Sequential([
        Embedding(vocab_size, embedding_dim),
        SimpleRNN(rnn_units),
        Dense(vocab_size, activation='softmax')
    ])

    # 모델 컴파일
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

    # 모델 학습
    model.fit(input_sequences, target_words, epochs=10, batch_size=1)

    # 새로운 텍스트 데이터
    new_text = "I enjoy learning"

    # 새로운 텍스트를 단어 인덱스 시퀀스로 변환
    new_sequence = tokenizer.texts_to_sequences([new_text])

    # 패딩 추가 (모델 입력 길이에 맞추기)
    new_sequence = pad_sequences(new_sequence, maxlen=5)

    # 다음 단어 예측
    predictions = model.predict(new_sequence)

    # 예측 결과에서 가장 높은 확률을 가진 단어 인덱스 찾기
    predicted_word_index = np.argmax(predictions, axis=-1)

    # 단어 인덱스를 단어로 변환
    predicted_word = tokenizer.index_word.get(predicted_word_index[0], "UNK")

    print("새로운 입력 시퀀스:", new_sequence)
    print("예측된 다음 단어:", predicted_word)
    ```

### 3.3 **트랜스포머 기반 모델**: BERT, GPT, T5
- 트랜스포머 기반 모델은 self-attention 메커니즘을 사용하여 시퀀스 데이터를 처리합니다.

1. **BERT를 사용한 텍스트 분류**

    ```python
    
    from transformers import BertTokenizer, TFBertForSequenceClassification
    import tensorflow as tf

    # BERT 토크나이저 로드
    # bert-base-uncased는 영어 텍스트에 특화된 모델이므로, 
    # 한국어 텍스트를 처리하려면 bert-base-multilingual-uncased 또는 한국어 특화 모델(예: klue/bert-base)을 사용하는 것이 좋습니다.
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    # 샘플 텍스트
    text = "I love natural language processing"

    # 토크나이징
    inputs = tokenizer(text, return_tensors='tf', truncation=True, padding=True)

    # BERT 모델 로드
    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')

    # 예측
    outputs = model(inputs)
    logits = outputs.logits

    # 결과 출력
    print(logits)
    ```

2. **GPT를 사용한 텍스트 생성**
    ```python
    from transformers import GPT2Tokenizer, TFGPT2LMHeadModel
    import tensorflow as tf

    # GPT-2 토크나이저 로드
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

    # GPT-2는 기본적으로 pad_token이 없으므로 eos_token을 pad_token으로 설정
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

    # GPT-2 모델 로드
    model = TFGPT2LMHeadModel.from_pretrained('gpt2')

    # 샘플 텍스트
    text = "Once upon a time"

    # 토크나이징 (attention_mask 포함)
    inputs = tokenizer(text, return_tensors='tf', return_attention_mask=True, padding=True)

    # 텍스트 생성
    outputs = model.generate(
        input_ids=inputs['input_ids'],
        attention_mask=inputs['attention_mask'],
        max_length=50,
        num_return_sequences=1,
        do_sample=True,  # 샘플링 활성화
        top_k=50,        # 상위 50개 토큰에서 샘플링
        top_p=0.95,      # 누적 확률 95% 내에서 샘플링
        no_repeat_ngram_size=2  # 2-gram 반복 방지
    )

    # 결과 출력
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(generated_text)
    ```

3. **T5를 사용한 텍스트 요약**
    ```python
    from transformers import T5Tokenizer, TFT5ForConditionalGeneration

    # T5 토크나이저 로드
    tokenizer = T5Tokenizer.from_pretrained('t5-small')

    # T5 모델 로드
    model = TFT5ForConditionalGeneration.from_pretrained('t5-small')

    # 샘플 텍스트
    text = "Natural language processing is a field of artificial intelligence."

    # 토크나이징
    inputs = tokenizer.encode("summarize: " + text, return_tensors='tf', max_length=512, truncation=True)

    # 요약 생성
    outputs = model.generate(inputs, max_length=50, num_beams=4, early_stopping=True)

    # 결과 출력
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(summary)
    ```

---

# 2. 실습 예제

## 1. WordCloud
- **WordCloud**는 텍스트 데이터에서 빈도수가 높은 단어를 크게, 빈도수가 낮은 단어를 작게 표현한 시각화 도구입니다.
- 텍스트 데이터의 주요 키워드를 한눈에 파악할 수 있어, 데이터 분석의 첫 단계로 많이 사용됩니다.

```python

from wordcloud import WordCloud
from konlpy.tag import Okt
from collections import Counter
import matplotlib.pyplot as plt

# 한국어 텍스트 데이터
text = """
자연어 처리(NLP)는 인공지능의 한 분야로, 컴퓨터가 인간의 언어를 이해하고 처리할 수 있게 하는 기술입니다.
NLP는 텍스트 분석, 감정 분석, 기계 번역, 챗봇 등 다양한 분야에서 활용됩니다.
한국어는 띄어쓰기와 조사 처리에 주의해야 합니다.
"""

# 형태소 분석기 초기화
okt = Okt()

# 명사 추출
nouns = okt.nouns(text)

# 불용어 제거
stopwords = ["는", "을", "를", "이", "가", "의", "에", "와", "과", "입니다", "합니다"]
words = [word for word in nouns if word not in stopwords]

# 단어 빈도수 계산
word_count = Counter(words)

# WordCloud 객체 생성
# !apt install fonts-nanum
wordcloud = WordCloud(
    font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',  # 한국어 폰트 경로
    width=800,
    height=400,
    background_color='white'
).generate_from_frequencies(word_count)

# WordCloud 시각화
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
```

🔹 **출력 예시:**  
![wordcloud](/assets/images/wordcloud_image.png)


## 2. **감성분석**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# 1. 샘플 데이터 생성 (실제로는 파일이나 API에서 가져올 수 있음)
data = {
    'text': [
        '이 영화는 정말 재미있었어요!', 
        '연기가 너무 좋았습니다.', 
        '스토리가 흥미롭고 감동적이었어요.',
        '시간 낭비였습니다. 정말 별로였어요.',
        '연기도 스토리도 모두 실망스러웠습니다.',
        '돈과 시간이 아까웠어요.',
        '배우들의 연기가 인상적이었습니다.',
        '다시는 보고 싶지 않은 영화입니다.',
        '음악과 영상미가 아름다웠어요.',
        '기대했던 것보다 훨씬 재미없었습니다.'
    ],
    'sentiment': [1, 1, 1, 0, 0, 0, 1, 0, 1, 0]  # 1: 긍정, 0: 부정
}

df = pd.DataFrame(data)
print("데이터 샘플:")
print(df.head())

# 2. 텍스트 전처리 함수
def preprocess_text(text):
    # 소문자 변환 (영어의 경우)
    text = text.lower()
    # 특수문자 제거
    text = re.sub(r'[^\w\s]', '', text)
    # 불필요한 공백 제거
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# 데이터에 전처리 적용
df['clean_text'] = df['text'].apply(preprocess_text)
print("\n전처리된 텍스트 샘플:")
print(df[['text', 'clean_text']].head())

# 3. 데이터 분할 (학습 및 테스트용)
X_train, X_test, y_train, y_test = train_test_split(
    df['clean_text'], df['sentiment'], test_size=0.3, random_state=42)

print(f"\n학습 데이터 크기: {len(X_train)}")
print(f"테스트 데이터 크기: {len(X_test)}")

# 4. 특성 추출: TF-IDF 벡터화
tfidf_vectorizer = TfidfVectorizer(min_df=2)  # 최소 2개 문서에서 등장하는 단어만 포함
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# 주요 특성(단어) 시각화
feature_names = tfidf_vectorizer.get_feature_names_out()
print(f"\n추출된 특성(단어) 수: {len(feature_names)}")
print(f"주요 특성(단어): {', '.join(feature_names[:10])}")

# 5. 모델 학습: 나이브 베이즈 분류기
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train_tfidf, y_train)

# 6. 모델 평가
y_pred = nb_classifier.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
print(f"\n모델 정확도: {accuracy:.4f}")
print("\n분류 보고서:")
print(classification_report(y_test, y_pred, target_names=['부정', '긍정']))

# 7. 새로운 텍스트에 대한 예측
def predict_sentiment(text):
    # 전처리
    clean_text = preprocess_text(text)
    # 벡터화
    text_tfidf = tfidf_vectorizer.transform([clean_text])
    # 예측
    prediction = nb_classifier.predict(text_tfidf)[0]
    prob = nb_classifier.predict_proba(text_tfidf)[0]
    
    sentiment = "긍정" if prediction == 1 else "부정"
    confidence = prob[prediction]
    
    return sentiment, confidence

# 예제 텍스트로 테스트
test_texts = [
    "이 제품은 가격 대비 성능이 매우 좋습니다.",
    "배송이 너무 늦고 서비스가 불친절했어요.",
    "디자인은 괜찮지만 성능이 기대에 미치지 못했습니다."
]

print("\n새로운 텍스트에 대한 감성 예측:")
for text in test_texts:
    sentiment, confidence = predict_sentiment(text)
    print(f"텍스트: '{text}'")
    print(f"예측 감성: {sentiment} (확률: {confidence:.4f})")
    print("-" * 50)
```

## 3. **단어 임베딩 및 텍스트 유사도 분석**

```python
# 필요한 라이브러리 설치 (최초 실행 시)
# pip install gensim numpy scikit-learn

from gensim.models import Word2Vec
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# 1. 샘플 텍스트 데이터 준비
# 간단한 문장 리스트로 단어 임베딩 학습
sentences = [
    ["고양이", "강아지", "좋아해"],
    ["고양이", "귀여워"],
    ["강아지", "충성스럽다"],
    ["고양이", "독립적이다"],
    ["강아지", "친구"]
]

# 2. Word2Vec 모델 학습
# 단어 임베딩 생성 (벡터 크기: 10, 윈도우: 2, 최소 빈도: 1)
model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=0)

# 3. 단어 벡터 확인
word1 = "고양이"
word2 = "강아지"
vector1 = model.wv[word1]  # "고양이"의 벡터
vector2 = model.wv[word2]  # "강아지"의 벡터

print(f"'{word1}'의 벡터: {vector1}")
print(f"'{word2}'의 벡터: {vector2}")

# 4. 코사인 유사도 계산
# 벡터를 2D 배열로 reshaping
vector1_reshaped = vector1.reshape(1, -1)
vector2_reshaped = vector2.reshape(1, -1)
similarity = cosine_similarity(vector1_reshaped, vector2_reshaped)[0][0]

print(f"'{word1}'와 '{word2}'의 유사도: {similarity:.4f}")

# 5. 모델에서 비슷한 단어 찾기
similar_words = model.wv.most_similar(word1, topn=3)
print(f"'{word1}'와 유사한 단어: {similar_words}")
```

🔹 **출력 예시:**  
```
'고양이'의 벡터: [ 0.07379206 -0.01533812 -0.04534608  0.06552739 -0.0486109  -0.01816626
  0.02878772  0.00990492 -0.08285812 -0.09450678]
'강아지'의 벡터: [-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809
  0.06458873  0.08972988 -0.05015428 -0.03763372]
'고양이'와 '강아지'의 유사도: 0.5437
'고양이'와 유사한 단어: [('강아지', 0.5436561107635498), ('친구', 0.3293631672859192), ('좋아해', -0.18002544343471527)]
```

## 4. **토픽 모델링**
- LDA를 활용한 토픽 모델링

```python
# 필요한 라이브러리 설치 (최초 실행 시)
# pip install gensim nltk

import gensim
from gensim import corpora
from nltk.tokenize import word_tokenize
import nltk

# NLTK 데이터 다운로드 (최초 실행 시)
nltk.download('punkt')

# 1. 샘플 텍스트 데이터 준비
# 간단한 문서 리스트 (한국어로 예시)
documents = [
    "고양이가 귀여워서 좋아해요 매일 고양이와 놀아요",
    "강아지는 충성스럽고 친구 같은 동물이에요",
    "고양이는 독립적인 성격을 가지고 있어요",
    "강아지와 산책하는 게 정말 재밌어요",
    "고양이와 강아지 둘 다 사랑스러워요"
]

# 2. 텍스트 전처리
# 토큰화 및 불필요한 단어 제거 (간단히 공백 기준으로 분리)
tokenized_docs = [word_tokenize(doc) for doc in documents]

# 3. 사전(Dictionary) 생성
dictionary = corpora.Dictionary(tokenized_docs)

# 4. 문서-단어 행렬(Bag of Words) 생성
corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]

# 5. LDA 모델 학습
# 토픽 수: 2, 반복 횟수: 10
lda_model = gensim.models.LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=2,
    passes=10,
    random_state=42
)

# 6. 토픽 출력
topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(f"토픽 {topic[0] + 1}: {topic[1]}")

# 7. 각 문서의 토픽 분포 확인
print("\n문서별 토픽 분포:")
for i, doc_bow in enumerate(corpus):
    doc_topics = lda_model.get_document_topics(doc_bow)
    print(f"문서 {i + 1}: {doc_topics}")
```

🔹 **출력 예시:**  
```
토픽 1: 0.065*"고양이와" + 0.063*"매일" + 0.063*"좋아해요" + 0.063*"고양이가" + 0.063*"놀아요"
토픽 2: 0.054*"충성스럽고" + 0.054*"친구" + 0.054*"강아지와" + 0.054*"재밌어요" + 0.054*"정말"

문서별 토픽 분포:
문서 1: [(0, 0.9229462), (1, 0.077053785)]
문서 2: [(0, 0.087666884), (1, 0.9123331)]
문서 3: [(0, 0.9136158), (1, 0.0863842)]
문서 4: [(0, 0.08766774), (1, 0.9123323)]
문서 5: [(0, 0.09521585), (1, 0.90478414)]
```
