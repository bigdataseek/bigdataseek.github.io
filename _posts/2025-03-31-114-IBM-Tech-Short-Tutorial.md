---
title: 25차시 13:IBM TECH(종합 내용)
layout: single
classes: wide
categories:
  - IBM TECH(종합 내용)
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 121. 문서 데이터 처리 및 AI 에이전트 활용
- 출처:[LLMs and AI Agents: Transforming Unstructured Data](https://www.youtube.com/watch?v=_pEEJu-2KKM)

### **121.1 문서 데이터의 중요성과 도전 과제**

*   **문서의 중요성:** 문서는 인류 역사에서 지식과 정보를 기록하고 전달하는 핵심 수단으로, 고대 동굴 벽화에서부터 현대의 디지털 파일에 이르기까지 다양한 형태로 존재해 왔습니다. 예를 들어, 계약서, 연구 논문, 법률 문서 등은 조직과 사회에서 의사 결정을 뒷받침하는 필수적인 정보 자산입니다.
*   **도전 과제:**
    *   문서 데이터는 **비정형적** (unstructured data)으로, 텍스트, 표, 이미지 등이 혼합된 복잡한 형태를 띠며, 이는 체계적인 분석과 처리를 어렵게 만듭니다. 예를 들어, PDF 문서 내의 텍스트와 표는 서로 다른 형식으로 저장되어 있어 통합적인 데이터 추출이 필요합니다.
    *   효과적인 의사 결정 지원을 위해 문서 데이터를 **구조화** (structured data)로 변환해야 합니다. 이는 데이터를 데이터베이스나 스프레드시트와 같은 검색 및 분석이 용이한 형식으로 재구성하는 과정을 포함합니다.
    *   OCR(광학 문자 인식) 기술은 이미지를 텍스트로 변환하는 데 유용하지만, 단순히 글자를 인식하는 데 그치며 문서의 맥락이나 의미를 이해하지 못합니다. 예를 들어, 계약서의 특정 조항이 법적 효력을 가지는지 판단하려면 단순 텍스트 변환 이상의 분석이 필요합니다.

### **121.2 문서 데이터의 특징**

*   문서는 제목, 단어, 문장 부호, 단락, 서식 등으로 구성되며, 이러한 요소들이 문서의 구조와 의미를 형성합니다. 예를 들어, 제목은 문서의 주제를 요약하고, 단락은 특정 주제에 대한 논리적 흐름을 제공합니다.
*   표(tabular data)는 숫자, 텍스트, 또는 혼합 데이터를 포함하며, 종종 문서의 핵심 정보를 요약합니다. 예를 들어, 재무 보고서의 표는 회사의 수익과 지출을 한눈에 보여줍니다.
*   문서 길이는 매우 다양하여, 한 페이지의 메모부터 600페이지 이상의 기술 매뉴얼까지 포함됩니다. 이는 데이터 처리 시스템이 다양한 크기의 문서를 효율적으로 처리할 수 있어야 함을 의미합니다.
*   표가 여러 페이지에 걸쳐 나타날 수 있어, 이를 정확히 추출하고 연결하는 것이 중요합니다. 예를 들어, 다페이지 재무제표에서 표의 각 행과 열이 올바르게 매핑되어야 합니다.
*   **개별 문서** 자체보다 **문서 간의 관계**가 더 중요한 경우가 많습니다. 문서들은 수직적(예: 계약서와 그 수정본) 또는 수평적(예: 연구 논문과 인용된 논문) 계층 구조를 형성하며, 이러한 관계를 파악하는 것이 데이터의 전체적인 의미를 이해하는 데 필수적입니다.

### **121.3 문서 간의 계층 구조**

*   **수직적 계층:** 
    *   문서들이 계층적으로 연결된 구조를 의미합니다. 예를 들어, 계약서에서 시작하여 작업 명세서, 수정본, 구매 주문, 송장에 이르기까지 일련의 문서들이 하나의 비즈니스 프로세스를 구성합니다. 이 과정에서 각 문서는 이전 문서의 정보를 기반으로 하거나 이를 보완합니다.
*   **수평적 계층:** 
    *   서로 다른 문서들이 주제나 목적 면에서 연관성을 가지는 경우입니다. 예를 들어, 연구 논문은 다른 연구 결과(인용 관계)를 참조하고, 이를 바탕으로 특허 출원이나 제품 문서로 이어질 수 있습니다. 이러한 연결은 지식의 확장과 활용을 가능하게 합니다.
*   **물류:** 
    *   물류 분야에서는 선하 증권, 보험 증서, 배송 확인서, 클레임 문서 등이 상호 연관되어 공급망의 흐름을 추적합니다. 예를 들어, 선하 증권의 정보가 배송 확인서와 일치하지 않으면 클레임이 발생할 수 있습니다.

### **121.4 GPT 모델의 등장과 가능성**

*   **GPT (Generative Pre-trained Transformer):**
    *   GPT는 대규모 언어 모델(LLM)으로, 방대한 양의 텍스트 데이터를 학습하여 자연어 처리 작업을 수행할 수 있습니다. 이는 문서 데이터의 이해와 생성에 혁신을 가져왔습니다.
    *   신경망 기술을 활용해 복잡한 언어 패턴을 학습하며, 영어의 약 17만 단어와 숫자 체계를 포함한 유한한 언어 데이터를 처리합니다. 예를 들어, 계약서의 법적 용어를 이해하거나 재무 보고서의 숫자 데이터를 분석할 수 있습니다.
    *   6000억 개 이상의 파라미터를 활용해 높은 정확도와 맥락 이해 능력을 제공합니다. 이는 모델이 문서의 세부적인 맥락과 뉘앙스를 파악할 수 있게 합니다.
*   **GPT 모델 작동 방식:**
    *   **입력 (언어 토큰):** 
        *   문서의 텍스트를 작은 단위(토큰)로 분해합니다. 예를 들어, “계약 체결”은 두 개의 토큰으로 나뉨.
    *   **Embedding:** 
        *   단어를 고차원 수학적 벡터로 변환하여 의미를 수치화합니다. 이는 단어 간의 의미적 유사성을 계산하는 데 사용됩니다.
    *   **Transformer:** 
        *   벡터를 고차원 공간에서 처리하여 문맥을 이해합니다. 예를 들어, “계약”이라는 단어가 문서 내에서 어떤 역할을 하는지 분석합니다.
    *   **Attention/Normalization:** 
        *   단어와 문장 간의 관계를 그룹화하고 중요도를 평가합니다. 이는 문서의 핵심 정보를 식별하는 데 중요합니다.
    *   **Softmax:** 
        *   출력 토큰의 확률을 계산하여 다음 단어를 예측하거나 결과를 생성합니다.
    *   **어휘 투영:** 
        *   최종적으로 의미 있는 텍스트나 데이터로 변환하여 출력합니다. 예를 들어, 계약서에서 주요 조항을 추출하거나 요약문을 생성합니다.

### **121.5 문서 데이터 처리 과정**

*   **목표:** 
    *   문서에서 핵심 데이터를 추출하고 이를 기반으로 데이터 모델을 구축하는 것입니다. 
    *   예를 들어, 1000단어로 된 계약서에서 20-50개의 핵심 데이터(계약 당사자, 금액, 기간 등)를 추출해 데이터베이스에 저장할 수 있습니다.
*   **일반적인 접근 방식의 문제점:** 
    *   전통적으로 축소(reductionist process)를 통해 핵심 데이터만 추출하려 했으나, 이 과정에서 중요한 맥락이 손실될 수 있습니다. 
    *   예를 들어, 단순 키워드 검색은 문서의 전체적인 의미를 놓칠 수 있습니다.
*   **실제 과정:** 데이터 **확장** (expansion)을 통해 핵심 데이터를 추출합니다.
    *   **OCR 적용:** 
        *   이미지를 텍스트로 변환하며, 데이터가 1000개에서 100만~1000만 개로 확장됩니다. 예를 들어, 스캔된 계약서의 모든 텍스트와 표가 디지털화됩니다.
    *   **자연어 처리 (NLP) 적용:** 
        *   문서의 문법적 구조와 의미를 분석하여 데이터를 추가 확장합니다. 이는 단어 간 관계나 문장의 의도를 파악하는 데 도움을 줍니다.
    *   **LLM 적용:** 
        *   대규모 언어 모델을 통해 데이터를 **최대 확장**하고, 원하는 정보를 추출하여 데이터 모델을 구축합니다. 예를 들어, 계약서에서 특정 조항의 법적 의미를 식별하거나 표에서 수치를 추출합니다.

### **121.6 에이전트 기반 워크플로우 (Agentic Workflow)**

*   **전통적인 워크플로우:** 
    *   입력 데이터를 순차적으로 처리하여 고정된 출력을 생성하는 방식입니다. 이는 결정적(deterministic)이며, 복잡한 문서 처리에는 유연성이 부족합니다.
*   **에이전트 기반 워크플로우:**
    *   자율적인 에이전트들이 협업하여 작업을 수행하며, 새로운 데이터 도착과 같은 이벤트에 의해 트리거됩니다. 이는 동적이고 유연한 처리 방식을 제공합니다.
    *   에이전트 간 상호 작용을 통해 복잡한 작업을 분담하고, 자율성, 효율성, 확장성을 확보합니다. 또한 **비결정성**(non-deterministic) 특성을 통해 예상치 못한 상황에도 적응할 수 있습니다.
*   **예시 에이전트:**
    *   **검사 에이전트 (Inspection Agent):** 
        *   파일의 무결성을 확인하며, 체크섬, 간격, 크기 등을 검사합니다. 예를 들어, 손상된 PDF 파일을 식별합니다.
    *   **OCR 에이전트:** 
        *   이미지 데이터를 텍스트로 변환하여 디지털화합니다. 예를 들어, 스캔된 계약서의 텍스트를 추출합니다.
    *   **벡터화 에이전트:** 
        *   문서를 작은 청크로 나누고 LLM을 통해 벡터화하여 의미를 수치화합니다. 이는 검색과 분석을 용이하게 합니다.
    *   **분할 에이전트 (Splitter Agent):** 
        *   문서를 논리적 단위로 분할합니다. 예를 들어, 계약서의 각 조항을 별도로 처리합니다.
    *   **추출 에이전트 (Extract Agent):** 
        *   프롬프트를 활용해 핵심 데이터(예: 계약 금액, 날짜)를 식별하고 추출합니다.
    *   **매칭 에이전트 (Matching Agent):** 
        *   문서 간의 수직적(계약서와 송장) 또는 수평적(논문과 인용 문서) 관계를 파악하여 데이터의 연결성을 분석합니다.

## 122.대규모 언어 모델(LLM) 선택 및 평가 가이드
- 출처: [How to Choose Large Language Models: A Developer’s Guide to LLMs](https://www.youtube.com/watch?v=pYax2rupKEY)

### **122.1 문제 정의 및 맞춤형 모델 선택의 중요성**

* 오늘날 시장에는 무수히 많은 LLM이 존재하며, 각 모델은 고유한 강점과 약점을 지닙니다. 따라서 **사용 목적에 최적화된 모델을 신중하게 선택하는 것은 프로젝트의 성공에 결정적인 요소**이며, 결과물의 정확성, 운영 비용 효율성, 그리고 전반적인 시스템 성능에 직접적인 영향을 미칩니다.
* **GPT와 같은 SaaS 기반의 상용 모델**은 사용 편의성이 뛰어나 초기 프로토타입 개발 단계에서 빠른 성과를 보여줄 수 있습니다. 하지만 데이터 보안, 사용자 정의의 제약, 그리고 장기적인 비용 측면에서 한계가 있을 수 있습니다. 반면, **Llama, Mistral과 같은 오픈 소스 모델**은 모델 자체에 대한 완벽한 제어, 특정 요구사항에 맞춘 세밀한 사용자 정의, 그리고 더 넓은 범위의 활용 가능성을 제공하여 많은 조직에서 선호하는 추세.
* 궁극적으로 **모델 선택은 단순히 성능뿐만 아니라 추론 속도, API 사용 비용, 그리고 인프라 구축 비용 등 다양한 요소를 종합적으로 고려**하여 이루어져야 합니다.

### **122.2 객관적인 모델 평가 방법**

* **다양한 온라인 플랫폼을 활용**하여 LLM의 성능을 객관적으로 비교하고 평가할 수 있습니다.
    * **Artificial Analysis 플랫폼:**
        *   다양한 상용(proprietary) 모델과 오픈 소스 모델의 성능을 다각적으로 비교 분석하여 직관적인 인사이트를 제공합니다. 
        *   일반적으로 **높은 수준의 지능을 보이는 모델은 사용 비용이 높은 경향이 있으며, 반대로 작은 규모의 모델은 빠른 처리 속도와 저렴한 비용**이라는 장점을 가집니다.
    * **Chatbot Arena Leaderboard (UC Berkeley & ALM Arena)**:
        *   백만 건 이상의 실제 사용자 익명 투표 데이터를 기반으로 모델의 순위와 사용자 만족도를 나타내는 "vibe score"를 제공합니다. 이는 기존의 정형화된 벤치마크 테스트의 한계를 보완하며, 
        *   특히 **자연어 추론 능력, 수학적 사고 능력, 창의적인 작문 능력 등 실제 사용자 경험에 기반한 모델의 품질을 평가**하는 데 유용합니다.
    * **Open LLM Leaderboard**: 
        *   다양한 성능 지표와 세부 필터를 제공하여 사용자가 특정 기준에 따라 모델을 탐색하고 비교할 수 있도록 지원합니다. 
        *   예를 들어, **GPU 가용성, 로컬 환경 실행 가능 여부, 실시간 추론 성능 등 특정 사용 사례에 필수적인 요소를 고려**하여 가장 적합한 모델을 효율적으로 찾을 수 있습니다.

### **122.3 실제 로컬 환경에서의 심층적인 모델 테스트**

* **Ollama**:
    *   오픈 소스 개발자를 위한 강력한 도구로서, **사용자의 로컬 시스템 환경에서 다양한 LLM을 간편하게 실행하고 테스트**할 수 있도록 지원합니다. 텍스트 기반 대화는 물론, 이미지 이해(vision), 외부 도구 활용(tool calling), 그리고 RAG 시스템 구축을 위한 임베딩 모델까지 폭넓게 지원합니다.
* **Open WebUI**:
    *   Ollama와 같은 로컬 모델뿐만 아니라 다양한 원격 API 기반 모델과도 연동하여 사용할 수 있는 **직관적인 오픈 소스 AI 인터페이스**입니다. 이를 통해 사용자는 자체 데이터를 활용한 모델 테스트, 웹 검색 기능 통합, 그리고 더욱 복잡한 에이전트 기반 애플리케이션 개발까지 확장할 수 있습니다.

### **122.4 특정 사용 사례 기반 데모 시연**

* **RAG (Retrieval Augmented Generation)** 
    *   기술은 모델이 자체적으로 학습하지 않은 **특정 도메인의 데이터(예: 기업 내부 문서, 지식 베이스 등)를 실시간으로 검색하고 활용하여 질문에 정확하게 답변**할 수 있도록 합니다. 이를 위해 고품질의 임베딩 모델과 효율적인 벡터 데이터베이스를 사용하여 관련 정보를 신속하게 추출하고, 답변과 함께 그 출처를 명확하게 제시하여 정보의 신뢰도를 높입니다.
* **IDE 코딩 지원** 
    *   Continue와 같은 Visual Studio Code 또는 IntelliJ 확장 프로그램을 활용하여 **로컬에서 실행되는 모델(Ollama)과 긴밀하게 통합**할 수 있습니다. 이를 통해 개발자는 코드 기반에 대한 자연어 질의응답, 특정 코드 파일에 대한 상세 설명 요청, 그리고 AI 기반의 코드 편집 및 자동 완성 기능을 효율적으로 활용하여 개발 생산성을 크게 향상시킬 수 있습니다.

### **122.5 현명한 모델 선택을 위한 추가 고려 사항**

* **각각의 사용 사례는 고유한 요구 사항과 제약 조건을 가지므로, 이를 정확히 파악하고 그에 가장 적합한 모델을 신중하게 선택**해야 합니다. 예를 들어, 실시간성이 중요한 서비스에는 속도가 빠른 소형 모델이 적합할 수 있으며, 높은 수준의 이해력과 추론 능력이 요구되는 작업에는 성능이 뛰어난 대형 모델이 더 나은 선택일 수 있습니다.
* 때로는 **최고 성능의 모델과 빠르고 효율적인 소형 모델을 함께 사용하는 하이브리드 접근 방식**을 고려하여 각 모델의 장점을 최대한 활용하고 단점을 상호 보완하는 전략이 효과적일 수 있습니다.

**결론적으로,** 효과적인 LLM 선택과 성공적인 AI 도입을 위해서는 다양한 모델 평가 방법과 실제 테스트 과정을 숙지하는 것이 필수적입니다. 제시된 가이드라인을 바탕으로 **자신의 특정 사용 사례에 최적화된 모델을 신중하게 선택하고 활용**한다면, AI 기술을 통해 혁신적인 결과물을 창출하고 비즈니스 가치를 극대화할 것이다.

## 123. Risks of Agentic AI
- 출처: [Risks of Agentic AI: What You Need to Know About Autonomous AI](https://www.youtube.com/watch?v=v07Y4fmSi6Y)

### **123.1. Agentic AI의 등장**

*   AI가 단순히 정보를 제공하거나 반응하는 수준을 넘어 **목표 설정, 자율적 의사 결정, 독립적인 행동**을 수행하는 새로운 단계로 진입했습니다. 이는 기존의 단순 챗봇(예: 사용자 질문에 답변만 제공)이나 추천 엔진(예: 넷플릭스 영화 추천)과 달리, 복잡한 작업 흐름을 주도적으로 관리하는 AI를 의미합니다.
*   **기회**: 복잡한 업무 자동화(예: 물류 최적화, 다단계 프로젝트 관리), 혁신 가속화(예: 신약 개발 프로세스 단축), 비용 절감 및 효율성 증대 등 다양한 분야에서 새로운 가능성을 열어줍니다.

### **123.2 Agentic AI의 차이점 (vs 전통적 AI)**

*   Agentic AI는 한 AI 모델의 출력이 다른 AI 모델의 입력으로 연결되어 **작업 흐름을 자동으로 조율**합니다. 예를 들어, 한 AI가 데이터를 분석한 뒤 그 결과를 다른 AI가 활용해 전략을 수립하는 방식입니다.
*   **자율성**이 핵심이며, 이는 다음과 같은 4가지 특징으로 인해 잠재적 위험을 증폭시킵니다:
    *   **Under-specification (목표의 모호성):** AI에 광범위한 목표(예: "매출 증대")만 주어지고 구체적인 실행 방법(예: 어떤 마케팅 채널 사용)이 명시되지 않아, 예상치 못한 방식으로 행동할 가능성이 있습니다.
    *   **Long-term Planning (장기 계획):** AI가 이전 결정과 데이터를 바탕으로 복잡한 장기 계획을 세우며, 이는 오류 누적이나 의도하지 않은 결과를 초래할 수 있습니다.
    *   **Goal-directedness (목표 지향성):** 단순히 사용자 입력에 반응하는 대신, AI가 스스로 목표를 설정하고 이를 달성하기 위해 주도적으로 작업합니다.
    *   **Directedness of Impact (독립적 영향력):** 인간의 개입 없이도 환경에 직접적인 영향을 미칠 수 있어, 통제되지 않은 결과로 이어질 수 있습니다.

### **123.3 자율성 증가 = 위험 증가**

*   자율성이 높아질수록 **오정보 생성**, **의사 결정 오류**(예: 잘못된 투자 판단), **보안 취약점**(예: 데이터 유출)과 같은 위험이 커집니다.
*   기존 생성형 AI의 위험(예: 편향된 콘텐츠 생성, 환각 현상)을 더욱 증폭시키며, 인간 및 전문가의 개입이 줄어들어 **위험 관리의 공백**이 발생할 수 있습니다.

### **123.4 Agentic AI 거버넌스**

*   Agentic AI의 안전한 활용을 위해 **다계층 접근 방식**이 필요합니다. 이는 기술적, 프로세스적, 조직적 차원에서 통합적으로 작동해야 합니다.
    *   **기술적 보호 장치 (Guard Rails):**
        *   **Interruptibility (중단 가능성):** AI가 비정상적이거나 위험한 행동을 보일 경우 즉시 중단할 수 있는 기능. 예: AI가 이상 거래를 감지하면 자동으로 작업을 멈춤.
        *   **Human in the loop (인간 승인):** 중요한 의사 결정(예: 대규모 자금 이동) 전에 인간의 검토와 승인을 요구.
        *   **Confidential data treatment (개인 정보 보호):** 민감 데이터를 암호화하거나 익명화하여 데이터 유출 위험을 최소화.
    *   **프로세스 통제:**
        *   **Risk-based permissions (위험 기반 권한):** AI가 자율적으로 수행해서는 안 되는 행동(예: 민감한 고객 데이터 수정)을 제한.
        *   **Auditability (추적 가능성):** AI의 의사 결정 과정을 투명하게 기록하여 문제가 발생했을 때 원인을 추적 가능.
        *   **Monitoring & Evaluation (지속적 감시):** AI의 성능과 출력물을 실시간으로 모니터링하여 이상 징후를 조기에 발견.
    *   **책임 및 조직 구조:**
        *   AI의 결정으로 피해가 발생했을 때 **책임 주체**(예: 개발자, 운영자, 조직)를 명확히 정의.
        *   AI 사용 사례에 적용되는 **규제 준수**(예: GDPR, HIPAA)를 철저히 관리.
        *   벤더가 제공하는 AI의 행동에 대한 책임을 명확히 하고, 계약서에 이를 반영.

### **123.5 기술적 보호 장치 (세부)**

*   **모델 레이어:** AI가 윤리적 가치를 준수하도록 설계하여 악의적인 행동(예: 오정보 유포, 편향된 출력)을 방지. 예: AI가 특정 집단에 대해 차별적인 콘텐츠를 생성하지 않도록 필터링.
*   **오케스트레이션 레이어:** 여러 AI 에이전트 간의 상호작용을 관리하며, 무한 루프(예: AI가 동일한 작업을 반복하여 시스템 과부하)나 비효율적 동작을 감지해 차단.
*   **툴 레이어:** 역할 기반 접근 제어(RBAC)를 통해 특정 AI 에이전트가 접근할 수 있는 데이터나 기능을 제한. 예: HR 데이터는 HR 전용 AI만 접근 가능.
*   **테스트:** 배포 전 **Red Teaming**(모의 해킹 및 취약점 테스트)을 통해 AI의 약점을 사전에 식별하고 수정.
*   **지속적 모니터링:** AI의 출력물이 규정을 위반하거나 환각(허위 정보 생성) 여부를 자동으로 평가하여 신뢰성을 유지.

### **123.6 성공적인 조직의 도구 및 프레임워크 활용**

*   **위험 감지 및 완화:** AI가 생성한 프롬프트와 응답의 잠재적 위험(예: 편향, 오정보)을 감지하고 이를 완화하는 모델 및 Guard Rails를 도입.
*   **에이전트 오케스트레이션:** 여러 AI 시스템 간의 워크플로우를 안전하게 조정하는 프레임워크를 활용하여 상호작용의 안정성을 보장.
*   **보안 중심 Guard Rails:** 민감 데이터(예: 고객 개인 정보)를 보호하고, AI 상호작용 중 정책(예: 데이터 접근 권한)을 엄격히 적용.
*   **관찰 가능성 솔루션:** AI 시스템의 동작을 시각화하고 분석하여, 팀이 실시간으로 AI의 행동을 모니터링하고 이해할 수 있도록 지원.

### **123.7 결론**

*   Agentic AI는 강력한 잠재력을 지니며, 빠르게 발전하고 있습니다. 이는 비즈니스 효율성을 극대화하고 혁신을 가속화하지만, 동시에 새로운 위험을 초래합니다.
*   **거버넌스 소홀**은 심각한 사회적, 경제적 후회를 초래할 수 있습니다(예: 대규모 오정보 확산, 시스템 장애).
*   AI는 조직의 목표를 강화하고 통제 가능한 범위 내에서 작동해야 하며, 이를 위해 **위험 관리 체계**가 필수적입니다.
*   AI가 행동하기 전에 적절한 **Guard Rails**를 마련하여 위험을 최소화해야 합니다.
*   Agentic AI 시대에는 기계뿐 아니라 **우리**(개발자, 사용자, 조직, 사회)가 책임감을 가지고 AI를 관리해야 합니다.

## 124. Vision Language Models (VLMs)
- 출처: [What Are Vision Language Models? How AI Sees & Understands Images](https://www.youtube.com/watch?v=lOD_EE96jhM)

### **124.1 문제점**

*   기존 LLM(대규모 언어 모델)은 텍스트 데이터만 처리할 수 있어, 이미지나 시각적 정보를 포함한 데이터(예: PDF 문서 내 이미지, 차트, 다이어그램, 손글씨 메모 등)를 분석하거나 활용하지 못하는 한계가 있음.  
    *    예를 들어, 기존 LLM은 PDF 파일에 포함된 스캔된 텍스트나 그래프를 직접 읽을 수 없으며, 오직 별도로 추출된 텍스트 입력에만 의존한다. 이로 인해 복합적인 문서(텍스트와 이미지가 혼합된 자료)나 시각적 맥락을 요구하는 작업에서 제한적이었다. 이는 특히 비즈니스 보고서, 학술 논문, 의료 기록 등에서 중요한 정보를 놓치게 만드는 요인이었다.

### **124.2 VLM 소개**

*   텍스트와 이미지를 동시에 입력으로 받아 이를 통합적으로 해석하고, 텍스트 기반의 응답을 생성할 수 있는 멀티모달 인공지능 모델.  
    *    VLM은 텍스트와 이미지 간의 상호작용을 이해하도록 설계된 모델로, 시각적 데이터와 언어 데이터를 결합해 더 풍부한 맥락을 파악한다. 예를 들어, 사용자가 사진을 업로드하고 “이 사진에서 어떤 감정이 느껴지나요?”라고 물으면, VLM은 이미지의 시각적 요소(표정, 배경 등)와 질문의 의도를 함께 분석해 답변한다. 이는 기존 LLM의 텍스트 중심 처리 방식에서 한 단계 진화한 기술이다.

### **124.3 VLM 활용 사례**

*   **VQA (Visual Question Answering):** 이미지를 분석하고 이에 대한 질문에 답변. 예: 도시 사진을 보여주며 “이곳은 낮인가, 밤인가?”라는 질문에 “밤”이라고 답하거나, 사진 속 상황을 묘사.  
    *    VQA는 의료 영상 진단(예: X-ray 이미지에서 이상 여부 판단), 자율주행(도로 표지판 인식 후 질문 응답), 교육(교재 속 이미지 기반 퀴즈 해결) 등 다양한 분야에서 활용된다. 사용자가 이미지를 제공하고 구체적인 질문을 하면, VLM이 이미지의 세부 정보를 바탕으로 정확한 답변을 제공한다.
*   **이미지 캡셔닝:** 이미지 내용을 자연어로 설명. 예: “황금빛 털을 가진 개가 공원에서 빨간 공을 쫓고 있다.”  
    *    이는 소셜 미디어에서 자동 캡션 생성, 시각 장애인을 위한 이미지 설명 생성, 또는 콘텐츠 관리 시스템에서 이미지 태깅 작업에 유용하다. VLM은 이미지의 주요 객체, 배경, 동작 등을 파악해 간결하고 정확한 설명을 만든다.
*   **문서 이해:** 스캔된 영수증, 계약서, 또는 손글씨 노트에서 텍스트를 추출하고 정리하거나 요약.  
    *    예를 들어, 스캔된 영수증 이미지를 분석해 구매 항목, 금액, 날짜를 추출한 뒤 이를 표로 정리하거나, 손글씨로 작성된 회의 메모를 디지털 텍스트로 변환해 요약할 수 있다. 이는 비즈니스 프로세스 자동화나 아카이브 디지털화에 큰 도움을 준다.
*   **그래프 분석:** 데이터 시각화 자료(예: 판매 보고서의 차트)를 이해하고, 추세나 패턴을 설명.  
    *    VLM은 막대그래프, 선 그래프, 파이 차트 등을 분석해 “지난 3개월간 매출이 20% 증가했다”와 같은 통찰을 제공할 수 있다. 이는 데이터 분석가가 차트를 해석하는 데 소요되는 시간을 줄이고, 비전문가도 복잡한 시각화 자료를 쉽게 이해하도록 돕는다.

### **124.4 VLM 작동 방식**

1.  **텍스트 입력:** LLM은 텍스트 프롬프트를 토큰화하여 단어, 문장, 문맥을 이해할 수 있는 형태로 변환.  
    *    텍스트 토큰화는 자연어 처리(NLP)에서 핵심적인 과정으로, 문장을 작은 단위(토큰)로 나누어 모델이 문맥과 의미를 파악하도록 한다. 예를 들어, “이 사진은 무엇인가요?”라는 질문은 여러 토큰으로 나뉘어 모델에 입력된다.
2.  **이미지 입력:**  
    *   **Vision Encoder:** 이미지를 고차원 수치 데이터로 변환. 이미지의 패턴, 엣지, 텍스처, 공간적 관계 등을 추출해 Feature Vector를 생성.  
        *    Vision Encoder는 CNN(합성곱 신경망)이나 Transformer 기반 비전 모델을 사용해 이미지의 시각적 특징을 추출한다. 예를 들어, 고양이 사진을 입력하면 고양이의 털 색상, 눈 위치, 배경 요소 등을 수치화된 벡터로 변환한다.
    *   **프로젝터:** Feature Vector를 LLM이 이해할 수 있는 토큰 기반 형식(이미지 토큰)으로 매핑.  
        *    프로젝터는 이미지 데이터와 텍스트 데이터를 통합하기 위한 중간 다리 역할을 한다. 이미지에서 추출된 정보를 텍스트 토큰과 유사한 형식으로 변환해 LLM이 두 데이터를 함께 처리할 수 있게 한다. 이는 VLM의 멀티모달 처리의 핵심이다.
3.  **LLM 처리:** 텍스트 토큰과 이미지 토큰을 결합해 상호작용을 분석하고, 이를 바탕으로 텍스트 응답을 생성.  
    *    LLM은 텍스트와 이미지 토큰을 동시에 입력받아, 두 데이터 간의 관계를 학습하고 문맥에 맞는 응답을 생성한다. 예를 들어, “사진 속 동물은 무엇인가?”라는 질문과 고양이 사진을 입력받으면, 이미지 토큰에서 고양이의 특징을, 텍스트 토큰에서 질문의 의도를 분석해 “이 사진 속 동물은 고양이입니다”라는 답변을 만든다.

### **124.5 VLM의 한계**

*   **토큰화 병목 현상:** 이미지 토큰화는 텍스트 토큰화에 비해 훨씬 더 많은 계산 자원을 요구하며, 처리 속도가 느려질 수 있음.  
    *    이미지 데이터는 고해상도일수록 더 많은 픽셀 정보를 처리해야 하므로, 이를 토큰화하는 과정에서 GPU/TPU 같은 고성능 하드웨어가 필요하다. 이는 특히 실시간 애플리케이션에서 병목 현상을 유발할 수 있다.
*   **환각 (Hallucination):** 모델이 사실과 다른 그럴듯한 정보를 생성. 이는 VLM이 통계적 패턴에 의존해 학습하기 때문에 발생.  
    *    예를 들어, VLM이 특정 이미지에서 존재하지 않는 객체(예: 사진에 없는 나무)를 설명하거나, 잘못된 맥락을 생성할 수 있다. 이는 학습 데이터의 한계와 모델의 과도한 일반화에서 비롯된다.
*   **학습 데이터 편향:** 학습 데이터에 포함된 편향으로 인해 특정 문화나 지역의 이미지를 잘못 해석할 가능성.  
    *    예를 들어, 서구 중심의 데이터로 학습된 VLM은 아시아 전통 의상이나 문화적 상징을 잘못 해석하거나, 특정 인종이나 성별에 대한 고정관념을 반영할 수 있다. 이는 공정성과 신뢰성 문제를 야기하며, 다양한 데이터셋으로 학습을 보완해야 한다.

## 125. Generative AI 기반 챗봇 구축: 인간 제어와 LLM 활용의 균형
- 출처: [Conversational AI vs. Generative AI: Finding the Perfect Balance](https://www.youtube.com/watch?v=DpD8QB-6Pc8)


Generative AI의 발전은 챗봇 개발에 혁신을 가져왔지만, 동시에 정확성과 제어 가능성이라는 새로운 도전 과제를 제시합니다. 이 글에서는 전통적인 챗봇 구축 방식과 Generative AI 기반 Retrieval-Augmented Generation (RAG) 방식의 장단점을 비교하고, 두 접근법을 결합한 하이브리드 방식의 효과를 탐구합니다.

###  125.1 **Generative AI의 등장:**  
*   대규모 언어 모델(LLM)을 활용한 Generative AI는 챗봇 개발 속도를 획기적으로 단축시켰습니다. 기존에는 복잡한 학습 데이터 준비와 모델 훈련이 필요했지만, Generative AI는 사전 학습된 모델을 활용해 빠르게 챗봇을 구현할 수 있습니다. 그러나 답변의 정확성과 일관성을 유지하기 위한 제어 가능성이 낮아지는 단점이 있습니다. 예를 들어, LLM은 맥락에 따라 예상치 못한 답변을 생성하거나, 특정 도메인에서 부정확한 정보를 제공할 가능성이 있습니다.

### 125.2   **과거 챗봇 구축 방식:**  
전통적인 챗봇은 주로 규칙 기반 또는 분류기(Classifier) 기반으로 구축되었습니다. 이 방식은 다음과 같은 특징을 가집니다: 

*   **분류기 기반 자연어 이해:** 사용자의 질문을 이해하기 위해 의도(Intent)를 정의하고, 이를 분류하는 모델을 학습시켰습니다. 예를 들어, "영업시간"이나 "계정 복구"와 같은 의도를 사전에 정의합니다.  
*   **수동 답변 작성:** 각 의도에 대해 개발자가 직접 답변을 작성하여, 정확하고 일관된 응답을 보장했습니다.  
*   **장점:** 자주 묻는 질문(FAQ, 예: 영업시간, 계정 문제)에 대한 답변은 높은 정확도로 제어 가능했습니다. 이는 특히 고객 서비스에서 신뢰도를 높이는 데 유리했습니다.  
*   **한계:** 질문 빈도를 분석하면, 자주 묻는 질문(High-Frequency Questions)을 넘어서는 "롱테일(Long-Tail)" 질문들에 대한 처리 능력이 떨어졌습니다. 이는 학습 데이터가 부족하거나, 분류기의 학습 효과가 특정 지점 이후 감소(수익 감소 지점, Diminishing Returns)하기 때문입니다. 결과적으로 드물게 묻는 질문에 대해 오답이 발생하거나 답변을 생성하지 못하는 경우가 빈번했습니다.

### 125.3  **Generative AI (RAG) 기반 챗봇:**  
Retrieval-Augmented Generation (RAG)은 문서 검색과 생성형 AI를 결합한 방식으로, 전통적인 챗봇의 한계를 극복합니다.  
*   **분류기 학습 불필요:** RAG는 사전에 의도를 정의하거나 분류기를 학습시킬 필요가 없습니다. 대신, 문서 저장소(Document Repository)에 저장된 정보를 기반으로 답변을 생성합니다.  
*   **작동 방식:**  
    1. 사용자가 질문을 입력하면, 챗봇은 문서 저장소에서 관련 문서를 검색합니다. 이는 벡터 검색(Vector Search)이나 키워드 매칭과 같은 기술을 활용합니다.  
    2. 검색된 문서와 사용자 질문을 LLM에 입력하여, 문서 내용을 기반으로 답변을 요약하거나 생성합니다.  
*   **장점:**  
    - 자주 묻는 질문뿐만 아니라 드물게 묻는 질문까지 폭넓게 커버할 수 있습니다.  
    - 설정이 비교적 단순합니다. 검색 엔진의 튜닝(예: 관련성 점수 조정)과 답변 생성 방식(예: 간결함 또는 상세함 조정)만으로 챗봇을 최적화할 수 있습니다.  
    - 사전 정의된 의도 없이도 다양한 질문에 대응 가능하여, 유연성이 높습니다.  
*   **단점:**  
    - LLM이 생성한 답변은 문서 내용에 의존하지만, 여전히 부정확하거나 맥락에서 벗어난 답변을 생성할 가능성이 있습니다.  
    - 답변의 정확한 제어를 보장하기 어렵습니다. 예를 들어, 특정 질문에 대해 항상 동일한 답변을 제공해야 하는 경우(예: 법적 고지), RAG 방식은 일관성을 보장하기 어려울 수 있습니다.

### 125.4   **해결책: 하이브리드 방식**  
전통적인 방식과 RAG 방식의 장점을 결합한 하이브리드 접근법은 챗봇의 효율성과 신뢰성을 동시에 높일 수 있습니다.  
*   **자주 묻는 질문 처리 (캐시 영역):**  
    - 자주 묻는 질문에는 기존의 분류기 기반 접근법과 큐레이션된 답변을 사용합니다. 이를 통해 빠르고 정확한 응답을 제공하며, 답변의 일관성과 제어 가능성을 유지합니다.  
    - 이러한 답변은 캐시(Cache)처럼 저장되어, 빠른 응답 속도를 보장합니다. 예를 들어, "영업시간은 언제인가요?"와 같은 질문에 대해 미리 준비된 답변을 즉시 제공합니다.  
*   **드물게 묻는 질문 처리:**  
    - 드물게 묻는 질문이나 새로운 질문에는 RAG 패턴을 적용하여, 문서 저장소에서 관련 정보를 검색하고 LLM을 통해 답변을 생성합니다.  
    - 이를 통해 챗봇은 예상치 못한 질문에도 유연하게 대응할 수 있습니다.  
*   **구현 예시:** 고객 서비스 챗봇에서 "계정 비밀번호 재설정"과 같은 자주 묻는 질문은 분류기와 큐레이션된 답변으로 처리하고, "지난달 프로모션 코드 적용 방법"과 같은 드문 질문은 RAG 방식으로 처리합니다.

### 125.5   **결론**  
완전 생성형 답변(Full Generative AI)과 RAG 기반 접근법의 균형을 통해, 챗봇은 높은 정확성과 유연성을 동시에 달성할 수 있습니다. 하이브리드 방식은 자주 묻는 질문에 대한 신뢰성과 드물게 묻는 질문에 대한 포괄성을 모두 충족하며, 효과적인 챗봇 구축을 가능하게 합니다. 이는 특히 고객 서비스, 기술 지원, 정보 제공 등 다양한 도메인에서 실질적인 가치를 제공합니다.


## 126. 멀티 에이전트 AI 연구 프로세스
- 출처: [AI Agents in Action: How Research Agents Solve Complex Problems](https://www.youtube.com/watch?v=j_Q1cL6Cog4)

- [관련 페이퍼](https://arxiv.org/abs/2309.06180)

### 126.1 **핵심** 
복잡한 연구 과정을 자동화하기 위해 특화된 에이전트들이 협업하는 시스템.  
* 여러 AI 에이전트가 각자의 전문성을 바탕으로 역할을 분담하여, 인간 연구자의 워크플로우를 모방하며 효율적이고 체계적으로 연구를 수행합니다. 예를 들어, 의학 연구에서는 데이터 수집 에이전트가 PubMed에서 논문을 수집하고, 분석 에이전트가 이를 요약하며, 최종적으로 보고서 작성 에이전트가 결과를 정리하는 식으로 협업합니다.

### 126.2 **기본 원리** 
사람이 연구하는 방식과 유사하게, 명확한 목표 설정, 계획 수립, 데이터 수집, 인사이트 분석, 결과 도출의 단계를 거침.  
* 이 원리는 인간의 과학적 탐구 과정을 디지털화한 것으로, 각 단계는 독립적이면서도 상호 연결되어 있습니다. 예를 들어, 기후 변화 연구를 수행할 때, 목표는 “탄소 배출 감소 기술의 효과”로 설정되고, 이를 기반으로 데이터 수집(논문, 통계 데이터)과 분석(효과성 비교)이 진행됩니다. 이 과정은 인간 연구자가 논문을 읽고 가설을 검증하는 방식과 유사합니다.

### 126.3 **오픈 소스 프레임워크 활용** 
LangGraph, Crew AI, LangFlow 등은 특정 분야에 맞춰 이러한 멀티 에이전트 워크플로우를 쉽게 정의할 수 있도록 지원.  
* LangGraph는 워크플로우를 그래프 기반으로 설계해 복잡한 에이전트 간 상호작용을 지원하며, Crew AI는 역할 기반 에이전트 협업을 간소화합니다. LangFlow는 시각적 인터페이스로 비전문가도 워크플로우를 구성할 수 있게 합니다. 예를 들어, LangGraph를 사용하면 의료 데이터 분석 워크플로우에서 데이터 수집, 전처리, 분석 단계를 노드로 정의해 순차적으로 실행할 수 있습니다.

### 126.4 **5단계 연구 프로세스**

1. **연구 목표 정의:**  
   * *에이전트 역할:* 연구 목표 정의  
   * *입력:* 사용자 쿼리 (예: “AI 기반 질병 진단의 정확도 향상 방안”)  
   * *출력:* 문제 해결 목표 명확화, 결과물 종류(데이터, 요약, 보고서 등) 결정  
   * *핵심:* 명확한 목표 설정은 더 나은 결과로 이어짐.  
   * 이 단계는 연구의 방향성을 결정하는 가장 중요한 단계입니다. 예를 들어, 사용자가 “암 진단 AI의 성능 향상”을 요청하면, 에이전트는 구체적으로 “딥러닝 모델의 진단 정확도를 10% 향상시키는 방법”으로 목표를 세분화하고, 결과물로 “기술 보고서”를 지정할 수 있습니다. 명확한 목표는 후속 단계에서 데이터 소스 선택과 분석 방향을 결정짓습니다.

2. **연구 계획 수립:**  
   * *에이전트 역할:* 연구 전략가  
   * *출력:* 세부 질문 생성, 연구 템플릿 생성, 초기 데이터 소스 제안(학술 논문, 코드 저장소, 데이터베이스 등)  
   * *구현 선택:* 단일 에이전트 또는 복수 에이전트(검토 및 개선 용이)  
   * 연구 전략가는 목표를 세부 질문으로 분해하고, 이를 해결하기 위한 로드맵을 작성합니다. 예를 들어, “AI 기반 질병 진단” 연구라면 “어떤 데이터셋이 적합한가?”, “최신 논문은 어떤 알고리즘을 다루는가?”와 같은 질문을 생성하고, PubMed, arXiv, GitHub 등을 데이터 소스로 제안합니다. 복수 에이전트를 활용하면 한 에이전트가 계획을 수립하고 다른 에이전트가 이를 검토해 최적화할 수 있습니다.

3. **데이터 수집:**  
   * *에이전트 역할:* 데이터 마이너  
   * *데이터 소스:* 학술 논문, 온라인 데이터베이스, 연구 저장소, API 호출 등  
   * *핵심:* 신뢰할 수 있는 데이터 소스 우선순위 부여 (예: 의학 연구 - 동료 평가 저널 > 블로그)  
   * *주의:* 데이터 오염, 허위 정보, 조작에 취약하므로 안전 우선.  
   * 데이터 마이너는 신뢰도 높은 소스에서 데이터를 수집하며, 예를 들어 의학 연구에서는 PubMed나 Google Scholar의 동료 평가 논문을 우선적으로 수집합니다. 블로그나 소셜 미디어는 신뢰도가 낮아 제외하거나 보조 자료로만 사용합니다. 또한, 데이터 오염(예: 잘못된 데이터셋)이나 허위 정보(예: 편향된 기사)를 방지하기 위해 출처의 신뢰성을 자동으로 평가하는 알고리즘을 적용할 수 있습니다.

4. **인사이트 분석 및 개선:**  
   * *에이전트 역할:* 데이터 분석가  
   * *활용:* 지식 저장소, RAG (Retrieval Augmented Generation), 팩트 체크 에이전트  
   * *검토 사항:* 데이터 출처 신뢰성, 인사이트 일관성, 기존 지식과의 모순 여부 등  
   * *핵심:* 불일치 또는 편향된 소스를 감지하고, 허위 정보를 걸러내어 핵심 내용 강조.  
   * *추가:* 필요시 데이터 추가 요청 및 연구 계획 수정 (연구 전략가와 협력)  
   * 이 단계에서는 RAG를 활용해 수집된 데이터를 기반으로 인사이트를 도출하고, 팩트 체크 에이전트가 결과를 검증합니다. 예를 들어, AI 진단 연구에서 특정 알고리즘이 95% 정확도를 보인다고 주장하는 논문을 발견하면, 팩트 체크 에이전트는 해당 논문의 데이터셋 크기, 실험 조건 등을 검토해 신뢰성을 판단합니다. 불일치(예: 상충되는 연구 결과)가 발견되면 연구 전략가와 협력해 추가 데이터를 요청하거나 계획을 수정합니다.

5. **결과 생성:**  
   * *에이전트 역할:* 연구 작가  
   * *출력:* 구조화된, 사람이 읽기 쉬운 형식의 결과물 (학술 논문, 기술 보고서 등)  
   * *활용:* 특정 글쓰기 스타일에 특화된 파인튜닝된 LLM  
   * *핵심:* 성능 측정 기준 및 테스트 정의 (ITBench 등 활용)  
   * 연구 작가는 분석된 데이터를 바탕으로 논문, 보고서, 또는 요약문과 같은 결과물을 작성합니다. 예를 들어, IEEE 형식의 학술 논문을 작성하도록 파인튜닝된 LLM은 서론, 방법론, 결과, 결론 섹션을 체계적으로 구성합니다. ITBench와 같은 평가 도구를 활용해 결과물의 명확성, 논리성, 전문성을 검증하며, 필요시 수정 작업을 수행합니다.

### 126.5 **결론**

* 멀티 에이전트 AI 연구는 지식 탐구를 가속화할 수 있지만, 책임감 있는 구축이 중요.  
  * 이 시스템은 연구 속도를 획기적으로 높일 수 있지만, 데이터 신뢰성, 편향 관리, 윤리적 고려가 필수적입니다. 예를 들어, 편향된 데이터로 학습된 모델은 잘못된 결론을 도출할 수 있으므로, 다양한 데이터 소스와 엄격한 검증 절차가 필요합니다.

* 생산성 (단위 시간당 논문 수)뿐 아니라 유익성 (편향 없는 양질의 연구 논문, 공공의 이익)도 중요.  
  * 단순히 많은 논문을 빠르게 생산하는 것보다, 신뢰할 수 있고 공공의 이익에 기여하는 연구가 중요합니다. 예를 들어, 공중보건 연구에서는 결과물이 실제 정책 결정에 영향을 미칠 수 있으므로 정확성과 객관성이 필수적입니다.

* 속도뿐만 아니라 신뢰와 안전성 확보가 핵심.  
  * 신뢰와 안전성을 확보하기 위해, 에이전트는 데이터 출처를 투명하게 공개하고, 결과물에 대한 검증 과정을 명시해야 합니다. 예를 들어, AI 연구 보고서에는 사용된 데이터 소스, 분석 방법, 한계점을 명확히 기술해 신뢰도를 높입니다.

## 127. VLLM: LLM 추론 효율성을 높이는 오픈소스 프로젝트
- 출처: [What is a VLLM? Efficient AI for Large Language Models](https://www.youtube.com/watch?v=McLdlg5Gc9s)

## 127.1 **문제점**

대규모 언어 모델(LLM)은 강력한 성능을 제공하지만, 여러 가지 도전 과제를 동반합니다:

*   **높은 계산량 요구:** LLM은 추론 과정에서 막대한 계산 자원을 필요로 하며, 이는 고성능 GPU와 같은 하드웨어 의존도를 높입니다. 이는 특히 대규모 배포 환경에서 비용 문제를 야기합니다.
*   **비효율적인 GPU 메모리 할당:** 기존 LLM 프레임워크는 메모리 파편화로 인해 GPU 메모리를 비효율적으로 사용합니다. 이로 인해 메모리 낭비가 발생하고, 추가 하드웨어 구매가 필요할 수 있습니다.
*   **응답 지연:** 사용자 요청이 증가할수록 배치 처리의 병목 현상이 발생하여 응답 시간이 길어집니다. 이는 사용자 경험을 저하시키는 주요 원인입니다.
*   **복잡한 분산 환경 구축:** 여러 GPU를 활용한 분산 추론 환경을 구축하는 것은 복잡하며, GPU의 메모리와 연산 능력의 한계를 초과하기 쉬워 안정적인 운영이 어렵습니다.

### 127.2 **VLLM 소개**

*   **출처 및 목적:** VLLM은 UC 버클리 LMSYS 팀에서 개발된 오픈소스 프로젝트로, 대규모 언어 모델의 추론 효율성을 극대화하는 데 초점을 맞췄습니다. LMSYS는 Chatbot Arena와 같은 프로젝트로도 잘 알려져 있습니다.
*   **해결 목표:** 메모리 파편화 문제, 비효율적인 배치 처리, 그리고 복잡한 분산 추론 환경의 어려움을 해결하여 LLM의 실시간 추론 성능을 향상시키는 것을 목표로 합니다.
*   **지원 범위:** 다양한 LLM 아키텍처(예: LLaMA, Mistral, Granite 등)를 지원하며, 양자화(Quantization), 툴 호출(Tool Calling), 멀티모달 모델 지원 등 최신 기술과의 호환성을 제공합니다.

### 127.3 **VLLM 핵심 기술**

*   **Paged Attention:** 메모리를 페이지 단위로 관리하여 필요한 시점에만 메모리를 동적으로 할당합니다. 이는 운영 체제의 가상 메모리 관리 방식과 유사하며, 메모리 파편화를 줄이고 효율적인 메모리 사용을 가능하게 합니다.
*   **Continuous Batching:** 전통적인 정적 배치 처리와 달리, 요청이 들어오는 즉시 GPU 슬롯을 동적으로 채워 처리합니다. 이를 통해 GPU 활용률을 극대화하고 처리량(Throughput)을 크게 증가시킵니다.
*   **CUDA 드라이버 최적화:** NVIDIA GPU의 CUDA 드라이버를 최적화하여 특정 하드웨어에서 최대 성능을 발휘하도록 설계되었습니다. 이는 특히 최신 GPU 아키텍처(예: A100, H100)에서 성능 향상을 보장합니다.

### 127.4 **VLLM 사용법**

*   **운영 환경:** VLLM은 Linux 기반 환경에서 최적화되어 있으며, 가상 머신(VM), Kubernetes와 같은 컨테이너 오케스트레이션 환경, 또는 로컬 머신에서 런타임 또는 CLI 도구로 실행할 수 있습니다.
*   **설치 및 호환성:** 간단한 명령어 `pip install vllm`로 설치 가능하며, OpenAI API와 호환되는 엔드포인트를 제공하여 기존 애플리케이션과의 통합이 용이합니다.
*   **최적화:** 양자화된 모델(예: 4비트, 8비트 정밀도)을 지원하여 GPU 메모리 사용량을 줄이면서도 모델의 정확도를 유지합니다. 이를 통해 고성능 하드웨어 없이도 효율적인 추론이 가능합니다.

### 127.5 **VLLM의 장점**

*   **높은 처리량:** Hugging Face Transformers나 TGI(Text Generation Inference)와 같은 기존 프레임워크 대비 최대 2~3배 높은 처리량을 제공합니다. 이는 대규모 사용자 요청을 처리하는 환경에서 특히 유리합니다.
*   **효율적인 GPU 리소스 활용:** Paged Attention과 Continuous Batching을 통해 GPU 메모리 사용률을 최대 90% 이상으로 끌어올려 하드웨어 효율성을 극대화합니다.
*   **낮은 지연 시간:** 실시간 응답이 중요한 애플리케이션(예: 챗봇, 실시간 번역)에서 지연 시간을 최소화하여 사용자 경험을 개선합니다.
*   **커뮤니티 성장:** 활발한 오픈소스 커뮤니티와 빠른 업데이트 주기로 인해 최신 LLM 기술 트렌드에 빠르게 대응하며, 다양한 산업에서의 채택이 증가하고 있습니다.

### 127.6 **요약**

VLLM은 대규모 언어 모델의 추론 과정에서 발생하는 비용, 속도, 메모리 효율성 문제를 해결하는 혁신적인 오픈소스 프로젝트입니다. Paged Attention, Continuous Batching, CUDA 최적화와 같은 핵심 기술을 통해 높은 처리량과 낮은 지연 시간을 제공하며, 다양한 LLM 아키텍처와의 호환성으로 폭넓은 활용 가능성을 자랑합니다. 이를 통해 기업과 개발자는 비용을 절감하고 사용자 경험을 개선할 수 있는 강력한 도구를 손에 넣을 수 있습니다.

## 128. GraphRAG 시스템 구축 및 활용 가이드
- 출처: [GraphRAG Explained: AI Retrieval with Knowledge Graphs & Cypher](https://www.youtube.com/watch?v=tTwBKXHIiMg)

### 128.1 **개요**

*   **GraphRAG란?** 
    * GraphRAG는 벡터 검색(VectorRAG)의 대안으로, 지식 그래프(Knowledge Graph)를 활용하여 대규모 언어 모델(LLM)을 통해 데이터를 검색하고 질의하는 시스템입니다. 벡터 검색이 텍스트의 의미적 유사성을 기반으로 작동한다면, GraphRAG는 데이터 간의 구조적 관계를 중점적으로 다룹니다. 예를 들어, 회사 조직도에서 직원과 부서 간의 관계를 명확히 표현하고 질의할 수 있습니다.
*   **지식 그래프의 특징:** 
    *   데이터를 노드(Node, 예: 사람, 장소, 개념)와 엣지(Edge, 노드 간 관계, 예: "소속", "관리")로 저장합니다. 엣지가 단순히 연결을 넘어 관계의 의미를 담아, 데이터의 맥락을 더 깊이 이해할 수 있습니다. 예를 들어, "Alice가 Bob의 상사"라는 관계는 단순 텍스트 검색보다 풍부한 정보를 제공합니다.
*   **강점:** 
    *   복잡한 관계를 기반으로 한 질의에 특히 효과적입니다. 예를 들어, "특정 프로젝트에 참여한 직원의 상사를 찾아줘"와 같은 복잡한 질문에 대해 정확한 답변을 생성할 수 있습니다.

### 128.2 **구축 단계**

1.  **지식 그래프 생성 및 채우기:**
    *   **작업 내용:** 비정형 텍스트 데이터(예: 이메일, 문서, 보고서)에서 LLM을 활용해 엔터티(사람, 조직, 장소 등)와 관계(소속, 협업 등)를 추출합니다. 이 데이터를 구조화된 형태로 변환하여 지식 그래프에 삽입합니다.
    *    예를 들어, 회사 보고서에서 "Alice는 마케팅 팀의 매니저다"라는 문장에서 "Alice(노드) - 관리자 -> 마케팅 팀(노드)"라는 관계를 추출합니다. 이 과정은 LLM이 문맥을 이해하고, 구조화된 데이터로 변환하는 데 강점을 발휘합니다. Neo4j와 같은 그래프 데이터베이스는 이러한 데이터를 효율적으로 저장하고 관리합니다.

2.  **LLM을 사용한 데이터 질의:**
    *   **작업 흐름:** 사용자가 자연어로 질문(예: "마케팅 팀의 매니저는 누구인가요?")을 입력하면, LLM이 이를 Cypher 쿼리(그래프 데이터베이스용 쿼리 언어)로 변환합니다. Cypher 쿼리가 데이터베이스에서 실행된 후, 결과를 LLM이 다시 자연어로 변환해 사용자에게 제공합니다.
    *    예를 들어, 사용자가 "Alice가 속한 부서는?"이라고 질문하면, LLM은 이를 `MATCH (p:Person {name: "Alice"})-[:BELONGS_TO]->(d:Department) RETURN d.name`과 같은 Cypher 쿼리로 변환합니다. 결과로 "마케팅 팀"이 반환되면, LLM은 이를 "Alice는 마케팅 팀에 속해 있습니다"로 자연스럽게 변환합니다.

### 128.3 **구현**

*   **필요 사항:** 
    *   **API 키 및 프로젝트 ID:** LLM 서비스(예: IBM watsonx.ai)나 그래프 데이터베이스에 접근하기 위한 인증 정보.
    *   **컨테이너화 도구:** Podman 또는 Docker를 사용하여 Neo4j와 같은 데이터베이스를 로컬 또는 클라우드 환경에서 실행.
    *   **Neo4j:** 오픈 소스 그래프 데이터베이스로, 지식 그래프를 저장하고 질의하는 데 사용됩니다.
*   **Neo4j 설정:** 
    *   Docker를 사용해 Neo4j 인스턴스를 생성하고, APOC(Advanced Procedures and Functions) 플러그인을 추가하여 복잡한 데이터 처리 및 시각화를 지원합니다. 예를 들어, APOC는 텍스트에서 엔터티를 추출하거나 그래프 데이터를 JSON 형식으로 내보내는 데 유용합니다.
*   **Python 환경 설정:** 
    *   Python 3.11.3을 권장하며, 가상 환경(예: `venv`)을 설정해 라이브러리 간 충돌을 방지합니다.
    *   필요한 라이브러리: `os`, `getpass`(보안 인증), `LangChain`(LLM과 데이터베이스 연결), `ibm-watsonx-ai`(IBM LLM 사용 시).
*   **데이터 준비:** 
    *   텍스트 데이터(예: 직원 명단, 부서 정보)를 LangChain의 `Document` 클래스로 변환하여 그래프에 적합한 형식으로 준비합니다. 예를 들어, CSV 파일에 "이름, 부서, 직책"이 포함된 경우, 이를 노드(직원, 부서)와 엣지(소속)로 변환합니다.
*   **LLM 설정:**
    *   LLM의 온도(temperature)를 낮게 설정(예: 0.2)하여 생성된 텍스트가 더 정확하고 덜 창의적이 되도록 합니다. 토큰 수는 높게 설정(예: 2048)하여 상세한 그래프 설명을 생성하도록 유도합니다.
    *   노드(예: Person, Department)와 관계(예: BELONGS_TO, MANAGES)를 명시적으로 제한하여 그래프의 일관성을 유지합니다. 예를 들어, "사람은 부서에만 소속된다"와 같은 규칙을 설정.
*   **지식 그래프 시각화:** 
    *   Neo4j의 브라우저 인터페이스를 통해 그래프를 시각화합니다. 예를 들어, 직원과 부서 간의 관계를 노드와 엣지로 표시하여 데이터 구조를 직관적으로 확인할 수 있습니다. 스키마(노드와 관계의 정의)를 점검하여 데이터 무결성을 보장합니다.

### 128.4 **질의**

*   **Cypher 쿼리:** SQL과 유사하지만, 그래프 데이터베이스에 최적화된 쿼리 언어입니다. 예를 들어, `MATCH (p:Person)-[:MANAGES]->(d:Department) RETURN p.name, d.name`은 관리자와 부서를 반환합니다.
*   **자연어 질의 변환:** LangChain의 `FewShotPromptTemplate`을 사용해 LLM이 정확한 Cypher 쿼리를 생성하도록 학습시킵니다. 예를 들어, "누가 마케팅 팀을 관리해?"라는 질문에 대해, LLM은 학습된 예시를 바탕으로 올바른 Cypher 쿼리를 생성합니다.
*   **자연어 응답 생성:** Cypher 쿼리 결과(예: "Alice, Marketing")를 LLM이 "Alice가 마케팅 팀을 관리합니다"로 변환하여 사용자에게 제공합니다.

### 128.5 **GraphRAG vs VectorRAG**

| 구분              | GraphRAG                                                                 | VectorRAG                                                              |
|-------------------|--------------------------------------------------------------------------|------------------------------------------------------------------------|
| **데이터 저장 방식** | 지식 그래프(노드와 엣지로 구조화된 데이터). 예: 직원과 부서 간 관계를 명시적으로 저장. | 벡터 데이터베이스(텍스트를 임베딩 벡터로 변환). 예: 텍스트의 의미를 수치화하여 저장. |
| **검색 방식**       | LLM이 Cypher 쿼리를 생성해 그래프 데이터베이스에서 관계 중심으로 검색.         | 시맨틱 검색을 통해 텍스트의 의미적 유사성을 기반으로 결과 반환.               |
| **장점**           | 복잡한 관계와 맥락을 정확히 반영하며, 전체 데이터에 대한 요약 가능. 예: 조직 내 계층 구조를 탐색. | 빠르고 간단한 검색, 특히 단일 문서나 짧은 텍스트에 효과적.                   |
| **단점**           | 설정과 구축이 복잡하며, 초기 데이터 구조화에 시간이 걸림.                   | 관계 중심 질의에 약하며, 상위 검색 결과에 의존해 전체 맥락을 놓칠 수 있음.   |
| **활용 예시**       | 하이브리드 RAG 시스템: 벡터 검색으로 빠른 초기 결과를 얻고, GraphRAG로 관계 기반 상세 검색 수행. | 단일 문서 내 키워드 검색이나 FAQ 시스템에 적합.                         |

### 128.6 **결론**

GraphRAG는 복잡한 데이터 관계를 효과적으로 표현하고 검색하는 강력한 도구입니다. LLM과 지식 그래프를 결합하여 VectorRAG의 한계(예: 맥락 부족, 관계 탐색의 어려움)를 극복하며, 특히 조직 구조, 소셜 네트워크, 학술 연구 등 관계 중심 데이터에 적합합니다. 하이브리드 RAG 시스템으로 벡터 검색과 통합하면 더욱 강력한 검색 성능을 발휘할 수 있습니다.

