---
title: 1차시 2(빅데이터분석):웹 스크래핑(BeautifulSoup,Selenium)
layout: single
classes: wide
categories:
  - web scraping
tags:
  - html
  - beautifulsoup
  - selenium
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---


## **1. HTML 기본 개념 소개**
HTML은 웹 크롤링의 기초입니다. 간단한 예제를 통해 확인하자.

### **1.1 HTML 기본 구조**
```html
<!DOCTYPE html>
<html>
<head>
    <title>예제 페이지</title>
</head>
<body>
    <h1>환영합니다!</h1>
    <p id="intro">안녕하세요. 이것은 예제입니다.</p>
    <ul>
        <li class="item">첫 번째 항목</li>
        <li class="item">두 번째 항목</li>
    </ul>
</body>
</html>
```

- `<html>`: 문서의 시작과 끝.
- `<head>`: 메타 정보(예: 제목, 스타일 등).
- `<body>`: 실제 콘텐츠가 포함된 부분.
- `<h1>`, `<p>`, `<ul>`, `<li>`: 각각 제목, 단락, 목록, 항목을 나타내는 태그.
- `id`와 `class`: 특정 요소를 식별하거나 그룹화하는 속성.

### **1.2 CSS 선택자**

*   클래스 선택자 (Class Selector)
    *  **표기법:** HTML에서는 `class="클래스명"`으로 지정하고, CSS나 선택자에서는 `.클래스명` 형식으로
    *   **그룹화:** 특정 공통 속성을 가진 요소들을 논리적으로 그룹화할 때 유용합니다. 예를 들어, 여러 종류의 "뉴스 기사"가 있다면 `class="news-item"`을 부여하여 묶을 수 있다.
    *  **특징:**
        * **중복 가능:** 하나의 HTML 문서 내에서 동일한 클래스 이름을 가진 요소가 여러 개 있을 수 있다.
        * **다중 클래스:** 하나의 HTML 요소는 여러 개의 클래스를 가질 수 있습니다. (예: `<p class="text important">`) 이 경우 CSS에서 `.text`와 `.important` 스타일이 모두 적용됩니다.
  
* ID 선택자 (ID Selector)
    * **표기법:** HTML에서는 `id="아이디명"`으로 지정하고, CSS나 선택자에서는 `#아이디명` 형식으로 사용
    * **고유한 요소 식별:** HTML 문서 내에서 **오직 하나만 존재하는 고유한 요소**를 식별할 때 사용합니다. 예를 들어, 웹 페이지의 메인 메뉴 영역, 푸터 영역 등은 보통 ID를 부여하여 고유하게 식별합니다.
    * **특징:**
        * **고유성:** 하나의 HTML 문서 내에서 **동일한 ID를 가진 요소는 오직 하나만 존재해야 합니다.** (기술적으로는 여러 개를 넣어도 웹 브라우저가 오류를 내지는 않지만, 이는 표준을 위반하며 예측 불가능한 동작을 초래할 수 있습니다.)
        * **높은 명시도:** CSS에서 ID 선택자는 클래스 선택자보다 더 높은 우선순위(명시도)를 가집니다.

## **2. BeautifulSoup 기본 사용법**
BeautifulSoup은 HTML을 파싱하고 원하는 데이터를 추출하기 위한 도구입니다. 

### **2.1 설치 및 임포트**
```bash
pip install bs4
```
```python
from bs4 import BeautifulSoup
```

### **2.2 find()와 select() 비교**

웹 크롤링을 할 때 `find()`, `find_all()`, `select()`, `select_one()`은 Beautiful Soup 라이브러리에서 자주 사용되는 메서드들로, HTML 또는 XML 문서에서 원하는 데이터를 추출하는 데 사용됩니다. 각 메서드의 차이점을 간단히 설명하면 다음과 같습니다:

1.**`find()`**
- **기능**: 주어진 조건에 맞는 첫 번째 요소만 반환합니다.
    - CSS선택자 사용불가
- **반환값**: 단일 객체 (조건에 맞는 요소가 없으면 `None` 반환).
- **사용 예**:
  ```python
  element = soup.find('div', class_='example') #class_ : 파이썬의 예약어를 피하려고
  ```
  - 위 코드는 클래스 이름이 `example`인 첫 번째 `<div>` 태그를 찾습니다.

2.**`find_all()`**
- **기능**: 주어진 조건에 맞는 모든 요소를 리스트 형태로 반환합니다.
- **반환값**: 리스트 (조건에 맞는 요소가 없으면 빈 리스트 `[]` 반환).
- **사용 예**:
  ```python
  elements = soup.find_all('a', href=True)
  ```
  - 위 코드는 `href` 속성을 가진 모든 `<a>` 태그를 리스트로 반환합니다.



3.**`select()`**
- **기능**: CSS 선택자를 사용하여 조건에 맞는 모든 요소를 리스트 형태로 반환합니다.
- **반환값**: 리스트 (조건에 맞는 요소가 없으면 빈 리스트 `[]` 반환).
- **사용 예**:
  ```python
  elements = soup.select('div.example a')
  ```
  - 위 코드는 클래스 이름이 `example`인 `<div>` 내부의 모든 `<a>` 태그를 리스트로 반환합니다.

  ```
  elements = soup.select('div.example > a') #직계자식 선택자(>)
  ```
  - div 태그이면서 example 클래스를 가진 요소의 직계 자식인 `<a>` 태그만을 찾습니다. 즉, div.example 바로 아래에 있는 `<a>` 태그만 선택하고, 그 `<a>` 태그 안에 또 다른 `<a>` 태그가 있다면 그것은 선택하지 않습니다.


4.**`select_one()`**
- **기능**: CSS 선택자를 사용하여 조건에 맞는 첫 번째 요소만 반환합니다.
- **반환값**: 단일 객체 (조건에 맞는 요소가 없으면 `None` 반환).
- **사용 예**:
  ```python
  element = soup.select_one('div.example a')
  ```
  - 위 코드는 클래스 이름이 `example`인 `<div>` 내부의 첫 번째 `<a>` 태그를 반환합니다.


### **3. 간단한 예제**
위에서 작성한 HTML 파일을 파싱해 보는 예제를 제공하세요.

```python
from bs4 import BeautifulSoup

# HTML 문서 (앞서 작성한 예제)
html = """
<!DOCTYPE html>
<html>
<head>
    <title>예제 페이지</title>
</head>
<body>
    <h1>환영합니다!</h1>
    <p id="intro">안녕하세요. 이것은 예제입니다.</p>
    <ul>
        <li class="item">첫 번째 항목</li>
        <li class="item">두 번째 항목</li>
    </ul>
</body>
</html>
"""

# BeautifulSoup 객체 생성
soup = BeautifulSoup(html, "html.parser")

# 데이터 추출(find()는 태그 이름, 클래스, ID, 속성 등 키워드 인자 기반)
title = soup.title.text  # <title> 태그의 텍스트
heading = soup.h1.text   # <h1> 태그의 텍스트
intro = soup.find(id="intro").text  # id="intro"인 요소의 텍스트
items = [item.text for item in soup.find_all(class_="item")]  # class="item"인 모든 요소의 텍스트

# 결과 출력
print("제목:", title)
print("헤딩:", heading)
print("소개:", intro)
print("항목들:", items)
```

* **`태그.text`**:
    * 간단히 **모든 텍스트 콘텐츠를 있는 그대로** 가져오고 싶을 때 사용합니다.
    * 옵션이 필요 없고, 공백이나 줄 바꿈이 있어도 상관없을 때 편리합니다.
    * 예: `<title>Hello World</title>`에서 "Hello World"를 가져올 때 `soup.title.text`처럼 깔끔한 경우.
* **`태그.get_text()`**:
    * 추출할 텍스트의 **불필요한 공백이나 줄 바꿈을 제거**하고 싶을 때 (가장 많이 사용).
    * 텍스트 조각들 사이에 특정 **구분자를 삽입**하여 하나의 문자열로 만들고 싶을 때.
    * 웹 스크래핑 시 대부분의 경우 **`get_text(strip=True)`**를 사용하는 것이 훨씬 깔끔한 결과를 얻을 수 있어 권장됩니다.


## **3. HTML태그와 CSS선택자 비교**

### **3.1 HTML 태그**
- **HTML 태그**는 웹 페이지의 구조를 정의하는 기본 요소입니다.
- 태그는 `<`와 `>`로 감싸져 있으며, 특정 콘텐츠를 나타내거나 의미를 부여합니다.

**예시**
```html
<h1>제목</h1>
<p>단락 텍스트입니다.</p>
<a href="https://www.example.com">링크</a>
```

- `<h1>`: 제목을 나타냅니다.
- `<p>`: 단락(문단)을 나타냅니다.
- `<a>`: 하이퍼링크를 나타냅니다.

**역할**
- 웹 페이지의 **구조**와 **콘텐츠**를 정의합니다.
- 각 태그는 고유한 의미를 가지며, 브라우저가 이를 해석하여 화면에 표시합니다.


### **3.2 CSS 선택자**
- **CSS 선택자**는 HTML 문서 내에서 특정 요소를 선택하기 위한 규칙입니다.
- CSS 선택자는 스타일을 적용하거나 JavaScript/jQuery, BeautifulSoup 등을 통해 특정 요소를 찾을 때 사용됩니다.

**예시**
```css
/* 클래스 선택자 */
.highlight {
    color: red;
}

/* ID 선택자 */
#main-title {
    font-size: 24px;
}

/* 태그 선택자 */
h1 {
    text-align: center;
}
```

### 3.3 **종류**
1. **태그 선택자**: HTML 태그 이름을 직접 사용합니다.
   - 예: `h1`, `p`, `div`
2. **클래스 선택자**: 클래스 속성(`class`)을 사용합니다.
   - 예: `.highlight`, `.container`
3. **ID 선택자**: ID 속성(`id`)을 사용합니다.
   - 예: `#main-title`, `#footer`
4. **속성 선택자**: 특정 속성을 가진 요소를 선택합니다.
   - 예: `a[href]`, `input[type="text"]`
5. **계층 선택자**: 요소 간의 관계를 이용합니다.
   - 예: `div span`, `.container > p`


**차이점 비교**

| **항목**         | **HTML 태그**                                   | **CSS 선택자**                              |
|------------------|-----------------------------------------------|--------------------------------------------|
| **정의**         | 웹 페이지의 구조와 콘텐츠를 정의               | HTML 문서 내 특정 요소를 선택하기 위한 규칙 |
| **사용 목적**     | 브라우저가 콘텐츠를 표시하기 위해 사용          | 스타일 적용 또는 특정 요소를 찾기 위해 사용 |
| **형식**         | `<태그명>`으로 작성                            | `.클래스`, `#아이디`, `태그명` 등으로 작성  |
| **예시**         | `<h1>`, `<p>`, `<a href="...">`               | `h1`, `.highlight`, `#main-title`          |
| **주요 도구**    | HTML 문서 작성                                | CSS, JavaScript, jQuery, BeautifulSoup     |

### **3.4 예제를 통한 비교**

**HTML 코드**
```html
<div id="header">
    <h1 class="title">환영합니다!</h1>
    <p class="description">이곳은 예제 페이지입니다.</p>
</div>
```

**HTML 태그**
- `<div>`: 컨테이너 역할을 하는 태그.
- `<h1>`: 제목을 나타내는 태그.
- `<p>`: 단락 텍스트를 나타내는 태그.

**CSS 선택자**
- `#header`: ID가 `header`인 `<div>`를 선택.
- `.title`: 클래스가 `title`인 `<h1>`을 선택.
- `.description`: 클래스가 `description`인 `<p>`를 선택.


### **3.5 BeautifulSoup에서의 활용**
BeautifulSoup에서는 CSS 선택자를 사용하여 HTML 문서에서 원하는 요소를 쉽게 찾을 수 있습니다.

**예제**
```python
from bs4 import BeautifulSoup

html = """
<div id="header">
    <h1 class="title">환영합니다!</h1>
    <p class="description">이곳은 예제 페이지입니다.</p>
</div>
"""
soup = BeautifulSoup(html, "html.parser")

# HTML 태그로 접근
print(soup.h1.text)  # 출력: 환영합니다!

# CSS 선택자로 접근
print(soup.select_one("#header .title").text)  # 출력: 환영합니다!
print(soup.select_one(".description").text)    # 출력: 이곳은 예제 페이지입니다.
```
<br>

## **4. 주의사항: 크롤링 정책(`robots.txt`)을 준수**
- **`robots.txt`**는 웹사이트의 관리자가 크롤러(예: 검색 엔진 봇)에게 어떤 페이지를 크롤링할 수 있고, 어떤 페이지를 크롤링하지 말아야 하는지를 지정하는 **텍스트 파일**입니다. 

- 이 파일은 웹사이트의 루트 디렉토리에 위치하며, 크롤러가 웹사이트를 방문할 때 가장 먼저 확인하는 규칙 파일입니다. 
- 법적 강제력 없음
- 네이버: `https://www.naver.com/robots.txt`

```
User-agent: *
Disallow: /
Allow: /$
Allow: /.well-known/privacy-sandbox-attestations.json
```

1.**`Disallow: /`**
- 이 규칙은 "모든 경로(`/`)에 대해 크롤링을 금지한다"는 의미입니다.
- 즉, 기본적으로 웹사이트의 모든 페이지와 리소스는 크롤러가 접근할 수 없습니다.

2.**`Allow: /$`**
- 이 규칙은 "루트 URL(예: `https://example.com/`)만 크롤링을 허용한다"는 의미입니다.
- 여기서 `$`는 정규식 기호로, "문자열의 끝"을 나타냅니다.
- 따라서, `/$`는 "루트 경로로 끝나는 URL"만 허용한다는 뜻입니다.
  - 예: `https://example.com/` → 허용
  - 예: `https://example.com/page1`, `https://example.com/subdir/` → 차단

3.**`Allow: /.well-known/privacy-sandbox-attestations.json`**
- 이 규칙은 특정 파일인 `.well-known/privacy-sandbox-attestations.json`에 대해서만 크롤링을 허용합니다.
- 이 파일은 Privacy Sandbox 관련 정보를 담고 있으며, 검색 엔진이나 외부 서비스에서 접근해야 할 필요가 있습니다.

4.**결론**
*   `Disallow: /`는 기본적으로 모든 경로를 차단하는 규칙이고,  
`Allow: /$`와 `Allow: /.well-known/privacy-sandbox-attestations.json`은 특정 경로(루트 URL과 특정 파일)만 예외적으로 허용하는 규칙입니다.  

*   즉, **"전체를 막고 필요한 부분만 개방"** 하는 방식으로 작동하며, 이를 통해 웹사이트 운영자는 크롤러의 접근을 세밀하게 제어할 수 있습니다.


## 5. 셀레늄 (Selenium) 웹 자동화: 웹사이트를 로봇처럼 조종하기

여러분, 웹사이트에서 반복적으로 해야 하는 작업들(로그인, 버튼 클릭, 정보 입력, 스크롤 등)이 너무 귀찮을 때 없으셨나요? 또는 어떤 웹사이트는 특정 버튼을 눌러야만 숨겨진 정보가 나타나는 경우가 있죠? 이럴 때 우리 대신 웹사이트를 조작해 줄 똑똑한 도구가 바로 \*\*셀레늄(Selenium)\*\*입니다\!

### 5.1. 셀레늄이 뭐예요?

셀레늄은 파이썬으로 웹사이트를 **사람처럼 조종**하고 필요한 정보를 찾아오는 강력한 도구예요.

  * **웹사이트와 직접 놀아줘요!**

      * 이전에는 웹사이트에 "정보 보여줘\!" 하고 요청만 보내고 받은 HTML 코드(웹사이트의 설계도)를 분석해서 데이터를 얻었어요. 이걸 "클래식 웹 스크래핑"이라고 불러요. (마치 웹사이트에 편지만 보내는 것과 같죠.)
      * 하지만 셀레늄은 달라요. 마치 실제 사람이 웹 브라우저(크롬, 파이어폭스 등)를 열고, 마우스로 클릭하고, 키보드로 글자를 입력하고, 페이지를 아래로 스크롤하는 것처럼 **웹사이트와 직접 상호작용**해요. 이걸 "인터랙티브 웹 스크래핑"이라고 해요.

  * **언제 셀레늄이 필요할까요?**

      * **숨겨진 정보 찾기:** 어떤 웹사이트는 여러분이 스크롤을 내리거나, 특정 버튼을 클릭하거나, 마우스를 올려놓아야만 정보가 나타나는 경우가 있어요. 클래식 스크래핑으로는 이런 정보를 가져올 수 없지만, 셀레늄은 가능해요!
      * **웹 게임 자동화:** 웹 기반 게임에서 반복적인 클릭이나 조작을 대신 해주는 '봇'을 만들 때도 셀레늄이 필요해요.
      * **자동 로그인, 게시글 작성:** 매번 같은 아이디/비밀번호를 입력하고 게시글을 쓰는 등의 반복 작업을 자동화하고 싶을 때 유용해요.
      * **웹사이트 기능 테스트:** 개발자가 웹사이트의 버튼들이 잘 작동하는지, 로그인 과정에 문제가 없는지 등을 자동으로 테스트할 때도 사용해요.

### **5.2 Selenium의 기본 개념**
Selenium은 웹 브라우저를 자동화하여 동적 콘텐츠를 포함한 웹 페이지와 상호작용

- **WebDriver**: 브라우저를 제어. selenium 4부터 자동으로 chrome을 설정(수동 다운로드 불필요)
- **요소 선택**: `find_element`와 `find_elements`를 사용하여 HTML 요소를 찾는 방법.
- **동작 수행**: 클릭, 입력, 스크롤 등의 기본 동작.
- **대기(Wait)**: 페이지 로딩이나 요소가 나타날 때까지 기다리는 방법.


## 6. **로그인(login) 연습**
로그인 페이지에서 사용자 이름과 비밀번호를 입력하고 로그인 버튼을 클릭하는 방법을 연습할 수 있습니다.

### 6.1 자동 로그인 실습 사이트

*   URL: [https://the-internet.herokuapp.com/login](https://the-internet.herokuapp.com/login)

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC

# WebDriver 설정, 

driver = webdriver.Chrome()

try:
    # 로그인 페이지 접속
    driver.get("https://the-internet.herokuapp.com/login")

    # 사용자 이름과 비밀번호 입력
    username_input = driver.find_element(By.ID, "username")
    password_input = driver.find_element(By.ID, "password")

    username_input.send_keys("tomsmith")  # 올바른 사용자 이름
    password_input.send_keys("SuperSecretPassword!")  # 올바른 비밀번호

    # 로그인 버튼 클릭
    login_button = driver.find_element(By.CSS_SELECTOR, "button[type='submit']")
    login_button.click()

    # 명시적 대기 방법 'flash' ID를 가진 엘리먼트가 나타날 때까지 최대 10초 대기
    # wait = WebDriverWait(driver, 10)
    # success_message_element = wait.until(EC.visibility_of_element_located((By.ID, "flash")))

    # 로그인 성공 메시지 확인
    # 고정 지연 시간 (Time.sleep) 사용
    time.sleep(2)
    # 출력시 close버튼의 x 문자를 제외
    success_message = driver.find_element(By.ID, "flash").text.split('\n')[0]
    print(success_message)

finally:
    # WebDriver 종료
    driver.quit()
```

### 6.2 **학습 포인트**
1. **웹 요소 조작**: `find_element`와 `send_keys`, `click`을 사용하여 입력 필드에 데이터를 입력하고 버튼을 클릭하는 방법 학습.  
2. **데이터 추출**: `text` 속성을 활용해 웹 페이지에서 텍스트 데이터를 추출하고 결과를 확인하는 방법 익히기.  
3. **특정 요소 선택**: `By.ID`, `By.CSS_SELECTOR` 등 다양한 로케이터 전략을 사용하여 원하는 웹 요소를 정확히 선택하는 기술 습득.  
4. **자동화 프로세스 구현**: 로그인과 같은 일련의 작업을 자동화하여 실용적인 웹 상호작용을 구현하는 방법 학습.  
5. **리소스 관리**: `try-finally` 구문과 `driver.quit()`을 통해 WebDriver 세션을 안전하게 종료하고 리소스를 정리하는 중요성 이해.


## **7. 데이터 수집 연습**

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
import time

# ChromeDriver 설정
driver = webdriver.Chrome()

try:
    # Wikipedia 접속
    driver.get("https://en.wikipedia.org/wiki/Squid_Game_season_2")
    time.sleep(2)  # 페이지 로딩 대기

    # 제목과 내용 추출
    title = driver.find_element(By.TAG_NAME, "h1").text
    contents = driver.find_elements(By.CSS_SELECTOR, "#mw-content-text p")
    print(f"제목: {title}")
    for i, content in enumerate(contents):
        if i in [1, 2]:
            print(content.text)       

finally:
    # WebDriver 종료
    driver.quit()

```

### 7.1 **학습 포인트**
1. **데이터 추출**: `text` 속성을 사용하여 텍스트 데이터 추출.
2. **특정 요소 선택**: `find_element`,`find_elements`를 사용하여 원하는 정보만 추출.
3. **실용성**: 실제 웹사이트에서 유용한 정보를 수집하는 방법 학습.


## **8. By 역할**
`By`의 역할은 **Selenium WebDriver에서 요소를 찾을 때 사용하는 선택자 전략(또는 위치 지정 방법)을 정의**하는 것입니다.

### **8.1 `By`의 사용 예시**
아래는 `By`를 사용하여 웹 요소를 찾는 예제입니다.

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
import time


driver = webdriver.Chrome()
driver.get("http://quotes.toscrape.com/")
print(f"현재 페이지 제목: {driver.title}\n")
time.sleep(2)

# 1. By.CLASS_NAME으로 요소 찾기 (예: 모든 인용구 텍스트)
print("--- By.CLASS_NAME 예시 ---")
quote_elements = driver.find_elements(By.CLASS_NAME, "text")
print(f"페이지에 있는 인용구 텍스트 개수: {len(quote_elements)}")
if quote_elements:
    print(f"첫 번째 인용구 텍스트: {quote_elements[0].text}\n")


# 2. By.CSS_SELECTOR로 요소 찾기 (예: 첫 번째 인용구의 작가)
print("--- By.CSS_SELECTOR 예시 ---")
# 'quote' 클래스를 가진 div 아래에 있는 'small' 태그를 찾음
author_element = driver.find_element(By.CSS_SELECTOR, ".quote small.author")
print(f"첫 번째 인용구의 작가: {author_element.text}\n")


# 3. By.XPATH로 요소 찾기 (예: 'love' 태그를 가진 모든 인용구)
print("--- By.XPATH 예시 ---")
# "클래스명이 'quote'인 <div> 엘리먼트 중에서, 
# 그 내부에 클래스명이 'tag'이고 텍스트 내용이 'love'인 <a> 태그를 포함하고 있는 모든 <div> 엘리먼트들"
# 뒤의 대괄호 [.//a[@class='tag' and text()='love']는 앞의 요소에 대한 조건임.
love_quotes = driver.find_elements(By.XPATH, "//div[@class='quote'][.//a[@class='tag' and text()='love']]")
print(f"'love' 태그를 가진 인용구 개수: {len(love_quotes)}")
if love_quotes:
    print(f"첫 번째 'love' 인용구 텍스트: {love_quotes[0].find_element(By.CLASS_NAME, 'text').text}\n")


# 4. By.LINK_TEXT로 요소 찾기 (예: 'Next' 버튼)
print("--- By.LINK_TEXT 예시 ---")
try:
    next_button = driver.find_element(By.LINK_TEXT, "Next »")
    print(f"'Next »' 버튼을 찾았습니다. 클릭합니다.")
    next_button.click()
    time.sleep(3)
    print(f"다음 페이지 제목: {driver.title}\n")
except Exception as e:
    print(f"'Next »' 버튼을 찾거나 클릭하는 데 실패했습니다: {e}\n")


# 5. By.TAG_NAME으로 요소 찾기 (예: 모든 a 태그)
print("--- By.TAG_NAME 예시 ---")
all_links = driver.find_elements(By.TAG_NAME, "a")
print(f"페이지에 있는 총 링크 개수: {len(all_links)}\n")


driver.quit()
print("브라우저 종료.")
```

## 9. 웹 요소 찾기 심화: XPath (가장 강력한 길 찾기)

`find_element(By.NAME, "q")`처럼 이름으로 찾을 수도 있지만, 웹 페이지의 모든 요소는 마치 주소처럼 고유한 **XPath**라는 경로를 가지고 있어요. XPath는 가장 강력하게 요소를 찾아낼 수 있는 방법이에요.

  * **XPath는 주소 같은 거예요:**

      * `/html/body/div/div[2]/h1` 처럼 웹 페이지의 가장 위부터 순서대로 내려가면서 '어떤 태그의 몇 번째 자식'이라는 식으로 요소를 찾아가는 경로예요.
      * `//div[@id='container']/h3` 처럼 `//`를 사용하면 웹 페이지 어디에 있든 'ID가 container인 div 아래의 h3 태그'를 바로 찾아갈 수도 있어요. `[@속성='값']`은 특정 속성과 값을 가진 요소를 의미해요.
      * `//a[text()='클릭할 링크']` 처럼 '텍스트 내용이 특정 값인 링크'를 찾을 수도 있어요.

  * **예시: 특정 링크 클릭하기**

    ```python
    # (위의 초기 설정 코드 driver = webdriver.Chrome(...)는 실행되어 있다고 가정)
    driver.get("https://books.toscrape.com/")
    time.sleep(2)

    # 'Travel'라는 텍스트를 가진 링크 찾기
    # XPath: //a[contains(text(), 'Travel')]
    # 'a' 태그 중에서 'Travel' 텍스트를 포함하는 링크를 찾아요.
    try:
        travel_link = driver.find_element(By.XPATH, "//a[contains(text(), 'Travel')]")
        print(f"'Travel' 링크를 찾았습니다: {travel_link.text}")
        travel_link.click() # 'Travel' 링크 클릭!
        print(" 'Travel' 링크를 클릭했습니다. 페이지 로딩 대기...")
        time.sleep(3)
        print(f"새 페이지 제목: {driver.title}")
    except Exception as e:
        print(f"'Travel' 링크를 찾거나 클릭하는 데 실패했습니다: {e}")

    # 작업 완료 후 브라우저 닫기
    driver.quit()
    ```
