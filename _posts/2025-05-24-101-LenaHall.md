---
title: 46차시 1:LenaHall
layout: single
classes: wide
categories:
  - LenaHall
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 1. Retrieval Augmented Generation (RAG) 아키텍처
- 출처:[RAG Architectures Crash Course](https://www.youtube.com/watch?v=FbGMxel0g2I&t=4s)

### 1.1 **목표**:더 정확하고 신뢰할 수 있으며 유용한 AI 애플리케이션 구축  
RAG는 대규모 언어 모델(LLM)의 생성 능력과 외부 지식 소스를 결합하여, 최신 정보에 기반한 정확하고 맥락에 맞는 응답을 제공하는 것을 목표로 합니다. 이를 통해 환각(hallucination) 문제를 줄이고, 사용자에게 신뢰할 수 있는 결과를 전달하며, 다양한 도메인에서 유용성을 극대화합니다.

### 1.2 **RAG의 이점**  
RAG는 LLM이 내부 지식에만 의존하지 않고 외부 데이터를 검색하여 응답을 생성함으로써 여러 장점을 제공.  
* **사용자 만족도 향상**: 관련성 높은 정보를 제공하여 사용자가 원하는 답변을 더 쉽게 얻을 수 있습니다. 예를 들어, 최신 뉴스나 특정 도메인 지식을 반영한 응답은 사용자 경험을 크게 개선합니다.  
* **시간 절약**: 대량의 정보를 수동으로 검색할 필요 없이, RAG가 자동으로 관련 문서를 찾아 제공함으로써 사용자의 시간과 노력을 줄입니다.  
* **정확한 AI 결과 보장**: 검색된 데이터에 기반한 응답 생성으로, LLM의 부정확하거나 허구의 정보를 최소화하여 신뢰성을 높입니다.

### 1.3 **기본 RAG vs 고급 RAG**  
RAG는 구현 방식에 따라 기본 RAG와 고급 RAG로 나뉩니다. 아래 표는 두 접근 방식의 차이를 비교하며, 고급 RAG가 더 복잡한 요구 사항을 충족하기 위해 추가적인 기능을 포함함을 보여줍니다.

| 특징            | 기본 RAG                                                                 | 고급 RAG                                                                                                                                                                                                 |
|---------------|-------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **프로세스**      | 문서 임베딩 → 쿼리 임베딩 → 유사 문서 검색 → 프롬프트 보강 → 응답 생성 | 다단계 검색, 동적 쿼리 재구성, 관련성 필터링/재정렬, 자체 수정 메커니즘, 특정 사용 사례를 위한 특수 아키텍처. 예: 쿼리 분해, 검색 결과의 재평가, 여러 소스에서 정보를 통합하여 응답 품질을 극대화. |
| **제한 사항**     | 모든 문서를 동일하게 취급, 쿼리 유형에 적응하지 않음, 검증/개선 메커니즘 부재, 단일 검색 단계에 의존. 이로 인해 복잡한 쿼리나 최신 정보 요구 시 성능이 저하될 수 있음. | 제한 사항을 극복하기 위해 다단계 검색과 피드백 루프를 도입. 예: 검색된 정보의 품질을 평가하고, 필요 시 추가 검색을 수행하거나 쿼리를 재구성하여 더 정확한 결과를 도출. |
| **적용 분야**     | 일반적인 애플리케이션 (예: 간단한 FAQ, 기본적인 정보 검색).              | 정확성이 중요한 애플리케이션 (예: 법률 문서 분석, 의료 진단 지원, 최신 뉴스 기반 응답 생성).                                                                                         |

### 1.4 **다양한 RAG 아키텍처**  
RAG는 특정 사용 사례와 요구 사항에 따라 다양한 아키텍처로 구현될 수 있습니다. 아래 표는 각 아키텍처의 설명, 장점, 적합한 사용 사례를 상세히 정리한 것입니다.

| 아키텍처                          | 설명                                                                                                                                                                                                 | 장점                                                                                                                                                                                                 | 적합한 사용 사례                                                                                                                                                                                                 |
|---------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **캐시 증강 생성 (Cache Augmented Generation)** | 관련 문서를 미리 로드하고 추론 상태(키-값 캐시)를 저장하여 중복 처리를 방지. 예: 자주 묻는 질문에 대해 캐시된 결과를 재사용하여 응답 속도를 높임.                                                                                         | 반복적인 쿼리에 대한 지연 시간 감소, 계산 효율성 증대. 특히 동일한 질문이 반복되는 환경에서 서버 부하를 줄이고 빠른 응답을 제공.                                                                                                  | 예측 가능한 쿼리 패턴, 빠른 응답 속도 최적화, 유사한 질문이 빈번한 경우 (예: 고객 지원 챗봇), 계산 효율성이 중요한 애플리케이션.                                                                                                           |
| **에이전트 기반 RAG (Agentic RAG)**        | LLM이 복잡한 쿼리를 하위 쿼리로 분해하고, 여러 검색 단계를 수행하며, 다양한 정보 유형에 대해 다른 검색 전략을 적용. 예: "회사 재무 상태" 쿼리를 재무 보고서 검색과 뉴스 검색으로 나누어 처리.                                                  | 복잡한 쿼리에 대한 다단계 추론, 다양한 정보 유형에 맞춘 검색, 불완전한 쿼리 처리, 정 Sophisticated 정보 합성. 이를 통해 복잡한 문제 해결 능력이 향상됨.                                                                                   | 복잡한 쿼리, 다양한 정보 유형에 대한 검색 필요 (예: 연구 보고서 작성), 초기 쿼리가 불완전한 경우, 정교한 정보 합성이 필요한 사용 사례 (예: 정책 분석).                                                                                              |
| **교정 RAG (Corrective RAG)**            | 검색된 정보를 검증하고 개선하는 메커니즘 추가. 관련성 및 정확성을 확인하고, 모순되는 정보를 필터링하며, 초기 검색 결과를 기반으로 쿼리 재구성. 예: 잘못된 정보가 포함된 검색 결과를 걸러내고 추가 검색 수행.                                       | 정확성 향상, 모순되는 정보 소스 처리, 다양한 정보 품질 관리. 특히 신뢰할 수 없는 소스에서 정보를 가져올 때 유용.                                                                                                       | 정확성이 중요한 애플리케이션 (예: 의료 정보 제공), 모순될 가능성이 있는 정보 소스, 정보 품질이 사용 사례마다 다른 경우 (예: 법률 자문).                                                                                                     |
| **자기 성찰 RAG (Self-Reflective RAG)**   | 모델이 자체 성능을 평가하고 검색 전략을 조정하는 피드백 루프 사용. 예: 응답이 부정확하면 쿼리를 재구성하거나 검색 매개변수를 조정. 성공 및 실패 사례를 학습하여 점진적으로 성능 개선.                                                  | 정확도 향상, 다양한 쿼리 유형 처리, 시간 경과에 따른 시스템 개선, 모호한 쿼리 의도 처리. 이를 통해 시스템이 스스로 학습하며 성능이 향상됨.                                                                                          | 정확도가 중요한 애플리케이션, 다양한 사용자 쿼리 유형, 시간이 지남에 따라 개선해야 하는 시스템 (예: 교육 플랫폼), 쿼리 의도가 모호한 경우.                                                                                                     |
| **RAG 퓨전 (RAG Fusion)**              | 밀집 검색, 희소 검색, 하이브리드 접근 방식을 병렬로 사용하고, 다양한 임베딩 모델을 활용하여 검색 결과의 품질과 다양성을 향상. 예: 텍스트와 이미지 콘텐츠를 함께 검색하여 종합적인 응답 생성.                                           | 다양한 콘텐츠 유형 처리, 검색 방법의 상호 보완적 강점 활용, 정확도와 재현율 모두 개선. 복잡한 쿼리에 대해 더 포괄적인 결과를 제공.                                                                                             | 다양한 콘텐츠 유형이 있는 애플리케이션 (예: 멀티미디어 검색), 서로 보완적인 검색 방법, 정확도와 재현율 모두 필요한 사용 사례, 복잡하고 다면적인 쿼리 처리 (예: 제품 추천 시스템).                                                                               |
| **시간 증강 검색 (Temporal Augmented Retrieval)** | 시간 민감한 정보를 처리하기 위해 시간 인식을 통합. 문서 최신성을 고려하고, 최신 정보에 가중치를 부여. 예: 최신 뉴스를 우선적으로 검색하여 실시간 정보 제공.                                                              | 시간 관련 정보 처리, 정보 최신성 유지, 시간 제약 조건이 있는 쿼리 처리. 특히 빠르게 변화하는 도메인에서 유용.                                                                                                     | 뉴스 애플리케이션, 최신 이벤트 분석, 금융 또는 시장 데이터 애플리케이션, 정보가 빠르게 변경되는 도메인 (예: 주식 시장 분석), 시간 컨텍스트가 중요한 경우.                                                                                       |
| **계획 후 RAG (Plan then RAG)**         | 검색 작업 전에 계획 단계를 도입. 복잡한 쿼리를 하위 질문으로 나누고, 검색 순서를 결정하며, 정보를 종합. 예: "기업 인수 분석" 쿼리를 재무, 법률, 시장 동향으로 분해하여 단계적으로 검색.                                       | 복잡한 연구 질문 처리, 다단계 추론 필요, 쿼리 분해를 통한 결과 개선. 체계적인 접근으로 복잡한 문제 해결 가능.                                                                                                     | 여러 검색 단계가 필요한 복잡한 연구 질문 (예: 학술 연구), 다중 홉 추론 질문을 처리하는 애플리케이션, 쿼리 분해가 결과를 향상시키는 사용 사례.                                                                                                |
| **그래프 RAG (Graph RAG)**             | 지식 그래프를 사용하여 검색 및 추론 프로세스 강화. 정보가 엔터티와 관계로 구성된 그래프로 표현되고, 관계 경로를 따라 검색 수행. 예: "기업 A와 B의 관계" 쿼리에 대해 기업 간 파트너십 정보를 그래프로 추적.                      | 엔터티 간 복잡한 관계 처리, 관계 정보 강조, 구조화된 컨텍스트 제공. 특히 구조화된 데이터가 중요한 경우 유용.                                                                                                     | 엔터티 간 복잡한 관계가 있는 애플리케이션 (예: 소셜 네트워크 분석), 관계 정보가 중요한 사용 사례 (예: 공급망 분석).                                                                                                          |
| **Flare**                             | 응답 품질 개선을 위해 활성 검색 증강 생성을 통합. 생성 중 주장을 식별하고 검증을 위해 추가 검색 수행. 예: 역사적 사건에 대한 응답 생성 시, 사실 확인을 위해 추가 자료 검색.                                              | 환각 최소화, 고수준의 사실 정확도 필요. 사실 확인을 통해 신뢰성을 높이고, 장문형 콘텐츠에서 오류를 줄임.                                                                                                      | 높은 사실 정확도를 요구하는 애플리케이션 (예: 학술 글쓰기), 환각을 피해야 하는 경우, 많은 사실 주장이 포함된 장문형 콘텐츠와 검증이 중요한 도메인 (예: 저널리즘).                                                                                   |
| **컨텍스트 검색 (Contextual Retrieval)** | 검색 전에 문서 청크에 메타데이터, 섹션 제목, 주변 텍스트 등 추가 컨텍스트를 포함하여 관련성 향상. 예: 법률 문서에서 특정 조항을 검색할 때, 문서 구조와 메타데이터를 활용해 정확한 조항을 찾음.                                     | 구조화된 문서 처리, 복잡한 문서 처리, 중요한 컨텍스트 유지. 단순 청킹에서 손실될 수 있는 정보를 보존하여 검색 품질 향상.                                                                                              | 구조화되고 복잡한 문서가 있는 애플리케이션 (예: 법률 또는 기술 문서 검색), 문서 구조가 중요한 컨텍스트를 제공하는 경우, 특수 용어 또는 개념이 있는 도메인 (예: 의학 문헌).                                                                 |

### 1.5 **적절한 RAG 아키텍처 선택 기준**  
RAG 아키텍처를 선택할 때는 애플리케이션의 요구 사항을 신중히 고려해야 합니다.  
* **쿼리 유형**: 단순 사실 쿼리(예: "수도 이름")는 기본 RAG로 충분하지만, 복잡한 다단계 추론 쿼리(예: "정책 변화의 경제적 영향")는 에이전트 기반 RAG나 계획 후 RAG가 적합.  
* **정확성 요구 사항**: 의료나 법률처럼 정확성이 중요한 경우, 교정 RAG나 Flare와 같은 아키텍처가 필요.  
* **콘텐츠 유형**: 텍스트, 이미지, 그래프 등 다양한 콘텐츠를 다룰 경우 RAG 퓨전이 적합.  
* **시간 관련성**: 최신 정보가 중요한 경우(예: 뉴스 앱), 시간 증강 검색이 필수.  
* **처리량 요구 사항**: 높은 처리량이 필요한 경우(예: 실시간 챗봇), 캐시 증강 생성으로 효율성을 높일 수 있음.

### 1.6 **구현 고려 사항**  
RAG 시스템을 설계하고 구현할 때 다음 요소를 고려해야 합니다.  
* **청킹 전략**: 문서를 어떻게 나눌지 결정.  
  * **고정 크기 청킹**: 일정한 크기로 문서를 분할. 구현이 간단하지만 의미적 맥락이 손실될 수 있음.  
  * **의미론적 청킹**: 문장의 의미를 고려하여 분할. 관련성을 높이지만 계산 비용이 증가.  
  * **계층적 청킹**: 문서의 구조(예: 섹션, 단락)를 활용해 분할. 복잡한 문서에 적합.  
  * **최적 청크 크기**: 사용 사례에 따라 100-1000 토큰 범위에서 실험. 예: 짧은 FAQ는 작은 청크, 긴 연구 보고서는 큰 청크가 적합.  
* **임베딩 모델**: 문서와 쿼리를 벡터로 변환하는 모델 선택. 모델 품질, 차원, 도메인 특성(예: 의료 데이터용 특화 모델), 다국어 지원, 비용 및 지연 시간을 고려.  
* **벡터 데이터베이스**: 확장성, 쿼리 속도, 유지 관리 복잡성, 고급 기능(메타데이터 필터링, 하이브리드 검색 등), 통합성을 평가. 예: Pinecone, Weaviate, FAISS.  
* **프롬프트 구조**: 컨텍스트 배치(검색된 문서의 위치), 지시 명확성(명확한 질문 유도), 소스 속성(출처 명시)으로 응답 품질을 높임.  
* **평가 지표**: 시스템 성능을 평가하기 위한 기준.  
  * **관련성 (Relevance)**: 검색된 문서가 쿼리와 얼마나 관련 있는지.  
  * **정확성 (Accuracy)**: 응답이 사실적으로 정확한지.  
  * **완전성 (Completeness)**: 쿼리의 모든 측면을 다루는지.  
  * **근거 (Groundedness)**: 응답이 검색된 컨텍스트에 의해 뒷받침되는지.  

### 1.7 **평가 방법**  
RAG 시스템의 성능을 평가하기 위해 다양한 방법을 사용할 수 있습니다.  
* **인간 평가**: 전문가가 응답의 품질을 직접 평가. 정확하지만 비용과 시간이 많이 듦.  
* **자동화된 지표**: ROUGE, BLEU 등으로 텍스트 유사성을 측정. 빠르고 객관적이지만 의미적 품질을 완전히 반영하지 못할 수 있음.  
* **A/B 테스트**: 두 가지 RAG 설정을 비교하여 사용자 선호도 확인.  
* **사용자 피드백**: 실제 사용자 경험을 바탕으로 시스템 개선.  

### 1.8 **RAG 프레임워크**  
* LangChain, LlamaIndex와 같은 프레임워크는 RAG 구현을 간소화합니다.  
  * **LangChain**: 데이터 인덱싱, 검색, 프롬프트 관리, 에이전트 기반 워크플로우를 지원.  
  * **LlamaIndex**: 고급 인덱싱 및 쿼리 처리 기능 제공, 특히 구조화된 데이터에 강력.  
  이러한 프레임워크는 데이터 소스 통합, 검색 최적화, 평가 도구를 제공하여 개발 속도를 높입니다.

### 1.9 **결론**  
RAG 시스템은 정확하고 관련성 높으며 유용한 응답을 제공하기 위해 반복적인 평가와 지속적인 개선이 필요합니다. 다양한 RAG 아키텍처(기본 RAG부터 고급 아키텍처까지)와 구현 고려 사항(청킹, 임베딩, 데이터베이스 등)을 신중히 선택하면, 특정 사용 사례와 요구 사항에 맞는 강력한 시스템을 구축할 수 있습니다. 예를 들어, 의료 애플리케이션에서는 교정 RAG를, 실시간 뉴스 앱에서는 시간 증강 검색을 활용하여 최적의 성능을 달성할 수 있습니다.

## 2. Java와 Spring AI를 활용한 검색 증강 생성(RAG) 시스템 구축
- 출처: [Master RAG Data Prep: ETL with Java, Spring AI, Pinecone and MongoDB](https://www.youtube.com/watch?v=wHF2SNPiFGQ)

### 2.1 **주요 내용**

*   **RAG 시스템의 중요성:**
    *   대부분 기업이 AI 기능을 원하지만, 실제로 엔터프라이즈급 RAG 시스템을 구축할 개발자는 부족.
        *  RAG 시스템은 단순한 챗봇을 넘어, 비즈니스 데이터를 활용해 정확하고 맥락에 맞는 답변을 제공하는 AI 시스템을 구현하는 데 필수적입니다. 예를 들어, 고객 지원 시스템에서 고객의 질문에 대해 회사 정책이나 제품 정보를 기반으로 정확한 답변을 제공할 수 있습니다.
    *   RAG 시스템을 마스터하면 AI를 활용하여 실질적인 결과를 도출하는 개발자가 될 수 있음.
        *  RAG 시스템을 능숙히 다루는 개발자는 고객 맞춤형 솔루션, 자동화된 문서 분석, 또는 데이터 기반 의사결정 지원 시스템을 구축하여 기업의 경쟁력을 높일 수 있습니다.

*   **RAG 시스템의 필요성:**
    *   LLM(Large Language Model)은 환각 현상(hallucination)을 일으키고, 특정 비즈니스 데이터를 이해하지 못함.
        *  LLM은 일반적인 지식에 기반한 답변을 생성하지만, 특정 기업의 내부 데이터나 최신 정보를 반영하지 못해 부정확한 답변을 내놓을 수 있습니다. 예를 들어, 최신 제품 매뉴얼이나 내부 규정을 반영하지 못하면 잘못된 정보를 제공할 가능성이 있습니다.
    *   RAG는 실제 데이터를 기반으로 AI 응답을 생성하여 이러한 문제를 해결.
        *  RAG는 외부 데이터 소스(예: 회사 문서, 데이터베이스)를 활용해 LLM의 응답을 보강함으로써 신뢰성과 정확성을 높입니다. 이는 특히 금융, 의료, 법률 등 정확도가 중요한 산업에서 필수적.
    *   RAG를 통해 고객 지원, 코드 어시스턴트, 재무 자문 등 다양한 분야에서 AI 활용 가능.
        *  예를 들어, 고객 지원에서는 고객 문의에 대해 회사의 최신 FAQ나 매뉴얼을 기반으로 답변을 생성하고, 코드 어시스턴트에서는 개발 문서를 참조해 정확한 코드 예시를 제공할 수 있습니다.

*   **벡터 데이터베이스의 역할:**
    *   전통적인 데이터베이스는 정확한 일치 또는 키워드 검색에만 유용하며, 의미론적 이해에 한계가 있음.
        *  전통적인 SQL 데이터베이스는 키워드 매칭에 의존하기 때문에, 사용자의 질문 의도를 파악하거나 문맥에 맞는 결과를 반환하는 데 한계가 있습니다. 예를 들어, "결제 오류 해결 방법"이라는 쿼리에 대해 단순히 "결제"라는 단어가 포함된 문서만 반환할 수 있습니다.
    *   벡터 데이터베이스는 텍스트뿐만 아니라 의미를 저장하여, 사용자의 의도를 파악하고 관련 문서 검색 가능.
        *  벡터 데이터베이스는 텍스트를 고차원 벡터로 변환하여 의미적 유사성을 계산합니다. 이를 통해 "결제 오류"와 관련된 문서뿐만 아니라 "결제 문제", "결제 실패"와 같은 유사한 문맥의 문서도 검색할 수 있습니다.

*   **RAG 시스템의 핵심 단계:**
    1.  **검색(Retrieval):** 벡터 데이터베이스에서 관련 문서 검색.
        *  이 단계에서는 사용자의 쿼리를 벡터로 변환한 뒤, 벡터 데이터베이스에서 가장 유사한 문서들을 찾아냅니다. 예를 들어, "주문 취소 방법"을 묻는 쿼리에 대해 관련 매뉴얼이나 FAQ를 검색합니다.
    2.  **증강(Augmentation):** 검색된 컨텍스트와 사용자 쿼리 결합.
        *  검색된 문서의 내용을 사용자 쿼리와 함께 LLM에 입력하여, 쿼리와 관련된 맥락을 제공합니다. 이는 LLM이 일반적인 답변 대신 데이터에 기반한 구체적인 답변을 생성하도록 돕습니다.
    3.  **생성(Generation):** LLM이 데이터 기반 응답 생성.
        *  LLM은 검색된 문서와 쿼리를 바탕으로 자연스럽고 정확한 답변을 생성합니다. 예를 들어, "주문 취소는 구매 후 7일 이내에 가능하며, 고객 포털에서 진행할 수 있습니다"와 같은 답변을 생성할 수 있습니다.

*   **Spring AI의 활용:**
    *   Spring AI는 RAG 시스템 구축의 복잡성을 추상화하고, 개발자에게 유용한 패턴 제공.
        *  Spring AI는 복잡한 벡터 데이터베이스 통합, 임베딩 생성, 검색 로직 등을 단순화하여 개발자가 비즈니스 로직에 집중할 수 있도록 돕습니다.
    *   다양한 벡터 데이터베이스(Pine Cone, Chroma, PostgreSQL, Azure Cosmos DB, MongoDB Atlas 등) 지원.
        *  Spring AI는 여러 벡터 데이터베이스를 공통 인터페이스로 지원하므로, 특정 데이터베이스에 종속되지 않고 유연하게 시스템을 설계할 수 있습니다.
    *   `VectorStore` 인터페이스를 통해 다양한 벡터 데이터베이스 구현과 상호 작용하는 공통 API 제공.
        *  `VectorStore` 인터페이스는 벡터 데이터의 저장, 검색, 관리 작업을 표준화된 방식으로 처리하여, 개발자가 데이터베이스별로 별도의 코드를 작성할 필요를 줄여줍니다.

*   **구현 예시:**
    *   **문서 수집(Document Ingestion):**
        *   `FileSystemResource`와 `TikaDocumentReader`를 사용하여 파일 시스템에서 문서를 읽어옴.
            *  예를 들어, PDF, Word, 텍스트 파일 등 다양한 형식의 문서를 읽어와 텍스트로 추출합니다. 이는 기업의 매뉴얼, 계약서, FAQ 문서 등을 처리하는 데 유용합니다.
        *   `TokenTextSplitter`를 사용하여 문서를 작은 청크로 분할.
            *  긴 문서를 작은 단위(예: 문장 또는 단락)로 나누어 벡터화하면 검색 정확도가 높아집니다. 예를 들어, 100페이지 분량의 매뉴얼을 500개의 작은 청크로 나눌 수 있습니다.
        *   `VectorStore.add()` 메서드를 사용하여 청크와 임베딩을 벡터 데이터베이스에 저장.
            *  각 청크는 텍스트를 벡터로 변환한 임베딩과 함께 저장되어, 이후 유사도 검색에 활용됩니다.
    *   **유사도 검색(Similarity Search):**
        *   `SearchRequest.Builder`를 사용하여 검색 파라미터(쿼리, Top K 등)를 설정.
            *  Top K는 검색 결과로 반환할 문서의 개수를 지정하며, 쿼리의 세부 설정(예: 유사도 임계값)을 통해 결과를 최적화할 수 있습니다.
        *   `VectorStore.similaritySearch()` 메서드를 사용하여 쿼리와 유사한 문서 검색.
            *  이 메서드는 쿼리를 벡터로 변환한 뒤, 데이터베이스에 저장된 벡터들과의 코사인 유사도 등을 계산하여 가장 관련성 높은 문서를 반환합니다.

*   **벡터 데이터베이스 교체:**
    *   Spring AI를 사용하면 코드 변경 없이 벡터 데이터베이스를 쉽게 교체 가능. (예: Pine Cone에서 MongoDB로 변경)
        *  Spring AI의 추상화된 인터페이스를 통해, 데이터베이스별로 다른 API를 학습할 필요 없이 동일한 코드로 다양한 데이터베이스를 사용할 수 있습니다. 이는 비용, 성능, 또는 스케일링 요구사항에 따라 데이터베이스를 선택할 때 유리합니다.

*   **향후 내용:**
    *   검색 결과를 지능적인 응답으로 변환하는 질문-답변 어드바이저 구축.
        *  질문-답변 어드바이저는 검색된 문서를 기반으로 사용자에게 자연스러운 대화 형식의 답변을 제공하며, 예를 들어, "주문 취소 정책이 궁금합니다"라는 질문에 대해 간결하고 정확한 답변을 생성합니다.
    *   더욱 정교한 RAG 파이프라인을 위한 검색 증강 어드바이저 구현.
        *  검색 증강 어드바이저는 검색 결과를 최적화하거나, 복잡한 쿼리를 여러 단계로 나누어 처리하여 더 정확한 결과를 도출합니다.
    *   개선된 검색을 위한 다중 쿼리 확장(Multi-Query Expansion) 기술 적용.
        *  다중 쿼리 확장은 사용자의 쿼리를 다양한 방식으로 재구성하여 더 많은 관련 문서를 검색하거나, 모호한 쿼리의 의도를 명확히 파악하는 데 사용됩니다.

### 2.2 **핵심 문장**

*   Spring AI를 활용하여 Java 환경에서 RAG 시스템을 구축하는 방법을 배우면, AI를 통해 실질적인 비즈니스 가치를 창출하는 개발자가 될 수 있습니다.
    *  Spring AI를 사용하면 복잡한 AI 시스템 구축 과정이 간소화되며, 비즈니스 데이터를 활용한 AI 솔루션을 빠르게 배포할 수 있어 개발자의 시장 경쟁력을 강화할 수 있습니다.
*   벡터 데이터베이스는 RAG 시스템에서 중요한 역할을 하며, Spring AI는 다양한 벡터 데이터베이스를 지원하여 개발자가 유연하게 선택할 수 있도록 합니다.
    *  벡터 데이터베이스는 의미 기반 검색을 가능하게 하며, Spring AI의 유연한 지원 덕분에 개발자는 프로젝트 요구사항에 맞는 최적의 데이터베이스를 선택할 수 있습니다.
*   문서 수집, 유사도 검색, 응답 생성 등 RAG 시스템의 핵심 단계를 Spring AI를 사용하여 구현하는 방법을 배우면, AI 기반 애플리케이션 개발 능력을 향상시킬 수 있습니다.
    *  이러한 기술은 단순한 챗봇을 넘어, 고객 지원, 문서 관리, 데이터 분석 등 다양한 도메인에서 AI를 활용한 고급 애플리케이션 개발로 이어질 수 있습니다.

## 3. AI의 새로운 지평을 열다: Java 및 Spring AI를 활용한 RAG (검색 증강 생성)

- 출처: [RAG with Java and Spring AI -- Tutorial](https://www.youtube.com/watch?v=RIQGv3u79xw)

최근 AI 분야에서 기업의 비즈니스 데이터를 AI가 실제로 이해하도록 돕는 솔루션인 **RAG(Retrieval Augmented Generation)**는 개발자들에게 가장 수요가 많은 기술 중 하나로 부상했습니다. RAG는 단순히 LLM(대규모 언어 모델)과 상호작용하는 것을 넘어, AI 응답을 실제 데이터에 **기반(grounding)**하여 **환각(hallucination)** 문제를 해결합니다. 이번 글에서는 Spring AI를 활용하여 RAG의 핵심인 **지능형 검색(intelligent retrieval)**과 **상황 인지 생성(context-aware generation)**을 구현하는 여러 패턴을 살펴보자.

### **3.1 가장 기본적인 LLM 상호작용: 직접 프롬프트**

가장 간단한 방법은 Spring AI의 `ChatClient`를 사용하여 LLM에 직접 문자열을 보내는 것입니다.

*   **장점**: 빠르고 간단하며, `ChatClient` 및 LLM 통합이 잘 작동하는지 신속하게 테스트하기에 좋습니다.
*   **단점**: **외부 컨텍스트나 문서 검색이 전혀 없습니다**. 모델은 훈련 데이터에만 기반하여 응답하며, 특정 프로젝트나 회사 데이터에 대해 질문하면 "모른다"고 답하거나 환각 현상을 보일 수 있습니다. 이는 일반적인 채팅에는 적합하지만, 도메인별 AI를 구축하는 데는 유용하지 않습니다.

### **3.2 기본 RAG 시스템 구축: `QuestionAnswerAdvisor` 활용**

LLM이 문서에 기반하여 답변하고 특정 지침을 따르도록 하려면 입력에 대한 더 많은 제어가 필요합니다. 이때 **프롬프트 템플릿**과 Spring AI의 **`QuestionAnswerAdvisor`**가 중요해집니다.

*   **프롬프트 템플릿**: 사용자의 질문(`query`)과 벡터 스토어에서 검색된 문서가 자동으로 주입될 `questionAnswerContext`와 같은 **플레이스홀더**를 정의합니다. 또한, LLM의 응답 스타일을 안내하는 특정 규칙을 포함할 수 있습니다.
*   **`QuestionAnswerAdvisor`**:
    *   Spring AI의 **어드바이저(Advisor)**는 일반적인 AI 패턴을 재사용 가능한 구성 요소로 캡슐화하는 역할을 합니다. 이는 마치 AI 애플리케이션을 위한 미들웨어와 같아서, 문서 검색, 컨텍스트 주입, 프롬프트 엔지니어링과 같은 반복적인 패턴을 자동으로 처리하여 개발자가 애플리케이션 로직에 집중.
    *   `QuestionAnswerAdvisor`는 벡터 스토어와 사용자 정의 프롬프트 템플릿을 인스턴스화하여 사용.
    *   사용자의 질문이 어드바이저에 전달되면, 어드바이저는 벡터 스토어에서 질문 관련 문서를 검색하고, 이 컨텍스트와 질문을 사용자 정의 프롬프트 템플릿에 채워넣은 후 LLM에 전송합니다.
    *   **검색 맞춤화**: `SearchRequest` 객체를 사용하여 검색을 맞춤 설정할 수 있습니다. 예를 들어, 문서가 고려되기 위한 **유사도 임계값(similarity threshold)**(예: 0.8)과 **가장 관련성이 높은 문서의 수(top K)**(예: 6개)를 지정할 수 있습니다.
*   **사용 사례**: 명확한 문서 기반 Q&A, 문서 챗봇, 내부 FAQ 시스템, 잘 정의된 지식 기반의 고객 지원에 탁월.
*   **제한 사항**: 편리하지만 다소 **블랙박스**처럼 느껴질 수 있습니다. 복잡한 검색(예: 다단계 검색, 정교한 재순위화, 하이브리드 검색 통합)이 필요할 경우 유연성이 부족할 수 있습니다.

### **3.3 더 복잡한 RAG 워크플로우를 위한 `RetrievalAugmentationAdvisor`**

`RetrievalAugmentationAdvisor`는 문서 검색을 `ChatClient` 호출에 통합하는 **더 강력한 방법**을 제공.

*   **핵심 강점**: **매우 모듈화된 설계**가 특징입니다. 이는 RAG 파이프라인 내에서 데이터 검색을 위한 **플러그 앤 플레이 시스템**으로 생각할 수 있습니다.
*   **`DocumentRetriever` 구성**: Spring AI의 `DocumentRetriever` 인터페이스의 모든 구현과 함께 구성.
    *   즉, 코어 RAG 로직을 다시 작성할 필요 없이 다양한 검색 전략이나 데이터 소스를 **원활하게 교체**할 수 있습니다. 예를 들어, 오늘은 PG 벡터나 Pinecone에서 데이터를 가져오기 위해 `VectorStoreDocumentRetriever`를 사용하고, 내일은 기존 CMS에서 문서를 가져오기 위해 사용자 정의 `ContentManagementSystemDocumentRetriever`를 구현하여 연결할 수 있습니다.
*   **데이터 검색 논리 디커플링**: 이 설계는 데이터 검색 논리를 AI 모델 상호 작용에서 효과적으로 **분리**합니다. 어드바이저의 역할은 문서를 가져오는 것이고, LLM이 문서를 사용하는 방법은 별개의 문제입니다.
*   **유연한 RAG 구현**: 다양한 데이터 소스와 검색 메커니즘을 쉽게 교체하거나 결합할 수 있어 진정으로 유연한 RAG 구현이 가능합니다. 이는 **단일 데이터 가져오기 방식에 얽매이지 않도록** 합니다.
*   **복잡한 RAG 구축 블록**: 모듈화되고 집중되어 있기 때문에 문서 검색이 별개의 교체 가능한 단계인 **더 복잡한 RAG 워크플로우**를 구성해야 할 때 탁월한 빌딩 블록 역할을 합니다.

### **3.4 고난이도 질문을 위한 `Multi-Query Expansion` (다중 쿼리 확장)**

단일 사용자의 쿼리에 대한 단일 통과 검색은 모호하거나 다면적인 어려운 질문에는 충분하지 않을 수 있습니다. 이때 **다중 쿼리 확장**과 같은 기술이 필요합니다.

*   **목표**: 모든 관련 정보를 찾을 가능성을 높이는 것입니다.
*   **작동 방식 (다단계 파이프라인)**:
    1.  **쿼리 확장**: `MultiQueryExpander` 컴포넌트를 사용하여 원래의 복잡한 질문의 여러 변형을 생성합니다 (이 컴포넌트 자체도 LLM을 사용함). 예를 들어, 4가지 변형을 요청할 수 있습니다.
    2.  **각 확장된 쿼리에 대한 문서 검색**: `DocumentRetriever`를 설정하고, 1단계에서 확장된 각 쿼리에 대해 문서를 검색하여 기본적인 **중복 제거**를 수행합니다. (문서가 발견되지 않으면 대체 로직이 있음).
    3.  **포괄적인 컨텍스트 및 최종 프롬프트 생성**: 쿼리 변형에서 검색된 모든 고유 문서를 단일 합성된 컨텍스트 문자열로 연결합니다. 그런 다음 LLM이 원래 쿼리에 대해 이 풍부한 컨텍스트를 사용하여 답변하도록 지시하는 `Synthesis Prompt Template`를 정의합니다.
    4.  **최종 답변 생성**: `ChatClient`를 호출하여 이 포괄적인 최종 프롬프트를 LLM에 보내 최종 답변을 생성합니다.
*   **장점**: 여러 쿼리를 생성하여 검색 **재현율(recall)**을 높이고, 특히 어려운 사용자 질문에 대한 모든 관련 문서를 찾을 가능성을 높입니다. 이는 일반적으로 더 높은 품질의 **더 완전한 답변**으로 이어집니다.
*   **트레이드오프**:
    *   쿼리 확장 단계에서 **추가 LLM 호출**이 발생하여 **지연 시간(latency)**과 **비용**이 증가할 수 있습니다.
    *   여러 쿼리 검색, 결과 집계, 잠재적인 중복 제거에 대한 **수동 오케스트레이션 노력**이 추가됩니다. 이는 매우 강력한 방법이지만, 추가 단계를 관리하기 위한 전체 파이프라인의 **신중한 설계**가 필요합니다.

### **3.5 패턴별 성능 및 핵심 원칙**

각 구현 패턴은 다른 성능을 보입니다.

*   **간단한 Q&A**: `QuestionAnswerAdvisor`나 `RetrievalAugmentationAdvisor` 모두 문서에 명확히 포함된 "What is Paid AI?"와 같은 질문에 잘 답변합니다.
*   **도전적인 질문**: "Which framework works with Java?"와 같이 명확한 답변을 제공하지 않는 질문에는 답변 품질이 저하되기 시작합니다.
*   **다중 쿼리 확장**: "Which framework works best with Gemini?"와 같이 복잡한 질문에 대해 **쿼리 확장기(query expander)**가 유사한 질문 4개를 생성하고, 컨텍스트를 종합하여 LLM에 보내 "정보가 부족하다"는 답변보다 **더 정교한 답변**을 제공하는 것을 확인할 수 있습니다.

**가장 중요한 점은 RAG가 단순히 하나의 도구가 아니라 `패턴`이라는 것입니다**. Spring AI든 LangChain이든 다른 프레임워크든, **검색(retrieval)**, **컨텍스트 증강(context augmentation)**, **안내된 생성(guided generation)**이라는 핵심 개념은 동일합니다.

*   **프롬프트 엔지니어링**: 어떤 RAG 시스템에서든 **효과적인 프롬프트 템플릿 설계**는 매우 중요합니다. LLM에게 어떻게 동작하고 제공된 컨텍스트를 어떻게 사용해야 하는지 **명확하게 지시**하는 것은 고품질 결과의 필수 요소입니다.
*   **단순하게 시작하여 확장**: 항상 **간단하게 시작하고 점진적으로 복잡성을 추가하는 것**이 좋습니다. 기본적인 Q&A 흐름으로 시작하고, 사용 사례가 요구할 때 사용자 정의 문서 검색 로직이나 다중 쿼리 확장과 같은 기술을 추가할 수 있습니다.
*   **좋은 데이터의 중요성**: 검색하는 컨텍스트가 더 좋고 포괄적일수록 LLM이 생성하는 답변의 품질도 향상됩니다. 문서 청크 모니터링, 다양한 파일 유형 테스트, 데이터 버전 관리, 대량 수집 배치 처리, 실패 처리 등 **데이터 스케일링 팁**도 중요합니다. 어떤 RAG 프레임워크나 기술을 사용하든 **좋은 데이터는 좋은 답변을 의미합니다**.

## 4. AI를 더 똑똑하게 만드는 방법: '프롬프트 공학'을 넘어 '맥락 공학'으로
- 출처: [Context Engineering vs Prompt Engineering Explained](https://www.youtube.com/watch?v=4q_oWQDOd9Q)

요즘 인공지능(AI)이 화두죠? 우리는 AI에게 질문하거나 지시를 내릴 때 '프롬프트'를 사용합니다. 마치 AI에게 "이것 좀 해줘!"라고 말하는 것과 같아요. 그런데 AI를 실제 비즈니스에 활용하려고 하면 단순히 이 프롬프트만으로는 부족하다는 사실을 깨닫게 됩니다. 여기에서 **'맥락 공학(Context Engineering)'**의 중요성이 드러납니다.

### 4.1 프롬프트 공학의 한계: 왜 AI는 자꾸 잊어버리고 헷갈릴까요?

우리가 아무리 완벽하게 프롬프트를 작성해도 AI는 때때로 중요한 내용을 잊어버리거나, 요구사항을 착각하고, 들쭉날쭉한 결과물을 내놓습니다. 왜 그럴까요?

* **하나의 프롬프트에 모든 것을 담으려다 보니:** 예를 들어, 고객 환불 시스템을 만든다고 생각해봅시다. 환불 정책 확인, 계산, 상품별 규칙, 답변 형식 등 모든 지시를 하나의 프롬프트에 구겨 넣으면 AI는 30일 환불 기간은 기억해도 전자기기 보증 요구사항은 잊거나, 옷 환불 규칙을 노트북에 적용하는 등의 실수를 저지를 수 있습니다.
* **정보 경쟁:** 프롬프트에 지시가 많아질수록 중요한 지시가 뒤로 밀려나 AI의 출력에 미치는 영향이 줄어들어요.
* **기억력 부족:** 프롬프트만으로는 AI가 이전에 나눈 대화나 결정을 기억하지 못합니다. 매번 새로운 대화처럼 시작하니, 같은 정책에 대해서도 다르게 해석할 수 있죠.

이러한 문제들은 마치 우리가 복잡한 문제를 풀 때, 질문 하나만 던져주고 다른 참고 자료나 경험은 전혀 주지 않는 것과 같습니다.

### 4.2 '맥락(Context)'이란 무엇일까요?

**맥락**은 AI 모델이 결정을 내릴 때 참고할 수 있는 **모든 정보 환경**을 말합니다. AI의 '작업 기억(working memory)'이라고 생각하면 쉬워요. 사람이 문제를 풀 때 질문(프롬프트) 외에 참고 자료, 과거 경험, 도구, 원하는 결과에 대한 이해 등 모든 정보를 활용하는 것과 똑같습니다.

**맥락의 핵심 구성 요소는 다음과 같아요:**

* **시스템 프롬프트:** 모든 대화에 적용되는 기본적인 행동 규칙 (예: "항상 친절하게 답변해줘.")
* **사용자 입력:** 지금 당장 AI에게 시키는 작업이나 질문
* **대화 기록:** 이전 대화 내용 (AI가 앞뒤 내용을 연결해서 이해하도록 돕습니다.)
* **장기 기억:** 사용자 선호도나 과거 결정처럼 여러 번의 대화에 걸쳐 AI가 기억해야 할 정보
* **검색된 정보:** 문서, 데이터베이스 등 외부에서 가져온 지식
* **사용 가능한 도구:** 텍스트 생성 외에 AI가 할 수 있는 다른 작업 (예: 계산하기, 날씨 정보 찾기)
* **출력 스키마:** AI가 답변을 어떤 형식으로 만들어야 하는지 (예: 항상 JSON 형식으로 대답해줘.)

### 4.3 '맥락 공학(Context Engineering)'이란?

**맥락 공학**은 AI 시스템이 제대로 작동하도록 이 **정보 환경을 설계하고 관리하는 기술**입니다. 단순히 프롬프트를 잘 쓰는 것을 넘어, AI에게 필요한 정보, 도구, 기억을 전체적으로 조율하는 것이죠. 목표는 AI 모델이 현재 작업에 필요한 정확한 정보만 받아들이도록 하여, 혼란을 막고 정보의 빈틈이 생기지 않도록 하는 것입니다.

**맥락 공학의 네 가지 주요 활동:**

1.  **맥락 창 외부에 정보 작성:** 나중에 필요할 때 꺼내 볼 수 있도록 정보를 따로 저장합니다.
2.  **관련 정보만 선택:** 특정 단계에 필요한 정보만 골라서 제공합니다.
3.  **긴 기록 압축:** 오래된 대화 기록은 요약하여 핵심만 남깁니다.
4.  **다른 맥락 격리:** 여러 작업이 서로 섞이지 않도록 구분합니다.

### 4.4 맥락 관리 실패 시 발생하는 문제들

맥락이 제대로 관리되지 않으면 AI는 다음과 같은 문제들을 겪을 수 있습니다.

* **맥락 오염 (Context Poisoning):** 잘못된 정보나 환각(거짓 정보)이 맥락에 스며들어 AI 전체에 퍼져나가는 경우.
* **맥락 분산 (Context Distraction):** 너무 많은 정보 때문에 AI가 제대로 추론하지 못하고 과거 행동을 반복하거나 새로운 해결책을 찾기 어려워지는 경우.
* **맥락 혼란 (Context Confusion):** 관련 없는 정보가 AI의 작업 기억을 채워서 사용할 수 있는 도구가 줄어들고 성능이 떨어지는 경우.
* **맥락 충돌 (Context Clash):** 맥락 내의 여러 정보가 서로 모순되어 AI가 결정을 내리지 못하고 혼란에 빠지는 경우.

### 4.5 맥락 공학의 핵심 기술

* **맥락 작성 (Writing Context):** 당장 AI에게 필요한 정보는 아니지만, 나중에 참조할 수 있도록 영구적인 저장 공간에 메모를 하거나 계획, 중간 결과를 저장하는 기술입니다.
* **맥락 선택 (Selecting Context):** 현재 단계에 꼭 필요한 정보만 똑똑하게 찾아오는 기술입니다. 불필요한 정보가 중요한 정보를 압도하는 '맥락 과부하'를 막을 수 있어요.
* **맥락 압축 (Compressing Context):** 토큰 비용(AI가 정보를 처리하는 데 드는 비용)을 줄이면서도 긴 대화 기록의 이점을 유지하는 방법입니다. 긴 대화를 자동으로 요약하여 핵심 결정만 남기는 방식이죠.
* **맥락 격리 (Isolating Context):** 서로 다른 작업들이 섞여서 간섭하는 것을 막는 기술입니다. 각 작업에 맞는 별도의 맥락 창을 만들거나, 특정 작업을 처리하는 데 특화된 AI 에이전트를 사용하는 방식 등이 있습니다.

### 4.6 맥락 공학을 돕는 도구들

맥락 공학은 하나의 도구로 해결되는 것이 아니라 여러 기술과 도구의 조합이 필요합니다.

* **DSPI, LangGraph:** AI 구성 요소를 모듈식으로 다루고, 복잡한 워크플로를 구현할 수 있도록 돕는 프레임워크입니다.
* **벡터 데이터베이스 (예: Pinecone, Weaviate, Chroma):** 수많은 문서나 기억에서 필요한 정보를 의미적으로 검색할 수 있도록 해줍니다.
* **구조화된 생성 도구:** AI가 답변을 특정 형식(JSON, XML 등)으로 출력하도록 강제하여 맥락 손상을 방지합니다.
* **메모리 시스템 (예: MEZero):** 정보 간의 관계를 이해하고, 오래된 정보를 자동으로 정리하며, 필요한 정보를 자동으로 제공하는 지능형 기억 시스템입니다.
* **오케스트레이션 플랫폼:** 여러 AI 모델, 데이터베이스, API를 조율하고 맥락 일관성을 유지하는 복잡한 작업을 처리합니다.

### 4.7 프롬프트 공학과 맥락 공학의 관계

**맥락 공학은 프롬프트 공학의 더 큰 개념 안에 포함된다고 볼 수 있습니다.** 마치 건물을 지을 때 프롬프트 공학이 벽돌 하나하나를 잘 쌓는 기술이라면, 맥락 공학은 건물의 튼튼한 기초(기억, 정보 검색, 상태 관리)를 다지고 전체 설계도를 그리는 기술입니다.

AI를 실제 비즈니스에 적용하여 **신뢰할 수 있고, 일관된 결과를 내며, 고객 정보를 기억하고, 외부 데이터와 연동하고, 복잡한 비즈니스 규칙을 따르게 하려면 맥락 공학이 필수적입니다.**

### 4.8 지금 바로 시작해볼까요?

AI 시스템의 성능을 향상시키기 위한 실행 계획은 다음과 같습니다.

1.  **프롬프트 공학의 한계점 찾기:** AI가 답변을 제대로 못 하거나, 이전 대화를 잊어버리거나, 여러 요구사항을 처리할 때 혼란을 겪는 부분을 찾아보세요.
2.  **영향이 큰 시스템부터 시작:** 개선했을 때 효과가 명확하고, 일관성이 특히 중요한 시스템을 선택합니다.
3.  **기본적인 기억 기능 구현:** AI가 대화 세션 간에 정보를 저장하고 다시 가져올 수 있는 기능을 추가해 보세요.
4.  **관련 문서 또는 과거 결정 검색 기능 추가:** AI가 필요한 정보를 찾아오고, 대화가 길어질 때 핵심만 요약하며, 다른 작업들이 섞이지 않도록 맥락을 분리하는 기능을 도입해 보세요.
5.  **성과 측정:** 정확도, 일관성, 사용자 만족도가 얼마나 개선되었는지 꼼꼼하게 확인하여 성공 여부를 판단합니다.
