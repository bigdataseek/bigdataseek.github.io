---
title: 4차시 1 :StatQuest(Statistics) 1
layout: single
classes: wide
categories:
  - Statistics 
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 1. 히스토그램(Histograms)
출처:[StatQuest: Histograms, Clearly Explained](https://www.youtube.com/watch?v=qBigTkBLU6g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=1)

데이터의 양이 많아질 때 개별적인 측정값을 시각화하는 것의 한계를 극복하고, 데이터의 전반적인 분포를 파악하는 방법을 제시합니다.

### **1.1 히스토그램이 필요한 이유**
*   사람들의 키와 같이 많은 측정값을 점으로 표시할 경우, **점들이 겹쳐서 보이지 않거나 정확한 위치에 겹치지 않아 전체적인 분포를 파악하기 어렵습니다**.
*   동일한 위치에 있는 점들을 쌓아 올리는 방식도 측정값이 정확하게 겹치는 경우가 드물기 때문에 여전히 많은 정보를 놓칠 수 있습니다.

### **1.2 히스토그램의 정의 및 작동 방식**
*   히스토그램은 측정값의 범위를 **'bin(빈)'이라는 구간들로 나눈 다음**, 각 bin에 속하는 측정값들을 쌓아 올리는 방식으로 만들어집니다.
*   즉, 각 bin은 특정 범위의 데이터를 대표하며, 해당 bin에 속하는 데이터의 개수를 그 **높이로 표현**합니다.

### **1.3 히스토그램 해석 및 활용**
*   **높이가 높은 bin은 해당 범위에 우리가 측정한 값이 많다는 것을 의미**합니다.
*   히스토그램을 이용하면 **미래의 측정값이 특정 범위에 있을 가능성을 예측**할 수 있습니다. 예를 들어, 특정 bin의 높이가 높으면 미래 측정값이 그 범위에 있을 가능성이 더 높다고 볼 수 있습니다.
*   히스토그램은 **데이터의 분포를 근사치를 구하기 위한 좋은 방법**이 됩니다. 예를 들어, 데이터의 형태를 보고 정규 분포나 지수 분포와 같은 특정 분포를 사용하여 미래 측정값을 예측하는 데 활용할 수 있습니다.

### **1.4 Bin 폭(Width)의 중요성**
*   **bin의 폭을 정하는 것은 매우 중요하고 까다로운 작업**입니다.
*   **bin이 너무 좁으면:**
    *   많은 값들이 각각 하나의 bin을 가지게 되어, 단순히 개별 점들을 나열한 것과 크게 다르지 않게 됩니다.
    *   이는 데이터를 통해 유용한 정보를 얻기 어렵게 만들어, 그다지 도움이 되지 않습니다.
*   **bin이 너무 넓으면:**
    *   측정값이 단 몇 개의 bin에만 나뉘어, 데이터의 세부적인 특징을 파악하기 어렵습니다.
    *   예를 들어, 측정값이 단 두 개의 넓은 bin에 나뉘면 '평균보다 높은 값의 개수'와 '평균보다 낮은 값의 개수' 정도만 알 수 있어, 더 많은 정보를 얻을 수 있는 기회를 놓치게 됩니다.
*   **최적의 bin 폭 찾기:**
    *   데이터를 명확하게 파악하기 위해서는 **다양한 폭의 bin을 가지고 테스트**를 해봐야 합니다.
    *   사용하는 프로그램의 **기본 설정(default setting)을 맹목적으로 믿지 말고**, 최고의 히스토그램을 얻었다고 확신하기 전에 여러 bin 폭 설정을 시도해보는 것이 중요합니다.

## 2. 확률 분포의 주요 개념: 데이터를 히스토그램으로 시각화
**"분포가 무엇인지"**에 대한 설명입니다. 많은 양의 데이터를 시각화하고 그 특성을 파악하는 데 필수적인 개념을 소개합니다.

### **2.1 분포의 기본 아이디어 (히스토그램을 통한 이해)**
*   **측정값들을 'bin(빈)'에 쌓기:** 많은 사람들의 키를 측정하는 예시를 통해 분포의 개념을 설명합니다. 개별 키 측정값을 일정한 구간(bin)에 넣고 쌓는 방식으로 시작합니다. 예를 들어, 5.2피트는 5~5.5피트 bin에, 5.8피트는 5.5~6.0피트 bin에 들어가는 식입니다.
*   **히스토그램 생성:** 이처럼 많은 측정값들을 bin에 쌓으면 **히스토그램(Histogram)**을 얻을 수 있습니다.
*   **히스토그램 해석:**
    *   히스토그램의 **높이가 높은 부분**은 해당 범위에 **대부분의 측정값들이 존재한다**는 것을 의미합니다. 예를 들어, 대부분의 키는 5피트와 6피트 사이에 분포하며, 5피트보다 작거나 6피트보다 큰 사람은 상대적으로 적다는 것을 보여줍니다.
    *   히스토그램을 통해 **무작위로 하나의 측정값을 뽑을 때, 그 값이 특정 범위에 있을 확률**을 가늠할 수 있습니다.
    *   데이터의 전반적인 형태를 파악하여 누가 정말 크거나 작은지, 혹은 평균에 가까운 키를 가졌는지 판단할 수 있게 해줍니다.

### **2.2 Bin 폭(Width)의 중요성**
*   **작은 Bin의 효과:** bin의 크기를 더 작게 만들면, 데이터가 어떻게 쌓이는지 **더 정확하고 정밀하게 평가**할 수 있습니다. 이는 데이터의 세부적인 분포를 더 잘 볼 수 있게 해줍니다.
*   (소스에 명시되어 있지는 않지만, 학습자의 이해를 돕기 위해 추가 설명) 만약 bin의 폭이 너무 넓거나 좁으면 데이터의 실제 분포를 왜곡하여 보여줄 수 있으므로, 적절한 bin 폭을 선택하는 것이 중요합니다.

### **2.3 히스토그램을 근사화하는 곡선의 이점 (확률 밀도 함수로의 전환)**
*   **곡선을 통한 근사화:** 히스토그램과 같은 모양을 가지면서, 그 형태를 **곡선으로 근사화(Approximation)**할 수 있습니다. 이 곡선은 히스토그램이 보여주는 측정값의 가능성 분포를 나타냅니다.
*   **곡선의 장점:**
    *   **미측정 bin의 확률 계산:** 아무리 많은 측정을 했어도 히스토그램의 특정 bin에 값이 없을 수 있는데, 이 곡선을 이용하면 해당 bin에서도 확률을 계산할 수 있습니다.
    *   **bin 폭 제한 없음:** 곡선은 bin 폭의 제한을 받지 않습니다. 예를 들어, 5.021피트와 5.317피트 사이와 같이 **매우 세밀한 범위의 값을 측정할 확률을 미적분학(또는 컴퓨터)을 이용해 정확하게 계산**할 수 있습니다. 이는 히스토그램처럼 가까운 bin에 맞춰 반올림할 필요 없이 정밀한 계산이 가능함을 의미합니다.
    *   **시간과 비용 절약:** 대량의 데이터를 측정하기 어려운 경우, 데이터의 평균과 표준편차에 기반한 근사 곡선을 사용하는 것이 보통 충분히 좋으며, 이는 **많은 시간과 돈을 절약**하게 해줍니다.
*   **분포의 정의 확장:** 히스토그램과 이러한 곡선 모두 "분포(Distribution)"라고 불리며, **측정값들의 가능성이 어떻게 분포되어 있는지**를 보여줍니다. 가장 높은 부분은 측정값들이 주로 존재하는 영역을, 낮은 부분은 측정값들이 거의 존재하지 않을 영역을 나타냅니다.

키 측정값의 분포를 예로 들었지만, **다른 모양을 가진 다양한 종류의 분포**들이 있다.
이 내용은 **데이터의 특성을 이해하고 예측 모델을 구축하는 데 필수적인 개념인 확률 분포를 시각적으로 이해하는 데 큰 도움**이 될 것입니다. 특히, 히스토그램과 그것을 근사하는 곡선이 데이터 분석에서 어떻게 활용되는지에 대한 기초를 제공합니다.

## 3. 정규분포(Normal Distribution)

### **3.1 정규분포란 무엇인가요?**
정규분포는 통계학에서 매우 중요한 개념으로, '가우시안 분포(Gaussian Distribution)' 또는 '종모양 곡선(Bell-shaped Curve)'이라고도 불립니다. 이름처럼 대칭적인 **종모양**을 띠는 곡선으로 나타납니다.

### **3.2 정규분포 곡선의 이해**
*   **X축**: 우리가 측정하고자 하는 값(예: 사람의 키, 몸무게, 통근 시간 등)을 나타냅니다.
*   **Y축**: 특정 측정값이 관찰될 **상대적 확률**을 나타냅니다.
    *   예를 들어, 키가 매우 작거나 매우 큰 사람을 볼 확률은 상대적으로 드물기 때문에, 곡선의 양쪽 끝 부분에서는 높이가 낮습니다.
    *   반면에, **평균 키에 가까운 사람**을 볼 확률은 꽤 흔하기 때문에, 곡선의 가운데 부분(평균값)에서 높이가 가장 높습니다.

### **3.3 정규분포를 정의하는 두 가지 핵심 요소**
정규분포 곡선을 정확하게 이해하고 그리기 위해서는 다음 두 가지를 반드시 알아야 합니다:

1.  **평균 (Mean)**:
    *   측정값들의 산술 평균입니다.
    *   정규분포 곡선의 **중심**이 어디에 위치하는지를 알려줍니다. 정규분포는 항상 평균값을 기준으로 대칭을 이룹니다.
    *   예시: 갓난아기의 평균 키는 20인치, 성인 남성의 평균 키는 70인치입니다.

2.  **표준편차 (Standard Deviation)**:
    *   측정값들이 평균으로부터 얼마나 퍼져 있는지를 나타내는 척도입니다.
    *   곡선이 **얼마나 넓어야 하는지**를 결정하며, 곡선의 넓이는 그 높이에도 영향을 미칩니다.
        *   **곡선이 넓을수록 높이는 낮아지고**, **곡선이 좁을수록 높이는 높아집니다**.
    *   예시: 갓난아기 키의 표준편차는 0.6인치로 성인 남성의 키 표준편차(4인치)보다 작습니다. 이 때문에 아기 키 분포 곡선은 성인 키 분포 곡선보다 더 좁고 높게 나타납니다.

### **3.4 정규분포의 중요한 특성: 95% 규칙**
정규분포의 중요한 특성 중 하나는 대부분의 측정값이 평균 근처에 몰려있다는 것입니다. 특히, 정규곡선은 **측정값의 약 95%**가 **평균을 중심으로 +/- 2 표준편차** 범위 내에 속하도록 그려집니다.

*   **예시**:
    *   갓난아기 키의 95%는 평균(20인치)을 중심으로 +/- 1.2인치 (표준편차 0.6인치 * 2) 범위 내에 있습니다.
    *   성인 키의 95%는 평균(70인치)을 중심으로 +/- 8인치 (표준편차 4인치 * 2) 범위 내에 있습니다.

### **3.5 정규분포의 중요성**
정규분포는 자연 현상에서 매우 흔하게 관찰될 뿐만 아니라, 통계학과 인공지능 분야에서 **데이터를 이해하고 모델링하는 데 필수적인 개념**입니다.
*   **데이터 분포 이해**: 정규분포를 통해 데이터가 어떻게 퍼져 있고, 어떤 값이 가장 흔하며, 이상치는 무엇인지 등을 파악할 수 있습니다.
*   **확률 추론**: 특정 구간에 측정값이 존재할 확률을 계산하는 기초가 됩니다.
*   **통계적 추론의 기반**: 통계학의 '마법'과 같다고 언급되는 **중심극한정리(Central Limit Theorem)**를 이해하는 데 핵심적인 배경이 됩니다. 중심극한정리는 여러 통계적 분석 및 머신러닝 모델(예: 선형 회귀, 신경망의 가중치 초기화 등)의 이론적 기반이 되므로, 정규분포에 대한 이해는 AI 학습자에게 매우 중요합니다.

## 4. The mean, the median, and the mode.
데이터가 정규분포 일때 , mean, median 그리고 mode는 모두 동일한 값을 갖는다.

이 Mean = Median = Mode 관계는 정규분포의 완벽한 대칭성에서 비롯된 가장 중요한 특징 중 하나입니다.이 관계는 데이터의 분포를 확인하는 데 유용하게 쓰입니다. 만약 어떤 데이터셋의 mean, median, mode 값이 크게 다르다면, 그 데이터는 치우침(Skewness)이 존재하여 정규분포를 따른다고 보기 어렵다는 중요한 증거가 됩니다.

* Mean > Median인 경우: 오른쪽으로 치우친 분포 (Positive Skew)
* Mean < Median인 경우: 왼쪽으로 치우친 분포 (Negative Skew)

따라서 정규분포를 가정하는 많은 통계적 방법론(예: t-검정, ANOVA)을 사용하기 전에 이 세 값의 관계를 확인하는 것은 데이터가 정규성을 만족하는지 검토하는 좋은 출발점이 됩니다.

## 5. 지수 분포(The Exponential Distribution)

지수 분포는 어떤 사건이 연속적으로 일어날 때, 그 사건 사이의 대기 시간(시간 간격)을 나타내는 연속 확률 분포입니다. 쉽게 말해, "다음 사건이 일어날 때까지 얼마나 기다려야 하는가?"에 대한 확률을 모델링합니다.

가장 큰 특징은 '기억력이 없다(Memoryless Property)'는 것입니다. 이는 과거에 얼마나 오래 기다렸는지와 관계없이, 다음 사건이 일어날 때까지의 남은 시간의 확률 분포가 동일하다는 의미입니다.

## 6. 모집단과 모수 추정

### **6.1 모집단(Population)의 이해**
*   **정의**: 모집단은 우리가 관심을 가지는 **모든 측정값의 전체 집합**을 의미합니다. 예를 들어, 한 사람의 2,400억 개에 달하는 모든 간세포 내 유전자 X의 mRNA 수, 또는 모든 가게의 녹색 사과 개수 등이 모집단이 될 수 있습니다.
*   **히스토그램 및 분포**: 모집단 내 모든 측정값을 세어 히스토그램을 그리면, 대부분의 데이터가 특정 범위에 분포하고 일부는 적게 분포하는 형태를 볼 수 있습니다. 이러한 히스토그램은 정규분포(평균=20, 표준편차=10인 예시), 지수분포(Rate=0.1인 예시), 감마분포(Shape, Rate에 의해 결정되는 예시) 등 다양한 **통계적 분포**에 대응될 수 있습니다.
*   **확률 계산**: 이러한 분포를 사용하여 특정 범위의 값이 관찰될 확률을 계산할 수 있습니다. 예를 들어, 30개 이상의 mRNA를 가진 간세포를 관찰할 확률은 해당 범위의 곡선 아래 면적을 전체 면적으로 나눈 값으로 구할 수 있습니다.

### **6.2 모수(Population Parameters)란 무엇인가요?**
*   **정의**: 모수(Population Parameters)는 **모집단의 분포 형태를 결정하는 숫자**들입니다. 이들은 모집단 전체의 특성을 요약해 줍니다.
*   **예시**:
    *   **정규분포**: 모집단의 **평균(Population Mean)**과 **표준편차(Population Standard Deviation, Population SD)**가 모수입니다. 평균은 분포의 중심을, 표준편차는 데이터가 평균 주변으로 얼마나 퍼져 있는지를 나타냅니다.
    *   **지수분포**: **Rate**가 모수입니다.
    *   **감마분포**: **Shape**와 **Rate**가 모수입니다.
*   **중요성**: 모수는 모집단의 핵심적인 통찰력(예: 특정 확률)을 제공하며, 이는 모든 실험 결과와 미래의 실험에도 적용될 수 있는 근거가 됩니다.

### **6.3 표본(Sample)과 모수 추정(Parameter Estimation)**
*   **현실적인 제약**: 현실적으로 우리는 시간과 돈이 충분하지 않아 모집단의 모든 것을 측정할 수 없습니다 (예: 2,400억 개의 간세포를 모두 세는 것은 불가능합니다).
*   **표본의 사용**: 따라서 우리는 모집단에서 **상대적으로 작은 일부를 추출한 표본(Sample)**을 사용하여 모집단의 모수를 **추정(Estimate)**합니다. 예를 들어, 전체 간세포 중 5개만 측정하는 것입니다.
*   **추정된 모수(Estimated Parameters)**: 표본으로부터 계산된 평균이나 표준편차는 '추정된 모집단의 평균', '추정된 모집단의 표준편차'라고 불립니다.
*   **추정치의 가변성**: 실험을 반복할 때마다 (즉, 다른 표본을 추출할 때마다) 모수의 추정값은 달라질 수 있습니다. 이러한 추정값들은 실제 모집단의 모수와는 다를 수 있습니다.

### **6.4 AI/머신러닝에서의 중요성**
*   **학습 데이터와의 연결**: 5개의 측정값과 같은 표본은 머신러닝의 **학습 데이터셋(Training Dataset)**으로 생각할 수 있습니다.
*   **예측의 목표**: 우리가 머신러닝을 통해 예측하고자 하는 것은 궁극적으로 **모집단을 나타내는 분포 곡선**, 즉 모집단의 모수입니다.
*   **결과의 재현성(Replicability)**: 모수를 추정하고 그 추정치를 결과의 근거로 사용함으로써, 우리는 다른 사람이 동일한 방식으로 실험했을 때 유사한 결과를 얻을 수 있도록 (모사할 수 있도록) 만듭니다.

### **6.5 추정치의 신뢰도 정량화**
*   **데이터 양의 영향**: 일반적으로 **더 많은 데이터를 가질수록** 추정된 모수는 실제 모수에 더 가까워지며, 따라서 **추정 값에 대한 신뢰도가 높아집니다**.
*   **신뢰도 측정**: 통계학에서는 **p-value**와 **신뢰구간(Confidence Intervals)**과 같은 도구를 사용하여 모수 추정치에 대한 신뢰도를 정량화합니다. 이는 추정치와 실제 값의 차이가 얼마나 큰지, 그리고 그 차이를 얼마나 신뢰할 수 있는지를 알려줍니다.
*   **결론**: 모집단의 모수를 추정하고 그 신뢰도를 수치화함으로써, 우리는 미래의 실험에서 재현 가능한(모사할 수 있는) 결과를 만들어낼 수 있습니다.

## 7. 평균, 분산, 그리고 표준편차 계산

### 7.1  **데이터의 이해 및 예시**
*   영상에서는 간세포의 유전자 X mRNA 개수를 예시로 들며, 초록색 점들(3, 13, 19, 24, 29)이 개별 측정값을 나타냅니다.
*   만약 모든 2,400억 개의 간세포를 측정할 수 있다면, 이는 **모집단** 데이터가 됩니다.
*   하지만 현실적으로는 5개와 같은 **상대적으로 적은 수의 표본**을 사용해 모집단을 추정합니다.

### 7.2  **모집단 평균 (μ, 뮤)**
*   **정의**: 전체 모집단의 모든 측정값의 평균입니다.
*   **계산**: 모든 측정값의 합을 측정값의 총 개수(N)로 나눕니다.
*   **특징**: 모집단 전체를 측정하는 것은 거의 불가능하므로, **실제 모집단 평균을 계산할 일은 거의 없습니다**.

### 7.3  **표본 평균 (x̄, x바) 또는 추정 평균**
*   **정의**: 모집단 평균을 추정하기 위해 표본 데이터의 평균을 사용한 값입니다.
*   **계산**: 표본 측정값의 합을 표본 측정값의 개수(n)로 나눕니다.
*   **특징**: 표본의 수가 많아질수록 **x̄는 μ에 가까워집니다**. 우리가 실제로 가장 많이 사용하는 값입니다.

### 7.4  **모집단 분산 (σ²)**
*   **정의**: 모집단 데이터들이 모집단의 평균(μ)으로부터 얼마나 퍼져있는지를 나타내는 값입니다.
*   **계산**: 각 측정값(X)과 모집단 평균(μ)의 차이를 제곱한 후, 이 제곱 값들의 평균을 구합니다 (Σ(X - μ)² / N).
*   **제곱하는 이유**: 차이가 음수(-)인 경우와 양수(+)인 경우가 서로 상쇄되는 것을 막기 위함입니다.
*   **단위 문제**: 결과값의 단위는 원래 측정값의 단위의 제곱이 됩니다 (예: mRNA²).

### 7.5  **모집단 표준편차 (σ, 시그마)**
*   **정의**: 모집단 분산에 루트(√)를 씌운 값입니다.
*   **특징**: 단위가 원래 측정값과 같아지므로, **그래프에 직접 표시하여 데이터의 퍼짐 정도를 직관적으로 이해**할 수 있습니다. 모집단 분산과 마찬가지로 거의 계산할 일이 없습니다.

### 7.6  **추정 분산 (s²) 또는 표본 분산**
*   **정의**: 모집단 분산을 추정하기 위해 표본 데이터를 사용하여 계산한 값입니다.
*   **핵심 차이점 (모집단 분산 공식과 비교)**:
    1.  **모집단 평균 (μ) 대신 표본 평균 (x̄)을 사용**합니다.
    2.  **측정값의 개수(n) 대신 (n-1)로 나눕니다**.
*   **(n-1)로 나누는 이유**: 표본 평균(x̄)은 모집단 평균(μ)보다 표본 데이터에 더 가깝기 때문에, (X - x̄)² 값들이 (X - μ)² 값들보다 작아지는 경향이 있습니다. 만약 n으로 나누면 분산이 과소평가될 수 있으므로, **(n-1)로 나누어 이러한 과소평가를 보상**하고 더 정확한 추정치를 얻습니다.

### 7.7  **추정 표준편차 (s) 또는 표본 표준편차**
*   **정의**: 추정 분산에 루트(√)를 씌운 값입니다.
*   **특징**: 단위가 원래 측정값과 같아 그래프에 표시하기 용이하며, 추정된 평균과 함께 데이터의 분포를 시각화하는 데 사용됩니다.

### 7.8  **실용적인 중요성 (AI 학생을 위한 조언)**
*   데이터 분석에서 우리는 **거의 항상 모집단의 모수(평균, 분산, 표준편차)를 표본을 통해 '추정'하게 됩니다**.
*   엑셀과 같은 소프트웨어에서 분산을 계산할 때, **'VAR.P'(모집단 분산) 대신 'VAR.S'(추정 분산)를 사용해야 합니다.** 이는 우리가 대부분 표본 데이터를 다루기 때문입니다.
*   적은 수의 측정값만으로도 모집단의 특성을 상당히 잘 추정할 수 있으며, 이는 막대한 시간과 비용을 절약해 줍니다.

실제 데이터를 다룰 때 **모집단 모수(평균, 분산, 표준편차)를 '계산'하는 것이 아니라 표본을 통해 '추정'하는 것**이 일반적이라는 점을 명확히 이해해야 합니다. 특히 **추정 분산을 계산할 때 (n-1)로 나누는 이유**를 아는 것은 통계적 추정의 핵심 원리를 파악하는 데 매우 중요합니다.

## 8. 수학적 모델(model)이란 무엇인가?

### 8.1  **모델의 정의 및 의미**
*   **"모델"이라는 단어는 다양한 의미로 사용될 수 있습니다.** 어린 시절의 장난감 조립 모델, 옷을 입는 사람인 패션 모델 등 여러 의미가 있지만, 통계 및 수학 분야에서 **모델은 관계를 나타냅니다**.
*   예를 들어, 쥐의 무게와 크기 사이의 관계를 분석하는 데 사용될 수 있습니다.

### 8.2  **모델의 역할: 관계 분석 및 예측**
*   **모델은 관계를 분석하는 데 사용됩니다.** 유전자 X와 쥐의 크기 사이의 관계를 알고 싶을 때처럼 말입니다.
*   **모델은 방정식의 형태를 가질 수 있습니다.** 데이터에 맞는 선형 방정식이 그 예시입니다.
*   이러한 **수학적 모델 또는 방정식은 아직 측정되지 않은 새로운 데이터에 대해 예측을 제공할 수 있습니다.** 예를 들어, 4유닛 무게를 가진 쥐의 크기를 3.3으로 예측하는 것과 같습니다.

### 8.3  **모델의 특성: 실제 데이터의 근사화**
*   **모델 또는 방정식은 실제 데이터를 근사화(approximation)한 것입니다**.
*   **점선으로 표시되는 실제 데이터와 모델 간의 차이**는 모델이 데이터를 얼마나 잘 설명하는지 보여줍니다.
*   **많은 통계적 방법들은 모델이 실제 데이터에 얼마나 잘 근사화되었는지 여부를 판별하는 데 집중합니다**. 이는 선형 회귀, 일반 선형 모델, t-검정, ANOVA, F-검정 등 다양한 통계 기법들을 통해 이루어집니다.

### 8.4  **모델의 형태와 복잡성**
*   **모델은 항상 직선 형태일 필요는 없습니다.** 약 복용량과 머리카락 성장 사이의 관계처럼 특정 지점 이후로는 효과가 감소하는 비선형 관계를 모델링할 수도 있습니다.
*   **모델은 간단할 수도 있고 복잡할 수도 있습니다.** 쥐의 크기를 모델링하는 데 '유전자 X'와 '유전자 Y' 두 가지 유전자를 사용하는 것과 같이 여러 변수를 포함할 수 있습니다.

### 8.5  **실용적 중요성**
*   **AI 및 머신러닝 분야에서 모델은 데이터로부터 패턴을 학습하고 예측을 수행하는 핵심적인 도구입니다.** 선형 회귀, 로지스틱 회귀, 신경망 등 다양한 머신러닝 알고리즘들은 본질적으로 데이터를 모델링하는 방법입니다.
*   **데이터를 기반으로 한 예측 모델을 구축하고, 통계적 방법을 통해 이 모델의 유용성과 신뢰도를 평가하는 능력은 AI 엔지니어에게 매우 중요합니다**.
*   주어진 데이터를 통해 미지의 데이터를 예측하고, 모델의 성능을 정량적으로 평가하는 과정은 AI 프로젝트의 필수적인 부분입니다.

**모델이 데이터 간의 관계를 파악하고 예측하는 수학적 도구**임을 이해해야 합니다. 또한, **모델은 실제 데이터를 완벽하게 나타내는 것이 아니라 '근사화'하는 것이며, 통계적 방법을 통해 모델의 유용성과 신뢰도를 꾸준히 평가**해야 한다는 점을 기억해야 합니다.

## 9. 가설검정과 귀무가설

### **9.1 가설검정의 필요성: 무작위성(Randomness)과 변동성(Variability)**
*   바이러스 치료제 A와 B의 효과를 비교하는 가상의 시나리오를 통해 설명합니다.
*   동일한 약을 투여하더라도, 사람마다 회복 시간은 다릅니다. 이는 운동량, 식습관, 스트레스, 면역 체계 등 **우리가 통제할 수 없는 임의의 요인들** 때문입니다.
*   이러한 무작위적 변동성 때문에, 소규모 실험에서 얻은 결과(예: A약이 B약보다 평균 15시간 더 빠르게 회복)만으로 성급하게 가설을 세우고 확신하기 어렵습니다.

### **9.2 초기 가설 설정과 검증의 어려움**
*   **초기 가설**: 소규모 데이터(예: 각 3명의 실험 참가자)를 바탕으로 "A약이 B약보다 평균 15시간 빠르게 회복한다"는 가설을 세울 수 있습니다.
*   **반복 실험의 중요성**: 이 가설을 검증하기 위해 실험을 반복하게 됩니다.
*   **예상치 못한 결과**: 하지만 반복 실험 결과는 처음 세운 가설과 **완전히 반대**되거나, 혹은 **약간 다른** 결과를 보여줄 수 있습니다. 예를 들어, A약이 오히려 회복에 35시간 더 걸리거나, C약이 D약보다 13시간이 아닌 12시간 또는 13.5시간 빠르게 회복하는 식입니다.
*   **문제점**:
    *   만약 반복 실험에서 일관되게 기존 가설과 **반대되는 결과**가 나온다면, 처음 세운 가설은 **틀렸다고 자신 있게 말할 수 있습니다**.
    *   그러나 반복 실험 결과가 가설과 **완전히 같지는 않지만, 크게 다르지도 않다면** (예: 13시간 가설 vs 12시간/13.5시간 결과), 우리는 그 가설이 틀렸다고 **확실히 말할 수도, 또 완전히 맞다고 확신할 수도 없습니다**.
    *   이는 초기 가설이 단지 **하나의 특정 실험 결과에만 기반**했기 때문이며, 만약 다른 실험 결과를 바탕으로 가설을 세웠다면 또 다른 특정 가설(예: 12시간 빠르다, 13.5시간 빠르다)이 되었을 것이기 때문입니다. 즉, **많은 합리적인 가설들이 존재**할 수 있습니다.

### **9.3 귀무가설(Null Hypothesis)의 도입**
*   **어떤 가설을 검증해야 하는가?** 수많은 합리적인 가설 중에서 어떤 것을 선택하여 검증할지 모호하다는 문제가 발생합니다.
*   이러한 모호함을 해결하기 위해 **귀무가설(Null Hypothesis)**이라는 개념이 사용됩니다.
*   **귀무가설의 정의**: 귀무가설은 **"어떤 것들 사이에 차이가 없다"**는 가설을 의미합니다. 예를 들어, E약과 F약 사이에 회복 시간의 **차이가 없다**는 것입니다.
*   **귀무가설의 장점**:
    *   특정 시간(예: 0.5시간, 0.25시간)과 같은 **정확한 수치를 알 필요 없이**, 단순히 **'차이가 0이다'**는 가설을 세울 수 있습니다.
    *   이는 **소규모 데이터에 의존하여 가설을 세울 필요가 없게 합니다.** '차이가 없음'이라는 기준점 '0'은 데이터 없이도 설정할 수 있기 때문입니다.

### **9.4 귀무가설 검증의 원리**
*   **데이터가 귀무가설을 '압도적으로 설득하지 못한다면'**: 소규모 데이터에서 E약과 F약 사이에 0.5시간 또는 0.25시간의 작은 차이가 관찰될 수 있습니다. 하지만 이러한 작은 차이는 우리가 통제할 수 없는 임의의 요인들(예: 건강 상태, 식습관) 때문에 **쉽게 바뀔 수 있습니다**. 이런 경우, 데이터는 "E약과 F약 사이에 차이가 없다"는 귀무가설이 틀렸다고 **강력하게 주장하지 못합니다**. 따라서 우리는 귀무가설을 **기각할 수 없습니다** (즉, 차이가 없다고 계속 가정하는 것이 합리적입니다).
*   **귀무가설을 '기각할 수 있다면'**: 만약 충분히 많은 사람들에게 약을 테스트하고, 작은 임의의 요인들이 결과에 큰 영향을 미치지 않는 상황에서 **일관된 차이**가 관찰된다면, "모든 사람이 우연히 좋은 식습관을 가졌거나 운동을 열심히 했다"와 같은 설명을 상상하기 어려워집니다. 이럴 때 우리는 **귀무가설을 기각할 수 있습니다.** 귀무가설을 기각한다는 것은 **약 종류 간에 통계적으로 유의미한 차이가 존재한다**고 결론 내릴 수 있음을 의미합니다.

가설검정은 데이터의 무작위적 변동성을 이해하고, 이를 바탕으로 세워진 가설이 타당한지 아닌지를 통계적으로 판단하는 과정입니다. 특히 **귀무가설**은 "아무런 차이가 없다"는 기준점을 제시함으로써, 복잡하고 불확실한 데이터 속에서 **특정 효과의 존재 여부를 명확하게 판단할 수 있도록 돕는 강력한 도구**입니다.

## 10. 대립가설(Alternative Hypotheses)

### **10.1 귀무가설(Null Hypothesis)의 간략한 복습**
*   가설검정의 목적은 여러 가설에 대한 혼란을 피하고, 단순히 **"차이가 있는지 없는지"**를 결정하는 것입니다.
*   귀무가설은 **"어떤 것들 사이에 차이가 없다"**는 가설입니다. 예를 들어, 두 약(C약과 D약) 사이에 회복 시간의 **차이가 없다**고 가정하는 것입니다.
*   만약 많은 사람을 대상으로 한 실험에서 C약을 먹은 사람들이 D약을 먹은 사람들보다 **훨씬 더 짧은 회복 시간을 보인다면**, 그리고 이러한 차이가 **임의의 요인(예: 건강한 식습관)만으로는 설명하기 힘들 정도로 크다면**, 우리는 귀무가설이 **틀렸다**고 판단합니다. 즉, C약과 D약 사이에 **실제로 차이가 있음**을 알게 됩니다.
*   반대로, 만약 관찰된 차이가 **약간의 임의의 요인으로도 쉽게 바뀔 수 있을 정도라면**, 우리는 귀무가설이 틀렸다고 말하지 않습니다.

### **10.2 통계검증(Statistical Test)의 역할**
*   수집된 모든 데이터의 목적은 **"귀무가설이 틀렸다"고 할 것인지, 아니면 "귀무가설이 틀렸다고 하지 않을 것인지"**를 결정하는 것입니다.
*   이 결정을 내리기 위해 데이터를 **통계검증(Statistical Test)**이라는 과정에 넣게 되며, 통계검증의 결과가 바로 귀무가설에 대한 결정이 됩니다.
*   **통계검증을 수행하기 위해서는 세 가지 요소가 필요합니다**:
    1.  **데이터**
    2.  **귀무가설** (혹은 일차 가설, 즉 틀렸다고 할지 말지 결정할 어떤 것)
    3.  **대립가설**

### **10.3 대립가설(Alternative Hypothesis)의 이해**

*   **정의**: 대립가설은 이 경우, **단순히 귀무가설을 반대로 한 것**입니다. 즉, "차이가 있다"는 가설입니다. (하지만 모든 대립가설이 이렇게 단순하지는 않습니다. 아래 5번 참조)
*   **대립가설이 중요한 이유 (개념적 이해)**:
    *   C약과 D약의 차이가 없다는 **귀무가설**을 검증하는 한 가지 방법은, **두 약의 데이터를 합친 전체 평균(귀무가설을 나타냄)**과 각 **약 그룹의 개별 평균(대립가설을 나타냄)**을 비교하는 것입니다.
    *   만약 각 약 그룹의 개별 평균을 사용하는 것이 전체 평균을 사용하는 것보다 데이터를 더 잘 설명한다면(즉, 그룹 간 평균 차이가 통계적으로 유의미하고, 그룹 내 분산이 전체 분산보다 작다면), 이는 두 약 사이에 효과의 차이가 있음을 시사합니다. 이 경우 우리는 귀무가설을 기각하고, 두 약의 효과가 다르다는 증거가 있다고 결론지을 수 있습니다
    *   반대로, 전체 평균과 개별 약 그룹 평균 사이의 차이가 **크게 다르지 않다면**, 이는 관찰된 미묘한 차이가 **임의의 요인(예: 운동량, 식습관)에 의해 발생했을 수 있다**는 것을 의미합니다. 이 경우 우리는 **귀무가설이 틀렸다고 말하지 않습니다**.
*   머신러닝과의 연결: 유의미한 차이가 없음에도 불구하고 개별 그룹 평균에 과도하게 의존하는 것은, 샘플링 변동에 과도하게 적합되는 과적합(overfitting) 현상과 유사한 측면이 있습니다(머신러닝 용어에 익숙하지 않다면 이 부분은 넘어가도 좋습니다).

### **10.4 3개 이상의 그룹이 있을 때의 대립가설**
*   데이터 그룹이 두 개만 있을 때(예: C약 vs D약)는 대립가설이 단순히 귀무가설의 반대였기 때문에 **꽤나 명확합니다**.
*   그러나 **3개 혹은 그 이상의 그룹이 있다면 대립가설은 좀 더 흥미로워집니다**.
    *   예를 들어, 귀무가설이 "C, D, E 세 약 모두 차이가 없다"일 때:
        *   **하나의 대립가설**은 "**모든 약이 다 다르다**"가 될 수 있습니다. 이 경우, 각각의 약에 대한 평균과의 차이를 측정합니다.
        *   **또 다른 대립가설**은 "**C약과 D약은 차이가 없고, E약만 차이가 있다**"가 될 수 있습니다. 이 경우, C약과 D약은 합친 평균과의 차이를, E약은 따로 구한 평균과의 차이를 계산합니다.
*   이렇게 **여러 가지 대립가설이 존재**할 수 있으며, 통계검증에서 **어떤 대립가설을 사용하느냐에 따라 귀무가설에 대한 다른 결정을 내릴 수 있습니다**.

### **10.5 대립가설의 중요성 및 주의사항**
*   어떤 대립가설을 사용할지 **명확하게 하는 것이 중요**합니다.
*   하지만 검증에서 어떤 대립가설을 사용했든지 상관없이, 우리는 궁극적으로 **귀무가설이 틀렸다고 할지 말지만 결정**합니다.
*   **귀무가설을 '틀렸다'고 하더라도, 대립가설을 '받아들였다'고 말하지는 않습니다.** 왜냐하면 귀무가설을 '틀렸다'고 할 수 있는 다른 대립가설이 더 좋거나 많을 수 있기 때문입니다. 즉, 옳은 대립가설을 받아들였는지 알기 위한 가능한 가설들이 너무 많기 때문입니다.
*   결론적으로, 통계검증은 **"귀무가설이 틀렸다"라고 할지 말지만 결정**합니다.

**요약하자면,**
*   통계검증에는 **데이터, 귀무가설, 그리고 대립가설**의 세 가지 요소가 필요합니다.
*   두 그룹만 있을 때는 대립가설이 명확하게 귀무가설의 반대이지만, **3개 이상의 그룹이 있을 때는 여러 대립가설이 존재**할 수 있으며, 어떤 대립가설을 사용하느냐에 따라 귀무가설에 대한 결정이 달라질 수 있습니다.

## 11. P-value(P-값)

**P-value**는 0과 1 사이의 값으로, 두 그룹(예: A약과 B약) 간에 관찰된 차이가 **우연히 발생했을 가능성**을 정량화하는 통계적 도구입니다. AI 분야에서는 모델 간 성능 비교, A/B 테스트 결과 해석 등 다양한 상황에서 통계적 유의미성을 판단하는 데 활용됩니다.

### **11.1 P-value가 필요한 이유**
처음에는 A약과 B약을 각각 한 명에게 투여하거나, 두 명에게 투여하는 등 적은 수의 사람에게 실험하면, 플라시보 효과, 희귀 알레르기, 약물 오용, 약물 표기 오류 등 **이상하고 무작위적인 요인**들이 결과에 큰 영향을 미칠 수 있습니다. 따라서 A약이 B약보다 좋다고 쉽게 결론내릴 수 없습니다.

수많은 사람에게 약을 테스트하여 A약이 거의 모든 사람(1,046명 중 99.7%)을 치료하고 B약은 소수(1,434명 중 0.001%)만 치료했다면, A약이 B약보다 명백히 좋다고 볼 수 있습니다. 그러나 만약 A약이 37%를 치료하고 B약이 29%를 치료하는 것과 같이 **두 약의 차이가 크지 않을 때**, 무작위적인 요인들이 항상 존재하는 상황에서 A약이 더 우수하다고 어떻게 확신할 수 있을까요? 바로 이럴 때 P-value가 필요합니다.

### **11.2 P-value의 해석 방법**
*   **P-value는 0과 1 사이의 값**입니다. P-value가 0에 가까울수록 A약과 B약 사이에 **실제로 차이가 있다는 확신이 커집니다**.
*   **임계점(Threshold)**: "A약과 B약에 차이가 있다"고 결론 내리기 위한 P-value의 기준점을 **임계점**이라고 합니다.
    *   가장 일반적으로 사용되는 임계값은 **0.05**입니다. 이 임계값은 **"두 약의 차이가 없고, 이러한 실험을 아주 많이 했을 때, 5%의 실험 결과만이 우연히 틀리게 나온다"**는 것을 의미합니다.
    *   만약 P-value가 0.05보다 작으면, 우리는 두 약 사이에 통계적으로 유의미한 차이가 있다고 판단합니다.

### **11.3 거짓양성(False-positive)과 임계점의 의미**
*   **거짓양성(False-positive)**은 **실제로는 차이가 없음에도 불구하고** 작은 P-value를 얻어 "차이가 있다"고 잘못 결론 내리는 상황을 말합니다.
*   임계점 0.05를 사용한다는 것은, **실제로는 차이가 없는 경우에도 100번의 실험 중 5번은 거짓양성을 얻을 수 있다**는 뜻입니다.
*   **임계점 조절**:
    *   만약 "약의 차이가 있다"고 말하는 것이 **매우 중요하다면** (예: 생명과 직결되는 신약 개발), 0.00001과 같이 **더 작은 임계값**을 사용하여 거짓양성의 발생 확률을 10만 번 중 1번으로 줄일 수 있습니다.
    *   반대로 **그리 중요하지 않은 문제라면** (예: 아이스크림 트럭 도착 시간), 0.2와 같이 **더 큰 임계값**을 사용하여 10번 중 2번의 거짓양성을 허용할 수도 있습니다.
    *   대부분의 경우 **0.05**가 거짓양성을 줄이는 노력 대비 효율성 측면에서 가장 합리적인 임계값으로 사용됩니다.

### **11.4 가설검증(Hypothesis Testing)**
통계학에서는 약에 차이가 있는지 없는지를 결정하기 위한 이러한 과정을 **가설검증**이라고 부릅니다.
*   **귀무가설(Null Hypothesis)**: "약들 간에 차이가 없다"는 가설입니다.
*   P-value는 이 귀무가설을 받아들일지 말지 결정하는 데 도움을 줍니다. P-value가 충분히 작으면 귀무가설을 기각하고, 두 약 사이에 차이가 있다고 결론 내립니다.

### **11.5 P-value가 알려주지 않는 것**
*   **작은 P-value는 'A약과 B약의 차이가 있다'는 것을 알려주지만, 그 차이의 크기(효과의 크기)가 얼마나 되는지는 말해주지 않습니다**.
*   예를 들어, A약과 B약의 차이가 8%임에도 P-value가 0.24로 크게 나올 수 있고, 반대로 아주 많은 사람을 대상으로 한 실험에서는 A약과 B약의 차이가 1%밖에 나지 않는데도 P-value가 0.04로 작게 나올 수 있습니다.
*   따라서 P-value가 작다고 해서 반드시 두 그룹 간의 차이가 크다는 것을 의미하는 것은 아닙니다.

요약하자면, P-value는 관찰된 결과가 우연에 의해 발생했을 확률을 나타내며, 특정 임계점(주로 0.05)을 기준으로 통계적 유의미성을 판단하는 데 사용됩니다. 하지만 P-value만으로는 효과의 크기를 알 수 없다는 점을 유의해야 합니다.

## 12. P-value(P-값)의 개념, 계산 방법, 해석

**P-value**는 0과 1 사이의 값으로, 특정 결과가 **우연히 발생했을 확률**을 나타내는 통계적 측정치입니다. AI 분야에서는 모델 간 성능 비교, A/B 테스트 결과 분석 등 다양한 상황에서 통계적 유의미성을 판단하는 데 필수적으로 사용됩니다.

### **12.1 P-value의 목적: 귀무가설 검증**
P-value를 계산하는 주된 이유는 **귀무가설(Null Hypothesis)**을 검증하기 위함입니다. 귀무가설은 기본적으로 "관찰된 현상이 특별하지 않다" 또는 "두 그룹 간에 차이가 없다"는 가설입니다. 예를 들어, 동전 던지기 실험에서 "연속으로 2번 앞면이 나왔더라도, 내 동전은 일반 동전과 차이가 없다"는 것이 귀무가설이 됩니다.
**작은 P-value**는 이 귀무가설을 **기각**하라고 말해줍니다. 즉, 관찰된 현상이 우연히 발생했다고 보기 어렵고, **무언가 특별한 일**이 일어났다고 판단할 수 있다는 의미입니다.

### **12.2 P-value의 구성 요소 (계산 원리)**
P-value는 다음 세 가지 확률의 합으로 결정됩니다:
1.  **무작위로 관찰했을 때 나올 확률**: 우리가 실제로 관찰한 결과가 나올 확률입니다.
2.  **동일하게 희귀한 다른 관찰들의 확률**: 관찰된 결과만큼 희귀하지만, 다른 방식으로 나타날 수 있는 결과들의 확률입니다. (예: 2번의 앞면이 나올 확률만큼 2번의 뒷면이 나올 확률)
3.  **더 희귀하거나 더 극단적인 다른 관찰들의 확률**: 관찰된 결과보다 더 희귀하거나 극단적인 결과들의 확률입니다.
이 두 번째, 세 번째 부분을 P-value에 더하는 이유는, **동일하게 희귀하거나 더 희귀한 많은 것들이 존재할수록** 우리가 관찰한 것이 덜 특별해 보이기 때문입니다.

### **12.3 P-value 계산 예시**

*   **동전 두 번 던지기 (2회 앞면)**
    *   가능한 모든 결과는 (앞면, 앞면), (앞면, 뒷면), (뒷면, 앞면), (뒷면, 뒷면)의 4가지입니다. 각 결과의 확률은 0.25입니다.
    *   **귀무가설**: "2회 연속 앞면이 나왔더라도, 동전은 일반 동전과 다르지 않다".
    *   **P-value 계산**:
        1.  2회 앞면이 나올 확률: 0.25.
        2.  동일하게 희귀한 다른 관찰 (2회 뒷면)의 확률: 0.25.
        3.  더 희귀하거나 극단적인 관찰의 확률: 0 (없음).
        *   **결과**: P-value = 0.25 + 0.25 + 0 = **0.5**.
    *   **해석**: P-value 0.5는 일반적으로 사용되는 임계값 0.05보다 크므로, 귀무가설을 기각하는 데 실패합니다. 즉, 2회 연속 앞면이 나왔다는 데이터만으로는 동전이 특별하다고 확신할 수 없습니다.

*   **동전 다섯 번 던지기 (4회 앞면 / 1회 뒷면)**
    *   총 32가지 가능한 결과가 있습니다.
    *   **P-value 계산**:
        1.  4회 앞면 / 1회 뒷면이 나올 확률: 5/32.
        2.  동일하게 희귀한 다른 관찰 (1회 앞면 / 4회 뒷면)의 확률: 5/32.
        3.  더 희귀하거나 극단적인 관찰 (5회 앞면 + 5회 뒷면)의 확률: 1/32 + 1/32 = 2/32.
        *   **결과**: P-value = 5/32 + 5/32 + 2/32 = 12/32 = **0.375**.
    *   **해석**: P-value 0.375 또한 0.05보다 크므로, 귀무가설을 기각하는 데 실패합니다. 이 데이터 역시 동전이 특별하다고 확신하기 어렵습니다.

*   **연속적인 데이터 (키 분포)**
    *   키와 같이 연속적인 데이터를 다룰 때는 **통계 분포(statistical distributions)**를 사용합니다. 분포 곡선 아래의 영역은 특정 범위 내의 확률을 의미합니다.
    *   **예시**: 브라질 여성의 키 분포(평균 155.7cm)에서 키가 142cm인 사람의 P-value 계산.
        *   **귀무가설**: "키 142cm인 사람이 이 파란색 분포(평균 155.7cm)에서 나왔다".
        *   **P-value 계산**: 평균(155.7cm)으로부터 142cm만큼 극단적인 양쪽 꼬리 영역의 확률을 합합니다.
            *   142cm 이하의 영역 (2.5%) + 169cm 이상(142cm와 동일하게 극단적)의 영역 (2.5%).
            *   **결과**: P-value = 0.025 + 0.025 = **0.05**.
        *   **해석**: P-value가 정확히 0.05이므로, "이 분포에서 나왔을 수도, 아닐 수도 있다"는 애매한 결론을 내립니다.
    *   **예시 2**: 키가 141cm인 사람의 P-value
        *   141cm 이하의 영역 (0.016) + 141cm와 동일하게 극단적인 반대편 영역 (0.016).
        *   **결과**: P-value = 0.016 + 0.016 = **0.03**.
        *   **해석**: P-value 0.03은 0.05보다 작으므로 귀무가설을 기각합니다. 즉, 141cm처럼 작은 사람을 측정한 것은 꽤나 특별하며, 다른 키 분포가 더 적합하다는 것을 의미합니다.
    *   **예시 3**: 키가 평균에 가까운 (155.4cm~156cm) 사람의 P-value
        *   P-value는 1에 가까운 값이 나옵니다. 이는 해당 측정값이 이 분포에서 나오는 것이 전혀 이상하지 않다는 것을 의미합니다.

### **12.4 P-value의 임계점과 해석**

*   **일반적인 임계점**: 일반적으로 **0.05**를 P-value의 임계점으로 사용합니다.
    *   **P-value < 0.05**: 귀무가설을 **기각**합니다. 즉, 관찰된 결과는 우연히 발생했을 가능성이 낮고, **통계적으로 유의미한 차이**가 있다고 판단합니다.
    *   **P-value > 0.05**: 귀무가설을 **기각하지 못합니다**. 즉, 관찰된 결과가 우연히 발생했을 가능성을 배제할 수 없으며, 통계적으로 유의미한 차이가 있다고 보기 어렵습니다.

### **12.5 단측 P-value와 양측 P-value (One-sided vs. Two-sided P-value)**

*   **양측 P-value (Two-sided P-value)**:
    *   일반적으로 더 많이 사용되며, 특별한 이유가 없다면 양측 P-value를 계산하는 것이 권장됩니다.
    *   관찰된 결과가 평균으로부터 **양방향으로 얼마나 극단적인지**를 고려하여 P-value를 계산합니다. 즉, 변화의 방향(더 좋아졌는지, 더 나빠졌는지)에 관계없이 **"무언가 특별한 일이 일어났는지"**를 감지할 수 있습니다.
    *   예시: 신약이 회복 시간을 단축시키거나(더 짧은 회복 시간), 혹은 오히려 길게 하거나(더 긴 회복 시간) 관계없이, 두 방향 모두에서 특별한 변화를 감지합니다.

*   **단측 P-value (One-sided P-value)**:
    *   **잠재적으로 위험하며, 거의 쓰이지 않고 전문가에 의해서만 사용되어야 합니다**.
    *   미리 정해둔 **특정 방향으로의 변화만** 고려하여 P-value를 계산합니다. 예를 들어, 신약이 회복 시간을 **단축시켰을 때**만 유의미하다고 판단하고 싶을 때 사용됩니다.
    *   **문제점**: 만약 약이 회복 시간을 단축시키지 않고 **오히려 길게 했다면** (즉, 예상과 반대 방향으로 변화가 일어났다면), 단측 P-value는 이 반대 방향의 변화를 감지하지 못할 수 있습니다. 이는 실제로 약이 해로웠음에도 불구하고 "효과가 없다"고 잘못 결론 내릴 위험이 있습니다.

요약: P-value는 관찰된 결과가 우연히 발생했을 확률을 나타내며, 귀무가설을 기각하여 통계적 유의미성을 판단하는 데 사용됩니다. 주로 0.05를 임계점으로 사용하며, P-value가 0.05보다 작으면 유의미한 차이가 있다고 봅니다. 
그러나 P-value는 **효과의 크기**를 직접적으로 알려주지 않으며 P-value의 정의와 동전 던지기 예시를 통해 P-value는 특정 관찰이 얼마나 희귀한지를 나타낼 뿐, 그 차이의 실제적인 크기나 중요성을 직접적으로 알려주지 않음을 유추할 수 있다. 
*   **양측 P-value**가 일반적으로 안전하고 더 널리 사용됩니다.

## 13. p-해킹: 통계적 오용과 거짓 양성(False-positive)의 위험성

"p-해킹"은 **분석 기술의 오용 및 남용**을 의미하며, 결과적으로 **거짓 양성(false-positive)**에 속는 결과를 초래할 수 있는 통계적 관행입니다. 이는 연구의 신뢰도를 심각하게 떨어뜨릴 수 있으므로 반드시 피해야 합니다.

### **13.1 p-해킹이란 무엇인가? (예시: 약물 검증)**

*   **문제 상황**: 여러 후보 약물 중 바이러스 회복 시간을 줄이는 효과적인 약물을 찾고자 할 때.
*   연구자가 많은 약물(예: 약 A, B, C... Z)을 차례로 테스트하면서, **통계적으로 유의미한 결과(p-value가 0.05보다 작은 경우)**가 나올 때까지 계속해서 테스트를 진행하고, 유의미하게 보이는 약물(예: 약 Z)만을 보고하는 행위가 p-해킹의 한 형태입니다.
*   예를 들어, 약 Z가 회복 시간을 단축한 것처럼 보이고, 두 그룹의 평균을 비교한 통계 검증에서 p-value가 0.02로 나와 귀무가설(약 복용 유무에 따른 차이가 없다)을 기각하더라도, 이것은 p-해킹의 결과일 수 있습니다.

### **13.2 p-해킹이 문제가 되는 이유 (거짓 양성의 위험)**

*   **다중 검정 문제 (Multiple Testing Problem)**: 동일한 분포에서 나온 데이터일지라도, 여러 번의 통계 검정을 수행하면 **우연히 작은 p-value를 얻을 확률이 증가**합니다.
    *   p-value 0.05를 임계값으로 설정한다는 것은, 동일한 분포에서 나온 데이터에 대해 약 **5%의 통계 검증이 거짓 양성으로 나타날 수 있음**을 의미합니다.
    *   즉, 100번 검정하면 5번, 10,000번 검정하면 500번의 거짓 양성이 예상됩니다. 검정이 많아질수록 거짓 양성을 다룰 가능성이 커집니다.
    *   영상에서는 같은 분포에서 무작위로 추출한 그룹들을 계속 비교했을 때, 결국 p-value가 0.02로 나오는 거짓 양성 사례를 보여줍니다.

*   **데이터 추가의 유혹**: p-value가 0.05에 가깝지만 그보다 큰 값(예: 0.06)이 나왔을 때, **유의미한 결과를 얻기 위해 데이터를 조금 더 추가**하는 유혹에 빠질 수 있습니다.
    *   예시에서, 원래 p-value가 0.06인 두 그룹에 각각 하나의 측정값을 추가했더니 p-value가 0.02로 작아지는 결과를 얻었지만, 이것 역시 p-해킹이며 거짓 양성일 가능성이 매우 높습니다.
    *   이는 결론을 내리기 위해 **단 하나의 p-value만 계산한다는 이론적인 가정**을 위반하기 때문입니다. 실제로는 여러 p-value를 계산하여 결정을 내린 것이 됩니다.

### **13.3 p-해킹을 피하는 방법**

p-해킹과 거짓 양성의 위험을 피하기 위한 두 가지 주요 방법이 제시됩니다:

*   **가. 다중 검정 보정 (Multiple Testing Correction)**:
    *   만약 여러 가지 가설을 검정해야 한다면, **각 테스트마다 p-value를 계산하고, 모든 p-value를 False Discovery Rate(FDR)와 같은 방법으로 조정**해야 합니다.
    *   FDR은 원래의 p-value보다 일반적으로 더 큰 조정된 p-value를 생성하며, 이전에 거짓 양성이었던 일부 검정들이 0.05보다 큰 p-value를 갖도록 조정됩니다.
    *   **중요한 것은 "좋아 보이는 데이터"나 "좋아 보이는 테스트"만 보고하는 것이 아니라, 모든 검정에 대한 모든 p-value를 포함해야 한다는 것입니다**. 이것은 거짓 양성을 보고할 확률을 줄여줍니다.

*   **나. 검정력 분석(Power Analysis)을 통한 적절한 표본 크기 결정**:
    *   실험을 시작하기 전에 **적절한 표본의 크기를 결정**해야 합니다.
    *   **검정력 분석**은 실험 전에 수행되며, 귀무가설을 높은 확률로 올바르게 기각하기 위해 **얼마나 많은 모사(replicates)가 필요한지** 알려줍니다.
    *   따라서, 검정 중 p-value가 0.05에 가깝게 나왔을 때 추가적으로 관찰값을 넣는 대신, **검정력 분석을 통해 알맞은 표본 크기를 결정**하여 거짓 양성에 속는 것을 예방해야 합니다.

요약하자면, 연구나 모델 평가 시 통계적 유의미성을 맹목적으로 추구하기보다는, p-해킹의 위험성을 인지하고, 여러 가설을 동시에 검정할 때는 적절한 보정 방법을 사용하며, 실험 설계 단계에서 검정력 분석을 통해 충분한 표본 크기를 확보하는 것이 중요합니다.

## 14. FDR: 통계적 검정에서 거짓 양성(False Positives) 제어하기

**FDR(False Discovery Rate)**은 **겉보기에 좋아 보이지만 실제로는 잘못된 데이터(bad data that looks good)를 걸러내는 도구**입니다. 특히 대규모 유전자 시퀀싱과 같이 **동시에 수많은 통계 검정을 수행할 때 발생하는 거짓 양성(false positives)의 수를 제어**하는 데 사용됩니다.

### **14.1 거짓 양성(False Positives) 문제의 발생**

*   **p-값과 유의성**: 통계 검정에서 p-값은 두 표본이 동일한 분포에서 나왔을 확률을 나타냅니다. 일반적으로 p-값이 0.05보다 작으면 통계적으로 유의미하다고 판단하여 귀무가설(차이가 없다)을 기각합니다. 이는 두 표본이 서로 다른 분포에서 나왔다고 가정하게 만듭니다.
*   **단일 검정에서의 거짓 양성**: 만약 두 표본이 실제로 동일한 분포에서 나왔는데도 불구하고 p-값이 0.05보다 작게 나오는 경우가 발생할 수 있습니다. 이를 **거짓 양성(false positive)**이라고 합니다. 이러한 거짓 양성은 드물게 발생하지만, **5%의 시간 동안 발생**할 수 있습니다 (즉, 95%의 경우 p-값이 0.05보다 크게 나옴).
*   **다중 검정 문제**: 인간과 쥐의 세포에는 최소 10,000개의 전사된 유전자가 있습니다. 만약 동일한 유형의 쥐에서 추출한 두 표본을 비교하여 10,000개의 모든 유전자에 대해 통계 검정을 수행한다면, **10,000개의 5%는 500개**에 해당합니다. 이는 **500개의 거짓 양성**이 발생할 수 있음을 의미하며, 실제로는 흥미롭지 않은 500개의 유전자가 흥미롭게 보일 수 있다는 것입니다. **FDR은 이 많은 수의 거짓 양성을 제어하는 데 사용됩니다**.

### **14.2 p-값의 분포 이해**

*   **동일한 분포에서 추출된 표본**: 표본들이 동일한 분포에서 나왔을 때(예: 약물 처리하지 않은 쥐), 10,000번의 검정을 통해 얻은 p-값들은 **균등하게 분포**됩니다. 즉, 0부터 1까지의 p-값 구간에서 각 구간에 약 5%의 p-값들이 고르게 분포합니다 (예: 0~0.05, 0.05~0.1 등 각 구간에 약 500개의 p-값이 존재).
*   **다른 분포에서 추출된 표본**: 표본들이 서로 다른 분포에서 나왔을 때(예: 대조군과 약물 처리군 쥐), 10,000번의 검정을 통해 얻은 p-값들은 **0에 가깝게 치우쳐 분포**됩니다. 즉, 대부분의 p-값들이 0.05보다 작게 나옵니다.
*   **실제 실험 상황**: 실제 실험에서는 약물의 영향을 받는 유전자(다른 분포)와 영향을 받지 않는 유전자(동일한 분포)가 혼합되어 있습니다. 따라서 얻게 되는 p-값의 히스토그램은 이 두 가지 분포의 합으로 나타나게 됩니다. 히스토그램의 왼쪽 부분(0에 가까운 p-값)은 약물의 영향을 받은 유전자와 영향을 받지 않은 유전자 모두에서 발생한 p-값의 혼합이고, 오른쪽 부분(균등하게 분포된 p-값)은 주로 약물의 영향을 받지 않은 유전자에서 나온 것입니다.

### **14.3 Benjamini-Hochberg(벤자미니-혹버그) 방법으로 FDR 적용하기**

*   **FDR의 역할**: FDR, 특히 **Benjamini-Hochberg 방법**은 **유의미하다고 보고되는 거짓 양성의 수를 제한하는 방식으로 p-값을 조정**합니다. 이 방법은 p-값을 더 크게 만들어서, 원래 유의미했던 p-값(예: 0.04)이 조정 후에는 더 이상 유의미하지 않게(예: 0.06) 만들 수 있습니다.
*   **Benjamini-Hochberg 방법의 핵심**: 만약 유의성 판단 기준이 **FDR 조정 p-값이 0.05보다 작은 경우**라면, **조정된 p-값이 0.05보다 작은 유의미한 결과들 중 5% 미만이 거짓 양성**이 될 것입니다. 즉, 보고된 유의미한 결과의 95%는 참 양성(true positives)이라는 의미입니다.
*   **계산 과정(간략화)**:
    1.  모든 p-값들을 **작은 값부터 큰 값 순서로 정렬**합니다.
    2.  각 p-값에 **순위(rank)**를 매깁니다.
    3.  가장 큰 p-값부터 시작하여 순서대로 **조정된 p-값(FDR adjusted p-value)**을 계산합니다. 조정된 p-값은 다음 두 값 중 작은 값을 선택합니다:
        *   직전의 조정된 p-값
        *   현재 p-값 × (총 p-값의 개수 / 현재 p-값의 순위)
*   **결과**: 이 조정을 통해 원래 0.05보다 작았던 거짓 양성 p-값들은 0.05보다 커져서 더 이상 유의미하지 않게 되는 반면, **참 양성 p-값들은 0.05보다 작게 유지**되어 여전히 유의미하게 남는 경우가 많습니다.

요약하자면, AI를 배우는 학생들은 대량의 데이터를 다루는 연구나 모델 평가 시, 단순히 낮은 p-값에만 의존하는 것이 아니라, 다중 검정으로 인한 거짓 양성의 위험성을 인지하고 Benjamini-Hochberg와 같은 FDR 보정 방법을 활용하여 통계적 결과의 신뢰성을 높이는 것이 중요합니다.

## 15. 통계적 검정력(Statistical Power) 이해하기

### **15.1 검정력이란 무엇인가?**
*   **검정력은 귀무가설(Null Hypothesis)을 올바르게 기각할 확률입니다**.
*   다른 말로, 검정력은 '올바르게 작은 값의 p-value(< 0.05)를 얻을 확률'이라고도 할 수 있습니다.

### **15.2 높은 검정력의 예시 (쥐 실험 A)**
*   **시나리오**: 특별 식단을 한 쥐와 일반 식단을 한 쥐의 무게 분포가 **거의 겹치지 않고 명확히 구분될 때**.
*   **결과**: 이때는 두 식단 간의 차이를 쉽게 볼 수 있으며, 매우 작은 p-value (예: 0.0004)를 얻게 됩니다.
*   **해석**: 이 작은 p-value (< 0.05)는 '두 세트가 동일 분포로부터 나왔다'는 귀무가설을 **올바르게 기각**할 수 있도록 합니다.
*   **결론**: 이처럼 귀무가설을 올바르게 기각할 확률이 높을 때, 우리는 **아주 좋은 검정력**을 가졌다고 말합니다.
*   **주목**: 만약 특별 식단과 일반 식단 사이에 실제 차이가 없어서 두 그룹이 동일한 분포를 공유한다면, '올바르게 귀무가설을 기각한다'는 개념 자체가 적용되지 않으므로 검정력은 이러한 상황에 해당하지 않습니다.

### **15.3 낮은 검정력의 예시 (쥐 실험 B)**
*   **시나리오**: 특별 식단이 쥐의 무게를 약간만 줄여 **두 분포가 많이 겹치는 경우**.
*   **결과**: 이 경우, 작은 수의 측정값(예: 각 그룹 3마리 쥐)으로 비교하면 큰 p-value (예: 0.34)를 얻게 될 가능성이 높습니다. 이것은 '두 그룹이 같은 분포로부터 나왔다'는 귀무가설을 **기각하는 데 실패**한다는 의미입니다.
*   **문제점**: 실제로는 두 분포가 다르다는 것을 알고 있음에도 불구하고, 대부분의 경우 귀무가설을 기각하지 못합니다. 가끔 데이터가 겹치지 않아 작은 p-value를 얻고 올바르게 귀무가설을 기각하는 경우도 있지만, 이는 드뭅니다.
*   **결론**: 두 분포 간의 겹치는 부분이 많고 표본(샘플)의 크기가 작다면, 상대적으로 **낮은 검정력**을 갖게 됩니다.

### **15.4 검정력을 높이는 방법**
*   **측정값의 수 증가**: **측정값의 수를 늘림으로써 검정력을 항상 증가시킬 수 있습니다**. 즉, 더 많은 데이터를 수집하는 것이 중요합니다.
*   **검정력 분석**: **검정력 분석(Power Analysis)**은 좋은 검정력을 갖기 위해 얼마나 많은 측정을 해야 하는지 알려줍니다.

**핵심 요약:**
*   **검정력은 귀무가설을 올바르게 기각할 확률**입니다.
*   두 분포의 겹치는 부분이 적으면 **높은 검정력**을 가지며, 귀무가설을 올바르게 기각할 확률이 높습니다.
*   두 분포가 많이 겹치고 표본 크기가 작으면 **낮은 검정력**을 가집니다.
*   **표본 크기를 늘리면 검정력을 향상시킬 수 있습니다**.
*   **검정력 분석**은 필요한 표본 크기를 결정하는 데 도움을 줍니다.

## 16. 검정력 분석(Power Analysis) 이해하기

### **16.1 왜 검정력 분석을 하는가? (p-해킹의 문제점 극복)**
*   **시나리오**: 약 A와 약 B의 회복 시간을 비교하는 실험에서, 각 약을 복용한 3명의 데이터로 통계 검정을 했을 때 p-value가 0.06이 나왔다고 가정해 봅시다. 일반적으로 사용하는 유의수준(threshold) 0.05보다 p-value가 크기 때문에, 통계적으로 유의미한 차이가 있다고 말할 수 없습니다.
*   **p-해킹의 유혹**: 이때, '측정값이 다른 분포에서 나왔을 것 같다'는 의심이 들고 p-value가 0.05에 가깝기 때문에, 각 약을 복용한 사람을 한 명씩 추가하여 다시 통계 검정을 하고 싶은 유혹을 느낄 수 있습니다. **그러나 이것은 p-해킹이며, 올바르지 않은 방법입니다**.
*   **검정력 분석의 역할**: p-해킹을 피하고 **올바른 결정을 내리기 위해** 다음 실험에 필요한 **표본(Sample)의 크기를 정하는 것이 바로 검정력 분석입니다**. 검정력 분석은 '두 그룹 간의 차이가 없다'는 **귀무가설(Null Hypothesis)을 올바르게 기각할 높은 확률을 보장하기 위해 표본의 크기를 결정합니다**. 즉, 충분한 데이터를 사용했음을 알 수 있게 해줍니다.

### **16.2 검정력에 영향을 미치는 주요 요인**
검정력은 여러 요인에 영향을 받지만, **주된 요인은 두 가지**입니다:
1.  **두 분포 사이의 겹침 정도**: 우리가 식별하기 원하는 두 분포 사이가 얼마나 많이 겹쳐져 있는가?
2.  **표본 크기(Sample Size)**: 각 그룹에서 몇 개의 측정값을 얻을 것인가?

### **16.3 표본 크기가 검정력에 미치는 영향**
*   **평균 추정의 변동성**: 통계 검정은 개별 측정값보다는 **데이터의 요약값(예: 평균값)**을 사용합니다.
    *   **표본 크기가 1일 때**: 하나의 측정값만으로 모집단 평균을 추정하면, 추정된 평균값은 모집단 평균으로부터 매우 멀리 떨어져 있을 수 있으며, **변동성이 매우 큽니다**. 이는 추정된 평균값에 대한 확신을 주기 어렵게 하고, 결과적으로 상대적으로 큰 p-value를 얻어 귀무가설을 올바르게 기각하지 못하게 할 가능성이 높습니다 (즉, 낮은 검정력).
    *   **표본 크기가 2개 이상일 때**: 2개 이상의 측정값으로 평균을 추정하면, 극단적인 측정값이 추정된 평균값에 미치는 영향이 줄어듭니다. 다른 측정값들이 극단적인 값을 보상하여, **추정된 평균값들이 모집단의 평균에 더 가깝게 됩니다**.
    *   **표본 크기가 클수록**: 각 추정값에 더 많은 측정값을 사용할수록, 추정된 평균값들은 **모집단 평균에 더욱 가까워지고 변동성이 줄어듭니다**.
*   **분포의 겹침 감소 및 검정력 증가**: 표본 크기가 커져서 추정된 평균값들이 모집단의 평균에 더 가까워지면, **서로 다른 분포의 평균이 덜 겹치게 됩니다**. 이는 '두 표본이 같은 분포로부터 나왔다'는 귀무가설을 올바르게 기각할 확률을 높여주며, **높은 검정력**을 갖게 됩니다.
*   **중심 극한 정리**: 이 결과는 정규분포뿐만 아니라, 평균값을 가지는 모든 분포에 대해 중심 극한 정리에 의해 적용 가능하다고 언급됩니다.

### **16.4 검정력 분석을 하는 방법**
검정력 분석은 좋은 검정력을 얻기 위해 필요한 표본 크기를 알려줍니다.
1.  **원하는 검정력 결정 (Desired Power)**:
    *   검정력은 0에서 1 사이의 값을 선택할 수 있지만, **일반적으로 0.8(80%)**을 사용합니다. 이는 80%의 확률로 귀무가설을 올바르게 기각하는 것을 의미합니다.
2.  **유의수준 임계값 설정 (Significance Level, Alpha)**:
    *   마찬가지로 0에서 1 사이의 값을 선택할 수 있지만, **매우 일반적인 값은 0.05**입니다.
3.  **두 분포의 겹침 정도 추정 (Effect Size)**:
    *   겹침 정도는 두 모집단 평균 사이의 거리와 표준편차에 의해 영향을 받습니다.
    *   이를 하나의 지표로 합치는 일반적인 방법은 **효과 크기(Effect Size) 'd'를 계산하는 것**입니다.
    *   **효과 크기 공식**: (추정된 평균 값의 차이) / (합동 추정 표준편차).
    *   **합동 추정 표준편차 계산**: 각 표준편차의 제곱의 합을 2로 나누고, 그 값에 루트를 씌웁니다 (예: `sqrt((s1^2 + s2^2)/2)`).
    *   평균의 차이와 표준편차는 초기 데이터나 합리적인 추측을 통해 추정될 수 있습니다. 예를 들어, 평균 차이 10, 각 표준편차 7과 6일 때, 합동 표준편차는 6.5가 되고 효과 크기는 1.5가 됩니다.
4.  **검정통계량 계산기 사용**:
    *   위에서 결정한 검정력(0.8), 유의수준(0.05), 그리고 계산된 효과 크기(1.5)를 **온라인 검정통계량 계산기**에 입력합니다.
    *   예시에서는 각 그룹당 **9개의 측정값**을 얻으면 80%의 확률로 귀무가설을 올바르게 기각할 수 있다는 결과가 나옵니다.

**핵심 요약:**
*   **검정력 분석**은 p-해킹을 피하고, 다음 실험에 필요한 **최적의 표본 크기를 결정**하여 귀무가설을 올바르게 기각할 확률을 높이는 통계적 방법입니다.
*   **표본 크기가 커질수록** 추정된 평균값은 모집단 평균에 가까워지고, 극단값의 영향이 줄어들어, **추정치의 변동성이 감소합니다**.
*   이는 서로 다른 분포의 평균이 겹치는 정도를 줄여주어, **높은 검정력**을 갖게 만듭니다.
*   검정력 분석을 위해서는 **원하는 검정력, 유의수준, 그리고 효과 크기(분포의 겹침 정도)**를 결정해야 합니다.

## 17. 공분산(Covariance)

### **17.1 공분산의 주요 아이디어**

공분산은 두 측정값 사이의 **관계의 유형을 분류**하는 데 사용됩니다.
예를 들어, 5개의 서로 다른 세포에서 'X 유전자에 대한 mRNA 개수'와 'Y 유전자에 대한 mRNA 개수'를 측정하거나, 5개의 가게에서 '녹색 사과 개수'와 '빨간 사과 개수'를 세는 것처럼 두 가지 다른 측정값을 **짝 지어** 분석할 때 유용합니다. 이러한 짝 지어진 측정값들은 x축과 y축 상에 하나의 점으로 표시될 수 있습니다.

공분산은 다음과 같은 세 가지 형태의 관계로 분류할 수 있습니다:

1.  **양의 트렌드를 가진 관계**: X 유전자의 값이 상대적으로 낮을 때 Y 유전자도 낮은 값을 가지고, X 유전자의 값이 상대적으로 높을 때 Y 유전자도 높은 값을 가지는 경우입니다. 이는 두 변수가 함께 증가하는 **양의 기울기**를 가진 관계를 의미합니다.
2.  **음의 트렌드를 가진 관계**: X 유전자의 값이 상대적으로 낮을 때 Y 유전자가 높은 값을 가지고, X 유전자의 값이 상대적으로 높을 때 Y 유전자가 낮은 값을 가지는 경우입니다. 이는 X 유전자의 개수가 증가하면 Y 유전자의 개수가 감소하는 **음의 기울기**를 가진 관계를 의미합니다.
3.  **트렌드가 없는 관계**: 모든 X 유전자에 대해 Y 유전자의 개수가 동일하거나, 그 반대의 경우처럼 X와 Y 사이에 명확한 양 또는 음의 트렌드가 없는 경우입니다. 이때는 **아무 관계도 아니다**라고 분류합니다.

### **17.2 공분산 값과 관계의 분류**

공분산을 계산하면 다음과 같은 결과로 관계를 분류할 수 있습니다:

*   **공분산이 양(+)의 값**이면, 관계는 **양의 기울기**를 가집니다. 이는 두 변수가 함께 증가하는 경향을 보임을 의미합니다. (예: 116)
*   **공분산이 음(-)의 값**이면, 관계는 **음의 기울기**를 가집니다. 이는 한 변수가 증가할 때 다른 변수가 감소하는 경향을 보임을 의미합니다. (예: -105.15)
*   **공분산이 0**이면, 두 변수 사이에 **관계가 없습니다**. 이는 양의 기여와 음의 기여가 상쇄되어 트렌드가 없기 때문입니다.

### **17.3 공분산의 계산 직관**

공분산은 `(x - x_bar) * (y - y_bar)` 항들을 합산하여 `(n - 1)`로 나누는 식으로 계산됩니다.

*   **양의 공분산**:
    *   X, Y 모두 각 평균보다 작을 때, `(음수) * (음수)` = **양수**가 됩니다.
    *   X, Y 모두 각 평균보다 클 때, `(양수) * (양수)` = **양수**가 됩니다.
    *   이 경우, 데이터가 그래프의 왼쪽 아래 사분면과 오른쪽 위 사분면에 주로 분포하며 공분산에 양의 값으로 기여합니다.
*   **음의 공분산**:
    *   하나의 값은 평균보다 작고 다른 값은 평균보다 클 때, `(음수) * (양수)` 또는 `(양수) * (음수)` = **음수**가 됩니다.
    *   이 경우, 데이터가 그래프의 왼쪽 위 사분면과 오른쪽 아래 사분면에 주로 분포하며 공분산에 음의 값으로 기여합니다.
*   **공분산이 0**:
    *   X나 Y의 값이 평균과 같으면 차이가 0이 되어, 곱셈 결과도 **0**이 됩니다.
    *   또는 양의 기여와 음의 기여가 서로 상쇄되어 전체 합이 0이 될 수 있습니다.

### **17.4 공분산의 한계점과 중요성**

공분산은 그 자체만으로는 몇 가지 이유로 **해석하기 어렵습니다**:

*   **규모(Scale)에 민감**: 공분산 값은 데이터의 규모에 따라 크게 변합니다. 예를 들어, 데이터에 2를 곱하면 공분산은 4배가 될 수 있습니다. 실제 관계가 변하지 않았음에도 불구하고 공분산 값이 바뀌기 때문에, 그 값 자체만으로는 관계의 강도나 선의 기울기 가파르기, 데이터 점들이 추세선에 얼마나 가까운지 등을 알 수 없습니다.
*   **분산과의 관계**: 어떤 변수(X)와 그 자신(X)의 공분산을 구하는 것은 X의 분산을 구하는 것과 같습니다.

그럼에도 불구하고 공분산은 **매우 중요한 통계량**입니다:

*   **상관관계 계산의 디딤돌**: 공분산은 상관관계(correlation)와 같이 **더 흥미로운 통계량을 계산하기 위한 첫 번째 단계이자 디딤돌**로 사용됩니다. 상관관계는 공분산과 달리 데이터의 규모에 민감하지 않아 관계를 더 잘 설명할 수 있습니다.
*   **다양한 분석에 활용**: 공분산은 주성분 분석(PCA, Principal Component Analysis) 등 **다양한 통계 및 AI 분석에서 핵심적인 역할**을 합니다.

요약하자면, 공분산은 두 변수 간의 관계가 양의 트렌드인지, 음의 트렌드인지, 혹은 트렌드가 없는지를 분류하는 데 사용되며, 그 자체의 값 해석은 어렵지만, 상관관계나 PCA와 같은 고급 통계 및 AI 기법의 중요한 기초가 됩니다.

## 18. 피어슨 상관관계(Pearson's Correlation): 관계의 강도 정량화

### **18.1 관계의 이해: X축과 Y축 데이터**

영상에서는 두 가지 종류의 데이터를 예로 들어 설명합니다:
*   **X 유전자의 mRNA 개수**와 **Y 유전자의 mRNA 개수** (서로 다른 5개의 세포에서 측정)
*   **녹색 사과의 개수**와 **빨간 사과의 개수** (서로 다른 5군데 가게에서 측정)
이러한 짝 지어진 측정값들은 x축과 y축 상에 하나의 점으로 나타낼 수 있습니다.

### **18.2 관계의 종류와 예측**

데이터는 다음과 같은 트렌드를 보일 수 있습니다:

*   **양의 관계 (Positive Relationship)**: X 유전자 값이 낮으면 Y 유전자 값도 낮고, X 유전자 값이 높으면 Y 유전자 값도 높은 경향을 보입니다. 이는 **양의 기울기**를 가진 직선으로 나타낼 수 있으며, 한 변수의 값을 통해 다른 변수의 값을 **예측**하는 데 사용될 수 있습니다.
*   **음의 관계 (Negative Relationship)**: 영상에서는 직접적으로 설명되진 않지만, 양의 관계와 반대로 한 변수가 증가할 때 다른 변수가 감소하는 경향을 나타냅니다.

데이터가 트렌드 선에 **가까울수록** X 유전자가 Y 유전자에 대해 **더 잘 설명해 준다**고 할 수 있으며, 이를 **상대적으로 강한 관계**라고 합니다. 반대로 데이터가 트렌드 선에서 **멀리 떨어져 있을수록** **상대적으로 약한 관계**라고 합니다.

**주의**: 상관관계는 X가 Y의 원인이라는 **인과 관계를 말하는 것이 아닙니다**. 단순히 관찰된 트렌드를 제시하며, 합리적인 추측을 돕는 역할을 합니다. 다른 어떤 요인이 관찰된 트렌드를 유발했을 가능성을 배제하지 않습니다.

### **18.3 상관관계 값과 관계의 강도**

상관관계는 관계의 강도를 **정량화**하며, 그 값의 범위는 **-1에서 1**입니다.

*   **상관관계 = 1**: **양의 기울기**를 가진 직선이 **모든 데이터 점의 중간을 통과할 때** (이상적으로는 모든 점을 지날 때) 나타나는 **최대값**입니다. 이는 가장 강한 양의 선형 관계를 의미하며, X를 알면 Y를 매우 좁은 범위 내에서 추측할 수 있습니다.
*   **상관관계 = -1**: **음의 기울기**를 가진 직선이 **모든 데이터 점의 중간을 통과할 때** 나타나는 **최소값**입니다. 이는 가장 강한 음의 선형 관계를 의미하며, X를 알면 Y를 매우 좁은 범위 내에서 추측할 수 있습니다.
*   **상관관계 = 0**: **직선으로 나타낼 수 없는 관계**가 있을 때 나타납니다. 이는 X축의 값이 Y축에 대해 아무것도 예측할 수 없음을 의미합니다. 관계가 약해질수록, 즉 데이터가 직선에서 멀어질수록 상관관계 값은 **0에 가까워집니다**.

**주목**: 상관관계는 데이터의 **규모에 좌우되지 않습니다**. 데이터에 어떤 값을 곱하거나 단위를 변경해도 상관관계 값은 변하지 않습니다. 이는 공분산과 구별되는 중요한 특징으로, 상관관계를 훨씬 더 **해석하기 쉽게** 만듭니다. 기울기가 크거나 작더라도 직선이 모든 데이터를 지나면 상관관계는 여전히 1 또는 -1입니다.

### **18.4 데이터 양과 P-value, 그리고 예측에 대한 확신**

상관관계 값 자체는 관계의 강도를 나타내지만, 그 **예측에 대한 확신**은 **데이터의 양(표본 크기)**과 **p-value**에 따라 달라집니다.

*   **데이터의 양**: 일반적으로 **더 많은 데이터**가 있을수록, 이 선으로 만든 예측에 대해 **더 확신**할 수 있습니다. 왜냐하면 임의의 점들이 우연히 강한 관계를 이룰 확률이 점점 더 작아지기 때문입니다.
*   **P-value**: **p-value**는 임의로 그려진 점들이 관찰된 것과 같이 강하거나 더 강한 관계를 가질 확률을 나타냅니다. 따라서 **p-value가 작을수록** (예: 2.2 * 10^-16), 해당 예측에 대한 **확신이 더 커집니다**. (예: p-value가 0.03이면, 3%의 확률로 임의의 데이터가 유사한 강도의 관계를 만들 수 있다는 의미입니다).

**주의**: 데이터 양을 증가시키는 것은 상관관계를 증가시키지 않으며, 단지 **추측에 대한 확신을 증가**시킵니다. 상관관계 값이 작으면 아무리 데이터가 많고 확신이 크더라도, 그 추측은 여전히 좋지 않을 수 있습니다.

### **18.5 피어슨 상관관계의 계산**

상관관계를 계산하려면 **분산(Variance)**과 **공분산(Covariance)**에 대한 이해가 필요합니다.
피어슨 상관관계는 다음 공식으로 계산됩니다:

**상관관계(Correlation) = X와 Y의 공분산(Cov(X,Y)) / (sqrt(X의 분산(Var(X))) * sqrt(Y의 분산(Var(Y))))**

*   **분자(공분산)**: 양의 무한대에서 음의 무한대 사이의 값을 가질 수 있으며, 관계선의 기울기, 데이터의 평균으로부터의 거리, 데이터의 규모에 따라 달라집니다.
*   **분모**: 공분산 값을 **-1부터 1사이의 값으로 압축**하여 상관관계가 데이터의 규모에 영향을 받지 않도록 만듭니다.

**예시 계산**: 특정 데이터에 대한 공분산이 116이고, X 유전자의 분산이 101.8, Y 유전자의 분산이 160.3일 경우, 상관관계는 약 **0.9**가 됩니다.

### **18.6 상관관계의 한계 및 R-제곱(R-Square)**

상관관계 값은 공분산보다 해석하기 쉽지만, 여전히 직관적으로 관계의 강도를 비교하기 어려울 수 있습니다 (예: 상관관계 0.9가 0.64보다 두 배 더 좋은 예측을 의미하는지는 명확하지 않습니다).

이러한 문제를 해결하기 위해 **R-제곱(R-Square)**이라는 개념이 사용됩니다. R-제곱은 상관관계와 관련이 있으며, 단순한 직선 관계를 넘어 **더 복잡한 관계** (비선형 관계)에 대해서도 정량화할 수 있습니다.

요약하자면, 상관관계는 두 변수 간의 선형 관계의 강도를 -1부터 1까지의 값으로 정량화합니다. 1은 가장 강한 양의 선형 관계, -1은 가장 강한 음의 선형 관계, 0은 선형 관계가 없음을 의미합니다. 상관관계는 데이터의 규모에 독립적이며, 예측에 대한 확신은 데이터의 양과 p-value에 따라 결정됩니다. 공분산을 기반으로 계산되며, R-제곱과 같은 더 고급 통계량의 기초가 됩니다.

## 19. 조건부 확률 (Conditional Probability)

### **19.1 조건부 확률이란 무엇인가?**
조건부 확률은 **어떤 사건(B)이 이미 발생했다는 전제 하에 다른 사건(A)이 발생할 확률**을 의미합니다. 즉, 우리가 관심 있는 사건의 발생 가능성을 평가할 때, 특정 정보나 조건이 주어졌을 때 그 확률이 어떻게 달라지는지를 다룹니다.

### **19.2 표기법**
조건부 확률은 일반적으로 **P(A|B)**로 표기됩니다. 이는 "B가 주어졌을 때 A의 확률" 또는 "B라는 조건 하에 A의 확률"이라고 읽습니다.

### **19.3 핵심 아이디어: 표본 공간의 변화**
조건부 확률의 가장 중요한 개념은 **주어진 조건(B)이 발생함으로써 전체 표본 공간(사건이 발생할 수 있는 모든 경우의 수)이 조건 B에 해당하는 범위로 축소된다는 점**입니다. 일반적인 확률 계산에서는 전체 사건 중에서 특정 사건이 발생하는 비율을 보지만, 조건부 확률에서는 조건 B에 해당하는 사건들 중에서만 사건 A가 발생하는 비율을 봅니다.

### **19.4 계산 방식 (수식의 기본 개념)**
비디오에서 명시적으로 수식이 언급되지는 않지만, 설명의 흐름을 통해 다음과 같은 의미를 유추할 수 있습니다:
**P(A|B) = P(A and B) / P(B)**
*   **분자 (P(A and B))**: 사건 A와 B가 **모두 발생할 확률** 또는 A와 B 조건을 동시에 만족하는 경우의 수.
*   **분모 (P(B))**: 조건이 되는 사건 B가 **발생할 확률** 또는 B 조건을 만족하는 모든 경우의 수.

가장 중요한 것은 분모가 **전체 사건의 수가 아니라 조건 B를 만족하는 사건의 수로 바뀐다**는 점입니다.

### **19.5 예시 (Head & Shoulders 비유)**
비디오에서는 'Head & Shoulders를 사랑하는 사람들을 찾는' 비유를 사용하여 조건부 확률을 설명합니다.
*   예를 들어, "Head & Shoulders를 사랑하는 사람들"을 찾는다고 했을 때, 이것은 전체 사람들 중에서 해당 조건을 만족하는 사람을 찾는 것입니다.
*   하지만 "이웃 중 일부를 더 사랑하는 사람이라는 **조건 하에** Head & Shoulders를 사랑하는 사람들"을 찾는다고 하면, 이제 우리는 전체 사람이 아니라 '이웃 중 일부를 더 사랑하는 사람들'이라는 특정 그룹 안에서 Head & Shoulders를 사랑하는 사람을 찾아야 합니다. 여기서 **분모가 '전체 사람 수'에서 '이웃 중 일부를 더 사랑하는 사람 수'로 바뀌게 됩니다**.

### **19.6 데이터 추적 도구**
비디오는 사건들을 체계적으로 추적하기 위해 '콘텐츠 업데이트 테이블(content update table)' 즉, **분할표(contingency table)**와 같은 도구의 중요성을 강조합니다. 이는 각 사건의 발생 여부와 조합을 시각적으로 파악하는 데 유용합니다.

결론적으로, 조건부 확률은 이미 알고 있는 정보를 바탕으로 미지의 사건에 대한 우리의 믿음이나 예측을 업데이트하는 강력한 도구이며, AI 시스템이 데이터를 학습하고 의사결정을 내리는 데 필수적인 개념입니다.

## 20. 베이즈 정리(Bayes' Theorem)

베이즈 정리는 이미 발생한 정보(증거)를 바탕으로 어떤 사건에 대한 우리의 믿음(확률)을 업데이트하는 데 사용되는 강력한 통계 도구입니다. AI 분야에서 예측 모델링, 스팸 필터링, 의료 진단, 패턴 인식 등 다양한 분야에서 핵심적인 역할을 합니다.

### **20.1 선행 지식: 조건부 확률(Conditional Probability) 복습**

베이즈 정리를 이해하기 위해서는 조건부 확률에 대한 명확한 이해가 필수적입니다.

*   **정의**: 어떤 사건 B가 이미 발생했다는 전제 하에 다른 사건 A가 발생할 확률을 의미합니다.
*   **표기**: **$P(A\|B)$**로 표기하며, "B가 주어졌을 때 A의 확률"이라고 읽습니다.
*   **핵심 아이디어**: 조건부 확률은 **전체 표본 공간이 주어진 조건(B)으로 축소된다**는 개념을 바탕으로 합니다. 즉, 조건 B를 만족하는 사건들 중에서만 사건 A가 발생하는 비율을 계산합니다.
*   **계산**: $P(A\|B) = P(A and B) / P(B)$
    *   여기서 **P(A and B)**는 A와 B가 동시에 발생할 확률을, **P(B)**는 B가 발생할 확률을 나타냅니다.
*   **명확성을 위한 표기**: 동영상에서는 조건부 확률을 더 명확하게 이해하기 위해 "사탕을 싫어하지만 소다를 좋아하는 사람들을, 소다를 좋아하는 조건 하에 찾는 것"과 같이 다소 중복되는 표기법을 사용하여 분자(두 사건이 동시에 발생하는 확률)와 분모(주어진 조건의 확률)의 관계를 강조합니다. 이는 베이즈 정리를 이해하는 데 도움이 됩니다.

### **20.2 베이즈 정리(Bayes' Theorem)란 무엇인가?**

조건부 확률은 특정 정보(B)가 주어졌을 때 우리가 알고 싶은 사건(A)의 확률을 계산합니다. 하지만 때로는 **A와 B가 동시에 발생할 확률(P(A and B))**을 직접 알지 못하는 경우가 있습니다. **베이즈 정리는 이처럼 특정 정보를 알지 못하더라도, 하나의 조건부 확률을 다른 조건부 확률을 사용하여 유도할 수 있게 해줍니다**.

### **20.3 베이즈 정리 유도 과정**

베이즈 정리는 간단한 대수학을 통해 유도됩니다.

*   우리는 두 가지 조건부 확률 식을 알고 있습니다:
    *   `(1) P(A|B) = P(A and B) / P(B)`
    *   `(2) P(B|A) = P(A and B) / P(A)`
*   두 식을 모두 P(A and B)에 대해 정리하면 다음과 같습니다:
    *   `P(A and B) = P(A|B) * P(B)`
    *   `P(A and B) = P(B|A) * P(A)`
*   두 식의 좌변이 같으므로, 우변도 같습니다:
    *   `P(A|B) * P(B) = P(B|A) * P(A)`
*   이 식을 우리가 원하는 조건부 확률에 대해 정리하면 **베이즈 정리**가 됩니다. 예를 들어, `P(A|B)`에 대해 정리하면 다음과 같습니다:
    *   `P(A|B) = [P(B|A) * P(A)] / P(B)`
    *   또는 `P(B|A)`에 대해 정리하면:
    *   `P(B|A) = [P(A|B) * P(B)] / P(A)`

### **20.4 베이즈 정리의 중요성 및 활용**

*   **데이터 부족 상황에서의 활용**: 실제 세상에서는 모든 데이터를 가지고 있지 않은 경우가 많습니다. 예를 들어, 인도의 모든 사람에게 소다를 좋아하는지 물어볼 수는 없습니다. 베이즈 정리는 이처럼 **데이터가 불완전하거나 추측(guess)이 필요한 상황에서 확률을 계산하고 추론하는 데 매우 유용**합니다.
*   **"추측"의 과학 (베이즈 통계학)**: 베이즈 정리는 **베이즈 통계학(Bayesian statistics)**의 근간을 이룹니다. 베이즈 통계학은 '추측'이나 '사전 지식(prior knowledge)'을 확률 계산에 통합하는 철학을 담고 있으며, 이는 AI 모델이 새로운 데이터를 통해 자신의 믿음을 지속적으로 업데이트하는 방식과 유사합니다.
*   **새로운 정보로 믿음 업데이트**: 베이즈 정리는 특정 사건(B)에 대한 새로운 정보가 주어졌을 때, 우리가 원래 알고 있던 다른 사건(A)에 대한 확률을 어떻게 업데이트해야 하는지를 알려줍니다. 이는 **기계 학습(Machine Learning)**에서 모델이 훈련 데이터를 통해 가중치를 조정하고 예측을 개선하는 과정과 본질적으로 동일합니다.

### **20.5 표준 표기법 이해하기**

대부분의 경우 조건부 확률은 `P(A|B)`와 같이 간결하게 표기됩니다. 동영상에서는 이러한 표준 표기법이 두 개의 다른 조건부 확률`(P(A|B)`와 `P(B|A))`이 실제로 **동일한 사건(예시의 '노란색 영역')**을 다른 관점(주어진 조건)에서 바라보고 있다는 것을 간과하게 만들 수 있다고 지적합니다. 따라서 어떤 정보가 조건으로 주어졌는지 명확히 인지하며, 그 조건이 전체 표본 공간을 어떻게 축소시키는지를 이해하는 것이 중요합니다.

결론적으로, 베이즈 정리는 AI 학습자가 확률론적 추론과 데이터 기반 의사결정의 원리를 이해하는 데 있어 핵심적인 개념입니다. 데이터가 불완전하거나 불확실한 상황에서 지식을 업데이트하고 현명한 결정을 내리는 데 필수적인 도구임을 기억하시기 바랍니다.

## 21. 기대값 (Expected Values)

특정 사건이 발생했을 때 장기적으로 우리가 무엇을 "기대"할 수 있는지 이해하는 데 중점을 둡니다. 기대값은 통계 및 머신러닝에서 의사결정 시 불확실성을 정량화하는 데 매우 중요한 개념입니다.

### **21.1 기대값의 기본 아이디어**
기대값은 어떤 이벤트가 여러 번 반복될 때, **평균적으로 얻거나 잃을 것으로 예상되는 값**을 나타냅니다. 이는 단 한 번의 이벤트 결과와 다를 수 있으며, 장기적인 관점을 제공합니다.

### **21.2 기대값 계산 예시: "트롤 2" 영화 내기**

*   **배경 설정**: Statland라는 가상의 지역에서 "트롤 2" 영화를 아는 사람과 모르는 사람에 대한 데이터가 있습니다. 총 213명 중 37명(17%)이 영화를 알고 있고, 176명(83%)이 모릅니다.
*   **확률 계산**:
    *   임의의 사람이 "트롤 2"를 들어봤을 확률: 37 / 213 = **0.17**
    *   임의의 사람이 "트롤 2"를 들어본 적 없을 확률: 176 / 213 = **0.83**
*   **내기 시나리오 1**: "트롤 2"를 들어봤을 경우 $1를 잃고(-$1), 들어본 적 없을 경우 $1를 얻는($1) 내기를 한다고 가정합니다.
    *   **100번 내기 시 예상 결과**:
        *   잃을 확률(0.17) * 100회 = 약 **17회** 잃을 것으로 예상되며, 총 $17를 잃게 됩니다.
        *   얻을 확률(0.83) * 100회 = 약 **83회** 얻을 것으로 예상되며, 총 $83를 얻게 됩니다.
        *   총 예상 수익/손실: -$17 (손실) + $83 (수익) = **$66** (수익).
    *   **내기 한 번당 평균 기대값**: $66 / 100회 = **$0.66** (66센트).
    *   결론: 이 내기를 여러 번 반복하면, 평균적으로 한 번 내기당 66센트를 얻을 것으로 예상됩니다. 즉, 장기적으로는 이득을 보는 내기입니다.

### **21.3 기대값의 통계적 표기법**

*   기대값은 **E(X) = 0.66** 또는 **E(bet) = 0.66**으로 표기할 수 있습니다.
*   **시그마(Σ) 표기법을 사용한 일반 공식**:
    **`E(X) = Σ [각 결과(x) × 각 결과를 관찰할 확률(P(x))]`**
    *   위 내기 시나리오 1에 적용하면:
        E(X) = (-$1) * 0.17 + ($1) * 0.83 = -0.17 + 0.83 = **0.66**

### **21.4 기대값 계산 예시 2: 다른 배당금**

*   **내기 시나리오 2**:
    *   "트롤 2"를 들어봤을 경우 **$10를 얻고** ($10),
    *   들어본 적 없을 경우 **$1를 잃는** (-$1) 내기를 한다고 가정합니다.
*   **기대값 계산**:
    E(X) = ($10) * 0.17 + (-$1) * 0.83
    E(X) = 1.70 - 0.83 = **0.87**
*   결론: 이 경우 한 번 내기당 평균적으로 87센트를 얻을 것으로 예상됩니다.

### **21.5 핵심 시사점**
기대값은 개별적인 사건의 결과가 예측 불가능하더라도, **충분히 많은 횟수만큼 반복될 경우 장기적으로 어떤 결과가 나타날지 예측**하는 데 사용됩니다.


**AI를 배우는 학생들에게 이 개념이 왜 중요한가요?**
AI 및 머신러닝 분야에서 기대값은 **의사결정 프로세스**의 핵심을 이룹니다.

*   **강화 학습(Reinforcement Learning)**: 에이전트가 특정 행동을 했을 때 미래에 얻을 것으로 기대되는 보상(expected reward)을 계산하여 최적의 정책을 학습합니다.
*   **확률적 모델링**: 데이터의 불확실성을 다루는 다양한 모델(예: 베이지안 네트워크, 마르코프 모델)에서 특정 이벤트의 기대 결과를 추정하는 데 사용됩니다.
*   **위험 분석 및 최적화**: 특정 결정이 가져올 수 있는 이득과 손실의 기대값을 평가하여 위험을 최소화하고 이득을 최대화하는 전략을 수립합니다.
*   **손실 함수(Loss Function)**: 모델의 예측과 실제 값 사이의 차이를 측정하는 데 사용되는 손실 함수의 기대값을 최소화하는 방식으로 모델을 학습시킵니다.

기대값을 이해하는 것은 불확실한 환경에서 합리적인 의사결정을 내리고, 복잡한 AI 시스템을 설계하고 분석하는 데 필수적인 기초 지식입니다.

## 22. 기대값 (Expected Values): 이산 변수와 연속 변수

이전 대화에서 우리는 **이산 변수(Discrete Variables)**의 기대값 개념을 다루었습니다. 이산 변수는 '트롤 2' 영화 내기처럼 결과가 명확히 구분되는 유한하거나 셀 수 있는 값들을 가집니다 (예: $1를 잃거나 $1를 얻는 경우). 이산 변수의 기대값은 각 결과값에 해당 확률을 곱한 값들을 모두 합하여 계산합니다: **`E(X) = Σ [x * P(x)]`**.

### **22.1 연속 변수의 이해**

*   **정의**: 연속 변수는 측정되는 값들에서 나오며, 결과값이 '연속적'입니다. 예를 들어, 두 사람을 만나는 사이에 '기다려야 하는 시간'과 같이 측정 가능한 값들이 연속 변수에 해당합니다.
*   **예시**: Statland를 걷다가 사람들을 만나는 '기다리는 시간'에 대한 기대값을 알고 싶다고 가정합니다. 처음에는 10초 후에, 다음에는 30초 후에, 그리고 즉시, 또 10초 후에 사람을 만나는 식의 데이터를 수집할 수 있습니다.

### **22.2 연속 변수 모델링: 지수 분포 (Exponential Distribution)**

*   **데이터의 한계**: 실제 데이터를 수집하는 것은 시간이 오래 걸리고 데이터에 '틈(gaps)'이 생길 수 있습니다. 또한, 어떤 간격(예: 10초, 5초)으로 데이터를 볼지에 따라 결과가 달라질 수 있습니다.
*   **지수 분포의 도입**: 이러한 문제를 해결하기 위해 **기다리는 시간(waiting times)**과 같은 연속 변수를 **지수 분포(Exponential Distribution)**로 모델링할 수 있습니다. 지수 분포는 데이터의 상단을 매끄럽게 지나가는 곡선 형태로 나타납니다.
*   **람다(λ) 매개변수**: 지수 분포의 모양은 **람다(λ)**라는 매개변수에 의해 정의됩니다. 람다는 '초당 만나는 사람 수'를 나타내는 비율(rate)입니다.
    *   λ가 0.05일 때 데이터에 가장 잘 맞는 곡선이 나옵니다.
    *   λ가 0.1이면 더 많은 사람을 만나므로 곡선이 0에 가깝게 더 가파릅니다.
    *   λ가 0.01이면 더 적은 사람을 만나므로 곡선이 덜 휘고 오른쪽에 더 높게 나타납니다.
*   **확률 계산**: 지수 분포에서 특정 시간 범위 내에서 사람을 만날 **확률**은 해당 구간 아래의 **곡선 면적(area under the curve)**을 계산하여 얻습니다. 이는 **적분(integration)**을 통해 수행됩니다. 예를 들어, 0초에서 10초 사이에 사람을 만날 확률은 0.39이며, 이는 해당 구간의 곡선 아래 면적입니다.
*   **Likelihood (가능도)**: y축 값은 **가능도(likelihood)**를 나타냅니다. 지수 분포의 y축 좌표는 최대 가능도 추정(maximum likelihood estimation)에 사용되는 가능도 값입니다.
*   **총 면적 = 1**: 곡선 아래의 전체 면적은 1입니다. 이론적으로 x축은 양의 무한대까지 이어집니다.

### **22.3 연속 변수의 기대값 계산**

*   **근사(Approximation)**: 연속 분포의 기대값을 근사하기 위해, 각 10초 간격을 이산적인 결과로 가정하고 각 간격의 확률을 해당 직사각형의 면적으로 근사하여 계산할 수 있습니다. 이 직사각형들의 면적은 너비(10초)에 중앙값에서의 높이(가능도)를 곱하여 구합니다.
    *   예를 들어, 첫 10초 간격(x=5초)의 확률은 약 0.4입니다.
    *   이산 변수 공식처럼 (결과 * 확률)을 합산하여 기대값 22초를 얻을 수 있습니다.
*   **정확한 계산 (Integral)**: 근사치의 정확도를 높이려면 간격의 너비를 계속 줄여야 합니다 (예: 10초에서 5초로, 그리고 0으로). 간격의 너비가 0으로 가고 직사각형의 수가 무한대로 갈 때, 우리는 더 이상 근사하는 것이 아니라 **정확한 면적을 적분하여 계산**하게 됩니다.
*   **일반 공식**:
    *   이산 변수: **`E(X) = Σ [x * P(x)]`** (합계 사용)
    *   연속 변수: **`E(X) = ∫ [x * f(x) dx]`** (적분 사용)
        *   여기서 **f(x)**는 **확률 대신 가능도(likelihood)**, 즉 확률 밀도 함수(Probability Density Function, PDF)의 y축 좌표를 나타냅니다.

### **22.4 지수 분포의 기대값 유도 (수학적 증명)**

*   **지수 분포의 공식**: 지수 분포의 확률 밀도 함수는 $f(x) = λe^{(-λx)}$ 입니다.
*   **적용**: 기대값 공식에 지수 분포의 가능도 함수를 대입하고, x가 0부터 무한대까지 정의되므로 0부터 무한대까지 적분합니다.
    **E(X) = ∫ (from 0 to ∞) $\[x * λe^{(-λx)} dx\]$**
*   **부분 적분 (Integration by Parts)**: 이 적분을 풀기 위해 **부분 적분** 기법을 사용합니다.
    * 부분적분의 공식은 다음과 같습니다. $∫$ u dv = uv - $∫$ v du
    *   $f(x) = x, g'(x) = λe^{(-λx)}$ 로 설정하고 각각 미분 및 역미분(antiderivative)을 구합니다.
    *   부분 적분 공식을 적용하고 x=무한대와 x=0에서 평가합니다.
    *   극한값 계산 (로피탈의 정리 사용)을 통해 최종적으로 **$E(X) = 1/λ$**라는 결과를 얻습니다.
*   **예시 적용**: 만약 λ=0.05라면, E(X) = 1 / 0.05 = **20초**가 됩니다. 즉, 평균적으로 사람들을 만나는 데 20초를 기다릴 것으로 예상됩니다.

### **22.5 기대값의 핵심 요약**

*   이산 변수와 연속 변수의 기대값은 매우 유사합니다.
*   **두 가지 주요 차이점**:
    1.  **합계(sum)를 적분(integral)으로 대체**합니다.
    2.  **확률(probability)을 가능도(likelihood)로 대체**합니다.


**AI를 배우는 학생들에게 이 개념이 왜 중요한가요?**

연속 변수의 기대값은 AI와 머신러닝의 수많은 영역에서 근간을 이룹니다.

*   **확률 분포**: 연속적인 데이터를 모델링하는 데 사용되는 가우시안 분포, 지수 분포 등 다양한 확률 분포의 특성을 이해하고 예측하는 데 필수적입니다.
*   **확률적 모델링**: 베이즈 추론, 마르코프 연쇄 몬테카를로(MCMC) 등 복잡한 확률적 모델에서 사후 분포의 기대값이나 예측값의 기대값을 계산하는 데 사용됩니다.
*   **최적화**: 비용 함수(cost function)나 손실 함수(loss function)의 기대값을 최소화하는 방식으로 모델 파라미터를 학습시킵니다.
*   **강화 학습**: 에이전트가 환경과 상호작용할 때 미래에 얻을 것으로 기대되는 보상(expected reward)을 계산하여 최적의 행동 정책을 결정합니다.
*   **불확실성 정량화**: 예측 모델이 특정 범위 내에서 값을 예측할 때, 해당 예측의 기대값과 불확실성을 정량화하여 모델의 신뢰도를 평가합니다.

연속 변수에 대한 기대값 개념을 숙지하는 것은 AI 시스템이 현실 세계의 연속적인 데이터를 어떻게 이해하고, 예측하며, 의사결정하는지 파악하는 데 매우 중요합니다.

## 23. 이항 분포와 이항 검정

### **23.1 이항 분포란 무엇인가? – 무작위성을 이해하는 도구**

*   **동기**: 흔히 이항 분포를 동전 던지기 예시로 설명하지만, 이 동영상에서는 **환타 맛 선호도**라는 더 실용적인 질문으로 시작합니다. 예를 들어, "사람들이 오렌지 환타를 포도 환타보다 더 좋아하는가?" 또는 "두 맛을 똑같이 좋아하는가?" 와 같은 질문에 답하는 데 사용될 수 있습니다.
*   **핵심 질문**: 만약 7명 중 4명이 오렌지 환타를 선호하고 3명이 포도 환타를 선호한다면, 이것만으로 사람들이 오렌지 환타를 더 좋아한다고 확신할 수 있을까요? 아니면 이것이 단순히 무작위적인 우연과 적은 샘플 크기 때문일까요?
*   **이항 분포의 역할**: 이항 분포는 **"어떤 선호도도 없는 경우(무차별 가정)"에 어떤 결과가 나올지 예측**하는 데 사용됩니다. 통계 용어로, 두 가지 맛을 똑같이 좋아한다는 가설(귀무가설) 하에 데이터를 모델링하고, 실제 관측된 데이터가 이 모델에 얼마나 잘 맞는지 평가합니다.

### **23.2 이항 분포의 계산 이해하기**

이항 분포의 계산 과정을 손으로 직접 해보는 방법과 공식을 사용하는 방법을 모두 보여주어 이해를 돕습니다.

*   **간단한 예시 (3명 중 2명이 오렌지 선호)**:
    *   만약 사람들이 어떤 맛도 특별히 선호하지 않는다면, 오렌지를 선택할 확률은 0.5, 포도를 선택할 확률도 0.5라고 가정합니다.
    *   특정 순서로 2명이 오렌지를, 1명이 포도를 선호할 확률은 0.5 * 0.5 * 0.5 = 0.125 입니다.
    *   그러나 "3명 중 어떤 2명"이 오렌지를 선호하는 경우는 여러 순서(예: 오렌지-오렌지-포도, 오렌지-포도-오렌지, 포도-오렌지-오렌지)가 가능하며, 이 모든 순서의 확률을 합하면 0.375가 됩니다. 즉, 무작위 우연으로 3명 중 2명이 오렌지 환타를 선호할 확률이 0.375라는 의미입니다.
*   **이항 분포 공식**: 복잡해 보이는 공식은 실제로는 위에서 손으로 계산한 과정을 체계화한 것입니다.
    *   `X`: 오렌지 환타를 선호하는 사람의 수 (예시에서는 2)
    *   `n`: 총 질문한 사람의 수 (예시에서는 3)
    *   `P`: 오렌지 환타를 선택할 확률 (무선호 가정 하에서는 0.5)
    *   공식은 `X`명이 `n`명 중 `P`의 확률로 특정 결과를 얻을 확률을 계산합니다.
    *   **팩토리얼(Factorials)** 부분: `n! / (X! * (n-X)!)`은 `n`개의 항목 중 `X`개를 선택하는 **조합의 수**를 나타냅니다. 즉, 3명 중 2명이 오렌지를 선호하는 **다른 방법의 수**를 계산합니다 (이 예시에서는 3가지 방법).
    *   **확률 부분**: `P^X * (1-P)^(n-X)`는 `X`명이 오렌지를 선호하고 `n-X`명이 포도를 선호할 **실제 확률**을 계산합니다.
    *   이 모든 부분을 결합하면 손으로 계산한 결과와 동일하게 `0.375`를 얻습니다.

### **23.3 이항 검정이란 무엇인가? – 통계적 결론 도출**

이항 분포를 사용하여 **p-값(p-value)**을 계산하는 것을 **이항 검정**이라고 합니다.

*   **예시 (7명 중 4명이 오렌지 선호)**:
    *   만약 7명에게 물었을 때 4명이 오렌지를 선호했다면, 무작위 우연으로 이 결과가 나올 확률(P=0.5 가정 하에)은 0.273입니다.
*   **p-값 계산**: p-값은 "관측된 데이터(4명 중 7명)의 확률"과 "그것과 같거나 더 희귀한(극단적인) 모든 다른 가능성들의 확률"을 합한 값입니다.
    *   이 동영상에서는 양측 검정(two-sided p-value)을 설명하며, 오렌지가 더 선호될 확률(4, 5, 6, 7명 선호)과 포도가 더 선호될 확률(0, 1, 2, 3명 선호)을 모두 포함합니다.
    *   모든 가능성의 확률을 합산하면 1.0이 됩니다.
*   **결론**: 이 예시에서 p-값이 1이라는 것은 "선호도가 없다(P=0.5)"는 모델이 관측된 데이터에 **잘 맞는다**는 의미입니다. 따라서 샘플 크기 7로는 오렌지 환타와 포도 환타가 동등하게 사랑받는다는 가능성을 배제할 수 없습니다. 즉, 4대3이라는 결과만으로는 오렌지 환타가 더 인기 있다고 결론 내리기 어렵다는 것입니다.

### **23.4 이항 분포의 조건**

이항 분포는 다음과 같은 조건에서만 유효하게 작동합니다:

*   **독립성**: 한 사람의 선호도가 다른 사람의 선호도에 영향을 미치지 않아야 합니다. 즉, 확률이 독립적이고 일정해야 합니다.

## 24. 중심극한정리(Central Limit Theorem)

### **24.1 중심극한정리란 무엇인가?**

중심극한정리는 **'당신이 평범하지 않더라도, 당신의 평균은 평범합니다!'** 라는 문구로 요약될 수 있습니다. 이는 어떤 분포에서 표본을 뽑든, 그 표본들의 **평균값들은 정규분포를 따른다**는 놀라운 원리입니다.

이 개념을 이해하기 위해서는 정규분포와 통계분포에서 표본의 개념을 미리 아는 것이 도움이 됩니다.

### **24.2 중심극한정리의 작동 원리 (예시)**

비디오에서는 두 가지 예시를 통해 중심극한정리를 설명합니다:

*   **균일분포(Uniform Distribution) 예시**: 0부터 1까지의 범위에서 모든 값이 동일한 확률로 선택되는 균일분포에서 시작합니다.
    *   여기서 20개의 임의의 표본을 수집하고, 그 표본의 평균을 계산합니다.
    *   이 과정을 반복하여 10개, 20개, 100개의 평균값을 구한 뒤 히스토그램으로 그려보면, **100개의 평균값들이 정규분포를 따르는 것을 명확하게 확인할 수 있습니다**.
    *   원래 데이터는 균일분포에서 나왔지만, **그 평균값들은 균일분포를 따르지 않고 대신 정규분포를 따릅니다**.

*   **지수분포(Exponential Distribution) 예시**: 균일분포와는 전혀 다른 형태의 지수분포에서도 동일한 과정을 반복합니다.
    *   마찬가지로 20개의 임의의 표본을 뽑아 평균을 계산하고, 이 평균값들을 100개까지 모아 히스토그램으로 그리면, **이 평균값들 또한 정규분포를 따릅니다**.
    *   이 평균들이 지수분포에서 나온 데이터를 사용했더라도, **평균값 자체는 지수분포를 따르지 않고 정규분포를 따릅니다**.

결론적으로, 어떤 분포(평균값을 계산할 수 있는 한)로 시작하든 상관없이, **이러한 분포에서 표본들을 뽑아 그 평균값을 구하면, 그 평균값들은 정규분포를 따를 것입니다**. (단, 코쉬분포처럼 평균이 없는 분포는 예외입니다.)

### **24.3 중심극한정리가 왜 중요한가?**

중심극한정리는 통계학에서 매우 중요한 개념이며, AI 분야에서도 데이터 분석의 기초가 될 수 있습니다.

*   **원시 분포에 대한 불확실성 해소**: 우리가 실험을 할 때, 데이터가 어떤 분포에서 나왔는지 항상 알 수 없습니다. 하지만 중심극한정리는 "무슨 상관이야? 표본의 평균들은 정규분포를 따를 거야!"라고 말해줍니다.
*   **다양한 통계 검증의 기반**: 표본의 평균들이 정규분포를 따른다는 것을 알기 때문에, 우리는 원시 분포에 대해 너무 많이 걱정할 필요가 없습니다. 이를 통해 **신뢰구간(confidence intervals)을 만들 수 있고**, **t-test(두 표본 평균의 차이 검정)**나 **ANOVA(세 개 이상의 표본 평균의 차이 검정)**와 같이 **표본의 평균을 사용하는 많은 통계 검증들을 수행할 수 있습니다**.

### **24.4 실용적인 조언**

*   일반적으로 중심극한정리가 잘 성립하기 위한 **표본의 크기는 최소 30개**가 되어야 한다고 알려져 있습니다. 이는 경험 법칙이며 안전하다고 여겨지지만, 비디오에서는 20개 표본 크기에서도 이 법칙이 깨지는 예시를 보여주었습니다.
*   중심극한정리가 제대로 작동하기 위해서는 **표본에서 평균값을 계산할 수 있어야 합니다**. 코쉬분포와 같이 평균을 가지지 않는 분포는 예외입니다.

## 25. 기술 복제(Technical Replicates)"와 "생물학적 복제(Biological Replicates)

### **25.1 기술 복제(Technical Replicates)란 무엇인가?**

**기술 복제는 동일한 샘플에 대해 동일한 실험을 여러 번 반복하는 것을 의미합니다**. 이는 주로 측정 방법의 정확도나 특정 개체의 특성을 파악하는 데 중점을 둡니다.

**예시:**
*   한 사람에게서 혈액 샘플을 채취하여 유전자 발현을 측정하고, 동일한 샘플로 이 과정을 여러 번 반복하는 경우.
*   한 사람에게서 동시에 세 개의 샘플을 채취하여 각각 유전자 발현을 측정하는 경우. 이 경우에도 여전히 한 개인에 대한 이야기만 다루기 때문에 기술 복제로 간주됩니다.

**기술 복제를 통해 알 수 있는 것:**
*   **측정의 정확도**: 각 복제 측정값이 크게 다르다면, 단일 측정값을 신뢰하기 어렵다는 것을 알 수 있습니다. 이는 새로운 측정 방법을 발표할 때 그 방법의 정확성을 입증하는 데 유용합니다.
*   **개인의 특성**: 특정 개인에 대한 유전자 발현과 같은 정확한 측정값을 얻을 수 있습니다. 만약 전체 인구에게 결과를 일반화할 필요 없이 이 개인에 대한 이야기만 하고 싶다면 기술 복제가 적절합니다.

**핵심**: 기술 복제는 **개인 또는 측정 방법**에 대한 이야기를 들려줍니다.

### **25.2 생물학적 복제(Biological Replicates)란 무엇인가?**

**생물학적 복제는 서로 다른 생물학적 원천(예: 다른 사람, 다른 동물, 다른 식물, 다른 세포주 등)에서 채취한 샘플을 사용하여 실험하는 것을 의미합니다**. 이는 결과를 더 넓은 집단에 **일반화**하는 데 필수적입니다.

**예시:**
*   세 명의 다른 사람(dude)에게서 샘플을 채취하여 각 샘플에서 유전자 발현을 측정하는 경우.

**생물학적 복제를 통해 알 수 있는 것:**
*   **집단의 특성**: 특정 그룹(예: 사람들, 동물, 식물 또는 세포주)의 유전자 발현에 대한 정보를 얻을 수 있습니다.
*   **결과의 일반화**: 만약 당신이 Y-염색체에 관심이 있어 남성들의 유전자 발현에 대한 이야기를 하고 싶다면, 남성 샘플만으로도 충분합니다. 하지만 남성과 여성 모두에게 일반화되는 결과를 얻고 싶다면, 여성 샘플도 측정해야 합니다. 마찬가지로, 특정 인종이나 연령대에만 국한되지 않고 "일반적인 사람들"에 대한 이야기를 하고 싶다면, 다양한 그룹에서 샘플을 수집해야 합니다.

**핵심**: 생물학적 복제는 **그룹**에 대한 이야기를 들려줍니다.

### **25.3 기술 복제와 생물학적 복제의 혼합 사용**

기술 복제와 생물학적 복제를 함께 사용할 수 있지만, 그 유용성은 실험의 종류에 따라 달라집니다. 예를 들어, RNA 시퀀싱(RNA-seq)과 같은 특정 실험에서는 기술 복제보다는 **생물학적 복제를 더 많이 추가하는 것이 더 효율적**일 수 있습니다. 어떤 복제 전략이 더 나은지는 해당 실험 유형에 대해 얼마나 많은 정보가 이미 알려져 있는지에 따라 달라집니다.

### 25.4 **어떤 이야기를 하고 싶은가?**

데이터를 수집하고 분석할 때 가장 중요한 것은 **"어떤 이야기를 하고 싶은가?"**를 결정하는 것입니다.

*   **개인 또는 측정 방법**에 대해 이야기하고 싶다면 **기술 복제**를 사용하세요.
*   **그룹 또는 더 넓은 모집단**에 대해 이야기하고 싶다면 **생물학적 복제**를 사용하세요.

데이터 수집 단계에서부터 이 두 가지 복제 개념을 명확히 이해하고 적용하는 것은 **신뢰할 수 있고 일반화 가능한 AI 모델을 구축하는 데 매우 중요**합니다. 이 지식은 여러분의 데이터 분석과 실험 설계에 큰 도움이 될 것입니다!

## 26. 표본 크기(Sample Size, N)와 유효 표본 크기(Effective Sample Size)

### 26.1 표본 크기(N)의 기본 개념

*   **표본 크기(N)**는 일반적으로 연구하고자 하는 대상의 **독립적인 개체 수**를 의미합니다. 예를 들어, '파란색 사람(blue dude)'들의 유전자 발현에 관심이 있다면, 세 명의 다른 파란색 사람들로부터 측정값을 얻었다면 N은 3이 됩니다.
*   **기술적 반복(Technical Replicates)**과 구분해야 합니다. 기술적 반복은 동일한 개체에서 여러 번 측정하는 것으로, 측정 방법의 정확도에 대한 정보를 제공합니다.
    *   만약 우리가 사람이나 쥐 개체 간의 차이가 아닌, **측정 방법의 정확도**를 설명하는 데 관심이 있다면, 기술적 반복 횟수를 표본 크기(N)로 계산할 수 있습니다.
    *   하지만 만약 '파란색 사람'들의 유전자 발현을 보고하고 싶다면, 한 파란색 사람에게서 두 번 측정했더라도 N은 여전히 3입니다. 이 경우 기술적 반복은 표본 크기에 포함되지 않습니다.

### 26.2 유효 표본 크기(Effective Sample Size)

*   **배경**: 때로는 우리가 얻는 샘플들이 완전히 독립적이지 않고 **높은 상관관계**를 가질 수 있습니다. 예를 들어, 쌍둥이의 유전자 발현은 다른 사람들에 비해 매우 높은 상관관계를 가질 것입니다. 이러한 경우, 단순히 개체 수를 세는 것은 실제 '독립적인 정보'의 양을 과대평가할 수 있습니다.
*   **개념**: 유효 표본 크기는 **상관관계가 있는 샘플들을 독립적인 샘플로 환산했을 때의 실제 표본 크기**를 나타냅니다. 상관관계가 높은 샘플들은 완전히 독립적인 샘플로 간주되지 않습니다.
*   **계산 방법**: 유효 표본 크기는 다음 공식으로 계산됩니다.
    `유효 표본 크기 = N / (1 + (N - 1) * 상관관계)`
    여기서 N은 샘플 수이고, '상관관계'는 샘플 간의 상관계수입니다.
*   **상관관계의 영향**:
    *   **상관관계가 높을 때 (예: 0.7)**: 두 샘플이 1.18명으로 계산됩니다. 이는 두 명의 사람이지만, 정보의 관점에서는 1.18명의 독립적인 정보만을 제공한다는 의미입니다.
    *   **상관관계가 낮을 때 (예: 0.1)**: 두 샘플이 1.82명으로 계산됩니다. 이는 두 명의 사람이 거의 두 명에 가까운 독립적인 정보를 제공한다는 의미입니다.
    *   즉, **상관관계가 높을수록 유효 표본 크기는 감소**하고, 상관관계가 낮을수록 실제 표본 크기에 가까워집니다.

### 26.3 요약 및 적용 시나리오

*   **방법의 정확도 보고**: 표본 크기(N)는 기술적 반복의 수입니다.
*   **특정 그룹에 대한 보고 (예: '파란색 사람'이나 특정 계통의 쥐)**: 표본 크기(N)는 해당 그룹의 독립적인 개체 수.
*   **일반적인 모집단에 대한 보고 (예: 모든 종류의 사람이나 쥐)**: 상관관계를 고려하여 **유효 표본 크기**를 계산.

## 27. 표준편차(Standard Deviation)와 표준오차(Standard Error)

### 27.1 표준편차(Standard Deviation)의 이해

*   **정의**: 표준편차는 **하나의 측정값 세트에서 데이터가 얼마나 퍼져있는지**를 정량화합니다. 즉, 개별 측정값들이 평균으로부터 얼마나 떨어져 있는지를 나타내는 지표입니다.
*   **예시**: 만약 5마리의 쥐 무게를 측정했다면, 이 5개의 측정값들의 평균과 그 평균 주위에 데이터가 얼마나 퍼져있는지를 나타내는 것이 표준편차입니다. 이는 우리가 측정한 특정 데이터 세트의 분산도를 설명합니다.

### 27.2 표준오차(Standard Error)의 이해

*   **정의**: 표준오차는 **여러 개의 측정 세트의 평균에서 데이터가 얼마나 퍼져있는지**를 정량화한 것입니다. 즉, 여러 번 반복된 실험에서 얻은 평균값들이 전체 평균으로부터 얼마나 떨어져 있는지를 나타내는 지표입니다.
*   **예시**: 5마리의 쥐 무게를 재는 동일한 실험을 매번 다른 쥐를 사용하여 5회 반복했다고 상상해 봅시다. 이 5번의 실험 각각에서 5개의 쥐 무게 평균과 표준편차를 얻게 됩니다. 이 5개의 평균값들을 다시 모았을 때, 이 평균값들의 평균을 기준으로 이 평균값들이 얼마나 퍼져있는지를 나타내는 것이 **표준오차**입니다. "평균들의 표준편차"라고도 불립니다.

### 27.3 핵심 차이 및 적용

*   **표준편차**: **개별 데이터 포인트의 분산도**를 나타냅니다. "데이터가 얼마나 퍼져있는지"에 대한 정보를 제공합니다.
*   **표준오차**: **표본 평균의 정밀도**를 나타내며, 만약 실험을 반복한다면 그 평균값이 얼마나 변동할 것인지를 추정하는 데 사용됩니다.

### 27.4 흔한 혼동 및 권장 사항

*   **혼동**: 표준오차는 여러 세트의 평균으로 설명되지만, **하나의 측정값 세트만 가지고도 추정할 수 있습니다**. 이 때문에 하나의 데이터 세트만 있을 때도 종종 표준오차를 표시하는 경우가 있습니다.
*   **표시 지침**:
    *   대부분의 경우, 그래프가 주로 **측정된 데이터를 설명**하려는 것이라면 **표준편차를 표시**해야 합니다. 이는 데이터가 얼마나 넓게 분포되어 있는지 직접적으로 보여줍니다.
    *   표준오차는 주로 **표본 평균이 모집단 평균을 얼마나 잘 대표하는지**를 추정할 때 사용됩니다.

## 28. 표준오차(Standard Error)와 부트스트랩(Bootstrap)

### **28.1 오차 막대의 종류 및 데이터 표현 방식**

데이터를 시각화할 때 일반적으로 사용되는 '오차 막대'에는 세 가지 주요 종류가 있습니다.

*   **표준편차(Standard Deviation):**
    *   **개념:** 평균 주위로 **개별 데이터 점들이 얼마나 퍼져있는지**를 나타냅니다. 표준편차가 크면 데이터 점들이 평균으로부터 많이 떨어져 있다는 의미입니다.
    *   **사용 목적:** 당신이 직접 수집한 **데이터 자체의 분포**를 설명할 때 사용됩니다.
    *   **그래프:** 주로 평균과 함께 빨간색 오차 막대로 표시되며, 종종 '다이너마이트 그래프'라고 불리는 형태로 실제 데이터 점 대신 평균과 표준편차만을 보여주기도 합니다.
*   **표준오차(Standard Error):**
    *   **개념:** **평균이 얼마나 퍼져있는지**를 나타냅니다. 즉, 여러 번 표본을 추출했을 때 그 표본들의 평균이 얼마나 다를 수 있는지를 알려줍니다.
    *   **사용 목적:** 모집단으로부터 얻은 **표본 평균의 정확도**를 추정할 때 사용됩니다.
*   **신뢰구간(Confidence Interval):**
    *   **개념:** 표준오차와 관련이 있으며, 나중에 더 자세히 다룰 예정입니다.


### **28.2 표준오차(Standard Error) 자세히 알아보기**

표준오차는 '평균의 표준오차'를 의미하는 경우가 많지만, 다른 통계량(예: 중앙값, 최빈값 등)의 표준오차도 계산할 수 있습니다.

*   **정의:** 표준오차는 **동일한 모집단에서 여러 번 추출한 표본들의 평균값들이 얼마나 퍼져있는지**를 나타내는 표준편차입니다. 다시 말해, 독립된 여러 표본을 뽑았을 때, 각 표본의 평균값들이 얼마나 다양하게 나타날지 짐작하게 해줍니다.
*   **계산 과정 (개념적 이해):**
    1.  **여러 개의 표본 추출:** 모집단에서 동일한 크기(N)의 측정값을 가지는 여러 개의 표본을 뽑습니다 (예: N=5인 표본 여러 개).
    2.  **각 표본의 평균 계산:** 각 표본에 대해 평균을 계산합니다.
    3.  **평균들의 표준편차 계산:** 이렇게 얻은 여러 표본 평균값들의 표준편차를 계산합니다. 이 값이 바로 **표준오차**입니다.
*   **특징:** 표준오차는 일반적으로 개별 데이터 점들의 표준편차보다 훨씬 더 작습니다. 이는 평균값들이 실제 데이터 점만큼 넓게 퍼지지 않기 때문입니다.

### **28.3 부트스트랩(Bootstrap) 기법**

실험을 여러 번 반복할 시간이나 돈이 없을 때, 또는 표준오차를 추정할 간단한 공식이 없을 때 **부트스트랩**이라는 강력한 기법을 사용할 수 있습니다. 부트스트랩은 개념적으로 매우 간단하고 컴퓨터로 구현하기 쉽다는 장점이 있습니다.

*   **필요성:** 대부분의 통계량(평균 외의 중앙값, 최빈값 등)에 대한 표준오차는 간단한 공식으로 추정하기 어렵기 때문에 부트스트랩이 유용합니다.
*   **과정:** 하나의 표본(주로 10개 이상의 측정값이 있는 표본이 좋음)이 있을 때, 다음 단계를 따릅니다.
    1.  **원본 표본에서 무작위 측정값 선택:** 가지고 있는 **원본 표본에서 임의의 측정값 하나를 뽑습니다**. (새로운 측정값이 아님)
    2.  **값 기록:** 선택된 값을 기록합니다.
    3.  **반복 (복원 추출):** 원본 표본의 크기와 동일한 개수의 측정값을 얻을 때까지 1, 2단계를 반복합니다. **이미 뽑았던 값을 다시 뽑을 수 있습니다** (이를 복원 추출이라고 합니다). 이렇게 생성된 새로운 표본을 '부트스트랩 표본'이라고 합니다.
    4.  **통계량 계산:** 이 부트스트랩 표본에서 우리가 알고 싶은 통계량(예: 평균, 중앙값)을 계산합니다.
    5.  **반복:** 1-4단계를 수천 번 반복하여 수많은 부트스트랩 표본과 그에 해당하는 통계량(예: 부트스트랩 평균들)을 얻습니다.
    6.  **표준오차 계산:** 이렇게 얻은 **모든 통계량(예: 부트스트랩 평균들)의 표준편차를 구합니다.** 이 값이 바로 **부트스트랩을 통한 표준오차 추정치**입니다.

표준오차는 여러 다른 표본에서 얻은 평균이 얼마나 다양할지 예측할 수 있는 척도입니다. 그리고 만약 표준오차를 구하기 위한 직접적인 공식이 없을 경우, 부트스트랩 기법을 사용하여 스스로 표준오차를 추정할 수 있습니다. 이는 AI 모델의 성능을 평가하거나, 실험 결과의 신뢰도를 측정하는 등 다양한 인공지능 분야에서 필수적인 도구로 활용될 수 있다.

## 29. 부트스트랩(Bootstrapping)

### 29.1 **부트스트랩(Bootstrapping)이란 무엇이며 왜 필요한가?**

데이터를 분석할 때, 우리가 얻은 결과(예: 평균값)가 우연히 나온 것인지, 아니면 실제 의미 있는 결과인지를 파악하는 것이 중요합니다.

*   **문제 제기:** 영상에서는 새로운 약의 효과를 8명의 환자에게 투여했을 때의 반응을 예시로 듭니다. 5명은 호전되었고 3명은 악화되어 평균 반응이 0.5로 나타났습니다. 여기서 의문은 이 0.5라는 평균이 약 때문인지, 아니면 단순히 무작위적인 요인(예: 원래 환자의 건강 상태) 때문인지 알 수 없다는 것입니다.
*   **이상적인 해결책 (하지만 비실용적):** 이 문제를 해결하는 한 가지 방법은 실험을 여러 번 반복하는 것입니다. 반복된 실험에서 얻은 평균값들을 히스토그램으로 만들면, 약효가 없는 0에 가까운 평균이 얼마나 자주 나타나는지, 그리고 0에서 멀리 떨어진 평균이 얼마나 드물게 나타나는지 알 수 있습니다. 하지만 실제로는 실험을 여러 번 반복하는 것은 비용과 시간이 많이 드는 일입니다.
*   **부트스트랩의 등장:** 바로 이때, 비용과 시간을 절약하면서도 동일한 효과를 얻을 수 있는 방법이 **부트스트랩(Bootstrapping)**입니다. 부트스트랩은 하나의 데이터셋만 가지고도, 실험을 수천 번 반복한 것과 같은 효과를 시뮬레이션하여 결과의 신뢰도를 추정할 수 있게 해줍니다.


### 29.2 **부트스트랩 과정 상세 설명**

부트스트랩은 다음 네 가지 주요 단계를 거쳐 진행됩니다:

1.  **부트스트랩 데이터셋 생성 (Make a bootstrapped data set):**
    *   **원본 데이터 선택:** 먼저 가지고 있는 원본 데이터셋(예: 8명의 환자 반응 데이터)을 준비합니다.
    *   **무작위 복원 추출:** 원본 데이터셋에서 **측정값 하나를 무작위로 선택하여 새로운 '부트스트랩 데이터셋'에 추가합니다**.
    *   **반복 (복원 추출):** 이 과정을 원본 데이터셋의 크기와 동일한 수의 측정값을 얻을 때까지 반복합니다. **중요한 점은 이미 선택한 값을 다시 선택할 수 있다는 것입니다 (복원 추출, sampling with replacement)**. 예를 들어, 원본 데이터가 8개라면, 8개의 새로운 측정값으로 구성된 부트스트랩 데이터셋을 만듭니다.
    *   이렇게 생성된 새로운 데이터셋을 **부트스트랩 데이터셋(bootstrapped dataset)**이라고 부릅니다. 이 데이터셋은 원본 데이터셋과는 다르기 때문에, 평균을 계산하면 보통 다른 값을 얻게 됩니다.

2.  **통계량 계산 (Calculate something):**
    *   새롭게 생성된 부트스트랩 데이터셋으로부터 우리가 관심 있는 통계량(예: **평균**, 중앙값, 표준편차 등)을 계산합니다. 영상에서는 주로 평균을 예로 들지만, 다른 통계량에도 적용 가능합니다.

3.  **계산 결과 기록 (Keep track of that calculation):**
    *   계산된 통계량 값을 기록해둡니다. 이를 통해 나중에 히스토그램을 만들 수 있습니다.

4.  **단계 1-3 반복 (Repeat steps 1-3 a bunch of times):**
    *   위의 1단계부터 3단계까지의 과정을 **수천 번 반복합니다**. 일반적으로 컴퓨터를 사용하여 수천 개의 부트스트랩 표본을 만들고 각각의 통계량을 계산합니다.

### 29.3 **부트스트랩의 '대단한 점' (Awesome Advantages)**

수천 번의 반복을 통해 얻은 통계량 값들을 히스토그램으로 만들면, 여러 이점을 얻을 수 있습니다.

*   **분포의 시각화:** 이 히스토그램은 우리가 실험을 여러 번 반복했다면 얻었을 통계량 값들의 분포를 보여줍니다. 이를 통해 원본 평균에서 멀리 떨어진 값이 얼마나 드물게 나타날지 시각적으로 파악할 수 있습니다.
*   **표준오차 추정:** 이 분포의 **표준편차를 계산하면, 그것이 바로 원본 데이터셋의 평균값(또는 다른 통계량)에 대한 표준오차**가 됩니다.
*   **신뢰구간 계산:** 부트스트랩으로 얻은 분포를 사용하여 **신뢰구간(Confidence Interval)**을 계산할 수 있습니다. 예를 들어, 95% 신뢰구간은 부트스트랩 평균들의 95%를 포함하는 구간을 의미합니다. 이 신뢰구간은 특정 가설을 검정하는 데 사용될 수 있습니다. 약물 예시에서는, 95% 신뢰구간이 0을 포함한다면, 약이 아무런 효과가 없다는 가설을 기각할 수 없다는 결론을 내릴 수 있습니다.
*   **뛰어난 유연성:** 부트스트랩의 가장 큰 장점은 **어떤 통계량에도 적용될 수 있다**는 점입니다. 평균뿐만 아니라 중앙값, 표준편차 등 복잡한 통계량에 대해서도 표준오차나 신뢰구간을 추정할 수 있으며, 이를 위해 별도의 복잡한 공식이 필요 없습니다.

## 30. 부트스트래핑(Bootstrapping)을 활용한 p-값 계산

### **30.1 부트스트래핑의 기본 단계 (핵심 요약)**

부트스트래핑은 다음 4단계로 진행됩니다:
1.  **부트스트랩 데이터 세트 생성**: 원본 데이터에서 **복원 추출(sampling with replacement)** 방식으로 데이터를 무작위로 뽑아 새로운 데이터 세트(부트스트랩 데이터 세트)를 만듭니다. 이 과정에서 중복된 값이 생길 수 있다.
2.  **통계량 계산**: 각 부트스트랩 데이터 세트에서 원하는 통계량(예: 평균, 중앙값, 표준편차 등)을 계산합니다.
3.  **계산값 기록**: 계산된 통계량을 기록해 둡니다.
4.  **반복**: 첫 세 단계를 수천 번 반복하여 통계량의 분포를 만듭니다.

이전에는 이 분포를 활용하여 원본 통계량의 95% 신뢰 구간을 만들었음을 상기시킵니다.

### **30.2 부트스트래핑으로 p-값 계산하는 방법**

p-값 계산은 귀무 가설(Null Hypothesis)을 검정하는 데 초점을 맞춥니다. 동영상에서는 약물의 효과를 예로 들어 설명합니다.

1.  **귀무 가설 설정**: 귀무 가설은 "약물이 질병에 아무런 효과가 없다"는 것입니다. 이는 약물 투여 후 기대되는 평균 효과가 0이라는 것을 의미합니다.
2.  **데이터 이동(Shifting Data)**: 원래 데이터의 관측된 통계량(예: 평균 0.5)을 귀무 가설의 기준점(예: 0)으로 **이동**시킵니다. 즉, 모든 측정값에서 관측된 평균값을 빼서, 데이터 세트의 평균이 0이 되도록 만듭니다. 이렇게 **이동된 데이터 세트는 귀무 가설이 참이라고 가정했을 때의 데이터 분포를 대표**하게 됩니다.
3.  **귀무 가설 분포 생성**: 평균이 0으로 이동된 이 데이터 세트로부터 위에서 설명한 부트스트래핑 과정을 수천 번 반복합니다. 이렇게 생성된 통계량(예: 평균)의 분포는 **귀무 가설이 참일 때 나타날 수 있는 통계량 값들의 분포**를 보여줍니다.
4.  **p-값 계산**:
    *   이제 **원래 데이터에서 관측된 통계량 값**(예: 평균 0.5)으로 돌아갑니다.
    *   **p-값은 귀무 가설 분포에서 관측된 통계량 값과 같거나, 그보다 더 극단적인(즉, 귀무 가설의 기준점 0에서 더 멀리 떨어진) 값을 얻을 확률**입니다.
    *   예를 들어, 원래 평균이 0.5였다면, 귀무 가설 분포에서 0.5보다 크거나 같은 평균을 얻을 확률과 -0.5보다 작거나 같은 평균을 얻을 확률을 더하여 p-값을 계산합니다.
    *   동영상 예시에서는 이 두 확률을 더해 0.63이라는 p-값을 얻었습니다.
5.  **가설 검정 결정**: 일반적으로 p-값이 유의수준(예: 0.05)보다 크면 귀무 가설을 기각하지 못합니다. 예시에서 0.63은 0.05보다 크므로, 약물이 아무런 효과가 없다는 귀무 가설을 기각할 수 없습니다.

### **30.3 방법의 유연성 및 AI 학생들을 위한 시사점**

*   **다양한 통계량에 적용 가능**: 이 부트스트래핑 기반 p-값 계산 방법은 평균(mean)뿐만 아니라 **중앙값(median)**, 표준편차 등 **거의 모든 통계량에 적용**될 수 있습니다.
*   **이상치(Outliers)에 강건**: 특히 데이터에 이상치가 존재할 경우, 평균은 이상치에 민감하게 반응하지만 **중앙값은 이상치에 더 강건(robust)합니다**. 따라서 이러한 경우 중앙값을 사용하여 더 신뢰할 수 있는 가설 검정을 수행할 수 있습니다.

**시사점:**

*   **비모수적 방법의 이해**: 부트스트래핑은 데이터의 특정 분포(예: 정규 분포)에 대한 가정이 필요 없는 **비모수적(non-parametric)** 방법입니다. 이는 AI 및 머신러닝 분야에서 실제 데이터가 종종 비정규적인 분포를 보일 때 매우 유용합니다.
*   **모델 성능 평가 및 통계적 유의성**: AI 모델의 성능을 평가하거나, A/B 테스트에서 여러 모델 간의 차이가 통계적으로 유의미한지 판단할 때 이 방법을 활용할 수 있습니다. 특히 복잡한 통계량(예: AUC, F1-score 등)에 대한 p-값 추정이 필요할 때 유용합니다.
*   **강건한 통계량 활용**: 이상치가 많은 데이터 세트에서 모델을 평가할 때, 평균 대신 중앙값과 같은 강건한 통계량을 사용하여 더 안정적인 결과를 얻고, 이에 대한 통계적 유의성을 부트스트래핑으로 검정할 수 있습니다.
*   **불확실성 정량화**: 모델 예측의 불확실성을 정량화하거나, 특정 하이퍼파라미터가 모델 성능에 미치는 영향의 유의성을 평가하는 데도 응용될 수 있는 강력한 도구입니다.

---