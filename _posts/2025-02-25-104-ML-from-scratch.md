---
title: 2ì°¨ì‹œ 4:ë¨¸ì‹ ëŸ¬ë‹ From Scratch 1
layout: single
classes: wide
categories:
  - ML
toc: true # ì´ í¬ìŠ¤íŠ¸ì—ì„œ ëª©ì°¨ë¥¼ í™œì„±í™”
toc_sticky: true # ëª©ì°¨ë¥¼ ê³ ì •í• ì§€ ì—¬ë¶€ (ì„ íƒ ì‚¬í•­)
---

## 1. K-ìµœê·¼ì ‘ ì´ì›ƒ(K-Nearest Neighbors, KNN)
- ì¶œì²˜:[How to implement KNN from scratch with Python](https://www.youtube.com/watch?v=rTEtEy5o3X0&list=PLcWfeUsAys2k_xub3mHks85sBHZvg24Jd&index=2)


### 1.1 `KNN.py` :K-ìµœê·¼ì ‘ ì´ì›ƒ(K-Nearest Neighbors) ì•Œê³ ë¦¬ì¦˜

```python
import numpy as np
from collections import Counter

def euclidean_distance(x1, x2):
    distance = np.sqrt(np.sum((x1-x2)**2))
    return distance
```

* **`euclidean_distance`** : ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜

  * KNNì—ì„œ â€˜ê°€ì¥ ê°€ê¹Œìš´â€™ ë°ì´í„°ë¥¼ ì°¾ê¸° ìœ„í•´ ê±°ë¦¬ ê³„ì‚°ì´ í•„ìš”í•©ë‹ˆë‹¤.
  * ì—¬ê¸°ì„œëŠ” **ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³µì‹** ì‚¬ìš©

    $ê±°ë¦¬ = \sqrt{\sum (x_1 - x_2)^2}$


```python
class KNN:
    def __init__(self, k=3):
        self.k = k
```

* **`__init__`** : ëª¨ë¸ì´ ëª‡ ê°œì˜ ì´ì›ƒ(k)ì„ ê³ ë ¤í• ì§€ ì €ì¥

  * ì˜ˆ: `k=3` â†’ ê°€ì¥ ê°€ê¹Œìš´ 3ê°œì˜ ë°ì´í„° ë³´ê³  ë‹¤ìˆ˜ê²° ê²°ì •



```python
    def fit(self, X, y):
        self.X_train = X
        self.y_train = y
```

* **`fit`** : í•™ìŠµ ë°ì´í„°ë¥¼ ì €ì¥

  * KNNì€ **í›ˆë ¨ ê³¼ì •ì—ì„œ ë³„ë„ì˜ ê³„ì‚°ì„ í•˜ì§€ ì•Šê³ **, ë‹¨ìˆœíˆ ë°ì´í„°ë§Œ ì €ì¥í•©ë‹ˆë‹¤.
  * ì˜ˆì¸¡ ì‹œì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ê±°ë¦¬ ê³„ì‚°


```python
    def predict(self, X):
        predictions = [self._predict(x) for x in X]
        return predictions
```

* **`predict`** : ì…ë ¥ëœ ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´ `_predict` í˜¸ì¶œ

  * ê²°ê³¼ëŠ” ê° ìƒ˜í”Œì˜ ì˜ˆì¸¡ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸


```python
    def _predict(self, x):
        # compute the distance
        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]
    
        # get the closest k
        k_indices = np.argsort(distances)[:self.k]
        k_nearest_labels = [self.y_train[i] for i in k_indices]

        # majority vote
        most_common = Counter(k_nearest_labels).most_common()
        return most_common[0][0]
```

* **`_predict`** : í•œ ê°œ ìƒ˜í”Œì— ëŒ€í•œ ì˜ˆì¸¡ ê³¼ì •

  1. ëª¨ë“  í›ˆë ¨ ë°ì´í„°ì™€ ê±°ë¦¬ ê³„ì‚°
  2. ê±°ë¦¬ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ **ê°€ì¥ ê°€ê¹Œìš´ kê°œì˜ ì¸ë±ìŠ¤** ì„ íƒ
  3. ê·¸ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë¼ë²¨ ëª©ë¡ ì¶”ì¶œ
  4. **ë‹¤ìˆ˜ê²°**ë¡œ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë¼ë²¨ ì„ íƒ

ğŸ’¡ **í•µì‹¬ ê°œë… ì •ë¦¬**

* **KNNì€ í›ˆë ¨ ì‹œ ê³„ì‚°ì´ ê±°ì˜ ì—†ê³ , ì˜ˆì¸¡ ì‹œ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹**
* k ê°’ì´ ì‘ìœ¼ë©´ ëª¨ë¸ì´ ë¯¼ê°(ë…¸ì´ì¦ˆ ì˜í–¥ í¼), í¬ë©´ ë¶€ë“œëŸ½ì§€ë§Œ ì •í™•ë„ ì €í•˜ ê°€ëŠ¥
* ê±°ë¦¬ ê³„ì‚° ë°©ì‹ì€ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì™¸ì—ë„ ë§¨í•´íŠ¼ ê±°ë¦¬, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë“± ê°€ëŠ¥

### 1.2 `train.py` â€” ëª¨ë¸ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from KNN import KNN
```

* í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
* `KNN` í´ë˜ìŠ¤ëŠ” ìš°ë¦¬ê°€ ë§Œë“  `KNN.py`ì—ì„œ ë¶ˆëŸ¬ì˜´

```python
cmap = ListedColormap(['#FF0000','#00FF00','#0000FF'])
```

* ì‚°ì ë„(Scatter plot) ìƒ‰ìƒ ì§€ì •

```python
iris = datasets.load_iris()
X, y = iris.data, iris.target
```

* **Iris ë°ì´í„°ì…‹ ë¡œë“œ**

  * X: ê½ƒë°›ì¹¨/ê½ƒì ê¸¸ì´Â·ë„ˆë¹„(4ê°œ íŠ¹ì„±)
  * y: ê½ƒ í’ˆì¢…(0, 1, 2)

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1234
)
```

* ë°ì´í„°ì…‹ì„ 80% í›ˆë ¨ / 20% í…ŒìŠ¤íŠ¸ë¡œ ë¶„ë¦¬

```python
plt.figure()
plt.scatter(X[:,2], X[:,3], c=y, cmap=cmap, edgecolor='k', s=20)
plt.show()
```

* ê½ƒì ê¸¸ì´($X\[:,2]$)ì™€ ë„ˆë¹„($X\[:,3]$)ë§Œ ì´ìš©í•˜ì—¬ ì‹œê°í™”

```python
clf = KNN(k=5)
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)
```

* **KNN ê°ì²´ ìƒì„±(k=5)**
* `fit`ìœ¼ë¡œ í›ˆë ¨ ë°ì´í„° ì €ì¥
* `predict`ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡

```python
print(predictions)

acc = np.sum(predictions == y_test) / len(y_test)
print(acc)
```

* ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •í™•ë„ ì¶œë ¥

### 1.3 scikit-learnì„ ì‚¬ìš©í•´ì„œ ê°™ì€ ì‘ì—…ì„ í›¨ì”¬ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# 1. ë°ì´í„° ë¡œë“œ
iris = datasets.load_iris()
X, y = iris.data, iris.target

# 2. í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1234
)

# 3. sklearn KNN ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
clf = KNeighborsClassifier(n_neighbors=5)  # k=5
clf.fit(X_train, y_train)

# 4. ì˜ˆì¸¡
predictions = clf.predict(X_test)
print(predictions)

# 5. ì •í™•ë„ ê³„ì‚°
acc = np.mean(predictions == y_test)
print(acc)
```

## 2. **Linear Regression(ì„ í˜• íšŒê·€)**
- ì¶œì²˜: [How to implement Linear Regression from scratch with Python](https://www.youtube.com/watch?v=ltXSoduiVwY&list=PLcWfeUsAys2k_xub3mHks85sBHZvg24Jd&index=3)

### 2.1 `LinearRegression.py` â€” ì„ í˜• íšŒê·€ ëª¨ë¸ êµ¬í˜„

```python
import numpy as np

class LinearRegression:
    def __init__(self, lr=0.001, n_iters=1000):
        self.lr = lr              # í•™ìŠµë¥  (learning rate)
        self.n_iters = n_iters    # ë°˜ë³µ íšŸìˆ˜
        self.weights = None       # ê°€ì¤‘ì¹˜
        self.bias = None          # ì ˆí¸

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)  # ì²˜ìŒì—ëŠ” ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        self.bias = 0

        for _ in range(self.n_iters):
            # ì˜ˆì¸¡ ê°’ ê³„ì‚°
            y_pred = np.dot(X, self.weights) + self.bias

            # ê°€ì¤‘ì¹˜ì™€ ì ˆí¸ì— ëŒ€í•œ ê¸°ìš¸ê¸°(gradient) ê³„ì‚°
            # í¸ë¯¸ë¶„ì—ì„œ ë‚˜ì˜¨ ê³±í•˜ê¸° 2ëŠ” í•™ìŠµë¥ ì— í¬í•¨ì‹œí‚¬ ìˆ˜ ìˆëŠ” ìƒìˆ˜ì´ê¸° ë•Œë¬¸ì—, 
            # ì½”ë“œì—ì„œëŠ” ìƒëµí•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤
            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))
            db = (1 / n_samples) * np.sum(y_pred - y)

            # ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
            self.weights -= self.lr * dw
            self.bias -= self.lr * db

    def predict(self, X):
        y_pred = np.dot(X, self.weights) + self.bias
        return y_pred
```

* **`fit` ë©”ì„œë“œ**

  * ë°ì´í„°ë¥¼ ë³´ê³  **ìµœì ì˜ ì§ì„ **(y = wx + b)ì„ ì°¾ëŠ” ê³¼ì •
  * `np.dot(X, self.weights)` : X(ì…ë ¥)ì™€ w(ê°€ì¤‘ì¹˜)ë¥¼ ê³±í•´ ì˜ˆì¸¡ê°’ì„ êµ¬í•¨
  * **ê¸°ìš¸ê¸° dw, db**ë¥¼ ê³„ì‚° â†’ ê²½ì‚¬ í•˜ê°•ë²•ìœ¼ë¡œ ì¡°ê¸ˆì”© ìˆ˜ì •
  
    ```python
    #ê¸°ìš¸ê¸° ê³„ì‚° ë¶€ë¶„:
    dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
    ```
    - ì˜¤ì°¨ `(y_pred - y)`ë¥¼ ê³„ì‚°
    - ê° íŠ¹ì„±ë³„ë¡œ ì˜¤ì°¨ì— ëŒ€í•œ ê¸°ì—¬ë„ë¥¼ êµ¬í•¨
    - í‰ê· ì„ ë‚´ì„œ ì•ˆì •ì ì¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°

* **ê²½ì‚¬ í•˜ê°•ë²•**
  * ì˜ˆì¸¡ê°’ì´ ì‹¤ì œê°’ê³¼ ê°€ê¹Œì›Œì§€ë„ë¡ wì™€ bë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì¡°ì •
  * í•™ìŠµë¥ (`lr`)ì´ ë„ˆë¬´ í¬ë©´ ë°œì‚°í•˜ê³ , ë„ˆë¬´ ì‘ìœ¼ë©´ í•™ìŠµì´ ëŠë¦¼
  ```python
    #ìµœì ê°’ ì°¾ì•„ê°€ëŠ” ë¶€ë¶„:
    self.weights = self.weights - self.lr*dw
    ```
    - dwê°€ ì–‘ìˆ˜ë©´ ê°€ì¤‘ì¹˜ë¥¼ ì¤„ì„ (ë¹¼ê¸° ë•Œë¬¸ì—)
    - dwê°€ ìŒìˆ˜ë©´ ê°€ì¤‘ì¹˜ë¥¼ ëŠ˜ë¦¼ (ìŒìˆ˜ë¥¼ ë¹¼ë¯€ë¡œ)
    - dwê°€ 0ì— ê°€ê¹Œìš°ë©´ ê°€ì¤‘ì¹˜ ë³€í™”ëŸ‰ë„ 0ì— ê°€ê¹Œì›€


### 2.2 `train.py` â€” ëª¨ë¸ í•™ìŠµ ë° ì‹œê°í™”

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import datasets
import matplotlib.pyplot as plt
from LinearRegression import LinearRegression

# 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„±
X, y = datasets.make_regression(
    n_samples=100,   # ë°ì´í„° ê°œìˆ˜
    n_features=1,    # ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜ (1ê°œ â†’ ë‹¨ìˆœ íšŒê·€)
    noise=20,        # ë°ì´í„°ì— ì¡ìŒ ì¶”ê°€
    random_state=4
)

# 2. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1234
)

# 3. ë°ì´í„° ì‹œê°í™” (ì‚°ì ë„)
plt.scatter(X[:, 0], y, color="b", marker="o", s=30)
plt.show()

# 4. ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
reg = LinearRegression(lr=0.01)  # í•™ìŠµë¥  0.01
reg.fit(X_train, y_train)

# 5. ì˜ˆì¸¡
predictions = reg.predict(X_test)

# 6. í‰ê°€ (í‰ê·  ì œê³± ì˜¤ì°¨, MSE)
def mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

print("MSE:", mse(y_test, predictions))

# 7. ì˜ˆì¸¡ ì§ì„  ê·¸ë¦¬ê¸°
y_pred_line = reg.predict(X)
plt.scatter(X_train, y_train, color="orange", s=10)
plt.scatter(X_test, y_test, color="blue", s=10)
plt.plot(X, y_pred_line, color="black", linewidth=2, label="Prediction")
plt.show()
```

*   ì‹¤í–‰ íë¦„
1. **ë°ì´í„° ì¤€ë¹„** â†’ `make_regression`ìœ¼ë¡œ ê°€ìƒì˜ ì„ í˜• ë°ì´í„° ìƒì„±
2. **ë°ì´í„° ë¶„ë¦¬** â†’ í•™ìŠµìš© 80%, í…ŒìŠ¤íŠ¸ìš© 20%
3. **ëª¨ë¸ í•™ìŠµ** â†’ ê²½ì‚¬ í•˜ê°•ë²•ìœ¼ë¡œ w, b ì°¾ê¸°
4. **ì˜ˆì¸¡** â†’ í•™ìŠµí•œ w, bë¡œ ìƒˆë¡œìš´ ê°’ ì˜ˆì¸¡
5. **í‰ê°€** â†’ MSE ê³„ì‚° (ê°’ì´ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
6. **ì‹œê°í™”** â†’ ë°ì´í„°ì™€ ì˜ˆì¸¡ ì§ì„ ì„ í•¨ê»˜ ê·¸ë¦¼

*   ì…ë¬¸ìë¥¼ ìœ„í•œ í•µì‹¬ ìš”ì•½
    * **ì„ í˜• íšŒê·€**: ë°ì´í„°ì— ê°€ì¥ ì˜ ë§ëŠ” ì§ì„ ì„ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜
    * **ê°€ì¤‘ì¹˜(w)**: ê¸°ìš¸ê¸°
    * **ì ˆí¸(b)**: ì§ì„ ì´ yì¶•ê³¼ ë§Œë‚˜ëŠ” ì§€ì 
    * **í•™ìŠµë¥ (lr)**: í•œ ë²ˆì— ì´ë™í•˜ëŠ” í­
    * **MSE**: ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€ ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ

### 2.3 **scikit-learn**ì„ í™œìš©í•´ ì„ í˜• íšŒê·€ë¥¼ í›¨ì”¬ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„±
X, y = datasets.make_regression(
    n_samples=100,   # ë°ì´í„° ê°œìˆ˜
    n_features=1,    # ë…ë¦½ ë³€ìˆ˜ ê°œìˆ˜
    noise=20,        # ì¡ìŒ ì¶”ê°€
    random_state=4
)

# 2. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1234
)

# 3. ëª¨ë¸ ìƒì„± & í•™ìŠµ
model = LinearRegression()
model.fit(X_train, y_train)

# 4. ì˜ˆì¸¡
y_pred = model.predict(X_test)

# 5. í‰ê°€ (MSE)
mse = mean_squared_error(y_test, y_pred)
print("MSE:", mse)
print("ê¸°ìš¸ê¸°(w):", model.coef_)
print("ì ˆí¸(b):", model.intercept_)

# 6. ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì§ì„ 
y_pred_line = model.predict(X)

# 7. ì‹œê°í™”
plt.figure(figsize=(8, 6))
plt.scatter(X_train, y_train, color="orange", s=10, label="Train data")
plt.scatter(X_test, y_test, color="blue", s=10, label="Test data")
plt.plot(X, y_pred_line, color="black", linewidth=2, label="Prediction line")
plt.legend()
plt.show()
```


*   ì‹¤í–‰ íë¦„
1. **ë°ì´í„° ìƒì„±** â†’ `datasets.make_regression`ìœ¼ë¡œ ì„ í˜• ê´€ê³„ê°€ ìˆëŠ” ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„
2. **ë°ì´í„° ë¶„ë¦¬** â†’ í•™ìŠµìš©/í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
3. **ëª¨ë¸ ìƒì„±** â†’ `LinearRegression()` ê°ì²´ ìƒì„±
4. **í•™ìŠµ** â†’ `.fit()`ìœ¼ë¡œ wì™€ b ìë™ ê³„ì‚°
5. **ì˜ˆì¸¡** â†’ `.predict()` ì‚¬ìš©
6. **í‰ê°€** â†’ `mean_squared_error`ë¡œ MSE ê³„ì‚°
7. **ì‹œê°í™”** â†’ í•™ìŠµ ë°ì´í„°, í…ŒìŠ¤íŠ¸ ë°ì´í„°, ì˜ˆì¸¡ ì§ì„  í•¨ê»˜ í‘œì‹œ

### 2.4 ì˜ˆì¸¡, í‰ê°€, ìµœì í™”ì˜ ìˆ˜í•™ì  í‘œí˜„

1.ì˜ˆì¸¡ê°’ ê³„ì‚° ìˆ˜ì‹
- ì„ í˜• íšŒê·€ì—ì„œ **ì˜ˆì¸¡ê°’** $\hat{y}$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•©ë‹ˆë‹¤.

> $\hat{y} = w \cdot x + b$

* $\hat{y}$ : ì˜ˆì¸¡ê°’ (prediction)
* $w$ : ê°€ì¤‘ì¹˜(Weight, ê¸°ìš¸ê¸°)
* $x$ : ì…ë ¥ ê°’
* $b$ : ì ˆí¸(Bias, yì¶•ê³¼ ë§Œë‚˜ëŠ” ê°’)

**ì—¬ëŸ¬ ê°œì˜ ì…ë ¥ ë³€ìˆ˜ê°€ ìˆì„ ê²½ìš°(ë‹¤ì¤‘ íšŒê·€)**:

> $\hat{y} = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b$

ë˜ëŠ” **ë²¡í„°/í–‰ë ¬** í˜•íƒœë¡œ:

> $\hat{y} = \mathbf{X} \cdot \mathbf{w} + b$

2.ì†ì‹¤ í•¨ìˆ˜ (MSE)

ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ë§ëŠ”ì§€ ì¸¡ì •í•˜ê¸° ìœ„í•´ **í‰ê·  ì œê³± ì˜¤ì°¨(Mean Squared Error)**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

> $\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2$

* $n$ : ë°ì´í„° ê°œìˆ˜
* $y_i$ : ì‹¤ì œê°’
* $\hat{y}_i$ : ì˜ˆì¸¡ê°’
* ê°’ì´ ì‘ì„ìˆ˜ë¡ ì˜ˆì¸¡ì´ ì˜ ëœ ê²ƒ

3.ê²½ì‚¬ í•˜ê°•ë²• (Gradient Descent)

ê²½ì‚¬ í•˜ê°•ë²•ì€ ì†ì‹¤(MSE)ì„ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ $w$ì™€ $b$ë¥¼ ì¡°ê¸ˆì”© ì¡°ì •í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

*   ê°€ì¤‘ì¹˜ $w$ ì—…ë°ì´íŠ¸ ìˆ˜ì‹

> $w := w - \alpha \cdot \frac{\partial \text{MSE}}{\partial w}$

*   ì ˆí¸ $b$ ì—…ë°ì´íŠ¸ ìˆ˜ì‹

> $b := b - \alpha \cdot \frac{\partial \text{MSE}}{\partial b}$

* $\alpha$ : í•™ìŠµë¥ (learning rate), í•œ ë²ˆì— ì´ë™í•˜ëŠ” í¬ê¸°
* $\frac{\partial \text{MSE}}{\partial w}$ : MSEë¥¼ $w$ì— ëŒ€í•´ ë¯¸ë¶„í•œ ê°’ (ê¸°ìš¸ê¸°)
* $\frac{\partial \text{MSE}}{\partial b}$ : MSEë¥¼ $b$ì— ëŒ€í•´ ë¯¸ë¶„í•œ ê°’

4.ê¸°ìš¸ê¸°(Gradient) ê³„ì‚°

**ë‹¨ì¼ ë³€ìˆ˜ì¼ ê²½ìš°**:

> $\frac{\partial \text{MSE}}{\partial w} = \frac{2}{n} \sum_{i=1}^{n} ( \hat{y}_i - y_i ) \cdot x_i$

> $\frac{\partial \text{MSE}}{\partial b} = \frac{2}{n} \sum_{i=1}^{n} ( \hat{y}_i - y_i )$

*  í¸ë¯¸ë¶„ì—ì„œ ë‚˜ì˜¨ ê³±í•˜ê¸° 2ëŠ” í•™ìŠµë¥ ì— í¬í•¨ì‹œí‚¬ ìˆ˜ ìˆëŠ” ìƒìˆ˜ì´ê¸° ë•Œë¬¸ì—, ì½”ë“œì—ì„œëŠ” ìƒëµí•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤
*   ê²½ì‚¬ í•˜ê°•ë²•ì€ **"ì •í™•í•œ ê¸°ìš¸ê¸° ê°’"**ì´ ì•„ë‹ˆë¼ **"ê¸°ìš¸ê¸°ì˜ ë°©í–¥"**ì´ ì¤‘ìš”

6.í•œ ì¤„ ìš”ì•½
* **ì˜ˆì¸¡**: $\hat{y} = w \cdot x + b$
* **í‰ê°€**: MSEë¡œ ì˜¤ì°¨ ê³„ì‚°
* **ìµœì í™”**: ê²½ì‚¬ í•˜ê°•ë²•ìœ¼ë¡œ w, bë¥¼ ë°˜ë³µ ì¡°ì •

7.np.dot() vs np.matmul() vs $*$
- $*$ ë˜ëŠ” np.multiply(): ìš”ì†Œë³„ ê³±ì…ˆ (Hadamard product).
- np.matmul(): í–‰ë ¬ ê³±ì…ˆì— íŠ¹í™”ë˜ì–´ ìˆìœ¼ë©°, ë¸Œë¡œë“œìºìŠ¤íŒ… ê·œì¹™ì´ ë‹¤ë¦…ë‹ˆë‹¤.
- np.dot(): ë” ì¼ë°˜ì ì¸ ì ê³± ì—°ì‚° (ìŠ¤ì¹¼ë¼, ë²¡í„°, í–‰ë ¬, í…ì„œ), NumPy 3.5+ì—ì„œëŠ” @ ì—°ì‚°ìë¡œ ëŒ€ì²´ ê°€ëŠ¥

8.í¸ë¯¸ë¶„ì´ë€?
- ì—¬ëŸ¬ ê°œì˜ ë³€ìˆ˜ë¥¼ ê°€ì§„ í•¨ìˆ˜ì—ì„œ í•œ ë³€ìˆ˜ë§Œ ë³€í™”ì‹œì¼°ì„ ë•Œ í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ë³´ëŠ” ê²ƒ
- ìˆ˜í•™ ê¸°í˜¸:
> $\frac{\partial f}{\partial x}$
  â†’ "í•¨ìˆ˜ $f$ë¥¼ $x$ì— ëŒ€í•´ í¸ë¯¸ë¶„"

-  ì™œ í•„ìš”í•œê°€?
    * MSEëŠ” $w$ì™€ $b$ë¼ëŠ” **ë‘ ê°œì˜ ë³€ìˆ˜**ë¥¼ ê°€ì§„ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    * $w$ë¥¼ ë°”ê¿¨ì„ ë•Œ MSEê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ë³´ë ¤ë©´
    $b$ëŠ” **ê³ ì •**í•˜ê³  $w$ë§Œ ë³€í™”ì‹œí‚¤ë©° ë¯¸ë¶„í•´ì•¼ í•©ë‹ˆë‹¤.
    * ë°˜ëŒ€ë¡œ $b$ë¥¼ ë°”ê¿¨ì„ ë•Œ ë³€í™”ë¥¼ ë³´ê³  ì‹¶ìœ¼ë©´
    $w$ëŠ” **ê³ ì •**í•˜ê³  $b$ë§Œ ë³€í™”ì‹œí‚¤ë©° ë¯¸ë¶„í•©ë‹ˆë‹¤.

9.ì²´ì¸ ë£°ì´ë€?
* **ì •ì˜**:
  í•˜ë‚˜ì˜ í•¨ìˆ˜ê°€ ì—¬ëŸ¬ í•¨ìˆ˜ë¡œ ì—°ê²°ë˜ì–´ ìˆì„ ë•Œ,
  ë°”ê¹¥ í•¨ìˆ˜ì™€ ì•ˆìª½ í•¨ìˆ˜ì˜ **ë³€í™”ëŸ‰(ë¯¸ë¶„ê°’)**ì„ ê³±í•´ì„œ ì „ì²´ ë³€í™”ëŸ‰ì„ êµ¬í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
* ê¸°í˜¸:
    > $\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$

    * $y$ëŠ” $u$ì— ì˜ì¡´í•˜ê³ ,
    * $u$ëŠ” $x$ì— ì˜ì¡´í•  ë•Œ ì ìš©

*  ì‰¬ìš´ ì˜ˆì‹œ
    >$y = (3x + 2)^2$

    *   **ê²‰ í•¨ìˆ˜**: $\square^2$  â†’ 2 Ã— (ì•ˆìª½)
    *   **ì•ˆìª½ í•¨ìˆ˜**: $3x + 2$ â†’ ë¯¸ë¶„í•˜ë©´ 3

    *   $\frac{dy}{dx} = 2(3x+2) \cdot 3= 6(3x+2)$

10.fit()ì—ì„œ Xê°€ ì•„ë‹Œ X.Të¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ : ì°¨ì› ê´€ì ì—ì„œ

ê°€ì •:
* $X$ : $(n_{\text{samples}}, n_{\text{features}})$ â†’ ì˜ˆ: (100, 3)
* $w$ : $(n_{\text{features}}, 1)$
* $y$ : $(n_{\text{samples}}, 1)$
* $y_{\text{pred}}$ : $(n_{\text{samples}}, 1)$

> `(y_pred - y)` ì˜ shape = $(n_{\text{samples}}, 1)$

ë§Œì•½ ê·¸ëƒ¥ `np.dot(X, (y_pred - y))`ë¥¼ í•˜ë©´:

* $(n_{\text{samples}}, n_{\text{features}})$ Ã— $(n_{\text{samples}}, 1)$ â†’ **ê³±ì…ˆ ë¶ˆê°€** (í–‰ë ¬ ê³± ì¡°ê±´ ë¶ˆë§Œì¡±)

ê·¸ë˜ì„œ $X$ ë¥¼ ì „ì¹˜(`X.T`)í•´ì„œ:

> $X.T$ : $(n_{\text{features}}, n_{\text{samples}})$

* $X.T$ Ã— `(y_pred - y)` : $(n_{\text{features}}, 1)$ â†’ ê° featureë³„ ê¸°ìš¸ê¸° ê³„ì‚° ê°€ëŠ¥


## 3. Logistic Regression
- ì¶œì²˜: [How to implement Logistic Regression from scratch with Python](https://www.youtube.com/watch?v=YYEJ_GUguHw)


### 3.1 LogisticRegression.py - ë¡œì§€ìŠ¤í‹± íšŒê·€ í´ë˜ìŠ¤ êµ¬í˜„

1.ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜
```python
def sigmoid(x):
    return 1/(1+np.exp(-x))
```
- ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ í•µì‹¬ì…ë‹ˆë‹¤
- ì–´ë–¤ ì‹¤ìˆ˜ê°’ì´ë“  0ê³¼ 1 ì‚¬ì´ì˜ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤
- Sì ëª¨ì–‘ì˜ ê³¡ì„ ì„ ê·¸ë¦¬ë©°, ì´ì§„ ë¶„ë¥˜ì— ì™„ë²½í•©ë‹ˆë‹¤
- np.exp()ëŠ” ì¸ìê°€ ìŠ¤ì¹¼ë¼ì´ë©´ ìŠ¤ì¹¼ë¼ë¥¼, ë²¡í„°ì´ë©´ ë²¡í„°ë¥¼ ë°˜í™˜

2.í´ë˜ìŠ¤ ì´ˆê¸°í™”
```python
def __init__(self, lr=0.001, n_iters=1000):
    self.lr = lr          # í•™ìŠµë¥  (learning rate)
    self.n_iters = n_iters # ë°˜ë³µ íšŸìˆ˜
    self.weights = None    # ê°€ì¤‘ì¹˜
    self.bias = None       # í¸í–¥
```
- `lr`: í•™ìŠµë¥ ì´ í´ìˆ˜ë¡ ë¹ ë¥´ê²Œ í•™ìŠµí•˜ì§€ë§Œ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- `n_iters`: ê²½ì‚¬í•˜ê°•ë²•ì„ ëª‡ ë²ˆ ë°˜ë³µí• ì§€ ì •í•©ë‹ˆë‹¤

3.ëª¨ë¸ í›ˆë ¨ (fit ë©”ì†Œë“œ)
```python
def fit(self, X, y):
    n_samples, n_features = X.shape  # ìƒ˜í”Œ ìˆ˜, íŠ¹ì„± ìˆ˜
    self.weights = np.zeros(n_features)  # ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
    self.bias = 0                        # í¸í–¥ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”
```

4.**ê²½ì‚¬í•˜ê°•ë²• ë°˜ë³µ:**
```python
for _ in range(self.n_iters):
    # 1. ì„ í˜• ì˜ˆì¸¡ê°’ ê³„ì‚°
    linear_pred = np.dot(X, self.weights) + self.bias
    
    # 2. ì‹œê·¸ëª¨ì´ë“œë¡œ í™•ë¥  ë³€í™˜
    predictions = sigmoid(linear_pred)
    
    # 3. ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° (ë¯¸ë¶„ê°’)
    dw = (1/n_samples) * np.dot(X.T, (predictions - y))
    db = (1/n_samples) * np.sum(predictions-y)
    
    # 4. ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ì—…ë°ì´íŠ¸
    self.weights = self.weights - self.lr*dw
    self.bias = self.bias - self.lr*db
```

ì´ ê³¼ì •ì€ **ê²½ì‚¬í•˜ê°•ë²•**ìœ¼ë¡œ:
- í˜„ì¬ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤
- ì´ ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ê¸ˆì”© ì¡°ì •í•©ë‹ˆë‹¤
- ì´ë¥¼ ë°˜ë³µí•˜ì—¬ ìµœì ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤

5.ì˜ˆì¸¡ (predict ë©”ì†Œë“œ)

```python
def predict(self, X):
    linear_pred = np.dot(X, self.weights) + self.bias  # ì„ í˜• ê³„ì‚°
    y_pred = sigmoid(linear_pred)                      # í™•ë¥ ë¡œ ë³€í™˜
    class_pred = [0 if y<=0.5 else 1 for y in y_pred] # 0 ë˜ëŠ” 1ë¡œ ë¶„ë¥˜
    return class_pred
```
- X : ì¼ë°˜ì ìœ¼ë¡œ (n_samples, n_features) í˜•íƒœì˜ 2ì°¨ì› ë°°ì—´
- self.weights : (n_features,) í˜•íƒœì˜ 1ì°¨ì› ë°°ì—´
- np.dot(X, self.weights)ì˜ ê²°ê³¼ëŠ” (n_samples,) í˜•íƒœì˜ 1ì°¨ì› ë°°ì—´
- í™•ë¥ ì´ 0.5 ì´ìƒì´ë©´ í´ë˜ìŠ¤ 1, ë¯¸ë§Œì´ë©´ í´ë˜ìŠ¤ 0ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤

### 3.2 train.py - ëª¨ë¸ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸

1.ë°ì´í„° ì¤€ë¹„
```python
bc = datasets.load_breast_cancer()  # ìœ ë°©ì•” ë°ì´í„°ì…‹ ë¡œë“œ
X, y = bc.data, bc.target          # íŠ¹ì„±ê³¼ ë¼ë²¨ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)
```
- ìœ ë°©ì•” ì§„ë‹¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (ì•…ì„±/ì–‘ì„± ì´ì§„ ë¶„ë¥˜)
- ë°ì´í„°ë¥¼ 80% í›ˆë ¨ìš©, 20% í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤

2.ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
```python
clf = LogisticRegression(lr=0.01)  # í•™ìŠµë¥  0.01ë¡œ ëª¨ë¸ ìƒì„±
clf.fit(X_train,y_train)           # ëª¨ë¸ í›ˆë ¨
y_pred = clf.predict(X_test)       # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
```

3.ì •í™•ë„ ê³„ì‚°
```python
def accuracy(y_pred, y_test):
    return np.sum(y_pred==y_test)/len(y_test)
```
- ì˜ˆì¸¡ì´ ë§ì€ ê°œìˆ˜ë¥¼ ì „ì²´ ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤

* í•µì‹¬ ê°œë… ì •ë¦¬
1. **ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜**ì…ë‹ˆë‹¤ - ì—°ì†ì ì¸ ê°’ì´ ì•„ë‹Œ ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤
2. **ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜**ë¡œ í™•ë¥ ì„ ê³„ì‚°í•˜ì—¬ 0ê³¼ 1 ì‚¬ì´ ê°’ì„ ë§Œë“­ë‹ˆë‹¤
3. **ê²½ì‚¬í•˜ê°•ë²•**ìœ¼ë¡œ ì˜¤ì°¨ë¥¼ ì ì§„ì ìœ¼ë¡œ ì¤„ì—¬ë‚˜ê°‘ë‹ˆë‹¤
4. **í•™ìŠµë¥ **ì´ ì¤‘ìš”í•©ë‹ˆë‹¤ - ë„ˆë¬´ í¬ë©´ ë°œì‚°í•˜ê³ , ë„ˆë¬´ ì‘ìœ¼ë©´ í•™ìŠµì´ ëŠë¦½ë‹ˆë‹¤

### 3.3 scikit-learnì„ ì‚¬ìš©í•œ ë¡œì§€ìŠ¤í‹± íšŒê·€ êµ¬í˜„

**1. ë°ì´í„° ì¤€ë¹„**
- `train_test_split`ì—ì„œ `stratify=y` ì˜µì…˜ìœ¼ë¡œ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ìœ ì§€í•©ë‹ˆë‹¤
- `StandardScaler`ë¡œ íŠ¹ì„±ë“¤ì„ í‘œì¤€í™”í•©ë‹ˆë‹¤ (í‰ê·  0, í‘œì¤€í¸ì°¨ 1)

    ```python
    import numpy as np
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
    from sklearn import datasets
    import matplotlib.pyplot as plt
    import seaborn as sns

    # 1. ë°ì´í„° ë¡œë“œ
    print("=== ë°ì´í„° ë¡œë“œ ===")
    bc = datasets.load_breast_cancer()
    X, y = bc.data, bc.target

    print(f"ë°ì´í„° í˜•íƒœ: {X.shape}")
    print(f"íŠ¹ì„± ìˆ˜: {X.shape[1]}")
    print(f"í´ë˜ìŠ¤ ë¶„í¬: ì•…ì„±={np.sum(y==0)}, ì–‘ì„±={np.sum(y==1)}")

    # 2. ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"\ní›ˆë ¨ ë°ì´í„°: {X_train.shape}")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")

    # 3. íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (ì„ íƒì‚¬í•­ì´ì§€ë§Œ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    ```
**2. ëª¨ë¸ ìƒì„±**
- `LogisticRegression` í´ë˜ìŠ¤ë¥¼ ë°”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤
- `max_iter=1000`ìœ¼ë¡œ ì¶©ë¶„í•œ ë°˜ë³µ íšŸìˆ˜ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤
- `random_state=42`ë¡œ ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ë§Œë“­ë‹ˆë‹¤
    ```python
    # 4. ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
    print("\n=== ëª¨ë¸ í›ˆë ¨ ===")
    model = LogisticRegression(
        random_state=42,
        max_iter=1000  # ìˆ˜ë ´ì„ ìœ„í•œ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
    )
    ```
**3. í›ˆë ¨ê³¼ ì˜ˆì¸¡**
- `.fit()`ìœ¼ë¡œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤
- `.predict()`ë¡œ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤
- `.predict_proba()`ë¡œ ê° í´ë˜ìŠ¤ì˜ í™•ë¥ ì„ êµ¬í•©ë‹ˆë‹¤
    ```python
    # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¡œ í›ˆë ¨
    model.fit(X_train_scaled, y_train)
    print("ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")

    # 5. ì˜ˆì¸¡
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)  # í™•ë¥  ì˜ˆì¸¡
    ```

**4. ì„±ëŠ¥ í‰ê°€**
- `accuracy_score`ë¡œ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤
- `classification_report`ë¡œ ì •ë°€ë„, ì¬í˜„ìœ¨, F1-scoreë¥¼ í™•ì¸í•©ë‹ˆë‹¤
- `confusion_matrix`ë¡œ í˜¼ë™í–‰ë ¬ì„ ë§Œë“­ë‹ˆë‹¤
    ```python
    # 6. ëª¨ë¸ í‰ê°€
    print("\n=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ì •í™•ë„: {accuracy:.4f}")

    # ìƒì„¸í•œ ë¶„ë¥˜ ë¦¬í¬íŠ¸
    print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(y_test, y_pred, target_names=['ì•…ì„±', 'ì–‘ì„±']))
    ```

**5. ì‹œê°í™”**
- í˜¼ë™í–‰ë ¬ì„ íˆíŠ¸ë§µìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤
- ì˜ˆì¸¡ í™•ë¥ ì˜ ë¶„í¬ë¥¼ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤
    ```python
    # 7. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
    plt.figure(figsize=(12, 5))

    plt.rcParams['font.family'] = 'AppleGothic'  # Mac ê¸°ë³¸ í•œê¸€ í°íŠ¸
    plt.rcParams['axes.unicode_minus'] = False   # ìŒìˆ˜ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

    # í˜¼ë™ í–‰ë ¬
    plt.subplot(1, 2, 1)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['ì•…ì„±', 'ì–‘ì„±'], 
                yticklabels=['ì•…ì„±', 'ì–‘ì„±'])
    plt.title('í˜¼ë™ í–‰ë ¬')
    plt.ylabel('ì‹¤ì œ')
    plt.xlabel('ì˜ˆì¸¡')

    # í™•ë¥  ë¶„í¬
    plt.subplot(1, 2, 2)
    plt.hist(y_pred_proba[:, 1], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    plt.title('ì–‘ì„± í´ë˜ìŠ¤ ì˜ˆì¸¡ í™•ë¥  ë¶„í¬')
    plt.xlabel('ì˜ˆì¸¡ í™•ë¥ ')
    plt.ylabel('ë¹ˆë„')
    plt.axvline(x=0.5, color='red', linestyle='--', label='ê²°ì • ê²½ê³„ (0.5)')
    plt.legend()

    plt.tight_layout()
    plt.show()
    ```

**6. íŠ¹ì„± ì¤‘ìš”ë„**
- ëª¨ë¸ì˜ ê³„ìˆ˜(`coef_`)ë¥¼ ë¶„ì„í•˜ì—¬ ì–´ë–¤ íŠ¹ì„±ì´ ì¤‘ìš”í•œì§€ í™•ì¸í•©ë‹ˆë‹¤
    ```python
    # 8. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ê³„ìˆ˜ì˜ ì ˆëŒ“ê°’ìœ¼ë¡œ íŒë‹¨)
    print("\n=== íŠ¹ì„± ì¤‘ìš”ë„ ===")
    feature_importance = pd.DataFrame({
        'íŠ¹ì„±ëª…': bc.feature_names,
        'ê³„ìˆ˜': model.coef_[0],
        'ì ˆëŒ“ê°’': np.abs(model.coef_[0])
    }).sort_values('ì ˆëŒ“ê°’', ascending=False)

    print("ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
    print(feature_importance.head(10))
    ```
**7. ì‹¤ì œ í™œìš©**
- ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì˜ˆì œë¥¼ í¬í•¨í•©ë‹ˆë‹¤
- í™•ë¥ ê³¼ ìµœì¢… ë¶„ë¥˜ ê²°ê³¼ë¥¼ ëª¨ë‘ ë³´ì—¬ì¤ë‹ˆë‹¤

    ```python
    # 9. ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡ ì˜ˆì œ
    print("\n=== ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡ ===")
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œ ì‚¬ìš©
    new_sample = X_test_scaled[0:1]  # 2D ë°°ì—´ í˜•íƒœ ìœ ì§€
    prediction = model.predict(new_sample)[0]
    probability = model.predict_proba(new_sample)[0]

    print(f"ì˜ˆì¸¡ ê²°ê³¼: {'ì–‘ì„±' if prediction == 1 else 'ì•…ì„±'}")
    print(f"ê° í´ë˜ìŠ¤ë³„ í™•ë¥ :")
    print(f"  ì•…ì„± í™•ë¥ : {probability[0]:.4f}")
    print(f"  ì–‘ì„± í™•ë¥ : {probability[1]:.4f}")
    print(f"ì‹¤ì œ ì •ë‹µ: {'ì–‘ì„±' if y_test[0] == 1 else 'ì•…ì„±'}")

    # 10. ëª¨ë¸ íŒŒë¼ë¯¸í„° í™•ì¸
    print("\n=== ëª¨ë¸ íŒŒë¼ë¯¸í„° ===")
    print(f"ì ˆí¸(bias): {model.intercept_[0]:.4f}")
    print(f"ê³„ìˆ˜ ê°œìˆ˜: {len(model.coef_[0])}")
    print(f"ìˆ˜ë ´ ì—¬ë¶€: {model.n_iter_[0]}ë²ˆ ë°˜ë³µ í›„ ìˆ˜ë ´")
```