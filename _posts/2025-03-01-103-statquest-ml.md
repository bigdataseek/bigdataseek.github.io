---
title: 4차시 3 :StatQuest(Machine Learning)
layout: single
classes: wide
categories:
  - Machine Learning
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 1. 머신러닝이란 무엇인가요?

### 1.1  **머신러닝이란 무엇인가요? (Machine Learning: 예측과 분류)**

머신러닝을 **예측(predictions)**과 **분류(classifications)**를 수행하는 것에 대한 모든 것이라고 정의합니다. 즉, 데이터를 기반으로 미래를 예측하거나 어떤 대상을 특정 범주로 나누는 것이 머신러닝의 주된 목표입니다.

1. 머신러닝의 예시: 의사결정 트리 (분류) \\
가장 먼저 소개되는 예시는 **의사결정 트리(Decision Tree)**입니다.
*   이 트리는 사람들이 "StatQuest"를 좋아할지 여부를 **분류**하는 데 사용됩니다.
*   질문(예: "재미있는 노래를 좋아하나요?", "머신러닝에 관심이 있나요?")에 대한 답변에 따라 "StatQuest를 좋아할 사람" 또는 "좋아하지 않을 사람"으로 분류합니다.
*   이처럼 의사결정 트리는 간단한 머신러닝 방법 중 하나이며, 이를 이해하는 것만으로도 머신러닝을 이해하는 데 큰 도움이 됩니다.

2. 머신러닝의 예시: 선형 회귀 (예측) \\
두 번째 예시는 **예측**에 대한 것입니다.
*   100미터 달리기 속도와 얌(yam) 섭취량 간의 가상 데이터를 사용합니다. 이 데이터는 얌을 더 많이 먹을수록 100미터 달리기 속도가 더 빨라지는 경향을 보여줍니다.
*   이러한 데이터의 **경향(trend)**을 보여주기 위해 **검은색 선(black line)**을 데이터에 맞출 수 있습니다.
*   이 검은색 선을 사용하여 특정 얌 섭취량을 가진 사람이 얼마나 빨리 달릴지 **예측**할 수 있습니다. 이 선은 예측을 할 수 있기 때문에 일종의 머신러닝이라고 할 수 있습니다.

### 1.2. **머신러닝의 주요 아이디어**

이제 머신러닝에서 예측과 분류를 수행하는 데 필요한 몇 가지 중요한 개념들을 살펴보겠습니다.

1. 훈련 데이터(Training Data)와 테스트 데이터(Testing Data)
*   머신러닝 용어로, 우리가 모델을 만드는 데 사용하는 원래 데이터를 **훈련 데이터(Training Data)**라고 합니다. 위 예시에서 검은색 선은 훈련 데이터에 맞춰졌습니다.
*   때로는 훈련 데이터에 더 잘 맞는 복잡한 곡선(예: 초록색 물결선)을 만들 수도 있습니다. 하지만 머신러닝의 목표는 **예측**을 하는 것이기 때문에, 이 곡선이 검은색 선보다 더 나은지 평가할 방법이 필요합니다.
*   이를 위해 우리는 **테스트 데이터(Testing Data)**를 사용합니다. 테스트 데이터는 새로운 사람들로부터 수집된, 모델이 이전에 보지 못한 데이터입니다.

2. 머신러닝 방법 평가하기: 오류 측정
*   테스트 데이터를 사용하여 검은색 선과 초록색 물결선이 얼마나 잘 예측하는지 비교합니다.
*   각 테스트 데이터 포인트에 대해, 모델이 예측한 값(예: 예측 속도)과 실제 값(예: 실제 속도) 사이의 **거리(distance)**를 측정합니다.
*   이러한 거리들의 **합계**를 구하여 각 모델의 성능을 평가합니다.
*   예시에서는 초록색 물결선이 훈련 데이터에는 더 잘 맞았지만, 테스트 데이터에 대한 거리의 합계는 검은색 선이 더 작았습니다. 이는 검은색 선이 새로운 데이터에 대해 더 나은 예측을 했다는 것을 의미합니다.
*   따라서 예측을 위해 두 모델 중 하나를 선택해야 한다면, 우리는 검은색 선을 선택할 것입니다.

3. 편향-분산 트레이드오프 (Bias-Variance Trade-off)
*   훈련 데이터에 너무 잘 맞지만 새로운 데이터에 대한 예측 성능이 떨어지는 현상을 **편향-분산 트레이드오프(bias-variance trade-off)**라고 부릅니다. 이는 훈련 데이터에만 과하게 맞춰져 일반화 성능이 떨어지는 **과적합(overfitting)**과 관련이 있습니다.

4. 모델의 복잡성보다 중요한 것: 테스트 데이터 성능
*   딥러닝이나 컨볼루션 신경망과 같은 **고급 머신러닝 방법**들이 많이 있습니다. 매년 새롭고 흥미로운 방법들이 등장합니다.
*   하지만 어떤 방법을 사용하든 가장 중요한 것은 그 방법이 얼마나 **화려한가**가 아니라, **테스트 데이터에서 얼마나 잘 작동하는지**입니다.
*   우리는 테스트 데이터를 사용하여 우리의 필요에 가장 잘 맞는 방법을 결정합니다.

5. 훈련 데이터와 테스트 데이터 분할
*   훈련 데이터와 테스트 데이터를 어떻게 나눌지는 중요한 질문입니다. 영상에서는 처음에는 임의로 나눴지만, 실제로 어떤 샘플을 훈련 데이터로 사용하고 어떤 샘플을 테스트 데이터로 사용할지 결정하는 방법들이 있다고 언급합니다.

**머신러닝은 예측과 분류를 다루는 학문입니다.** 수많은 화려한 머신러닝 방법들이 있지만, 가장 중요한 것은 그 방법들이 **새로운 데이터(테스트 데이터)에서 얼마나 잘 작동하는지**를 평가하여 우리에게 가장 적합한 모델을 선택하는 것입니다.

## 2. 교차 검증(Cross-validation): 머신러닝 모델의 성능을 평가하는 효과적인 방법

### **2.1 교차 검증의 필요성**
우리는 흉통이나 혈액 순환 같은 변수를 사용하여 심장병 예측과 같은 머신러닝 모델을 만들 수 있습니다. 로지스틱 회귀, K-최근접 이웃, 서포트 벡터 머신 등 다양한 머신러닝 방법이 있지만, 어떤 방법이 가장 좋은지, 그리고 실제 상황에서 얼마나 잘 작동할지 어떻게 알 수 있을까요? **교차 검증은 이러한 다양한 머신러닝 방법들을 비교하고, 실제 성능을 파악하는 데 도움을 줍니다**.

### **2.2 머신러닝의 두 가지 주요 단계: 훈련과 테스트**
머신러닝 모델을 만들기 위해 수집한 데이터로는 두 가지를 해야 합니다:
*   **훈련(Training)**: 데이터를 사용하여 머신러닝 방법의 **매개변수를 추정**하는 것입니다. 예를 들어, 로지스틱 회귀의 경우 데이터로 곡선의 형태를 추정합니다.
*   **테스트(Testing)**: 머신러닝 방법이 얼마나 잘 작동하는지 **평가**하는 것입니다. 즉, 모델이 새로운 데이터를 얼마나 정확하게 분류하는지 확인합니다.

### **2.3 데이터 분할의 중요성**
모든 데이터를 훈련에만 사용하는 것은 좋지 않습니다. 그렇게 하면 테스트할 데이터가 남지 않기 때문입니다. 훈련과 테스트에 동일한 데이터를 재사용하는 것 또한 나쁜 생각입니다. 왜냐하면 모델이 훈련되지 않은 데이터에 대해 얼마나 잘 작동할지 알아야 하기 때문입니다.
조금 더 나은 방법은 데이터의 첫 75%를 훈련에 사용하고, 나머지 25%를 테스트에 사용하는 것입니다. 그러나 어떤 방식으로 데이터를 분할하는 것이 최적인지는 알기 어렵습니다.

### **2.4 교차 검증의 작동 방식**
교차 검증은 데이터 분할 방식에 대해 너무 걱정하는 대신, **모든 데이터 블록을 한 번씩 테스트에 사용하고 그 결과를 종합합니다**.
예를 들어, 데이터를 4개의 블록으로 나눈다고 가정해봅시다 (이를 **4겹 교차 검증**이라고 합니다):
1.  첫 세 블록으로 모델을 **훈련**하고, 마지막 블록으로 모델을 **테스트**합니다. 테스트 데이터로 모델이 얼마나 잘 수행되었는지 기록합니다.
2.  다음에는 다른 조합의 블록을 훈련에 사용하고, 또 다른 블록을 테스트에 사용합니다. 이 과정에서 모델이 테스트 데이터로 얼마나 잘 수행되었는지 계속 기록합니다.
3.  결국, **모든 데이터 블록이 한 번씩 테스트에 사용됩니다**.
이렇게 함으로써 각 머신러닝 방법이 테스트 데이터 세트에서 얼마나 잘 작동했는지 비교할 수 있습니다. 예를 들어, 서포트 벡터 머신이 가장 분류를 잘했다면, 이 방법을 선택하게 될 것입니다.

### **2.5 다양한 교차 검증 방식**
*   **K-겹 교차 검증 (K-fold Cross-validation)**: 데이터를 K개의 블록으로 나누는 방식입니다. 예를 들어, **10겹 교차 검증(10-fold cross-validation)**은 데이터를 10개의 블록으로 나누어 사용하는 매우 흔한 방법입니다.
*   **하나 빼기 교차 검증 (Leave One Out Cross Validation)**: 극단적인 경우로, 각 개별 환자(또는 샘플)를 하나의 블록으로 간주하여 개별적으로 테스트하는 방식입니다.

### **2.6 튜닝 매개변수 최적화에도 활용**
교차 검증은 모델 비교뿐만 아니라 **튜닝 매개변수(tuning parameter)**를 찾는 데도 활용될 수 있습니다. 튜닝 매개변수는 모델이 스스로 추정하지 않고 우리가 직접 설정해야 하는 매개변수를 의미합니다. 예를 들어, 릿지 회귀(Ridge regression)에는 튜닝 매개변수가 있는데, **10겹 교차 검증을 사용하여 이 튜닝 매개변수의 최적 값을 찾을 수 있습니다**.

요약하자면, **교차 검증은 머신러닝 모델의 실제 성능을 신뢰성 있게 평가하고, 최적의 모델이나 매개변수를 선택하는 데 필수적인 기법입니다**.

## 3. 혼동 행렬(Confusion Matrix): 머신러닝 모델의 성능을 시각화하고 이해하는 핵심 도구

### **3.1 혼동 행렬의 필요성**
우리는 흉통, 혈액 순환, 동맥 막힘, 체중 등의 임상 측정값을 사용하여 **심장병 발병 여부를 예측하는 머신러닝 모델**을 만들 수 있습니다. 로지스틱 회귀, K-최근접 이웃, 랜덤 포레스트 등 다양한 머신러닝 방법이 있지만, 어떤 방법이 우리 데이터에 가장 적합한지 어떻게 결정할 수 있을까요?
이때, **혼동 행렬은 각 머신러닝 방법이 테스트 데이터에서 얼마나 잘 작동했는지를 요약하여 비교하는 데 도움을 줍니다**. 이전 대화에서 다룬 교차 검증은 이러한 훈련 및 테스트 데이터 분할에 유용하게 사용될 수 있습니다.

### **3.2 혼동 행렬의 구조**
혼동 행렬은 모델의 예측과 실제 값(정답)을 비교하여 만들어집니다.
*   **행(Row)**: 머신러닝 알고리즘이 **예측한 값**을 나타냅니다.
*   **열(Column)**: **실제 알려진 정답(known truth)**을 나타냅니다.

### **3.3 2x2 혼동 행렬의 네 가지 핵심 요소 (예: 심장병 예측)**
심장병 발병 여부(`심장병 있음` 또는 `심장병 없음`)와 같이 두 가지 범주를 예측할 때, 혼동 행렬은 네 부분으로 구성:

| | **예측: 심장병 있음 (Positive)** | **예측: 심장병 없음 (Negative)** |
| :--- | :--- | :--- |
| **실제: 심장병 있음 (Actual Positive)** | **참 양성 (TP)** | **거짓 음성 (FN)** |
| **실제: 심장병 없음 (Actual Negative)** | **거짓 양성 (FP)** | **참 음성 (TN)** |

*   **참 양성(True Positives, TP)**:
    *   **정의**: 환자가 실제로 **심장병이 있었고**, 알고리즘도 **심장병이 있다고 정확하게 예측**한 경우.
    *   **위치**: 왼쪽 상단.
    *   **의미**: 올바른 긍정 예측.

*   **참 음성(True Negatives, TN)**:
    *   **정의**: 환자가 실제로 **심장병이 없었고**, 알고리즘도 **심장병이 없다고 정확하게 예측**한 경우.
    *   **위치**: 오른쪽 하단.
    *   **의미**: 올바른 부정 예측.

*   **거짓 음성(False Negatives, FN)**:
    *   **정의**: 환자가 실제로 **심장병이 있었는데**, 알고리즘은 **심장병이 없다고 잘못 예측**한 경우.
    *   **위치**:  우측 상단.
    *   **의미**: **2종 오류(Type II error)**. 중요한 것을 놓쳤을 때 발생합니다. (예: 심장병 환자를 놓치는 경우).

*   **거짓 양성(False Positives, FP)**:
    *   **정의**: 환자가 실제로 **심장병이 없었는데**, 알고리즘은 **심장병이 있다고 잘못 예측**한 경우.
    *   **위치**: 왼쪽 하단.
    *   **의미**: **1종 오류(Type I error)**. 존재하지 않는 것을 있다고 잘못 예측한 경우.

### **3.4 혼동 행렬 해석의 예시**
랜덤 포레스트(Random Forest) 모델을 테스트 데이터에 적용했을 때, 다음과 같은 결과가 나올 수 있습니다:
*   **참 양성(TP)**: 142명 (실제 심장병 환자를 정확히 분류).
*   **참 음성(TN)**: 110명 (실제 심장병 없는 환자를 정확히 분류).
*   **거짓 음성(FN)**: 29명 (심장병이 있었지만 없다고 잘못 분류).
*   **거짓 양성(FP)**: 22명 (심장병이 없었지만 있다고 잘못 분류).

여기서 **대각선 상의 숫자(녹색 상자)**들은 알고리즘이 **정확하게 분류한 샘플의 수**를 나타내고, **대각선이 아닌 숫자(빨간색 상자)**들은 알고리즘이 **잘못 분류한 샘플의 수**를 나타냅니다.

### **3.5 모델 비교에 활용**
여러 머신러닝 방법(예: 랜덤 포레스트, K-최근접 이웃, 로지스틱 회귀)의 혼동 행렬을 비교하여 어떤 모델이 데이터에 더 적합한지 결정할 수 있습니다. 예를 들어, 랜덤 포레스트는 K-최근접 이웃보다 심장병 환자(142 대 107) 및 심장병 없는 환자(110 대 79) 모두를 더 잘 예측했으므로, 이 경우 랜덤 포레스트를 선택할 것입니다.

### **3.6 다양한 범주의 예측**
혼동 행렬의 크기는 예측하려는 범주의 수에 따라 달라집니다.
*   심장병 예측처럼 두 가지 범주가 있을 때는 2x2 혼동 행렬이 됩니다.
*   만약 영화 선호도 예측처럼 세 가지 범주(예: 트롤 2, 고어 폴리스, 쿨 애즈 아이스)가 있다면, 3x3 혼동 행렬이 됩니다.
*   네 가지 또는 그 이상의 범주가 있다면, 그에 맞춰 4x4, 40x40 등의 더 큰 혼동 행렬이 생성됩니다. 큰 혼동 행렬에서도 대각선은 올바른 예측을, 그 외의 부분은 잘못된 예측을 나타냅니다.

결론적으로, 혼동 행렬은 머신러닝 알고리즘이 무엇을 제대로 예측했고 무엇을 잘못 예측했는지를 명확하게 보여주는 핵심적인 시각화 도구입니다. 이는 모델의 성능을 이해하고, 여러 모델을 비교하며, 특정 유형의 오류(예: 거짓 음성)가 중요한 경우 모델을 최적화하는 데 매우 중요합니다

## 4. 머신러닝 기초: 민감도(Sensitivity)와 특이도(Specificity)

### **4.1 혼동 행렬(Confusion Matrix)의 이해**
민감도와 특이도를 계산하기 전에 **혼동 행렬**의 구성 요소를 아는 것이 중요합니다.
*   혼동 행렬에서 **행(rows)**은 **실제값(known truth)**을 나타내고, **열(columns)**은 **예측값**을 나타냅니다.

| | **예측: 심장병 있음 (Positive)** | **예측: 심장병 없음 (Negative)** |
| :--- | :--- | :--- |
| **실제: 심장병 있음 (Actual Positive)** | **참 양성 (TP)** | **거짓 음성 (FN)** |
| **실제: 심장병 없음 (Actual Negative)** | **거짓 양성 (FP)** | **참 음성 (TN)** |

두 가지 카테고리(예: 심장병 유무)가 있을 때, 혼동 행렬의 각 칸은 다음과 같이 정의됩니다:
*   **참 양성(True Positives, TP)**: 환자가 심장병을 가지고 있었고, 모델도 심장병이 있다고 정확하게 예측한 경우입니다. (좌측 상단)
*   **참 음성(True Negatives, TN)**: 환자가 심장병이 없었고, 모델도 심장병이 없다고 정확하게 예측한 경우입니다. (우측 하단)
*   **거짓 음성(False Negatives, FN)**: 환자가 심장병을 가지고 있었지만, 모델이 심장병이 없다고 잘못 예측한 경우입니다. (우측 상단)
*   **거짓 양성(False Positives, FP)**: 환자가 심장병이 없었지만, 모델이 심장병이 있다고 잘못 예측한 경우입니다. (좌측 하단)

### **4.2 민감도(Sensitivity) 계산 및 해석**
*   **민감도**는 **심장병이 있는 환자 중 실제로 모델이 심장병이 있다고 정확하게 식별한 비율**을 알려줍니다.
*   **계산식**: **민감도 = 참 양성 (TP) / (참 양성 (TP) + 거짓 음성 (FN))**.
*   **예시**:
    *   로지스틱 회귀 모델에서 TP가 139, FN이 32일 때, 민감도는 139 / (139 + 32) = 0.81이 됩니다. 이는 심장병 환자의 81%가 모델에 의해 정확하게 식별되었다는 의미입니다.
    *   **민감도가 높을수록 "양성(positives)"을 정확하게 식별하는 능력이 좋다**는 것을 나타냅니다.

### **4.3 특이도(Specificity) 계산 및 해석**
*   **특이도**는 **심장병이 없는 환자 중 실제로 모델이 심장병이 없다고 정확하게 식별한 비율**을 알려줍니다.
*   **계산식**: **특이도 = 참 음성 (TN) / (참 음성 (TN) + 거짓 양성 (FP))**.
*   **예시**:
    *   로지스틱 회귀 모델에서 TN이 112, FP가 20일 때, 특이도는 112 / (112 + 20) = 0.85가 됩니다. 이는 심장병이 없는 사람들의 85%가 모델에 의해 정확하게 식별되었다는 의미입니다.
    *   **특이도가 높을수록 "음성(negatives)"을 정확하게 식별하는 능력이 좋다**는 것을 나타냅니다.

### **4.4 머신러닝 모델 비교 및 선택**
민감도와 특이도는 여러 머신러닝 모델의 성능을 비교하고 선택하는 데 유용합니다.
*   예를 들어, 로지스틱 회귀 모델과 랜덤 포레스트 모델을 비교했을 때:
    *   **민감도**를 통해 랜덤 포레스트가 심장병 환자(양성)를 식별하는 데 약간 더 우수하다는 것을 알 수 있다.
    *   **특이도**를 통해 로지스틱 회귀가 심장병이 없는 환자(음성)를 식별하는 데 약간 더 우수하다는 것을 확인.
*   **모델 선택**:
    *   **음성 환자(심장병이 없는 환자)를 정확하게 식별하는 것이 더 중요하다면** 로지스틱 회귀 모델을 선택
    *   **양성 환자(심장병이 있는 환자)를 정확하게 식별하는 것이 더 중요하다면** 랜덤 포레스트 모델을 선택
    *   데이터로 무엇을 하는 것이 가장 중요한지에 따라 **민감도** 또는 **특이도** 중 어느 것에 더 중점을 둘지 결정해야

### **4.5 3개 이상의 카테고리를 가진 혼동 행렬에서의 민감도/특이도**
혼동 행렬이 3개 이상의 행과 열을 가질 경우(즉, 예측해야 할 카테고리가 3개 이상일 때) 계산 방식에 큰 차이가 있다.
*   전체 행렬에 대한 단일 민감도와 특이도 값은 존재하지 않습니다.
*   대신, **각 카테고리별로 별도의 민감도와 특이도를 계산**해야 합니다.

| **실제 선호 영화** / **예측 선호 영화** | **"Troll 2"** | **"Gore Police"** | **"Cool as Ice"** | **총 실제 인원** |
|:---:|:---:|:---:|:---:|:---:|
| **"Troll 2"** | **12 (TP_T2)** | 4 | 191 | 207 |
| **"Gore Police"** | 5 | **35 (TP_GP)** | 10 | 50 |
| **"Cool as Ice"** | 195 | 10 | **25 (TP_CAI)** | 230 |
| **총 예측 인원** | 212 | 49 | 226 | **487 (총 인원)** |

*   **예시 (영화 선호도 예측)**: "Troll 2", "Gore Police", "Cool as Ice" 세 가지 영화 중 가장 좋아하는 영화를 예측시:
    *   **"Troll 2"에 대한 민감도**: 12(TP) / (12(TP) + 195(FN)) = 0.058. 이는 "Troll 2"를 가장 좋아한 사람 중 6%만이 정확하게 식별되었다는 의미입니다.
    *   **"Troll 2"에 대한 특이도**: 80(TN) / (80(TN) + 200(FP)) = 0.286. 이는 "Troll 2"가 아닌 다른 영화를 더 좋아한 사람 중 52%가 정확하게 식별되었다는 의미입니다.
    *   "Gore Police"와 "Cool as Ice"에 대해서도 동일한 방식으로 각기 다른 민감도와 특이도를 계산해야

요약하자면, **민감도와 특이도는 머신러닝 모델의 성능을 평가하고, 특히 모델이 양성 및 음성 클래스를 얼마나 잘 분류하는지를 이해하는 데 필수적인 지표**입니다. AI 분야에 입문하는 학생들은 이 두 개념을 통해 모델의 강점과 약점을 파악하고, 특정 문제에 가장 적합한 모델을 선택하는 데 중요한 통찰력을 얻을 수 있습니다.

## 5. 머신러닝 기초: 민감도(Sensitivity), 특이도(Specificity), 정밀도(Precision), 재현율(Recall)

### **5.1 민감도(Sensitivity)**
*   **민감도**는 **실제 양성(actual positives) 중 정확하게 예측된 비율**입니다. 즉, 실제로 '참'인 것들을 모델이 얼마나 잘 '참'이라고 맞췄는지를 나타냅니다.

> $\text{민감도 (Sensitivity)} = \text{재현율 (Recall)} = \frac{\text{TP}}{\text{TP} + \text{FN}}$

### **5.2 특이도(Specificity)**
*   **특이도**는 **실제 음성(actual negatives) 중 정확하게 예측된 비율**입니다. 즉, 실제로 '거짓'인 것들을 모델이 얼마나 잘 '거짓'이라고 맞췄는지를 나타냅니다.

> $\text{특이도 (Specificity)} = \frac{\text{TN}}{\text{TN} + \text{FP}}$

### **5.3 정밀도(Precision)**
*   **정밀도**는 **예측된 양성(predicted positives) 중 정확하게 예측된 비율**입니다. 이는 모델이 '참'이라고 예측한 것들 중에서 실제로 얼마나 많은 것들이 '참'이었는지를 보여줍니다.

> $\text{정밀도 (Precision)} = \frac{\text{TP}}{\text{TP} + \text{FP}}$

### **5.4 재현율(Recall)**
*   **재현율**은 **민감도와 동일한 개념**입니다. 따라서 재현율 역시 **실제 양성(actual positives) 중 정확하게 예측된 비율**을 의미합니다.

## 6. 머신러닝의 기본: 편향(Bias)과 분산(Variance)

### 6.1  **문제 설정 및 데이터 준비**
*   우리의 목표는 생쥐의 체중으로 키를 **예측**하는 것입니다. 체중이 늘면 키도 커지지만, 특정 체중 이후에는 더 이상 키가 커지지 않고 살만 찐다는 실제 관계(진정한 관계)가 존재합니다.
*   하지만 우리는 이 진정한 관계를 정확히 나타내는 수학적 공식을 알지 못합니다.
*   머신러닝 모델을 훈련하고 평가하기 위해, 데이터는 **훈련 세트(training set)**와 **테스트 세트(testing set)**로 나뉩니다. 훈련 세트(파란색 점)로 모델을 학습시키고, 테스트 세트(녹색 점)로 모델의 성능을 평가합니다.

### 6.2  **편향(Bias)의 이해**
*   **편향**은 머신러닝 방법이 **진정한 관계를 포착하지 못하는 능력 부족**을 의미합니다.
*   **높은 편향(High Bias)의 예시: 선형 회귀(직선)**
    *   선형 회귀는 데이터에 **직선**을 맞춥니다. 이 직선은 아무리 잘 맞춰도 진정한 관계의 **곡선 형태를 유연하게 표현할 수 없습니다**. 따라서 직선은 진정한 관계를 완벽하게 포착할 수 없으며, 이는 상대적으로 **높은 편향**을 가집니다.
*   **낮은 편향(Low Bias)의 예시: 구불구불한 선**
    *   반면, **구불구불한 선(squiggly line)**을 맞추는 머신러닝 방법은 **매우 유연하여** 진정한 관계의 곡선에 따라 훈련 세트에 완벽하게 적응할 수 있습니다. 이러한 유연성 덕분에 이 방법은 **매우 낮은 편향**을 가집니다.

### 6.3  **분산(Variance)의 이해**
*   **분산**은 **다른 데이터 세트에 대한 예측 결과의 차이**를 의미합니다. 즉, 모델이 새로운 데이터에 얼마나 일관성 있게 작동하는지를 나타냅니다.
*   **높은 분산(High Variance)의 예시: 구불구불한 선**
    *   구불구불한 선은 훈련 세트에는 완벽하게 맞춰서 잔차 제곱합(sums of squares)이 거의 0에 가깝게 나옵니다. 그러나 **테스트 세트에서는 성능이 매우 나빴습니다**.
    *   이처럼 **다른 데이터 세트(훈련 세트 vs. 테스트 세트) 간에 적합도(fit)의 큰 차이**가 발생하면 **높은 분산**을 가졌다고 말합니다. 이는 구불구불한 선이 미래의 데이터 세트에서 얼마나 잘 수행될지 예측하기 어렵다는 것을 의미합니다.
*   **낮은 분산(Low Variance)의 예시: 직선**
    *   직선은 훈련 세트에 아주 잘 맞지는 않지만, 테스트 세트에서는 구불구불한 선보다 더 나은 성능을 보였습니다.
    *   직선의 잔차 제곱합은 다른 데이터 세트(훈련 세트와 테스트 세트)에 대해 **매우 유사**하며, 이는 **상대적으로 낮은 분산**을 의미합니다. 즉, 직선은 "훌륭한" 예측이 아닌 "괜찮은" 예측을 제공할지라도 **일관성 있게 좋은 예측**을 제공합니다.

### 6.4  **과적합(Overfitting)**
*   **과적합**은 모델이 **훈련 세트에는 아주 잘 맞지만, 테스트 세트에는 잘 맞지 않을 때** 발생합니다.
*   위의 예시에서 **구불구불한 선은 훈련 세트에 과적합**된 것입니다.

### 6.5  **이상적인 머신러닝 알고리즘**
*   이상적인 머신러닝 알고리즘은 다음 두 가지 특성을 가집니다.
    *   **낮은 편향**: 진정한 관계를 정확하게 모델링할 수 있어야 합니다.
    *   **낮은 분산**: 다른 데이터 세트에 걸쳐 일관된 예측을 생성하여 예측 결과가 안정적이어야 합니다.
*   이를 달성하기 위해서는 **너무 단순한 모델과 너무 복잡한 모델 사이의 "적절한 지점(sweet spot)"**을 찾아야 합니다.

### 6.6  **적절한 지점을 찾는 방법**
*   단순하고 복잡한 모델 사이의 적절한 균형점을 찾기 위한 세 가지 일반적인 방법으로 **정규화(regularization)**, **부스팅(boosting)**, **배깅(bagging)**이 언급됩니다.
    - 정규화: 모델의 복잡성에 직접적으로 '페널티'를 부여하여 균형을 잡습니다.
    - 부스팅:여러 개의 단순한(약한) 모델을 순차적으로 연결하여 지능적인 복잡함을 구축
    - 배깅: 여러 개의 단순한 모델을 결합하여 안정적인 복잡함을 구현합니다.

요약하자면, 머신러닝에서 모델을 만들 때 **편향(모델이 진정한 관계를 놓치는 정도)**과 **분산(모델이 새로운 데이터에 얼마나 일관적인지)**이라는 두 가지 중요한 요소를 고려해야 합니다. 이 둘 사이의 균형을 잘 잡는 것이 과적합을 피하고 실제 세계 데이터에서 좋은 성능을 내는 모델을 만드는 핵심입니다.

## 7. 데이터 과학을 위한 엔트로피(Entropy)

데이터 과학의 중요한 개념인 **엔트로피(Entropy)**를 쉽게 이해할 수 있도록 설명합니다. 엔트로피는 **분류 트리(classification trees)**를 만드는 데 사용되거나, 두 사물 간의 관계를 수량화하는 **상호 정보(Mutual Information)**의 기반이 되며, **상대 엔트로피(Relative Entropy, 쿨백-라이블러 거리)** 및 **교차 엔트로피(Cross Entropy)**와 같은 개념의 기초가 됩니다. 이들은 t-SNE 및 UMAP과 같은 차원 축소 알고리즘을 포함하여 다양한 분야에서 유사성과 차이를 정량화하는 데 활용됩니다.

엔트로피를 이해하기 위해 먼저 **놀라움(Surprise)**이라는 개념부터 살펴봅니다.

### 7.1  **놀라움(Surprise) 이해하기**
*   **확률과 놀라움의 관계**: 동영상은 주황색 닭과 파란색 닭의 예를 들어 설명합니다. 어떤 영역에서 주황색 닭을 고를 확률이 높으면 (예: 주황색 닭 6마리, 파란색 닭 1마리), 주황색 닭을 고르는 것은 **별로 놀랍지 않습니다**. 반대로 파란색 닭을 고르는 것은 **상대적으로 놀랍습니다**. 즉, **확률이 낮으면 놀라움은 높고, 확률이 높으면 놀라움은 낮습니다**.
*   **놀라움 계산**: 놀라움은 확률에 반비례한다는 직관이 있지만, 단순히 확률의 역수(`1/P`)를 사용하는 것은 문제가 있습니다. 예를 들어, 항상 앞면만 나오는 동전의 경우 앞면이 나올 확률은 1이지만, 놀라움은 0이어야 합니다. 그러나 `1/1`은 1이 되므로 우리가 원하는 0이 아닙니다.
*   이러한 이유로 놀라움을 계산하기 위해 **확률의 역수에 로그를 취합니다**. 일반적으로 두 가지 결과에 대한 놀라움을 계산할 때는 밑이 2인 로그(log base 2)를 사용합니다.
    *   **놀라움 = log₂(1 / 확률)** 또는 **-log₂(확률)**
    *   이 공식은 확률이 1일 때 놀라움을 0으로 만들고, 확률이 0에 가까워질수록 놀라움이 커지는 곡선을 제공
*   **연속된 사건의 놀라움**: 여러 번의 동전 던지기와 같이 연속된 사건의 전체 놀라움은 각 개별 던지기의 놀라움을 **합산**한 것과 같습니다.

### 7.2  **엔트로피(Entropy) 이해하기**
*   **엔트로피의 정의**: 엔트로피는 **한 번의 사건(예: 동전 던지기)당 예상되는 평균 놀라움의 양**입니다. 즉, 어떤 사건이 일어날 때 우리가 평균적으로 얼마나 놀랄 것인가를 나타내는 기대값입니다.
*   **계산 과정**: 100번의 동전 던지기 예시에서, 각 결과(앞면/뒷면)가 나타날 예상 횟수에 해당 놀라움 값을 곱한 후 합산합니다. 이 값을 총 동전 던지기 횟수(100)로 나누면 **사건당 평균 놀라움**, 즉 엔트로피를 얻게 됩니다.
*   **엔트로피 공식**: 이러한 과정을 통해 엔트로피는 다음과 같은 형태로 표현될 수 있습니다:
    *   $H(X) = - Σ [P(x) * log₂(P(x))]$ 
    *   여기서 P(x)는 특정 결과 x가 나올 확률입니다. 이는 클로드 섀넌(Claude Shannon)이 1948년에 발표한 표준 엔트로피 공식입니다.

### 7.3  **엔트로피로 유사성과 차이 정량화하기**
*   동영상에서는 닭들이 모여 있는 세 구역(A, B, C)의 엔트로피를 계산하여 엔트로피가 어떻게 유사성과 차이를 정량화하는지 보여줍니다.
*   **구역 A (주황 6, 파랑 1)**: 주황색 닭을 고를 확률이 매우 높고, 파란색 닭을 고를 확률은 낮다. 엔트로피 0.59
*   **구역 B (주황 1, 파랑 10)**: 파란색 닭을 고를 확률이 매우 높고, 주황색 닭을 고를 확률은 낮다. 엔트로피 0.44
*   **구역 C (주황 1, 파랑 1)**: 주황색 닭과 파란색 닭을 고를 확률이 **동일합니다 (각 0.5)**. 엔트로피는 1

*   **엔트로피의 특징**:
    *   **엔트로피가 가장 높은 경우**: 구역 C처럼 주황색 닭과 파란색 닭의 수가 **같을 때 (즉, 가장 예측하기 어려울 때)** 엔트로피가 가장 높습니다. 이는 가장 큰 불확실성 또는 다양성을 의미합니다.
    *   **엔트로피가 낮은 경우**: 구역 A와 B처럼 한 가지 색깔의 닭이 **대부분을 차지하여 결과를 예측하기 쉬울 때** 엔트로피가 낮습니다. 이는 더 높은 확실성 또는 균일성을 의미합니다.
*   결과적으로, 엔트로피를 사용하여 각 영역에서 주황색 닭과 파란색 닭의 수에 따른 **유사성 또는 차이를 수량화**

요약하자면, 엔트로피는 **어떤 사건의 결과에 대한 불확실성 또는 무질서도를 측정하는 척도**이며, 평균 놀라움의 개념을 통해 이해할 수 있습니다. 엔트로피 값이 높을수록 결과의 불확실성이 크고 다양하며, 엔트로피 값이 낮을수록 결과가 예측 가능하고 균일하다는 것을 의미합니다. 이러한 원리는 데이터 과학에서 데이터의 정보량을 측정하고, 분류 모델을 만들거나 데이터 간의 관계를 분석하는 데 광범위하게 사용됩니다.

## 8. 상호 정보(Mutual Information)

**상호 정보(Mutual Information, MI)는 두 변수 간의 관계를 수치로 나타내는 방법으로, 한 변수가 다른 변수에 대해 얼마나 많은 정보를 알려주는지 측정합니다.** 이는 AI 분야, 특히 데이터 분석 및 특성 선택(feature selection)에서 매우 유용한 개념입니다.

### **8.1 상호 정보를 사용하는 이유**
*   데이터셋에 많은 변수(특성)가 있을 때, 예측하려는 대상(예: 영화 '트롤 2'를 좋아하는지 여부)에 대해 각 변수가 얼마나 중요한지 알고 싶을 때 사용됩니다.
*   예를 들어, '팝콘을 좋아하는지 여부'와 '키' 같은 변수들이 '트롤 2'를 좋아하는지 예측하는 데 얼마나 큰 역할을 하는지 파악할 수 있습니다.
*   **R-제곱(R-squared)**과 같은 다른 관계 측정 도구는 연속형 데이터에만 작동하지만, 상호 정보는 **연속형 데이터와 이산형 데이터가 혼합된 경우에도 두 변수 간의 관계를 정량화할 수 있습니다**.

### **8.2 상호 정보의 기본 개념: 결합 확률과 주변 확률**
상호 정보를 계산하기 위해서는 **결합 확률(Joint Probability)**과 **주변 확률(Marginal Probability)**이라는 두 가지 개념을 이해해야 합니다.
*   **결합 확률**: 두 가지 사건이 동시에 발생할 확률. 예를 들어, 어떤 사람이 팝콘을 좋아하고 '트롤 2'도 좋아하는 확률.
*   **주변 확률**: 하나의 사건이 발생할 확률을 의미합니다. 예를 들어, 어떤 사람이 팝콘을 좋아하지 않을 확률.

이러한 결합 확률과 주변 확률은 표에 정리하여 쉽게 추적할 수 있습니다. 표의 가장자리(margins)에 주변 확률이 위치하기 때문에 '주변(marginal)'이라는 이름이 붙었습니다.

### 결합확률 & 주변확률 표

| | **트롤 2를 좋아함 (Y)** | **트롤 2를 좋아하지 않음 (N)** | **주변확률 (Marginal)** |
| :--- | :--- | :--- | :--- |
| **팝콘을 좋아함 (Y)** | P(팝콘 Y, 트롤 Y) | P(팝콘 Y, 트롤 N) | **P(팝콘 Y)** |
| **팝콘을 좋아하지 않음 (N)** | P(팝콘 N, 트롤 Y) | P(팝콘 N, 트롤 N) | **P(팝콘 N)** |
| **주변확률 (Marginal)** | **P(트롤 Y)** | **P(트롤 N)** | **총합 = 1** |


### **8.3 상호 정보 계산**
*   상호 정보는 복잡해 보이는 수식으로 계산되지만, 기본적으로는 **결합 확률과 주변 확률을 많이 더하는 과정(시그마 기호는 덧셈을 의미)**입니다.
*   수식에 결합 확률과 주변 확률을 대입하여 계산합니다.
*   수식에 사용되는 **로그 함수는 일반적으로 자연 로그(natural log)를 기본으로 합니다**.
*   만약 계산 과정에서 '0 * log(0)' 형태가 나타나더라도, x가 0에 가까워질 때 'x * log(x)'는 0이 되므로 해당 항은 0으로 처리됩니다.

### **8.4 상호 정보 값의 해석**
*   **상호 정보가 0일 때**: 두 변수 중 적어도 하나가 전혀 변하지 않는다면, 그 변수는 다른 변수에 대해 아무것도 알려주지 못하므로 상호 정보 값은 0이 됩니다. **변하지 않는 것은 아무런 놀라움(surprise)을 주지 않기 때문입니다**.
*   **상호 정보가 0보다 클 때 (값이 클수록 관계가 강함)**: 두 변수가 모두 변하고, 그 변화가 서로 관련되어 있다면 상호 정보 값은 0보다 커집니다.
    *   한 변수의 변화가 다른 변수의 변화에 대해 더 많은 것을 알려줄수록 상호 정보 값은 **더 커집니다**.
    *   두 변수가 **정확히 같은 방식**으로 변하거나 **정확히 반대되는 방식**으로 변할 때, 상호 정보 값은 **가장 커지며** 이 두 경우는 같은 상호 정보 값을 가집니다.

### **8.5 연속형 변수 다루기**
*   키(height)와 같은 **연속형 변수**가 포함된 경우, 먼저 **히스토그램**을 생성합니다.
*   히스토그램의 각 **구간(bin)**을 이산형 범주(discrete category)로 취급하여 결합 확률과 주변 확률을 계산하고 상호 정보를 구할 수 있습니다.

### **8.6 엔트로피(Entropy)와의 관계**
*   상호 정보 수식은 **엔트로피 수식과 유사한 점이 많습니다.** 둘 다 확률과 로그의 합으로 구성됩니다.
*   상호 정보는 엔트로피에서 파생될 수 있으며, **평균적으로 한 변수의 '놀라움' 또는 '변화'가 다른 변수의 '놀라움' 또는 '변화'와 어떻게 관련되어 있는지**를 알려줍니다.

요약하자면, 상호 정보는 다양한 유형의 변수들 간의 관계를 정량적으로 파악하여, 예측 모델 구축 시 어떤 특성(feature)이 가장 유용한지 판단하는 데 도움을 주는 강력한 도구입니다. AI 학습의 중요한 첫 걸음이 될 것입니다!

## 9. 최소 제곱법과 선형 회귀의 핵심 아이디어

AI 분야, 특히 기계 학습에서 데이터를 이해하고 예측 모델을 만드는 것은 매우 중요합니다. 선형 회귀는 가장 기본적인 기계 학습 알고리즘 중 하나로, 데이터의 경향을 파악하고 미래 값을 예측하는 데 사용됩니다.

### **9.1 왜 데이터에 선을 맞출까요?**
우리가 실험을 통해 얻은 데이터를 XY 그래프에 플로팅했을 때, 데이터의 전반적인 **추세(trend)를 파악**하기 위해 데이터를 가로지르는 선을 추가하고 싶어 합니다. 이 선을 통해 데이터가 어떤 경향을 보이는지 시각적으로 이해하고, 나아가 예측 모델을 만들 수 있습니다.

### **9.2 "가장 잘 맞는 선"은 어떻게 정의할까요?**
다양한 선들 중에서 어떤 선이 데이터에 가장 잘 맞는 선일까요? 이를 객관적으로 측정하기 위해 다음 방법을 사용.

*   **거리 측정**: 각 데이터 포인트에서 선까지의 수직 거리(잔차, residual)를 측정합니다.
*   **음수 문제 해결**: 단순히 이 거리들을 더하면, 선 위에 있는 포인트는 양수 거리를, 선 아래 있는 포인트는 음수 거리를 가져서 합계가 실제보다 작아지는 문제가 발생합니다.
*   **제곱 사용**: 이 문제를 해결하기 위해 각 거리(잔차)를 **제곱**하여 모두 양수로 만든 후 더합니다. 이 값이 바로 **잔차 제곱의 합(Sum of Squared Residuals)**입니다.
*   **목표**: 잔차 제곱의 합이 **가장 작은 선**이 데이터에 가장 잘 맞는 선이라고 정의합니다.

### **9.3 "최적의 선"을 찾는 방법: 최소 제곱법(Least Squares)**

우리가 찾으려는 선은 다음의 일반적인 방정식으로 표현될 수 있습니다:
**y = ax + b**
여기서:
*   **a**는 선의 **기울기(slope)**입니다.
*   **b**는 **y 절편(y-intercept)**입니다. 즉, x가 0일 때 선이 y축을 가로지르는 지점입니다.

우리의 목표는 **잔차 제곱의 합을 최소화하는 최적의 'a'와 'b' 값을 찾는 것**입니다. 이 방법을 **최소 제곱법(Least Squares)**이라고 부릅니다.

*   **기울기와 절편의 변화**: 기울기(a)와 절편(b)을 다양하게 변경함에 따라 잔차 제곱의 합이 어떻게 변하는지 시각화하면, 잔차 제곱의 합이 가장 작아지는 "최적의 지점(sweet spot)"이 존재함을 알 수 있습니다.
*   **미분(Derivative)의 활용**: 이 최적의 지점(잔차 제곱의 합이 최소가 되는 지점)을 수학적으로 찾기 위해 **미분**을 사용합니다. 잔차 제곱의 합을 나타내는 함수의 기울기가 **0이 되는 지점**이 바로 최소값이 되는 지점이기 때문입니다.
*   **컴퓨터의 역할**: 실제로 이 문제들을 손으로 푸는 사람은 거의 없으며, 대부분 **컴퓨터**를 사용하여 최적의 'a'와 'b' 값을 계산합니다.

### **9.4 핵심 개념 요약 (AI 입문자에게 중요한 포인트!)**

이 동영상에서 강조하는 두 가지 **가장 중요한 개념**은 다음과 같습니다:

1.  **관측값(실제 데이터)과 우리가 맞춘 선 사이의 거리(잔차)를 제곱하여 최소화하는 것**이 선을 맞추는 목표입니다. (수학적으로 "거리"를 측정하는 한 가지 방법)
2.  이 최소값을 찾기 위해 **미분(derivative)을 사용하여 함수의 기울기가 0이 되는 지점을 찾습니다**. (최적화 문제를 푸는 핵심 수학적 도구)

최종적으로 얻게 되는 선은 실제 데이터와 잔차 제곱의 합이 최소가 되는 "최소 제곱" 선이 됩니다. 이 과정을 이해하는 것은 AI의 기본적인 최적화 및 모델 학습 원리를 이해하는 데 매우 중요합니다.

## 10. 선형 회귀(Linear Regression)

선형 회귀는 **데이터 내의 관계를 정량화**하고, 그 관계가 **얼마나 신뢰할 수 있는지**를 결정하는 데 사용되는 통계적 기법입니다. 이는 **일반 선형 모델(General Linear Models)**이라고도 불리며, 여러 부분으로 구성되지만 핵심 개념은 다음 세 가지입니다.

1.  **최소 제곱법(Least Squares)을 사용하여 데이터에 선을 맞춥니다.**
2.  **R-제곱(r-squared) 값을 계산합니다.**
3.  **R-제곱에 대한 P-값(p-value)을 계산합니다.**

이 세 가지는 선형 회귀의 가장 중요한 개념입니다.

### **10.1 최소 제곱법을 이용한 선 맞춤 (Fitting a Line with Least Squares)**

선형 회귀의 첫 단계는 데이터에 가장 잘 맞는 선을 찾는 것입니다. 이 과정은 **최소 제곱법**이라고 불립니다.

*   **잔차(Residuals)**: 먼저 데이터에 선을 긋고, 선에서 각 데이터 포인트까지의 거리를 측정합니다. 이 거리를 **잔차(residual)**라고 부릅니다.
*   **잔차 제곱합(Sum of Squared Residuals)**: 각 잔차를 제곱한 후 모두 더합니다.
*   **최소화**: 선을 조금씩 회전시키면서 잔차 제곱합이 **가장 작아지는** 선을 찾습니다. 이 선이 바로 최소 제곱법으로 데이터에 맞춰진 선입니다.
*   **방정식 매개변수 추정**: 최소 제곱법은 이 선의 **y축 절편(y-axis intercept)**과 **기울기(slope)**라는 두 가지 매개변수를 추정합니다. 기울기가 0이 아니라는 것은 한 변수(예: 쥐의 무게)가 다른 변수(예: 쥐의 크기)를 예측하는 데 도움이 된다는 의미입니다.

### **10.2 R-제곱(R-squared) 계산**

R-제곱은 우리가 찾은 선형 모델이 데이터를 얼마나 잘 설명하는지, 즉 예측이 **얼마나 좋은지**를 나타내는 척도입니다.

*   **개념**: R-제곱은 독립 변수(예: 쥐의 무게)를 고려함으로써 종속 변수(예: 쥐의 크기)의 **전체 변동성(variation) 중 얼마만큼을 설명할 수 있는지**를 알려줍니다.
*   **계산**:
    *   먼저 **평균 주변의 제곱합(Sum of Squares around the mean, SS mean)** 또는 **평균 주변의 변동성(variation around the mean)**을 계산합니다. 이는 독립 변수를 고려하지 않았을 때 종속 변수의 전체 변동성을 나타냅니다.
    *   다음으로 **최소 제곱 적합 선 주변의 제곱합(Sum of Squares around the least squares fit, SS fit)** 또는 **적합 선 주변의 변동성(variation around the fit)**을 계산합니다. 이는 모델이 종속 변수를 예측한 후 남은 변동성(잔차의 제곱합)을 나타냅니다.
    *   R-제곱 공식은 `(평균 주변 변동성 - 적합 선 주변 변동성) / 평균 주변 변동성` 입니다.
*   **해석**:
    *   R-제곱 값이 **0.6 (60%)** 이라면, 쥐의 무게가 쥐의 크기 변동성의 60%를 설명한다는 의미입니다.
    *   R-제곱 값이 **1 (100%)** 이라면, 모델이 종속 변수의 모든 변동성을 완벽하게 설명한다는 의미입니다.
    *   R-제곱 값이 **0 (0%)** 이라면, 모델이 종속 변수의 변동성을 전혀 설명하지 못한다는 의미입니다.
*   **다중 매개변수 모델**: R-제곱 개념은 선형 방정식뿐만 아니라, **더 복잡한 방정식**이나 **여러 예측 변수(예: 쥐의 무게와 꼬리 길이로 몸통 길이 예측)**를 사용하는 다중 회귀(multiple regression) 모델에도 적용될 수 있습니다. 이 경우, 선 대신 **평면(plane)**을 데이터에 맞추게 됩니다.
*   **조정된 R-제곱(Adjusted R-squared)**: 모델에 너무 많은 "우스꽝스러운(silly) 매개변수"를 추가하면, 우연히 잔차 제곱합이 감소하고 R-제곱이 높아질 가능성이 있습니다. 이를 보완하기 위해 **매개변수 수를 고려하여 R-제곱 값을 조정하는 조정된 R-제곱(adjusted R-squared)**이 보고됩니다.

### **10.3 P-값(P-value) 계산**

R-제곱 값은 중요하지만, 그 관계가 통계적으로 **유의미한지, 즉 신뢰할 수 있는지**를 판단하는 방법이 필요합니다. 이를 위해 **P-값**을 계산합니다.

*   **F-통계량(F-statistic)**: R-제곱에 대한 P-값은 **F-통계량(F-statistic)**에서 나옵니다. F는 `무게로 설명되는 쥐 크기의 변동성 / 무게로 설명되지 않는 쥐 크기의 변동성`으로 정의됩니다.
    *   **분자(Numerator)**: 모델의 추가 매개변수(예: 쥐의 무게)에 의해 설명되는 변동성을 나타냅니다.
    *   **분모(Denominator)**: 적합 선에 의해 설명되지 않는, 즉 잔차에 해당하는 변동성을 나타냅니다.
*   **자유도(Degrees of Freedom)**: 이 개념은 제곱합을 변동성으로 바꾸는 역할을 하며, F-분포의 모양을 결정합니다.
*   **해석**:
    *   만약 모델이 데이터를 잘 설명한다면, 추가 매개변수에 의해 설명되는 변동성이 커지고, 설명되지 않는 변동성은 작아져 **F-값이 커집니다**.
    *   P-값은 F-분포를 사용하여 계산됩니다. F-값이 크다는 것은 해당 F-값이 우연히 발생할 확률이 낮다는 것을 의미하며, 이는 **P-값이 작다**는 것을 의미합니다.
    *   **P-값이 작으면**, R-제곱으로 나타나는 관계가 **통계적으로 유의미하다**고 판단합니다.

결론적으로, 선형 회귀는 데이터의 관계를 **R-제곱**으로 정량화하고, 그 관계의 **신뢰성**을 **P-값**으로 판단합니다. AI 분야에 입문하는 학생들에게 이 두 가지 개념은 데이터 분석과 모델 평가의 기본이자 핵심이 될 것입니다.

## 11. 다중 회귀(Multiple Regression)
**선형 회귀(Linear Regression)에 대한 이해를 바탕으로 다중 회귀의 개념을 설명**하고 있습니다.

### **11.1 다중 회귀란 무엇인가?**
*   **단순 선형 회귀(Simple Linear Regression)**가 데이터에 '선(line)'을 맞추는 것이라면, **다중 회귀는 데이터에 '평면(plane)' 또는 '고차원 객체(higher dimensional object)'를 맞추는 것**입니다.
*   '고차원 객체'라는 용어는 복잡하게 들릴 수 있지만, 이는 단순히 **모델에 추가적인 데이터를 추가하는 것을 의미**합니다. 예를 들어, 쥐의 몸무게로 몸 길이를 모델링하는 대신, 몸무게와 꼬리 길이, 심지어 먹은 음식의 양이나 쳇바퀴를 뛴 시간과 같은 **추가적인 요인들을 함께 사용하여 몸 길이를 예측하는 것**입니다.

### **11.2 R-제곱(R-squared) 값 계산**
*   R-제곱은 **단순 회귀와 다중 회귀 모두에서 계산 방식이 동일**하며, 모델이 데이터에 얼마나 잘 맞는지를 평가
*   다만, 다중 회귀의 경우 방정식에 **추가된 매개변수를 보상하기 위해 R-제곱을 '조정(adjust)'**합니다. 

### **11.3 P-값(P-value) 계산**
*   F 값과 P-값의 계산은 이전과 거의 동일합니다.
*   단순 회귀의 경우 방정식에 추정해야 할 매개변수가 2개이기 때문에 'P fit' 값은 2입니다.
*   다중 회귀의 경우, 꼬리 길이를 추가했다면 추정해야 할 매개변수가 3개이므로 'P fit' 값은 3이 됩니다. 모델에 데이터를 더 추가하면 'P fit' 값도 해당 매개변수 수에 맞춰 변경해야 합니다.
*   'P mean' 값은 단순 회귀와 다중 회귀 모두에서 1인데, 이는 몸 길이의 평균값 하나만 추정하면 되기 때문입니다.

### **11.4 다중 회귀의 진정한 강점: 모델 비교**
*   단순 회귀와 다중 회귀의 차이점은 크게 중요하지 않다고 말하면서도, 동영상에서는 **다중 회귀가 '진정으로 빛을 발하는' 지점**을 강조합니다.
*   그것은 바로 **서로 다른 모델들을 비교할 수 있다는 점**입니다.
*   예를 들어, 꼬리 길이 데이터를 수집할 가치가 있는지 알아보기 위해, 꼬리 길이가 없는 모델(단순 회귀)과 꼬리 길이가 있는 모델(다중 회귀)을 비교할 수 있습니다.
*   이 비교를 통해 계산된 F 값과 P-값을 해석하여, **두 모델 간의 R-제곱 값 차이가 크고 P-값이 작다면, 모델에 꼬리 길이를 추가하는 것이 그만한 가치가 있다**고 판단할 수 있습니다.


다중 회귀는 단순 선형 회귀의 확장으로, **더 많은 변수(데이터)를 사용하여 현상을 모델링하고 예측하는 강력한 도구**입니다. 특히, **변수를 추가하는 것이 모델의 성능을 얼마나 향상시키는지 통계적으로 평가할 수 있게 해준다**는 점에서 AI 학습에 있어 중요한 개념입니다.

## 12. 일반 선형 모델: 선형 회귀, t-검정, ANOVA의 통일된 접근법
선형 회귀(linear regression)에 사용되는 일반 선형 모델(General linear models)의 기법들이 어떻게 t-검정(t-tests)과 분산 분석(ANOVA)에도 동일하게 적용될 수 있는지를 **디자인 행렬(Design Matrix)**이라는 핵심 개념을 통해 명확하게 설명합니다.

통계 분석의 세 가지 주요 방법인 선형 회귀, t-검정, ANOVA가 본질적으로 **동일한 선형 모델 기법**을 사용하여 분석될 수 있음을 보여줍니다. 이 접근 방식은 컴퓨터가 다양한 통계 문제를 새로운 방법을 만들 필요 없이 효율적으로 해결할 수 있도록 합니다.

### **12.1 선형 회귀(Linear Regression) 간략 복습**
선형 회귀는 쥐의 몸무게로 쥐의 크기를 예측하는 것과 같은 문제에 사용됩니다.
*   **R-제곱(R-squared)**: 몸무게가 크기를 예측하는 데 얼마나 유용한지 알려줍니다.
*   **P-값(P-value)**: 관찰된 관계가 우연에 의한 것인지 아닌지 알려줍니다.

### **12.2 t-검정(T-test)에 선형 회귀 기법 적용하기**
t-검정의 목표는 두 그룹(예: 대조군 쥐와 돌연변이 쥐) 간의 유전자 발현 평균이 통계적으로 유의미하게 다른지 비교하는 것입니다. 선형 회귀 기법을 t-검정에 적용하는 단계는 다음과 같습니다.

1.  **전체 평균(Overall Mean) 계산 및 SS mean 계산**:
    *   먼저 x축을 무시하고 모든 데이터 포인트에 대한 전체 평균을 찾습니다.
    *   각 데이터 포인트와 전체 평균 사이의 거리(잔차)를 제곱하여 합산한 **잔차 제곱합(Sum of Squared Residuals around the mean, SS mean)**을 계산합니다.

2.  **데이터에 선 맞추기(Fit a Line to the Data)**:
    *   선형 회귀에서는 데이터에 단일 직선을 맞춥니다.
    *   t-검정에서는 각 그룹(대조군, 돌연변이군) 데이터에 대해 개별적으로 선을 맞춥니다. 이때 각 그룹의 **평균이 바로 최소제곱 적합(least squares fit)**이 됩니다.
    *   예를 들어, 대조군 평균은 y=2.2, 돌연변이군 평균은 y=3.6과 같은 수평선 방정식으로 표현될 수 있습니다.

3.  **하나의 방정식으로 결합 및 디자인 행렬(Design Matrix) 도입**:
    *   컴퓨터가 여러 개의 적합선을 단일 방식으로 처리할 수 있도록, 이 두 개의 적합선을 하나의 방정식으로 결합
    *   여기서 **디자인 행렬**이라는 개념이 등장합니다. 디자인 행렬은 1과 0으로 구성된 행렬로, 특정 그룹의 평균을 "켜거나(on)" "끄는(off)" 스위치 역할을 합니다. 예를 들어, 대조군 데이터 포인트의 경우 대조군 평균에 해당하는 열은 1, 돌연변이군 평균에 해당하는 열은 0이 됩니다.
        - 두 그룹(대조군과 돌연변이군)의 유전자 발현량을 비교하는 실험을 가정.총 5개의 샘플이 있으며, 그룹은 다음과 같습니다.
            *   샘플 1, 2, 3: **대조군** (Control)
            *   샘플 4, 5: **돌연변이군** (Mutant)

        - 디자인 행렬 (Design Matrix)

            | 샘플 | 그룹 | 절편 (Intercept) | 돌연변이군 (Mutant) |
            | :--- | :--- | :---: | :---: |
            | 1 | 대조군 | 1 | 0 |
            | 2 | 대조군 | 1 | 0 |
            | 3 | 대조군 | 1 | 0 |
            | 4 | 돌연변이군 | 1 | 1 |
            | 5 | 돌연변이군 | 1 | 1 |

        - . "켜짐(1)"과 "꺼짐(0)"의 역할
            - 절편 열 (Intercept): 이 열은 항상 1입니다. 통계 모델에서 이는 '기준이 되는 그룹의 평균' 을 나타냅니다. 여기서는 '대조군'이 기준입니다.
            - 돌연변이군 열 (Mutant): 이 열은 해당 샘플이 돌연변이군일 때만 "켜져서(1)" 값을 가지고, 대조군일 때는 "꺼져서(0)" 값을 가집니다. 이는 '대조군 평균과 돌연변이군 평균의 차이' 를 나타냄.
    *   이를 통해 `y = (대조군 평균) + (평균 차이) * $\[지시자\]$ + 잔차`와 같은 형태로 방정식을 표현할 수 있으며, 디자인 행렬이 각 데이터 포인트에 대해 $\[지시자\]$ 값을 0 또는 1로 설정함으로써 해당 그룹의 효과를 활성화 또는 비활성화하여 최종적으로 (대조군 평균) 또는 (대조군 평균 + 평균 차이), 즉 (돌연변이군 평균) 인 실제 적합값을 만들어냅니다.

4.  **SS fit 계산**:
    *   적합된 선(t-검정의 경우 각 그룹의 평균) 주변의 잔차 제곱합인 **SS fit(Sum of Squares of the Residuals around the fitted lines)**을 계산합니다.

5.  **F 값 및 P 값 계산**:
    *   계산된 SS mean과 SS fit, 그리고 P mean(평균 방정식의 매개변수 수)과 P fit(적합선 방정식의 매개변수 수)을 F 값 공식에 대입합니다.
    *   이 F 값을 통해 t-검정의 P 값을 얻을 수 있습니다.

### **12.3 분산 분석(ANOVA)에 선형 회귀 기법 적용하기**
ANOVA는 t-검정과 유사하지만, 세 개 이상의 범주(예: 대조군, 돌연변이군, 특이 식단 대조군, 특이 식단 돌연변이군, 이형접합체 쥐)의 평균이 모두 동일한지 여부를 테스트하는 데 사용됩니다.

*   **SS mean 계산**: 모든 범주의 데이터를 사용하여 전체 평균을 계산하고 SS mean을 구하는 과정은 t-검정과 동일
*   **SS fit 계산 및 디자인 행렬**: ANOVA에서도 각 범주의 평균을 맞추는 방식으로 SS fit을 계산합니다. 이때 **디자인 행렬은 각 범주마다 하나의 열**을 가지게 되며, 해당 범주의 평균을 활성화하는 역할을 합니다. ANOVA의 P fit은 범주의 수와 같습니다 (각 범주 평균이 하나의 매개변수이므로).
*   **F 값 및 P 값 계산**: 계산된 SS mean, SS fit, P mean, P fit 값을 F 값 공식에 대입하여 P 값을 얻습니다.

### 12.4 **시사점**

*   **모델의 통일성**: 언뜻 보기에 다른 통계 기법(선형 회귀, t-검정, ANOVA)들이 실제로는 **동일한 수학적 원리와 모델(최소제곱법 기반의 선형 모델)**을 공유한다는 점을 보여줍니다. 이는 AI/머신러닝 분야에서 다양한 문제에 대해 일반화된 모델을 구축하고 적용하는 방식과 유사합니다.
*   **디자인 행렬의 중요성**: 디자인 행렬은 범주형 데이터를 모델에 통합하고, 복잡한 그룹 비교를 단일 방정식 형태로 표현하는 강력한 방법입니다. 이는 AI 모델에서 **특성 공학(feature engineering)**이나 **원-핫 인코딩(one-hot encoding)**과 같은 기법의 기초적인 아이디어와 연결될 수 있습니다. 데이터를 모델이 이해할 수 있는 형태로 구조화하는 방법을 이해하는 데 필수적입니다.
*   **컴퓨터 자동화의 원리**: 이 동영상은 디자인 행렬을 통해 컴퓨터가 다양한 통계 분석을 자동으로 수행할 수 있는 원리를 설명합니다. 이는 AI 알고리즘이 복잡한 데이터 분석 작업을 효율적으로 자동화하는 방식의 기본적인 예시

이러한 이해는 통계적 모델링의 깊이를 더하고, AI 분야에서 데이터를 효과적으로 모델링하고 해석하는 데 필요한 통찰력을 제공할 것입니다. 비디오에서 소개된 디자인 행렬은 설명의 편의를 위한 것이며, 실제로는 더 다양한 형태의 디자인 행렬이 사용될 수 있다는 점도 참고해 주세요.

## 13. 일반 선형 모델과 디자인 행렬: 통계 분석의 통일된 접근법

**디자인 행렬(Design Matrix)**이라는 핵심 개념을 통해 일반 선형 모델(General linear models)이 어떻게 선형 회귀(linear regression), t-검정(t-tests), 분산 분석(ANOVA)과 같은 다양한 통계 분석 기법들을 통합적으로 다룰 수 있는지를 명확하게 설명합니다.

통계학의 여러 분석 방법들이 본질적으로 **동일한 선형 모델 기법**을 사용하여 분석될 수 있음을 보여주며, 그 중심에 **디자인 행렬**이 있음을 강조합니다. 디자인 행렬은 컴퓨터가 복잡한 통계 문제를 효율적으로 해결할 수 있도록 돕는 핵심 도구입니다.

### **13.1 t-검정을 위한 디자인 행렬**
t-검정은 두 그룹(예: 대조군과 돌연변이군) 간의 평균 차이가 통계적으로 유의미한지 확인하는 데 사용됩니다. 동영상에서는 t-검정을 위한 두 가지 형태의 디자인 행렬을 소개하며, 그중 더 일반적인 형태에 초점을 맞춥니다.

*   **표준 디자인 행렬**: 이 디자인 행렬에서 모든 측정값(대조군 및 돌연변이군)은 **대조군 평균 값에 대한 항을 활성화(turn on)**합니다 (디자인 행렬의 첫 번째 열이 모두 '1'로 구성됨). 반면, **돌연변이 측정값만 돌연변이 데이터의 평균과 대조군 데이터의 평균 간의 '차이'에 대한 항을 활성화**합니다 (두 번째 열이 돌연변이 데이터에 대해서는 '1', 대조군 데이터에 대해서는 '0'으로 구성됨). 이 '차이' 항은 돌연변이 데이터에 대한 오프셋(offset) 역할을 합니다.
*   **1과 0의 역할**: 디자인 행렬의 '1'은 해당 항을 '활성화'하고, '0'은 해당 항을 '비활성화'하여 결과에 영향을 미치지 않도록 합니다.
*   **동일한 결과**: 두 가지 형태의 디자인 행렬 모두 동일한 잔차 제곱합(sum of squares around the fit)과 매개변수 수(P fit)를 가지므로, 최종적으로 동일한 F 값과 P 값을 산출합니다.

### **13.2 선형 회귀를 위한 디자인 행렬**
디자인 행렬은 1과 0으로만 구성되는 것이 아니라 다른 숫자들도 포함할 수 있습니다. 선형 회귀에서 디자인 행렬은 다음과 같이 구성됩니다.

*   **구조**: 첫 번째 열은 모두 '1'로 채워져 Y 절편(y-intercept) 항을 활성화합니다. 두 번째 열에는 각 데이터 포인트의 **x축 위치(독립 변수 값)**가 들어갑니다. 이 x축 값은 기울기(slope) 항을 '스케일링'하는 역할을 합니다.
*   **계산**: 디자인 행렬의 각 행을 방정식에 대입하면 최소제곱 적합선(least squares fit line) 상의 해당 데이터 포인트에 대한 예측값을 얻을 수 있습니다. 이렇게 예측된 값들을 통해 잔차를 계산하고 P 값을 도출할 수 있습니다.

### **13.3 t-검정과 선형 회귀의 결합**
동영상은 쥐의 몸무게와 크기 사이의 관계(선형 회귀)를 보면서도, 동시에 두 가지 쥐 유형(정상 쥐와 돌연변이 쥐) 간의 차이(t-검정)를 분석하는 복합적인 예시를 제시합니다.

*   **복합 모델 방정식**: 이 모델은 정상 쥐의 Y 절편, 돌연변이 쥐의 오프셋(mutant offset), 그리고 두 쥐 유형에 동일하게 적용되는 기울기 항을 포함하는 방정식을 사용합니다.
*   **복합 디자인 행렬**:
    *   첫 번째 열은 모두 '1'로 Y 절편 항을 활성화합니다.
    *   두 번째 열은 돌연변이 오프셋을 '켜거나 끄는' 역할을 합니다. 정상 쥐에 대해서는 '0'으로 오프셋을 비활성화하고, 돌연변이 쥐에 대해서는 '1'로 오프셋을 활성화하여 돌연변이 쥐가 자체적인 Y 절편을 가질 수 있도록 합니다.
    *   세 번째 열에는 쥐 몸무게 데이터(x 좌표)가 들어갑니다.
*   **결과**: 이 디자인 행렬을 사용하면 각 쥐 유형에 대한 개별적인 선이 생성됩니다 (예: 정상 쥐는 빨간색 선, 돌연변이 쥐는 녹색 선).
*   **P 값 계산**: 이 **'고급 모델(fancy model)'**의 잔차 제곱합과 매개변수 수를 사용하여, 더 **'단순한 모델(simpler model)'**과 비교함으로써 P 값을 계산합니다. 예를 들어, 쥐 크기의 평균만 사용하는 모델, 몸무게만 고려하는 선형 회귀 모델, 또는 쥐 유형만 고려하는 t-검정 모델 등과 비교할 수 있습니다. 질문에 따라 어떤 '단순 모델'과 비교할지 결정됩니다.

### **13.4 배치 효과(Batch Effect) 보정을 위한 디자인 행렬 예시**
동영상은 실험실 간의 '배치 효과'를 보정하면서 돌연변이와 대조군 간의 차이를 분석하는 또 다른 복합적인 예시를 제시합니다.
*   배치 효과:  실험실마다 사용하는 장비의 보정 상태, 실험 시간, 실험자, 시약 로트 등 미세한 조건이 다르기 때문에 발생하는 시스템적인 오차입니다.
*   **목표**: 두 실험실 데이터를 결합하여 돌연변이와 대조군 간의 차이를 보되, 각 실험실의 전체적인 측정값 차이(배치 효과)를 보상하는 것입니다.
*   **방정식 항**: 이 모델은 '실험실 A의 대조군 평균 값', '실험실 B의 오프셋', 그리고 '돌연변이와 대조군 측정값 간의 차이'에 대한 항들을 포함합니다.
*   **디자인 행렬**: 각 항에 해당하는 열들을 디자인 행렬에 구성하여 배치 효과를 보정하고, 돌연변이-대조군 차이의 유의성을 평가합니다. 마지막 항(돌연변이-대조군 차이)이 중요한지 여부를 확인하기 위해, 해당 항을 무시한 단순 모델과 비교하여 P 값을 계산합니다.

### 13.5 **시사점**

*   **모델링의 통일성**: 선형 회귀, t-검정, ANOVA와 같은 겉보기에는 다른 통계 기법들이 **최소제곱법(least squares)**을 기반으로 하는 **동일한 선형 모델**을 공유한다는 점을 보여줍니다. 이는 AI/머신러닝에서 다양한 문제를 해결하기 위해 일반화된 모델 프레임워크를 적용하는 방식과 일맥상통합니다.
*   **디자인 행렬의 중요성 (데이터 표현)**: 디자인 행렬은 **범주형 데이터(categorical data)**와 **수치형 데이터(numerical data)**를 단일 모델에 통합하고, 복잡한 비교(그룹 간 차이, 배치 효과)를 단일 방정식 형태로 표현하는 강력한 방법입니다. 이는 AI 모델에서 **특성 공학(feature engineering)**이나 **원-핫 인코딩(one-hot encoding)**과 같은 기법의 기초적인 아이디어와 연결될 수 있습니다. 데이터를 모델이 이해하고 학습할 수 있는 효과적인 형태로 구조화하는 방법을 이해하는 데 필수적입니다.
*   **유연성과 확장성**: 디자인 행렬을 통해 다양한 종류의 독립 변수(연속형, 범주형)를 모델에 포함시킬 수 있는 유연성을 보여줍니다. 이는 AI 모델이 복잡한 데이터 관계를 학습할 수 있도록 하는 기본적인 메커니즘을 이해하는 데 도움이 됩니다.
*   **가설 검증의 기본 원리**: '고급 모델'을 '단순 모델'과 비교하여 P 값을 계산하는 과정은 AI 모델의 성능을 평가하거나, 특정 특성(feature)이 예측에 얼마나 중요한지 통계적으로 검증하는 기본 아이디어를 제공합니다.

## 14. Odds와 Log(Odds) 개념

**Odds(오즈)**와 **Log(Odds)(로그 오즈)**를 명확하게 설명합니다. 이 개념들은 AI 분야, 특히 분류 모델에서 자주 사용되는 **로지스틱 회귀(Logistic Regression)**의 기반이 된다.

### 14.1 Odds (오즈)란 무엇인가?

*   **정의**: Odds는 **어떤 사건이 일어날 가능성과 일어나지 않을 가능성의 비율**을 의미합니다. 예를 들어, 당신의 팀이 이길 확률이 1 대 4라고 하면, 이는 팀이 5번의 경기 중 1번 이기고 4번 진다는 의미입니다. 분수로는 1/4 (0.25)로 표현될 수 있습니다. 또 다른 예로, 5 대 3의 Odds는 팀이 8경기 중 5번 이기고 3번 진다는 뜻이며, 5/3 (약 1.7)로 계산됩니다.
*   **확률(Probability)과의 차이**:
    *   **확률**은 어떤 사건이 일어날 가능성 대 **모든 가능한 결과의 비율**입니다. 예를 들어, 팀이 5번 이기고 3번 진다면, 이길 확률은 5/8 (5승 / (5승 + 3패))입니다.
    *   **Odds**는 사건이 일어날 가능성 대 **사건이 일어나지 않을 가능성의 비율**입니다. 위 예시에서 이길 Odds는 5/3 (5승 / 3패)입니다. 즉, Odds와 확률은 서로 다른 개념입니다.
*   **계산 방법**:
    *   **횟수(Counts) 기반**: 예를 들어, 이기는 횟수를 지는 횟수로 나눕니다 (예: 5승 / 3패 = 1.7).
    *   **확률(Probabilities) 기반**: 어떤 사건이 일어날 확률을 그 사건이 일어나지 않을 확률로 나눕니다. 즉, `P(승리) / (1 - P(승리))` 와 같습니다. 두 방법 모두 동일한 Odds 값을 산출합니다.

### 14.2 Log(Odds) (로그 오즈)는 왜 필요한가?

*   **Odds의 비대칭성 문제**: Odds 값은 특정 문제가 있습니다.
    *   **사건이 일어나지 않을 경우**: Odds는 0과 1 사이의 값을 가집니다 (예: 1 대 4 = 0.25, 1 대 32 = 0.031). 팀이 약할수록 0에 가까워집니다.
    *   **사건이 일어날 경우**: Odds는 1과 무한대 사이의 값을 가집니다 (예: 4 대 3 = 1.3, 32 대 3 = 10.7). 팀이 강할수록 1에서 계속 증가합니다.
    *   이러한 **비대칭성(asymmetry)** 때문에 Odds를 비교하고 해석하는 것이 어렵습니다. 예를 들어, 1 대 6으로 질 Odds (0.17)와 6 대 1로 이길 Odds (6)는 숫자 크기가 매우 다르게 보입니다.
*   **Log(Odds)의 해결책**: Odds 값에 로그(log)를 취하면 이러한 비대칭성 문제를 해결할 수 있습니다.
    *   `log(1/6)`은 약 **-1.79**이고, `log(6/1)`은 약 **1.79**가 됩니다.
    *   이처럼 로그 함수를 사용하면, 원점(0)으로부터의 거리가 같아져 **대칭적(symmetrical)**으로 변하여 해석이 훨씬 쉬워집니다.

### 14.3 Log(Odds)의 중요성과 활용

*   **로지스틱 회귀의 기반**: 확률의 비율에 로그를 취한 것을 **로짓 함수(logit function)**라고 하는데, 이것이 바로 **로지스틱 회귀(Logistic Regression)**의 기초를 이룹니다. 로지스틱 회귀는 이진 분류(Binary Classification) 문제(예: 합격/불합격, 스팸/정상 메일)에서 널리 사용되는 AI/머신러닝 알고리즘입니다.
*   **통계 문제 해결에 유용**: Log(Odds)는 특정 통계 문제를 해결하는 데 매우 유용합니다. 무작위 숫자를 사용하여 Log(Odds)를 계산하고 히스토그램을 그리면 **정규 분포(normal distribution)**와 유사한 형태를 띠기 때문입니다. 이는 승/패, 예/아니오, 참/거짓과 같은 확률을 결정하려는 상황에서 특히 유용합니다.
*   **해석 및 통계 분석 용이성**: Log(Odds)는 값을 대칭적으로 만들고, 해석하기 쉽게 하며, 고급 통계 분석에 더 적합하게 만듭니다.

Odds는 어떤 사건이 일어날 가능성과 일어나지 않을 가능성의 비율이며, 확률과는 다릅니다. Log(Odds)는 이러한 Odds의 비대칭성 문제를 해결하고, 값을 대칭적으로 만들어 해석과 통계 분석을 용이하게 합니다. 특히, Log(Odds)는 **로지스틱 회귀의 핵심 개념**이디.

## 15. Odds Ratios와 Log(Odds Ratios) 개념

통계학에서 **Odds Ratio(오즈비)**와 **Log(Odds Ratio)(로그 오즈비)**라는 중요한 개념을 명확하게 설명합니다. 이 개념들은 AI 분야, 특히 **로지스틱 회귀(Logistic Regression)**와 같은 분류 모델에서 관계의 강도와 통계적 유의미성을 평가하는 데 필수적이다.

### 15.1 Odds Ratio (오즈비)란 무엇인가?

*   **Odds(오즈) 복습**: 이전 논의에서 Odds는 어떤 사건이 일어날 가능성과 일어나지 않을 가능성의 비율임을 언급했습니다. 예를 들어, 팀이 이길 확률 대 지지 않을 확률의 비율입니다.
*   **Odds Ratio의 정의**: Odds Ratio는 **두 가지 다른 Odds의 비율**을 의미합니다. 즉, 어떤 한 그룹의 Odds를 다른 그룹의 Odds로 나눈 값입니다.
*   **계산 및 범위**:
    *   예를 들어, `(2/4) / (3/1)`와 같이 계산될 수 있습니다.
    *   분모가 분자보다 크면 Odds Ratio는 0에서 1 사이의 값을 가지며, 분자가 분모보다 크면 1에서 무한대(infinity)까지의 값을 가집니다.

### 15.2 Log(Odds Ratio) (로그 오즈비)는 왜 필요한가?

*   **Odds Ratio의 비대칭성 문제**: Odds Ratio도 Odds와 유사하게 비대칭적인 문제를 가지고 있습니다. 0과 1 사이의 값과 1과 무한대 사이의 값으로 나뉘어 있어, 그 크기가 관계의 강도를 직관적으로 비교하기 어렵게 만듭니다.
*   **Log(Odds Ratio)의 해결책**: Odds Ratio에 로그(log)를 취하면 이러한 비대칭성 문제를 해결하고 값을 **대칭적(symmetrical)**으로 만듭니다.
    *   예를 들어, `log(2/4 / 3/1)`은 약 **-1.79**이고, 반대로 `log(3/1 / 2/4)`은 약 **1.79**가 됩니다.
    *   이처럼 로그를 취하면 0을 중심으로 대칭적인 값(음수 또는 양수)을 갖게 되어 해석이 훨씬 용이해집니다.

### 15.3 Odds Ratio와 Log(Odds Ratio)의 중요성 및 활용 (관계 및 효과 크기)

*   **두 가지 간의 관계 표시**: Odds Ratio와 Log(Odds Ratio)는 **두 가지 요소 간의 관계(relationship)**
    *   비디오에서는 **돌연변이 유전자(mutated gene)와 암(cancer)** 간의 관계를 예시로 들었습니다. 돌연변이 유전자를 가진 사람이 암에 걸릴 Odds가, 유전자가 없는 사람이 암에 걸릴 Odds보다 얼마나 높은지를 Odds Ratio로 계산합니다.
    *   예시에서, Odds Ratio는 **6.88**로, 돌연변이 유전자를 가진 사람이 암에 걸릴 Odds가 약 6.88배 더 높다는 것을 의미합니다. 이때 Log(Odds Ratio)는 **1.93**입니다.
*   **R-squared와의 유사성**: 이 값들은 통계학의 R-squared처럼 **효과 크기(effect size)**를 나타냅니다.
    *   **값이 클수록** 돌연변이 유전자가 암 예측에 **좋은 예측 변수(good predictor)**임을 의미합니다.
    *   **값이 작을수록** 돌연변이 유전자가 암 예측에 **좋은 예측 변수가 아님**을 의미합니다.

### 15.4 통계적 유의미성 (Statistical Significance) 확인 방법

Odds Ratio나 Log(Odds Ratio)가 보여주는 관계가 단순히 우연이 아닌 **통계적으로 유의미한지**를 확인하는 것이 중요합니다. 이를 위해 **p-value(p-값)**를 계산하는 세 가지 방법이 소개됩니다.

1.  **Fisher's exact test (피셔의 정확 검정)**:
    *   이 검정은 **하이퍼지오메트릭 분포(hypergeometric distribution)**를 사용하여 p-값을 계산합니다.
        - 하이퍼지오메트릭 분포: 유한한 모집단에서 비복원추출을 할 때, 특정 그룹에서 원하는 개체를 뽑을 확률을 설명하는 이산 확률 분포
    *   예시에서는 암 환자와 비환자, 돌연변이 유전자 보유자와 비보유자를 M&M(초콜릿) 주머니에 비유하여 설명합니다. 23개의 빨간 M&M과 117개의 파란 M&M을 한 줌 잡았을 때의 p-값을 계산하는 것과 유사합니다.
    *   이 예시에서 p-값은 0.00001로 계산되었습니다.

2.  **Chi-square test (카이제곱 검정)**:
    *   이 검정은 **관측된 값(observed values)**과 **기대되는 값(expected values)**을 비교하여 p-값을 계산
    *   기대되는 값은 돌연변이 유전자와 암 사이에 **아무런 관계가 없다고 가정**할 때의 예상치입니다.
    *   총 암 환자 수와 총 인원수를 바탕으로 암 발생 확률을 계산하고, 이 확률을 유전자 유무 그룹에 적용하여 기대값을 산출합니다.
    *   이 예시에서 p-값은 연속성 수정(continuity correction) 적용 시 0.00001, 미적용 시 0.000004로 계산

3.  **Wald test (월드 검정)**:
    *   이 검정은 **로지스틱 회귀(Logistic Regression)**에서 Odds Ratio의 유의미성을 판단하고 **신뢰 구간(confidence intervals)**을 계산하는 데 일반적으로 사용됩니다.

    *   $W = (β̂ - β₀) / SE(β̂)$
        - β̂: 추정된 계수
        - β₀: 귀무가설 하의 값 (보통 0)
        - SE(β̂): 계수의 표준 오차     
    *   Log(Odds Ratio)는 **정규 분포(normally distributed)**를 따른다는 사실을 활용합니다. 관계가 없을 경우 Log(Odds Ratio)의 분포는 0을 중심으로 합니다.
    *   관측된 Log(Odds Ratio)가 0으로부터 표준 편차(standard deviations)가 얼마나 떨어져 있는지를 계산하여 p-값을 도출합니다.
    *   예시에서 Log(Odds Ratio) 1.93은 0으로부터 4.11 표준 편차 떨어져 (z값 정의)있으며, p-값은 0.00005로 계산
        - Z-table에서 P(Z ≥ 4.11) = 0.00005

*   **방법 선택의 비일관성**: 이 세 가지 방법 중 **어떤 방법이 가장 좋은지에 대한 일반적인 합의는 없습니다**. 사람들은 p-값을 계산하기 위해 Fisher's exact test나 Chi-square test를 사용하고, 신뢰 구간을 위해 Wald test를 사용하는 등 혼합하여 사용하기도 합니다.
*   **테스트 결과의 일관성**: 영상에서 10,000개의 무작위 Log(Odds Ratio)에 세 가지 테스트를 모두 적용한 결과, 모든 테스트가 유의미한 p-값을 적절히 제한하는 좋은 성능을 보였습니다. 따라서 해당 분야에서 가장 일반적으로 사용되는 방법을 따르는 것이 권장됩니다.

Odds Ratio는 두 Odds의 비율을, Log(Odds Ratio)는 그 값에 로그를 취한 것으로, **두 가지 요소 간의 관계 강도**를 나타내는 중요한 지표입니다. 이러한 관계가 통계적으로 유의미한지 여부는 Fisher's exact test, Chi-square test, Wald test를 통해 p-값을 계산하여 판단합니다. 특히 Log(Odds Ratio)는 **로지스틱 회귀의 핵심 개념**이다.

## 16. 로지스틱 회귀 소개

### **16.1 선형 회귀(Linear Regression) 복습 (로지스틱 회귀 이해를 위한 배경)**

로지스틱 회귀를 이해하기 전에, 먼저 선형 회귀를 복습하는 것이 좋습니다.
*   **무엇을 하는가?** 데이터(예: 쥐의 몸무게와 크기)에 **직선을 맞추는(fit a line)** 방법입니다.
*   **주요 용도**:
    *   **R-제곱(r-squared) 계산**: 몸무게와 크기가 상관관계가 있는지 파악합니다. 큰 값은 큰 영향을 의미합니다.
    *   **p-값(p-value) 계산**: R-제곱 값이 통계적으로 유의미한지 판단합니다.
    *   **예측(Prediction)**: 새로운 쥐의 몸무게가 주어졌을 때, 그 크기를 예측하는 데 이 선을 사용할 수 있습니다. **데이터를 사용하여 무언가를 예측하는 것은 머신러닝의 한 형태**입니다.
*   **확장**: **다중 회귀(Multiple Regression)**는 몸무게와 혈액량과 같은 여러 변수를 사용하여 크기를 예측하는 더 복잡한 선형 회귀의 한 형태입니다. 유전자형과 같은 이산적인 측정값도 예측에 사용할 수 있습니다.
*   **모델 비교**: 선형 회귀에서는 단순한 모델(예: 몸무게만 사용)과 복잡한 모델(예: 몸무게와 혈액량 사용)을 비교하여, 예측 정확도를 위해 추가 변수가 필요한지 여부를 판단

### **16.2 로지스틱 회귀(Logistic Regression)의 핵심**

로지스틱 회귀는 선형 회귀와 유사하지만 몇 가지 중요한 차이점이 있습니다.
*   **무엇을 예측하는가?**: 선형 회귀가 '크기'와 같은 연속적인 값을 예측하는 것과 달리, 로지스틱 회귀는 **어떤 것이 '참(True)'인지 '거짓(False)'인지**를 예측합니다. 즉, **이진적인(binary) 결과**를 예측하는 데 사용됩니다 (예: 쥐가 비만이다/비만이 아니다).
*   **데이터에 무엇을 맞추는가?**: 직선 대신 **S자형 로지스틱 함수(S-shaped logistic function)**를 데이터에 맞춥니다. 이 곡선은 0에서 1 사이의 값을 가집니다.
*   **곡선의 의미**: 이 S자형 곡선은 특정 조건(예: 쥐의 몸무게)이 주어졌을 때 **어떤 사건이 발생할 확률(probability)**을 알려줍니다. 예를 들어, 몸무게가 매우 무거운 쥐는 비만일 확률이 높고, 중간 몸무게의 쥐는 50%의 확률로 비만일 수 있으며, 가벼운 쥐는 비만일 확률이 낮다는 식입니다.
*   **분류(Classification)에 활용**: 로지스틱 회귀는 확률을 알려주지만, 주로 **분류(classification)**에 사용됩니다. 예를 들어, 쥐가 비만일 확률이 50%보다 높으면 '비만'으로 분류하고, 그렇지 않으면 '비만 아님'으로 분류

### **16.3 로지스틱 회귀 모델의 복잡성과 변수 선택**

*   선형 회귀와 마찬가지로 로지스틱 회귀도 **단순한 모델**(예: 몸무게만으로 비만 예측)부터 **복잡한 모델**(예: 몸무게, 유전자형, 나이, 심지어 점성술적 별자리까지 사용)까지 만들 수 있습니다.
*   **연속형 데이터**(예: 몸무게, 나이)와 **이산형 데이터**(예: 유전자형, 점성술적 별자리)를 모두 사용할 수 있습니다.
*   **변수의 유용성 테스트**: 각 변수가 결과를 예측하는 데 유용한지 테스트할 수 있습니다. 선형 회귀와 달리 복잡한 모델과 단순한 모델을 직접적으로 쉽게 비교하기는 어렵지만, **Wald's test**와 같은 방법을 사용하여 특정 변수가 예측에 통계적으로 유의미한 영향을 미치는지 판단할 수 있습니다. 예를 들어, 동영상에서는 점성술적 별자리가 비만 예측에 전혀 도움이 되지 않는다고 설명합니다.

### **16.4 로지스틱 회귀에서 곡선을 맞추는 방법: 최대우도법(Maximum Likelihood)**

*   **선형 회귀와의 큰 차이점**: 선형 회귀는 **최소 제곱법(least squares)**을 사용하여 잔차(residuals) 제곱의 합을 최소화하는 직선을 찾습니다. 이를 통해 R-제곱을 계산하고 모델을 비교합니다.
*   **잔차 개념 없음**: 로지스틱 회귀에는 선형 회귀와 같은 '잔차' 개념이 없으므로 최소 제곱법을 사용할 수 없고 R-제곱도 계산할 수 없습니다.
*   **최대우도법 사용**: 대신 로지스틱 회귀는 **최대우도법(Maximum Likelihood)**을 사용합니다.
    *   이는 주어진 곡선에 따라 데이터를 관찰할 '우도(likelihood)'를 계산하는 방식입니다.
        - 우도: 데이터가 고정되어 있을 때, 다양한 모델 파라미터가 그 데이터를 설명할 수 있는 가능성
    *   다양한 S자형 곡선(선)을 시도하면서, 각 곡선이 실제 데이터를 얼마나 잘 설명하는지(즉, 데이터를 관찰할 우도가 얼마나 높은지) 계산합니다.
    *   모든 쥐에 대해 이 우도들을 곱한 다음, **가장 높은 우도 값(maximum likelihood)을 가진 곡선을 최종 모델로 선택**합니다.

**로지스틱 회귀는 샘플을 분류(classify)하는 데 사용되는 강력한 머신러닝 기법**입니다. 몸무게나 유전자형과 같은 다양한 유형의 연속형 및 이산형 데이터를 사용하여 분류를 수행할 수 있으며, 어떤 변수들이 분류에 유용한지 평가할 수도 있습니다. 확률을 제공하고 새로운 샘플을 분류하는 능력 때문에 매우 인기 있는 머신러닝 방법입니다.

## 17. 로지스틱 회귀: 계수(Coefficients)의 이해

### **17.1 로지스틱 회귀의 기본 복습**

*   **목적**: 로지스틱 회귀는 특정 결과가 발생할 **확률(probability)**을 예측하는 데 사용됩니다. 예를 들어, 쥐의 몸무게가 주어졌을 때 쥐가 비만일 확률을 예측합니다.
*   **그래프**: y축은 쥐가 비만일 확률(0부터 1까지)을 나타내며, 데이터에는 **S자형 점선(dotted line)**이 맞춰집니다.
    *   무거운 쥐는 비만일 확률이 1에 가깝고, 가벼운 쥐는 0에 가깝습니다. 중간 몸무게 쥐는 0.5에 가까운 중간 확률을 가집니다.

### **17.2 로지스틱 회귀의 일반화된 선형 모델(GLM) 특성**

*   로지스틱 회귀는 **일반화된 선형 모델(Generalized Linear Model, GLM)**의 특정 유형입니다.
*   이는 선형 모델(Linear Models)의 개념과 능력을 일반화한 것으로, 선형 모델에 익숙하다면 로지스틱 회귀를 이해하는 데 큰 도움이 됩니다.

### **17.3 연속형 변수(예: 몸무게)를 사용한 로지스틱 회귀 계수**

몸무게와 같은 연속형 변수를 사용하여 비만을 예측하는 경우에 로지스틱 회귀 계수를 어떻게 이해하는지 설명합니다.

*   **선형 회귀와의 비교**:
    *   선형 회귀는 데이터에 직선을 맞추며, y축은 이론적으로 어떤 숫자든 될 수 있습니다.
    *   로지스틱 회귀는 y축이 0과 1 사이의 확률 값으로 제한된다는 문제가 있습니다.
*   **Y축 변환: 로그 오즈(Log Odds) 사용**:
    *   이 문제를 해결하기 위해 로지스틱 회귀에서는 y축을 **비만 확률(probability of obesity)**에서 **비만의 로그 오즈(log odds of obesity)**로 변환합니다.
    *   **로짓 함수(logit function)**를 사용하여 이 변환을 수행하며, 이 변환을 통해 y축은 선형 회귀처럼 음의 무한대에서 양의 무한대까지 확장됩니다.
    *   이 변환 덕분에 S자형 곡선이 **직선**으로 바뀌게 됩니다.
    *   **중요**: 로지스틱 회귀의 계수는 이 **로그 오즈 그래프**를 기준으로 표현됩니다.
*   **계수의 해석**:
    *   **절편(Intercept)**: 몸무게(weight)가 0일 때 **비만의 로그 오즈**를 의미합니다.
    *   **기울기(Slope)**: 몸무게가 1단위 증가할 때마다 **비만의 로그 오즈**가 얼마나 증가하는지를 나타냅니다.
*   **통계적 유의성 평가 (Wald's Test)**:
    *   계수의 통계적 유의성은 **Z 값(Z value)**과 **p-값(p-value)**을 사용하여 **Wald's test**로 평가합니다.
    *   Z 값은 추정된 계수가 0에서 표준 편차의 몇 배만큼 떨어져 있는지를 나타냅니다. Z 값이 0으로부터 2 표준 편차 미만이면 통계적으로 유의미하지 않다고 판단할 수 있습니다.

### **17.4 이산형 변수(예: 변이 유전자 유무)를 사용한 로지스틱 회귀 계수**

변이 유전자 유무와 같은 이산형 변수를 사용하여 비만을 예측하는 경우 로지스틱 회귀 계수를 어떻게 이해하는지 설명합니다. 이는 선형 모델에서 **t-test**를 수행하는 방식과 매우 유사합니다.

*   **선형 모델의 t-test와의 유사성**:
    *   정상 유전자를 가진 쥐와 변이 유전자를 가진 쥐의 크기를 측정하고, 각 그룹의 평균 크기를 나타내는 두 개의 선을 데이터에 맞추는 방식으로 t-test를 수행할 수 있습니다.
*   **Y축 변환**: 이 경우에도 y축은 비만 확률에서 **비만의 로그 오즈**로 변환됩니다.
*   **두 개의 선 맞추기**:
    *   정상 유전자를 가진 쥐의 **비만 로그 오즈**를 나타내는 첫 번째 선.
    *   변이 유전자를 가진 쥐의 **비만 로그 오즈**를 나타내는 두 번째 선.
*   **계수의 해석**:
    *   **절편(Intercept)**: **정상 유전자를 가진 쥐의 비만 로그 오즈**를 의미합니다.
    *   **변이 유전자 항 (Log Odds Ratio)**: 변이 유전자를 가진 쥐의 로그 오즈에서 정상 유전자를 가진 쥐의 로그 오즈를 뺀 값입니다. 이는 **로그 오즈 비율(log odds ratio)**을 나타내며, 변이 유전자를 가지고 있는 것이 비만 오즈(odds)를 로그 스케일에서 얼마나 증가시키거나 감소시키는지 알려줍니다.
*   **통계적 유의성 평가**:
    *   변이 유전자 항(로그 오즈 비율)의 Z 값이 2보다 크면 통계적으로 유의미하다고 볼 수 있으며, 이는 변이 유전자가 비만과 관련이 있음을 시사합니다. p-값이 0.05 미만으로 확인될 수 있습니다.

결론적으로, 로지스틱 회귀는 선형 모델과 동일한 방식으로 작동하지만, **계수가 로그 오즈 스케일**로 표현됩니다.
*   이러한 특성 덕분에 **다중 회귀(multiple regression)**나 **분산 분석(ANOVA)**과 같은 선형 모델의 고급 기법들을 로지스틱 회귀에서도 동일하게 적용할 수 있습니다. 다만, 계수들이 로그 오즈 스케일이라는 점만 기억

## 18. 로지스틱 회귀: 최대 우도를 사용한 최적선 맞추기

로지스틱 회귀는 선형 회귀와 유사하지만, 주요 차이점은 Y축에 **로그 오즈(log odds)**를 사용한다는 점입니다.

### 18.1  **선형 회귀 복습 (최소 제곱)**:
*   선형 회귀에서는 데이터에 선을 맞출 때 **최소 제곱(least squares)** 방법을 사용합니다.
*   이 방법은 데이터 포인트와 선 사이의 거리인 잔차(residuals)를 측정하고, 이 잔차들을 제곱하여 음수 값이 상쇄되지 않도록 한 다음 모두 더합니다.
*   이 잔차 제곱의 합(sum of squared residuals)이 가장 작은 선이 가장 적합한 선으로 선택됩니다.

### 18.2  **로지스틱 회귀에서 최소 제곱을 사용할 수 없는 이유**:
*   로지스틱 회귀는 Y축을 **비만 확률(probability of obesity)에서 비만의 로그 오즈(log odds of obesity)**로 변환합니다.
*   이 변환으로 인해 원본 데이터가 양의 무한대 또는 음의 무한대로 밀려나게 됩니다. 따라서 데이터 포인트와 선 사이의 잔차도 양의 무한대 또는 음의 무한대가 됩니다.
*   결과적으로, 잔차가 무한대가 되므로 선형 회귀에서 사용하는 최소 제곱 방법을 로지스틱 회귀에서는 사용 불가.
*   이러한 이유로 로지스틱 회귀에서는 **최대 우도(maximum likelihood)** 방법을 사용합니다.

### 18.3  **최대 우도 방법 단계**:
*   **후보 선 설정**: 먼저 데이터에 맞을 것으로 예상되는 후보 선(후보 로그 오즈 선)을 그립니다.
*   **로그 오즈 추출 및 확률로 변환**:
    *   원본 데이터 포인트를 이 후보 선에 투영하여 각 샘플의 **후보 로그 오즈 값**을 얻습니다.
    *   이 후보 로그 오즈 값을 사용하여 **후보 확률**로 변환합니다. 이는 확률을 로그 오즈로 변환하는 공식의 재정렬(역변환)을 통해 이루어집니다.
*   **우도(Likelihood) 계산**:
    *   이제 관측된 데이터(예: 비만 쥐와 비만 아님 쥐)의 **우도**를 계산합니다.
    *   **비만 쥐**의 경우, 주어진 곡선 형태에서 쥐가 비만일 우도는 곡선 위의 Y축 값, 즉 **예측된 확률**과 동일합니다.
    *   **비만 아님 쥐**의 경우, 쥐가 비만일 확률이 낮을수록 비만이 아닐 확률은 높아집니다. 따라서 비만 아님 쥐의 우도는 **1 - (쥐가 비만일 확률)**로 계산됩니다.
*   **전체 우도 또는 로그-우도(Log-Likelihood) 계산**:
    *   모든 개별 우도들을 곱하여 **전체 우도**를 계산할 수 있습니다.
    *   하지만 통계학자들은 계산 편의상 개별 우도들의 로그 값을 취한 후 모두 더하는 **로그-우도(log-likelihood)**를 선호합니다. 우도와 로그-우도 모두 동일한 최적선을 가리킵니다.
*   **선 회전 및 반복**:
    *   이제 로그 오즈 선을 조금씩 회전시키고, 각 회전된 선에 대해 위 과정을 반복하여 새로운 로그-우도를 계산
    *   이때 사용되는 알고리즘은 로그-우도를 증가시키는 방향으로 선을 회전시키는 "스마트한" 방식을 사용
    *   이 과정을 반복하여 **로그-우도를 최대화하는 선**을 찾게 되며, 이 선이 데이터에 가장 잘 맞는 최적선으로 선택

### 18.4  **추가 고려 사항**:
*   선을 맞추는 것 외에도 로지스틱 회귀 모델의 유용성을 평가하기 위해 R-제곱(R-squared) 값과 p-값(p-value)을 알아야 합니다.
*   하지만 로지스틱 회귀에서는 일반적인 잔차를 사용할 수 없기 때문에, 이 값들을 계산하는 다른 방법이 있다.

요약하자면, 로지스틱 회귀는 선형 회귀와 달리 Y축에 로그 오즈를 사용하기 때문에 잔차 기반의 최소 제곱법을 쓸 수 없고, 대신 데이터의 우도를 최대화하는 **최대 우도** 방법을 사용하여 가장 적합한 모델 파라미터를 찾습니다. 이 개념은 AI 분야, 특히 분류(classification) 문제에서 매우 중요하게 사용되므로 잘 이해하는 것이 좋습니다.

## 19. 로지스틱 회귀: R-제곱과 p-값 이해하기
로지스틱 회귀(Logistic Regression) 모델의 **유용성**을 평가하는 중요한 방법인 **R-제곱(R-squared)**과 **p-값(p-value)** 계산에 대해 요약해 드리겠습니다. 이전 대화에서 로지스틱 회귀가 **최대 우도(Maximum Likelihood)**를 사용하여 최적선을 찾는 방법을 다루었다.

### 19.1  **문제 제기: R-제곱과 p-값 계산의 복잡성**
*   선형 회귀 모델의 경우, R-제곱과 p-값 계산 방법은 모두가 동의하는 표준화된 방식이 있습니다.
*   그러나 **로지스틱 회귀**에서는 이들을 계산하는 방법이 **10가지 이상**이며 합의된 방식이 없습니다. 이는 일반화 선형 모델(Generalized Linear Models)의 복잡성 때문입니다.
*   따라서 특정 분야에서 이미 사용되고 있는 R-제곱 계산 방법을 따르는 것이 좋은 시작점이 될 수 있습니다. 이 영상에서는 **McFadden의 의사 R-제곱(McFadden's pseudo R-squared)**을 다루는데, 이는 일반적으로 사용되며 R 소프트웨어에서 쉽게 계산할 수 있고, 선형 모델의 R-제곱 계산 방식과 유사합니다.

### 19.2  **선형 회귀의 R-제곱 복습 (간략하게)**
*   선형 회귀에서 R-제곱은 **잔차(residuals)**를 사용하여 계산됩니다.
*   **잔차 제곱의 합(SS fit)**은 최적 적합선(best fitting line) 주변의 잔차를 제곱하여 더한 값, "좋은 적합"의 척도.
*   **평균 제곱의 합(SS mean)**은 Y축 값의 평균 주변 잔차의 제곱합으로, "나쁜 적합"의 척도입니다.
*   R-제곱은 SS fit을 SS mean과 비교하여, 평균 주변의 변동성 중 모델이 설명하는 비율을 나타냅니다.
*   R-제곱은 0과 1 사이의 값을 가지며, 0은 관계가 없음을, 1은 완벽한 적합을 의미합니다.

### 19.3  **로지스틱 회귀의 R-제곱 (McFadden의 의사 R-제곱)**
*   **잔차 사용 불가**: 로지스틱 회귀에서는 잔차 값이 무한대가 될 수 있으므로, 선형 회귀처럼 잔차를 직접 사용 불가.
*   **로그 우도(Log Likelihood) 활용**: 대신 **로그 우도**를 사용하여 "좋은 적합"과 "나쁜 적합"을 측정합니다.
    *   **LL fit (fitted line의 로그 우도)**: 이는 최적 적합선(최대 우도로 찾아낸 선)을 기반으로 데이터의 로그 우도를 계산한 값입니다. 데이터 포인트를 최적 적합선에 투영하고, 로그 오즈를 확률로 변환한 다음 로그 우도를 계산합니다. **모델이 데이터에 잘 맞을수록 LL fit은 0에 가까운 값**을 가집니다.
    *   **LL overall probability (전체 확률의 로그 우도)**: 이는 가중치를 고려하지 않고 단순히 전체 비만 확률(또는 Y축 값의 평균)을 사용하여 데이터의 로그 우도를 계산한 값입니다. 이는 Y축에 대한 수평선에 해당합니다. **모델이 데이터에 잘 맞지 않을수록 LL overall probability는 더 큰 음수 값**을 가집니다.
*   **R-제곱 계산**: 로지스틱 회귀의 R-제곱은 LL fit과 LL overall probability를 사용하여 선형 회귀와 유사한 방식으로 계산됩니다.
*   **R-제곱 값의 범위**: 이 방식으로 계산된 R-제곱 역시 모델이 예측력이 없을 때는 0, 예측력이 완벽할 때는 1의 값을 가집니다. 로지스틱 회귀의 로그 우도는 0에서 음의 무한대 사이의 값을 가집니다.

### 19.4  **로지스틱 회귀의 p-값**
*   p-값 계산은 비교적 간단합니다.
*   **카이제곱(Chi-squared) 값**: `2 * (LL fit - LL overall probability)`의 차이가 카이제곱 값을 형성
*   **자유도(Degrees of Freedom)**: 자유도는 두 모델(LL fit 모델과 LL overall probability 모델)의 파라미터 수 차이와 같습니다. LL fit 모델은 y-절편과 기울기 두 개의 파라미터를 가지며, LL overall probability 모델은 y-절편 하나만 가지므로, 이 경우 자유도는 1입니다.
*   **해석**: 이 카이제곱 값과 자유도를 사용하여 p-값을 계산할 수 있습니다. p-값이 작을수록(예: 0.05 미만) 모델의 관계가 우연이 아님을 통계적으로 나타냅니다.

### 19.5  **기타 고려사항: 포화 모델(Saturated Model)**
*   실제로 이 R-제곱 및 p-값 공식은 "포화 모델(saturated model)" 항을 포함하는 경우가 많습니다.
    - 포화 모델은 데이터의 각 관측치마다 정확히 맞는 예측을 하는 ‘완벽한 모델’입니다.
    - 이런 모델은 현실적으로 쓸 수 없지만, 다른 모델과 비교할 때 기준점(baseline)으로 사용
    - 포화 모델항: 모델 평가 지표 계산식에 들어가는, 포화 모델의 로그 우도 값을 말합니다
*   그러나 로지스틱 회귀의 경우 포화 모델의 로그 우도가 0이므로, 이 항은 생략될 수 있습니다. 일반화 선형 모델에서는 0이 아닐 수 있기 때문에 일반적으로 포함됩니다.

**요약하자면,** 로지스틱 회귀에서 모델의 유용성을 평가하기 위해 선형 회귀와는 다른 접근 방식이 필요하다는 것을 이해하는 것이 중요합니다. 잔차 대신 **로그 우도**를 활용하여 **McFadden의 의사 R-제곱**을 계산하고, 이 로그 우도 차이를 통해 **p-값**을 도출하여 모델의 통계적 유의미성과 효과 크기를 평가합니다. 이 개념들은 분류(classification) 문제 해결에 있어 매우 중요한 기반 지식이 될 것입니다.

## 20. 포화 모델과 이탈도 통계량: 로지스틱 회귀 모델 평가의 심층 이해
**포화 모델(Saturated Model)**과 **이탈도 통계량(Deviance Statistics)**이 무엇이며, 로지스틱 회귀(Logistic Regression)에서 모델의 성능을 평가하는 R-제곱 및 p-값 계산에 어떻게 사용되는지 설명해 드리겠습니다. 이 개념들은 일반화 선형 모델(Generalized Linear Models)의 중요한 부분입니다.

### 20.1  **세 가지 핵심 모델 이해하기**:
모델의 적합도를 평가하기 위해 우리는 주로 세 가지 다른 유형의 모델을 고려하고 이들의 **로그 우도(log likelihood)**를 계산합니다.

*   **널 모델(Null Model)**:
    *   이것은 **가장 단순한 모델**입니다.
    *   예를 들어, 데이터의 평균을 추정하는 단일 매개변수만 가진 모델입니다.
    *   널 모델의 로그 우도는 데이터에 대한 **최악의 시나리오**를 나타내는 기준으로 사용됩니다.

*   **제안 모델(Proposed Model)**:
    *   이것은 우리가 **실제로 사용하고자 하는 모델**입니다.
    *   널 모델보다 더 많은 매개변수를 가집니다. 예를 들어, 두 개의 정규 곡선을 데이터에 맞추어 두 개의 평균을 추정하는 모델일 수 있습니다.

*   **포화 모델(Saturated Model)**:
    *   이것은 **가장 복잡한 모델**이며, 데이터에 **완벽하게 적합**합니다.
    *   **데이터 포인트당 하나의 매개변수**를 가집니다. 즉, 모든 데이터 포인트에 완벽하게 들어맞도록 매개변수의 수를 최대화합니다.
    *   포화 모델의 로그 우도는 데이터에 대한 **최고의 시나리오**, 즉 가능한 **최대 우도**를 나타내는 **상한선**

### 20.2  **로그 우도 기반 R-제곱에 포화 모델이 필요한 이유**:
*   선형 회귀의 일반적인 R-제곱은 잔차 제곱의 합(sums of squares of residuals)을 사용하여 계산되며, 완벽한 적합의 경우 잔차가 0이므로 고정된 '최고' 값(0)이 있습니다.
*   그러나 로그 우도 기반 R-제곱에서는 **어떤 상황에서나 적용될 수 있는 고정된 '최적' 값이 없습니다**.
*   이때 **포화 모델**이 등장하여 **이상적인 적합에 대한 상한선**을 제공합니다.
*   만약 제안 모델의 로그 우도가 포화 모델의 로그 우도와 같다면, R-제곱은 1이 됩니다.
*   포화 모델이 없으면 R-제곱 값이 0에서 1을 벗어날 수 있습니다 (예: 1.36). 포화 모델의 로그 우도를 포함함으로써 R-제곱 값은 항상 0에서 1 사이가 되도록 보장됩니다.

### 20.3  **이탈도 통계량(Deviance Statistics)의 개념**:
R-제곱의 p-값을 계산하기 위해 이탈도 통계량이라는 개념을 사용합니다. 이탈도는 로그 우도의 차이에 2를 곱하여 카이제곱(chi-squared) 분포를 따르도록 만듭니다.

*   **잔차 이탈도(Residual Deviance)**:
    *   **정의**: `2 * (포화 모델의 로그 우도 - 제안 모델의 로그 우도)`.
    *   **용도**: 이 값은 제안 모델이 포화 모델과 얼마나 유의미하게 다른지를 나타냅니다. 잔차 이탈도에 대한 p-값은 제안 모델이 포화 모델보다 더 나쁜 예측을 하는지 여부를 알려줍니다. 이 검사가 제대로 작동하려면 제안 모델이 포화 모델의 더 간단한 버전이어야 합니다 (내포된 관계).

*   **널 이탈도(Null Deviance)**:
    *   **정의**: `2 * (포화 모델의 로그 우도 - 널 모델의 로그 우도)`.
    *   **용도**: 이 값은 널 모델이 포화 모델과 얼마나 유의미하게 다른지를 나타냅니다. 널 이탈도에 대한 p-값은 널 모델이 포화 모델보다 훨씬 더 나쁜 예측을 하는지 여부를 알려줍니다.

### 20.4  **R-제곱의 p-값 계산**:
*   R-제곱의 p-값은 **널 이탈도와 잔차 이탈도의 차이**를 통해 계산할 수 있습니다. 이 차이 값은 카이제곱 분포를 따르며, 자유도(degrees of freedom)는 제안 모델과 널 모델의 매개변수 수 차이와 같습니다.
*   이 카이제곱 값과 자유도를 사용하여 p-값을 얻을 수 있습니다 (예: 0.002). 작은 p-값은 모델의 R-제곱 값이 우연이 아님을 통계적으로 나타냅니다.
*   대안으로, R-제곱의 유의미성은 제안 모델과 널 모델의 로그 우도를 직접 비교하여 `2 * (제안 모델의 로그 우도 - 널 모델의 로그 우도)`를 통해 카이제곱 값을 계산할 수도 있습니다. 그러나 널 이탈도와 잔차 이탈도가 자주 보고되기 때문에 이탈도 차이를 통한 방법도 중요하게 다뤄집니다.

### 20.5  **로지스틱 회귀에서 포화 모델을 무시할 수 있는 이유**:
*   로지스틱 회귀를 수행할 때는 데이터에 완벽하게 맞는 곡선(squiggle)을 상상합니다.
*   **로지스틱 회귀의 포화 모델은 데이터를 완벽하게 적합하므로, 데이터의 로그 우도는 항상 0이 됩니다** (로그(1)이 0이기 때문입니다).
*   따라서 로지스틱 회귀에서 R-제곱 및 이탈도 공식에서 포화 모델의 로그 우도 항은 0이 되어 **무시할 수 있습니다**.
*   결과적으로, 잔차 이탈도와 널 이탈도 공식은 `2 * -(로그 우도)`의 형태로 단순화됩니다. 로그 우도가 음수이므로 최종 이탈도 값은 양수가 됩니다.

**결론적으로**, 로지스틱 회귀 모델의 유용성을 평가할 때 **포화 모델**은 R-제곱 값의 합리적인 상한선을 제공하고, **이탈도 통계량**은 모델의 통계적 유의미성(p-값)을 계산하는 데 사용된다는 점을 이해하는 것이 중요합니다. 특히 로지스틱 회귀에서는 포화 모델의 로그 우도가 항상 0이므로, 계산을 단순화할 수 있다는 점을 기억해야 합니다.

## 21. R을 활용한 로지스틱 회귀 분석

R을 사용하여 **로지스틱 회귀(Logistic Regression)**를 수행하는 과정을 심장 질환 데이터셋을 예시로 들어 명확하게 설명합니다. 로지스틱 회귀는 특히 이진 분류 문제, 즉 어떤 사건이 발생할지 안 할지(예: 심장 질환 유무)를 예측시 유용

### **21.1 데이터 준비 및 전처리 (Data Preparation and Preprocessing)**

*   **데이터 로드**: UCI 머신러닝 저장소에서 심장 질환 데이터셋을 직접 다운로드하여 R로 읽어옵니다.
*   **초기 데이터 확인**: `head()` 함수로 데이터의 첫 여섯 행을 보고, `str()` 함수로 데이터의 구조(변수 유형)를 확인
*   **열 이름 지정**: 처음에 열 이름이 없으므로, UCI 웹사이트에 명시된 이름으로 열 이름을 지정하여 가독성을 향상
*   **데이터 타입 변환**: 로지스틱 회귀 모델에 적합하게 데이터를 정제합니다.
    *   `sex`(성별)와 `CP`(가슴 통증) 같은 수치형 변수를 **팩터(factor)** 타입으로 변환합니다. `sex`는 0(여성)과 1(남성)로, `CP`는 1~3(다른 유형의 통증)과 4(통증 없음)로 변환됩니다.
    *   `CA`와 `Thal` 열의 `?` (물음표) 값을 **결측값(NA)**으로 변경한 후, `CA`는 정수형으로, `Thal`은 팩터형으로 변환합니다.
    *   `HD`(심장 질환) 변수도 0(건강)과 1(심장 질환)을 `healthy`와 `unhealthy`로 변환하여 **팩터(factor)**로 만듭니다.
*   **결측값 처리**: `NA` 값을 가진 샘플(총 6개)을 식별하고, 이 예제에서는 해당 샘플들을 데이터셋에서 제거합니다. 결측값을 채우는 **결측값 대체(imputation)** 방법도 있지만, 여기서는 제거하는 방식을 사용합니다.
*   **데이터 균형 확인**: `xtabs()` 함수를 사용하여 성별(sex)과 심장 질환(heart disease), 가슴 통증(CP) 등 예측에 사용될 범주형 변수들의 분포가 모델 학습에 충분한지 확인합니다. 예를 들어, 심장 질환 환자가 특정 성별에만 치우쳐 있지 않은지 확인합니다.
    - xtabs(): 범주형 변수들의 조합별 빈도를 정리한 교차표를 만들 때 쓰는 함수

### **21.2 로지스틱 회귀 모델 구축 (Building Logistic Regression Models)**

*   **GLM 함수 사용**: R의 `GLM` (Generalized Linear Models) 함수를 사용하여 로지스틱 회귀를 수행합니다. 이 함수는 일반화 선형 모델을 수행하며, `family = binomial`을 지정하여 로지스틱 회귀를 실행합니다.

*   **간단한 모델 (Simple Model)**:
    *   `sex`(성별) 변수만을 사용하여 `heart disease`(심장 질환)를 예측하는 모델을 만듭니다.
    *   `summary()` 함수를 통해 모델의 상세 결과를 확인합니다.
    *   **계수(Coefficients) 해석**: 모델의 계수는 **로그 오즈(log odds)**와 **로그 오즈 비율(log odds ratio)**을 나타냅니다. 예를 들어, `sex`가 남성일 때 심장 질환을 가질 **로그 오즈**의 증가량(로그 오즈 비율)을 해석
    *   **P-값 (P-values) 확인**: 계수들의 p-값이 0.05 미만인지 확인하여 통계적 유의성을 판단합니다. p-값이 작으면 해당 변수가 예측에 통계적으로 유의미하다는 것을 의미합니다.
    *   **Deviance Residuals, AIC 등**: 모델의 적합도를 평가하는 **잔차 이탈도(deviance residuals)**, **분산 모수(dispersion parameter)**, **널 이탈도(null deviance)**, **잔차 이탈도(residual deviance)**, **AIC(Akaike Information Criterion)** 값들을 확인합니다. AIC는 모델 비교에 사용될 수 있습니다.
    *   AIC는 두 가지를 동시에 평가합니다:
        - 모델의 적합도 (Goodness-of-fit)  
            - 모델이 데이터를 얼마나 잘 설명하는가? (예: 로그 우도가 높을수록 좋음)
        - 모델의 복잡도 (Number of parameters)  
            - 변수가 너무 많으면 과적합(overfitting) 위험이 있음
        - 다음과 같은 식으로 계산됩니다:
        \[
        \text{AIC} = -2 \times (\text{로그 우도}) + 2 \times (\text{모델 파라미터 수})
        \]
        - **적합도와 단순함 사이의 균형**을 잡는 지표! 


*   **복합 모델 (Fancy Model)**:
    *   데이터셋의 **모든 나머지 변수**를 사용하여 `heart disease`를 예측하는 복합 모델을 구축합니다. `HD ~ .`와 같은 수식 문법을 사용합니다.
    *   이 모델의 `summary()` 결과를 통해 각 변수의 유의성을 확인합니다. 예를 들어, `age` 변수는 p-값이 커서 유용한 예측 변수가 아닐 수 있음을 보여줍니다.
    *   간단한 모델과 복합 모델의 **잔차 이탈도**와 **AIC** 값을 비교하여 복합 모델이 더 잘 맞는 경향이 있음을 확인합니다 (값이 작을수록 좋음).
    *   **McFadden의 유사 R-제곱 (Pseudo R-squared)**을 계산하여 모델의 전체 **효과 크기(effect size)**를 측정합니다 (예: 0.55). 또한 이 R-제곱 값의 p-값을 계산하여 우연이 아님을 확인합니다.

### **21.3 모델 평가 및 시각화 (Model Evaluation and Visualization)**

*   **예측 확률 시각화**: 각 환자에 대한 심장 질환 예측 확률과 실제 심장 질환 상태를 보여주는 그래프를 그립니다.
*   **결과 해석**: 그래프를 통해 심장 질환이 있는 환자들은 높은 예측 확률을, 심장 질환이 없는 환자들은 낮은 예측 확률을 보이는 것을 확인하며, 로지스틱 회귀 모델이 잘 작동했음을 보여줍니다.
*   **교차 검증(Cross-validation) 언급**: 새로운 데이터에 대한 모델의 성능을 더 잘 평가하기 위해 **교차 검증**을 사용할 수 있음
*   **GGPLOT2를 이용한 그래프**: `ggplot2` 라이브러리를 사용하여 예측 확률을 기반으로 데이터를 시각화하는 과정을 간략하게 보여줍니다.

데이터를 로드하고 전처리하는 과정부터, 로지스틱 회귀 모델을 구축하고 해석하며, 마지막으로 시각화를 통해 모델의 성능을 평가하는 전반적인 흐름을 잘 보여줍니다. **데이터 분석 파이프라인**과 **로지스틱 회귀의 기본 개념**을 이해하는 데 훌륭한 자료가 될 것입니다.

## 22. 로지스틱 회귀 분석 모델 평가의 핵심: 이탈도 잔차(Deviance Residuals)
이탈도 잔차는 로지스틱 회귀 모델의 성능을 평가하고 데이터 내의 특이점(outliers)을 이해하는 데 중요한 개념입니다.
이탈도 잔차는 모델이 개별 데이터 포인트를 얼마나 잘 설명하는지 보여주는 측정치로, 로지스틱 회귀 모델의 적합도를 평가하고 특이점을 식별하는 데 사용됩니다.

### **22.1 잔차 이탈도(Residual Deviance)란 무엇인가?**

*   **잔차 이탈도의 정의**: 잔차 이탈도(Residual Deviance)는 **포화 모델(Saturated Model)**의 로그 우도(log likelihood)와 **제안된 모델(Proposed Model)**의 로그 우도 사이의 차이에 2를 곱한 값으로 정의됩니다.
    *   **포화 모델**: 모든 데이터 포인트를 완벽하게 설명하는 이론적인 모델입니다. 각 데이터 포인트에 대해 별도의 파라미터를 가집니다.
    *   **제안된 모델**: 우리가 구축한 로지스틱 회귀 모델과 같이 특정 변수들을 사용하여 데이터를 설명하려는 모델
*   **계산 방법**: 잔차 이탈도는 개별 데이터 포인트에 대한 포화 모델과 제안된 모델 간의 우도 차이를 모두 합산하여 계산할 수 있습니다. 즉, 각 데이터 포인트가 전체 이탈도에 기여하는 정도를 합한 것과 같습니다.

### **22.2 이탈도 잔차(Deviance Residuals)의 개념**

*   **정의**: 이탈도 잔차는 **개별 데이터 포인트가 전체 잔차 이탈도에 기여하는 정도를 나타내는 값의 제곱근**입니다.
    *   각 이탈도 잔차는 해당 데이터 포인트에 대한 로그 우도 차이의 제곱근을 나타냅니다.
*   **일반 최소 제곱(OLS) 잔차와의 유사성**: 이탈도 잔차는 일반 최소 제곱(Ordinary Least Squares, OLS) 회귀에서 사용되는 잔차와 유사합니다.
    *   OLS에서 잔차를 제곱하여 합하면 모델의 적합도를 평가하는 데 사용되는 **제곱합(sums of squares)**
    *   마찬가지로, 이탈도 잔차를 제곱하여 합하면 **잔차 이탈도**가 되며, 이는 모델이 데이터를 얼마나 잘 설명하는지 평가하는 데 사용됩니다.

### **22.3 로지스틱 회귀에서 이탈도 잔차의 활용**

*   **계산의 유연성**: 로지스틱 회귀를 수행할 때, 이탈도 잔차에 대한 방정식은 데이터와 **최적 적합선(best fitting line)** 사이의 거리를 기반으로 재작성될 수 있습니다.
*   **부호의 의미**:
    *   데이터 포인트가 '구불구불한 선'(로지스틱 회귀의 S자 곡선) 위에 있으면 **양수(+)**의 이탈도 잔차
    *   데이터 포인트가 '구불구불한 선' 아래에 있으면 **음수(-)**의 이탈도 잔차를 가집니다.
*   **특이점(Outliers) 식별**:
    *   이탈도 잔차를 XY 그래프에 플로팅하면, 이들은 **중심에 위치하고 0에 상대적으로 가까워야 합니다**.
    *   **0에서 멀리 떨어진 이탈도 잔차**를 가진 데이터 포인트는 모델에 잘 맞지 않으며 **특이점일 가능성**이 있습니다. 이러한 특이점들은 데이터 라벨링 오류나 다른 문제의 징후일 수 있으므로 주의 깊게 확인해야 합니다.
*   **요약**: 이탈도 잔차는 각 데이터 포인트가 전체 잔차 이탈도에 얼마나 기여하는지를 나타내며, 주로 **특이점을 식별**하는 데 사용됩니다.

R을 활용한 로지스틱 회귀 모델 구축 방법뿐만 아니라, 모델의 핵심 평가 지표 중 하나인 **잔차 이탈도**와 **이탈도 잔차**의 개념 및 이를 통해 모델의 적합도를 평가하고 데이터의 특이점을 찾아내는 방법까지 이해할 수 있을 것입니다. 이는 실제 데이터 분석 프로젝트에서 모델을 진단하고 개선하는 데 필수적인 지식입니다.

## 23. ROC 곡선과 AUC
ROC(Receiver Operating Characteristic) 곡선과 AUC(Area Under the Curve)는 기계 학습 모델의 성능을 평가하고 이해하는 데 매우 중요한 도구입니다. 이 개념들은 **혼동 행렬(Confusion Matrix)**, **민감도(Sensitivity)**, **특이도(Specificity)**에 대한 기본적인 이해를 바탕으로 합니다.

### **23.1 로지스틱 회귀와 분류 임계값의 필요성**
비만인 쥐와 비만이 아닌 쥐를 분류하는 로지스틱 회귀(Logistic Regression) 모델을 예시로 들어 설명합니다. 로지스틱 회귀는 쥐의 몸무게를 기반으로 쥐가 비만일 **확률**을 예측합니다. 하지만 이 확률을 '비만' 또는 '비만 아님'과 같은 **이진 분류(binary classification)**로 바꾸려면 **임계값(threshold)**이 필요합니다.

*   **기본 임계값**: 일반적으로 0.5를 임계값으로 사용하여 확률이 0.5보다 높으면 비만으로, 낮거나 같으면 비만 아님으로 분류합니다.
*   **임계값의 유연성**: 하지만 임계값은 0.5가 아닌 다른 값으로 설정할 수도 있습니다. 예를 들어, 에볼라 감염 여부를 분류하는 상황에서는 **음성(negative)을 잘못 판단하여 감염자를 놓치는 것(False Negative)**이 치명적일 수 있으므로, **임계값을 낮춰(예: 0.1)** 모든 감염자를 올바르게 분류하는 것이 매우 중요합니다. 이는 **오탐(False Positive)**의 증가를 감수하더라도 필요할 수 있습니다. 반대로, 매우 보수적으로 판단해야 할 때는 임계값을 높일 수도 있습니다.

### **23.2 혼동 행렬과 평가 지표**
각기 다른 임계값을 사용하면 **혼동 행렬**의 내용이 달라지며, 이에 따라 **민감도(Sensitivity)**와 **특이도(Specificity)**와 같은 성능 지표도 변합니다. 혼동 행렬은 모델이 얼마나 정확하게 예측했는지(참 양성, 참 음성)와 얼마나 잘못 예측했는지(거짓 양성, 거짓 음성)를 보여줍니다.

### **23.3 ROC 그래프의 이해**
다양한 임계값에 대한 혼동 행렬을 일일이 비교하는 것은 번거롭고 혼란스러울 수 있습니다. 이러한 문제를 해결하기 위해 **ROC 그래프**가 사용됩니다. 모든 가능한 임계값에 대한 모델의 성능을 **간단하고 시각적인 방식**으로 요약합니다.

*   **Y축**: **참 양성 비율(True Positive Rate, TPR)**이며, 이는 **민감도(Sensitivity)**와 같습니다. TPR은 실제 양성 샘플 중에서 모델이 올바르게 양성으로 분류한 비율을 나타냅니다.
*   **X축**: **거짓 양성 비율(False Positive Rate, FPR)**이며, 이는 **1 - 특이도(1 - Specificity)**와 같습니다. FPR은 실제 음성 샘플 중에서 모델이 잘못해서 양성으로 분류한 비율을 나타냅니다.
*   **그래프 생성**: 임계값을 0부터 1까지 변화시키면서 각 임계값에 해당하는 (FPR, TPR) 쌍을 ROC 공간에 점으로 찍고, 이 점들을 연결하여 곡선을 만듭니다.
    *   예를 들어, 모든 샘플을 비만으로 분류하는 매우 낮은 임계값(예: 0)을 사용하면, 모든 실제 비만 쥐는 올바르게 분류되므로 TPR은 1이 되고, 모든 실제 비만 아님 쥐는 잘못 분류되므로 FPR도 1이 됩니다 (점: (1,1)).
    *   반대로, 모든 샘플을 비만 아님으로 분류하는 매우 높은 임계값(예: 1)을 사용하면, TPR은 0이 되고 FPR도 0이 됩니다 (점: (0,0)).
*   **해석**: ROC 그래프는 임계값에 따라 **참 양성 비율과 거짓 양성 비율이 어떻게 변하는지**를 보여줍니다. 대각선(y=x) 위에 있는 점들은 무작위 분류보다 좋은 성능을 의미하며, **곡선이 왼쪽 상단(0,1)에 가까울수록** 모델의 성능이 좋다고 할 수 있습니다. ROC 곡선은 특정 임계값에서 허용할 수 있는 거짓 양성 비율에 따라 **최적의 임계값을 식별**하는 데 도움을 줍니다.

### **23.4 AUC (Area Under the Curve)의 이해**
**AUC**는 **ROC 곡선 아래의 면적**을 나타내는 단일 값입니다.

*   **의미**: AUC 값은 0에서 1 사이의 값을 가지며, **값이 1에 가까울수록 모델의 분류 성능이 우수함**을 의미합니다.
*   **활용**: AUC는 서로 다른 **분류 방법(예: 로지스틱 회귀 대 랜덤 포레스트)**의 성능을 **쉽게 비교**하는 데 사용됩니다. 예를 들어, 빨간색 ROC 곡선의 AUC가 파란색 ROC 곡선의 AUC보다 크다면, 빨간색 모델이 더 좋은 성능을 가짐을 나타냅니다.

### **23.5 다른 평가 지표: 정밀도 (Precision)**
ROC 그래프는 TPR과 FPR을 사용하지만, 다른 지표를 활용할 수도 있습니다. 예를 들어, **정밀도(Precision)**는 종종 거짓 양성 비율을 대체하는 데 사용됩니다.

*   **정밀도**: 모델이 양성으로 예측한 결과 중 실제 양성인 비율을 의미합니다 (True Positives / (True Positives + False Positives)).
*   **정밀도의 유용성**: 샘플 불균형이 심한 경우(예: 희귀 질병 연구에서 질병이 없는 사람이 훨씬 많은 경우), 정밀도가 더 유용할 수 있습니다. 이는 정밀도 계산에 **참 음성(True Negatives)**이 포함되지 않아 데이터 불균형의 영향을 덜 받기 때문입니다.

**결론적으로,** ROC 곡선은 다양한 임계값에 대한 모델의 성능을 시각적으로 요약하여 **최적의 의사결정 임계값을 식별**하게 해주며, AUC는 모델의 전반적인 분류 성능을 단일 숫자로 제공하여 **서로 다른 분류 모델을 비교**하는 데 효과적인 도구입니다. AI 모델의 성능을 평가하고 개선하는 데 필수적인 개념이다.

## 24. ROC and AUC in R

### **24.1 사용되는 R 패키지**
*   **`pROC`**: ROC 그래프를 그리고 관련 계산을 수행하는 데 사용되는 핵심 라이브러리입니다.
*   **`randomForest`**: 예시에서 다른 분류 모델(랜덤 포레스트)을 만들기 위해 사용됩니다. 영상은 랜덤 포레스트가 샘플을 분류하는 방법이며, 임계값을 변경할 수 있다는 점만 알면 된다고 설명합니다.

### **24.2 예제 데이터셋 생성**
100개의 샘플로 구성된 가상의 데이터셋을 생성합니다.
*   **몸무게 데이터(`weight`)**: 평균 172, 표준편차 29의 정규 분포를 따르는 100개의 무작위 값을 생성한 뒤 정렬하여 몸무게 데이터를 만듭니다.
*   **비만 분류(`obese`)**: 생성된 몸무게를 기준으로 각 샘플이 '비만' 또는 '비만 아님'으로 분류됩니다. 몸무게를 랭크(1부터 100까지)로 변환하고 100으로 스케일링한 후, 0과 1 사이의 무작위 숫자와 비교하여 무작위 숫자가 스케일링된 랭크보다 작으면 '비만'(1)으로, 그렇지 않으면 '비만 아님'(0)으로 분류합니다. 가벼운 샘플은 대부분 '비만 아님'이고 무거운 샘플은 대부분 '비만'으로 분류됩니다.

### **24.3 로지스틱 회귀 모델 적용**
`glm` 함수를 사용하여 몸무게 데이터를 기반으로 로지스틱 회귀 곡선을 데이터에 적합시킵니다. 이 모델은 각 개체가 비만일 **예측 확률**을 제공합니다. `glm.fit$fitted.values`에 이 추정된 확률들이 저장됩니다.

### **24.4 ROC 곡선 그리기 및 AUC 계산**
*   **`roc` 함수 사용**: `pROC` 라이브러리의 `roc` 함수를 사용하여 ROC 그래프를 그립니다. 이 함수에 각 샘플의 **'알려진 분류'(obese 변수)**와 **'추정된 확률'(glm.fit$fitted.values)**을 전달합니다. `plot=TRUE` 옵션을 사용하여 그래프를 그리도록 지시합니다.
*   **결과 해석**: `roc` 함수를 실행하면 여러 정보가 출력되는데, 가장 중요한 부분은 **AUC(Area Under the Curve)** 값입니다.
*   **그래프 형태**: 기본적으로 `roc` 함수는 x축에 '특이도(Specificity)'를 표시하므로, x축이 1에서 0으로 역방향으로 진행됩니다. 이를 '1 - 특이도' 즉 '거짓 양성 비율(False Positive Rate, FPR)'로 바꾸고 싶다면 `legacy.axes=TRUE` 옵션을 설정해야 합니다.

### **24.5 ROC 그래프 커스터마이징**
*   **축 라벨 변경**: `percent=TRUE` 옵션을 사용하여 축 값을 백분율로 표시하고, x축을 'False Positive Percentage', y축을 'True Positive Percentage'로 명확하게 라벨링할 수 있습니다. 이는 민감도와 특이도가 무엇인지 기억하기 어려운 경우에 유용하다고 설명합니다.
*   **스타일 변경**: `col` 매개변수를 사용하여 ROC 곡선의 색상을 변경하고, `lwd` 매개변수를 사용하여 선의 두께를 조절할 수 있습니다.
*   **불필요한 패딩 제거**: `par(pty='s')` 설정을 통해 그래프 주변의 불필요한 여백을 제거하여 더 깔끔한 그래프를 얻을 수 있습니다.

### **24.6 임계값 추출 및 특정 영역 분석**
`roc` 함수가 수행하는 계산 결과를 변수에 저장하면, ROC 곡선을 구성하는 다양한 '참 양성 비율(TPP)', '거짓 양성 비율(FPP)', 그리고 이에 해당하는 **'임계값(thresholds)'**을 확인할 수 있습니다.
*   **임계값과 ROC 곡선**: 임계값이 음의 무한대(모든 샘플을 비만으로 분류)이면 TPP와 FPP 모두 100%가 되어 ROC 곡선의 오른쪽 상단에 해당하고, 임계값이 양의 무한대(모든 샘플을 비만 아님으로 분류)이면 TPP와 FPP 모두 0%가 되어 ROC 곡선의 왼쪽 하단에 해당합니다.
*   **관심 영역의 임계값**: 특정 TPP 범위(예: 60%에서 80% 사이)에 해당하는 FPP 및 임계값들을 분리하여, **참 양성과 거짓 양성의 최적 균형을 이루는 임계값을 선택**하는 데 활용할 수 있습니다.

### **24.7 부분 AUC(Partial AUC) 계산 및 표시**
모델이 소수의 거짓 양성만 허용하는 특정 영역에 초점을 맞추고 싶을 때 **부분 AUC**를 계산하고 그릴 수 있습니다.
*   `print.auc=TRUE`로 AUC 값을 그래프에 직접 출력하고, `partial.auc` 매개변수에 원하는 특이도(Specificity) 범위를 지정하여 부분 AUC를 계산합니다.
*   `auc.polygon=TRUE`를 설정하여 부분 면적을 시각적으로 표시하고, `auc.polygon.col`로 색상을 지정할 수 있습니다.

### **24.8 두 ROC 곡선 비교**
여러 모델의 성능을 비교하기 위해 두 개의 ROC 곡선을 겹쳐서 그릴 수 있습니다.
*   기존 로지스틱 회귀 ROC 곡선을 먼저 그린 후, `plot.roc` 함수를 사용하여 **랜덤 포레스트 모델**의 ROC 곡선을 추가합니다. `add=TRUE` 매개변수를 사용하여 기존 그래프에 추가하도록 지시합니다.
*   각 곡선의 AUC 값을 그래프에 출력하고 (겹치지 않도록 위치 조정), 범례(legend)를 추가하여 어떤 곡선이 어떤 모델을 나타내는지 명확히 합니다.

**결론적으로**, 이 영상은 AI 모델의 성능을 평가하는 데 필수적인 ROC와 AUC 개념을 R을 통해 실습하며 이해할 수 있도록 돕습니다. 데이터 생성부터 모델 학습, ROC 그래프 그리기, 커스터마이징, 그리고 여러 모델 비교에 이르기까지 **실제 데이터 과학 작업에서 ROC와 AUC를 어떻게 활용하는지에 대한 구체적인 가이드**를 제공합니다. 이론적 지식을 실용적인 R 코드와 연결하여 모델 평가 역량을 키울 수 있을 것입니다.

## 25. 정규화(Regularization) 기법의 이해: 릿지 회귀 (L2 정규화)

**정규화(Regularization)**라는 머신러닝 기법 중 하나인 **릿지 회귀(Ridge Regression)**를 명확하게 설명합니다. 릿지 회귀는 모델의 **과적합(overfitting)**을 방지하고 예측 성능을 향상시키는 데 목적이 있습니다.

이 내용을 이해하기 위해서는 머신러닝의 **편향(bias)**과 **분산(variance)** 개념, 선형 모델, 그리고 **교차 검증(cross-validation)**에 대한 기본적인 이해가 도움이 됩니다.

### **25.1 릿지 회귀가 필요한 이유: 과적합 문제 해결**

먼저, 선형 회귀(최소 제곱법)의 한계점을 살펴봅시다.

*   **선형 회귀 (최소 제곱법)**: 데이터에서 가중치와 크기 간의 관계를 모델링하기 위해 최소 제곱법(least squares)을 사용하여 선(방정식)을 찾습니다. 이 방법은 잔차 제곱의 합(sum of squared residuals)을 최소화하는 선을 찾습니다. 데이터 측정치가 많을 때는 이 선이 관계를 정확하게 반영할 수 있습니다.
*   **과적합(Overfitting) 발생**: 하지만 훈련 데이터(training data)가 매우 적을 때 (예: 두 개의 데이터 포인트만 있는 경우), 최소 제곱법으로 찾은 선은 이 두 점을 완벽하게 지나가 잔차 제곱의 합이 0이 될 수 있습니다. 문제는 이렇게 만들어진 선이 **훈련 데이터에 과적합(overfit)**되어 새로운 데이터(테스트 데이터)에 대한 예측에서 **높은 분산(high variance)**을 보이는 경향이 있다는 것입니다. 즉, 훈련 데이터에는 너무 잘 맞지만, 실제 예측에서는 성능이 떨어집니다.

**릿지 회귀의 핵심 아이디어**는 훈련 데이터에 완벽하게 맞지 않더라도 새로운 데이터를 더 잘 예측할 수 있는 선을 찾는 것입니다. 이는 모델에 **약간의 편향(bias)을 도입**하는 대신 **분산을 크게 줄여(significant drop in variance)** 장기적으로 더 나은 예측을 제공하는 방식입니다.

### **25.2 릿지 회귀의 작동 원리**

최소 제곱법이 잔차 제곱의 합을 최소화하는 반면, **릿지 회귀**는 다음과 같은 식을 최소화합니다:

**잔차 제곱의 합 + 람다(λ) × (기울기(slope)의 제곱)**

*   **패널티 항(Penalty Term)**: 여기서 **`람다(λ) × (기울기의 제곱)`** 부분이 **릿지 회귀 패널티(Ridge Regression penalty)** 또는 L2 패널티라고 불립니다. 이 항은 전통적인 최소 제곱법에 추가적인 제약을 가합니다.
*   **람다(λ)의 역할**: **람다(λ)**는 이 패널티의 강도를 결정하는 값입니다.
    *   **람다(λ)가 0일 경우**: 패널티 항이 0이 되므로 릿지 회귀는 최소 제곱법과 동일해집니다.
    *   **람다(λ)가 커질수록**: 기울기(slope)가 점점 작아져 0에 점근적으로 가까워집니다. 이는 예측이 독립 변수(예: 가중치)에 덜 민감해지도록 만들며, 모델의 분산을 줄이는 데 기여합니다.
*   **기울기 축소(Shrinking parameters)**: 릿지 회귀 패널티는 기울기가 작은 모델을 선호합니다. 기울기가 작다는 것은 독립 변수의 작은 변화에도 예측값이 크게 변하지 않아, 예측이 데이터에 덜 민감하다는 것을 의미합니다.
*   **패널티 적용 범위**: 일반적으로 릿지 회귀 패널티는 **y절편을 제외한 모든 매개변수(parameters)에 적용**됩니다. 각 매개변수는 해당 측정값에 의해 스케일링되기 때문에 y절편은 제외됩니다.

### **25.3 람다(λ) 값 결정 방법**

적절한 **람다(λ) 값**을 찾는 것이 중요합니다. 다양한 람다 값을 시도한 후, **교차 검증(cross-validation)**(일반적으로 10겹 교차 검증)을 사용하여 **가장 낮은 분산(lowest variance)**을 보이는 람다 값을 선택합니다.

### **25.4 릿지 회귀의 다양한 적용 및 강력한 장점**

릿지 회귀는 다양한 상황에서 활용될 수 있습니다:

*   **연속형 변수와 이산형 변수**: 체중(연속형)으로 크기를 예측하는 경우뿐만 아니라, 일반 식단/고지방 식단(이산형)과 같은 변수를 사용하여 크기를 예측하는 경우에도 작동합니다.
*   **로지스틱 회귀(Logistic Regression)**: 로지스틱 회귀에도 적용될 수 있으며, 이때는 잔차 제곱의 합 대신 우도(likelihoods)의 합을 최적화합니다. 릿지 회귀는 기울기 추정치를 축소하여 예측의 민감도를 낮춥니다.
*   **복잡한 모델**: 여러 매개변수를 가진 복잡한 모델에도 적용 가능하며, y절편을 제외한 모든 매개변수에 패널티를 적용하여 모델의 복잡성을 제어합니다.

**가장 놀라운 장점은 "해결 불가능한 상황"을 해결하는 능력입니다.**

*   **데이터 부족 문제 해결**: 최소 제곱법은 매개변수(parameter)의 개수만큼 최소한의 데이터 포인트가 필요합니다. 예를 들어, 10,001개의 매개변수를 추정하려면 10,001개 이상의 데이터 포인트가 필요합니다. 하지만 실제 데이터셋에서는 500개의 샘플만 있는 경우가 흔합니다. 이러한 **데이터 부족(not enough data)** 상황에서 최소 제곱법은 최적의 해를 찾을 수 없습니다.
*   **릿지 회귀의 마법**: **릿지 회귀**는 이 패널티 항을 추가하고 **교차 검증(cross-validation)**을 사용함으로써, **데이터가 충분하지 않더라도(예: 10,001개 매개변수와 500개 샘플)** 모든 매개변수에 대한 해를 찾을 수 있습니다. 이는 패널티가 작은 매개변수 값을 선호하도록 유도하기 때문입니다.

요약하자면, **릿지 회귀**는 샘플 크기가 상대적으로 작을 때 **새로운 데이터에 대한 예측을 개선(즉, 분산 감소)**하며, 예측이 훈련 데이터에 덜 민감하도록 만듭니다. 이는 **y절편을 제외한 모든 매개변수의 제곱 합에 람다(λ)를 곱한 패널티 항**을 최소화 대상에 추가함으로써 이루어집니다. 또한, 최소 제곱법으로 매개변수를 추정하기에 **데이터가 충분하지 않은 경우에도 릿지 회귀는 해결책을 찾을 수 있다**는 강력한 장점을 가집니다.

