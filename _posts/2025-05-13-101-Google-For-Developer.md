---
title: 41차시 1:Google For Developer
layout: single
classes: wide
categories:
  - Google For Developer
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---


## 1. Gemma 3 & Vertex AI 심층 분석: 강력한 오픈 모델 활용의 새로운 지평
- 출처: [Get started with Gemma 3 on Vertex AI](https://www.youtube.com/watch?v=pC2DhFJQocY&t=10s)
* Google의 혁신적인 오픈 모델 제품군인 Gemma 3를 Google Cloud의 통합 AI 플랫폼 Vertex AI를 통해 효율적으로 튜닝하고 간편하게 배포하는 방법에 대한 심층적인 논의

### 1.1 **Gemma 3** 
Google Gemini 2.0 아키텍처를 기반으로 탄생한 최첨단 오픈 모델 제품군

* **다양한 규모:** 10억, 40억, 120억, 270억 개의 파라미터를 가진 네 가지 모델 크기로 구성되어 사용 사례와 리소스 제약에 따른 최적의 선택지 제공
* **폭넓은 호환성:** 스마트폰과 같은 에지 장치부터 강력한 클라우드 환경까지 다양한 컴퓨팅 환경에서 유연하게 실행 가능
* **진보된 멀티모달 기능:** 대규모 모델(120억, 270억 파라미터)은 텍스트뿐만 아니라 이미지까지 이해하는 멀티모달 능력을 통해 더욱 풍부한 상호작용 지원
* **뛰어난 컨텍스트 처리 능력:** 다양한 언어 처리는 물론, 최대 128,000 토큰(수백 페이지 분량)에 달하는 방대한 정보 처리 능력을 통해 긴 문서 이해 및 복잡한 추론 작업 가능

### 1.2 **직면한 과제** 
Gemma 3와 같은 고성능 모델을 실제 서비스에 적용하기 위한 기술적 난관

* **복잡한 인프라 관리:** 강력한 모델을 안정적으로 배포하고 효율적으로 튜닝하기 위해서는 고성능 컴퓨팅 자원과 전문적인 프레임워크 관리가 필수적이며, 이는 개발자에게 상당한 부담으로 작용
* **특정 요구사항 맞춤형 모델 개발의 어려움:** 특정 사용 사례에 최적화된 모델을 만들기 위한 미세 조정 과정은 전문적인 지식과 상당한 컴퓨팅 자원을 요구하며, 일반 개발자가 접근하기 어려움

### 1.3 **Vertex AI의 혁신적인 솔루션** 
Google Cloud의 포괄적인 AI 플랫폼을 통한 문제 해결

* **강력한 클라우드 인프라 활용:** Google Cloud의 최첨단 인프라(GPU, TPU 등)를 통해 대규모 모델의 튜닝 및 배포에 필요한 강력한 컴퓨팅 파워를 손쉽게 활용 가능
* **Vertex AI Model Garden:** 엄격한 검증을 거쳐 즉시 사용 가능한 Gemma 3를 포함한 수백 개의 다양한 오픈 모델을 제공하여 모델 탐색 및 선택 과정 간소화
* **통합된 워크플로우:** Vertex AI 학습 및 예측 서비스와의 긴밀한 통합을 통해 모델 튜닝부터 안정적인 배포까지의 전 과정을 효율적으로 관리하고 기본적인 복잡성을 숨겨 개발 편의성 극대화
* **개발 생산성 향상:** 인프라 관리에 대한 부담을 최소화하고 핵심 비즈니스 로직 및 애플리케이션 개발에 집중할 수 있도록 지원

### 1.4 **Gemma 3 간편 배포 데모** 
Vertex AI Model Garden SDK를 활용한 놀라운 간편성

* **단 3줄의 코드:** 새로운 Vertex AI Model Garden SDK를 통해 복잡한 설정 없이 단 몇 줄의 파이썬 코드로 Gemma 3 모델을 클라우드에 배포하는 혁신적인 방법 제시
    1.  `model_garden` 라이브러리 임포트: 
        *   `import vertexai.language_models as lm`
    2.  배포할 Gemma 3 모델 지정: 
        *   `model = lm.TextGenerationModel.from_pretrained("gemma-pro-7b-it")` (예시: 70억 파라미터 Instruction Tuned 버전)
    3.  배포 명령 실행: 
        *   `endpoint = model.deploy()`
* **자동 인프라 프로비저닝:** Vertex AI가 모델 배포에 필요한 모든 인프라를 자동으로 구성하고 예측을 위한 API 엔드포인트를 생성하여 개발자는 인프라 관리에 신경 쓸 필요 없이 모델 활용에 집중 가능
* **표준 API 기반 멀티모달 활용:** 업계 표준 API(ChatCompletion API)를 통해 텍스트 생성뿐만 아니라 이미지 이해와 관련된 복잡한 멀티모달 작업까지 손쉽게 수행 가능

### 1.5 **Gemma 3 맞춤형 미세 조정 데모** 
Vertex AI를 활용한 효율적인 모델 커스터마이징

* **미세 조정의 중요성:** 특정 도메인 지식, 선호하는 언어 스타일, 특정 작업 수행 능력 등 모델의 행동 방식을 사용자의 요구에 맞게 수정하는 핵심 과정
* **Parameter Efficient Finetuning Techniques (PEFT):** LoRA, QLoRA와 같은 혁신적인 PEFT 기술을 활용하여 원래 모델의 대부분을 동결한 채 소량의 추가 파라미터만 학습시켜 튜닝에 필요한 시간과 컴퓨팅 자원을 획기적으로 절감
* **Vertex AI의 LoRA 기본 지원:** Vertex AI는 LoRA 기술을 활용한 Gemma 3 미세 조정 기능을 기본적으로 제공하여 개발자가 효율적으로 모델을 맞춤화할 수 있도록 지원
* **직관적인 UI 기반 미세 조정:** Vertex AI Model Garden 내 "Finetune" 버튼을 통해 복잡한 코딩 없이 그래픽 사용자 인터페이스(GUI)만으로 미세 조정을 수행할 수 있는 기능 제공 (초보자도 쉽게 시작 가능)
    1.  프로젝트 및 모델 식별을 위한 이름 지정
    2.  미세 조정의 기반이 될 기본 모델 선택 (예: Gemma 3 1b)
    3.  미세 조정된 모델의 저장 경로 설정
    4.  학습 횟수(Epochs), 학습률(Learning Rate) 등 미세 조정 관련 하이퍼파라미터 설정
    5.  미세 조정 완료 후 자동 배포 옵션 활성화

**결론:** Vertex AI는 강력한 성능의 Gemma 3와 같은 오픈 모델을 개발자가 쉽고 효율적으로 활용할 수 있도록 완벽하게 지원함으로써, 복잡한 인프라 관리 부담을 줄이고 혁신적인 AI 기반 애플리케이션 개발에 더욱 집중할 수 있는 환경을 제공한다.

## 2. 하이브리드 LLM: 온디바이스 및 클라우드 모델의 강점 결합
- 출처: [Hybrid LLMs: Utilizing Gemini and Gemma for edge AI applications](https://www.youtube.com/watch?v=PvKEHPbZ4-Y)

최근 AI 분야에서는 온디바이스(on-device) 모델과 클라우드 기반 모델의 장점을 결합한 **하이브리드 LLM 아키텍처**가 주목받고 있습니다. 이 아키텍처는 언어 모델, 데이터 소스, 처리 도구와 같은 구성 요소를 다양한 장치 및 서버에 전략적으로 분산하여 **성능, 프라이버시 및 모델 기능을 최적화**하는 것을 목표로 합니다.
* 이는 단일 처리 환경의 한계를 극복하고, 상황에 따라 가장 적합한 자원을 활용함으로써 사용자 경험을 향상시키려는 접근 방식입니다.

### 2.1 **왜 하이브리드 LLM 아키텍처를 사용해야 할까요?**
이 아키텍처는 여러 가지 중요한 이점을 제공합니다:

* **자원 효율성**: 특정 간단한 작업에는 로컬 컴퓨팅 자원을 활용하고, 추론이나 대규모 데이터 소스 처리와 같이 더 많은 컴퓨팅 자원이 필요한 작업은 클라우드에서 수행할 수 있습니다.
  * 예를 들어, 짧은 요약 생성이나 명령어 실행은 로컬에서 처리하고, 장문 생성이나 외부 정보 기반 응답은 클라우드에서 처리할 수 있습니다.

* **오프라인 기능**: 애플리케이션이 멀티모달 모델의 기능을 오프라인에서도 사용해야 할 때 유용합니다.
  * 예를 들어, 인터넷 연결이 제한된 의료 현장이나 군사용 장비처럼 네트워크 의존도가 낮은 환경에서 하이브리드 모델이 유용하게 쓰일 수 있습니다.

* **데이터 프라이버시**: 로컬 장치를 벗어날 수 없는 민감한 데이터를 처리해야 하는 경우에 특히 중요합니다.
  * 의료정보, 음성 녹음, 사용자 행동 데이터 등은 클라우드 전송 없이 로컬에서만 처리함으로써 규제 및 사용자 신뢰 측면에서 이점을 가질 수 있습니다.

* **성능과 기능의 균형**: 모바일 폰, 태블릿, 노트북 등에서 **Gemma 3 1B 및 4B 모델**과 같은 온디바이스 모델을 가속화하여 실행하면서도, 훨씬 더 많은 RAM과 처리가 필요한 모델의 기능을 유지할 수 있습니다. 이는 모델 아키텍처 및 장치 기능 개선 덕분에 가능해졌습니다.
  * 특히 모바일 SoC에 최적화된 Transformer 구조나 양자화(quantization) 기법 덕분에, 성능 저하 없이 모델을 실행할 수 있습니다.

### 2.2 **주요 구성 요소**
이러한 하이브리드 시스템에서 Google의 **클라우드 기반 멀티모달 모델군인 Gemini**와 **온디바이스에서 단일 GPU로 실행 가능한 오픈 모델군인 Gemma**를 결합하여 활용할 수 있습니다. 예를 들어, Gemini 2.5 Pro는 추론 및 실시간 데이터 액세스를 위한 클라우드 기반 모델로, Gemma 3 1B 파라미터 모델은 모바일 장치에서 성능과 기능을 균형 있게 제공하는 온디바이스 모델로 사용될 수 있습니다.
* 이 조합을 통해 하나의 앱에서도 빠른 반응성과 고급 기능을 동시에 제공할 수 있으며, 상황에 따라 유연하게 역할을 분담할 수 있습니다.

### 2.3 **하이브리드 아키텍처의 유형**
다양한 하이브리드 아키텍처 옵션이 있지만, 일반적으로 다음을 포함합니다:

* **순차적 아키텍처**: 온디바이스 LLM이 먼저 작업을 추론한 다음 클라우드 모델로 보낼지 결정합니다.
  * 사용자의 질문이 간단한 경우 로컬에서 끝내고, 복잡하거나 추가 정보가 필요한 경우에만 클라우드를 호출합니다.

* **병렬 아키텍처**: 두 모델 모두 주어진 작업을 추론한 다음 응답을 병합합니다.
  * 이 방식은 다중 후보 응답을 비교하여 더 나은 답을 선택하거나 결합할 수 있어 품질 향상에 기여.

* **라우팅 아키텍처**: 텍스트 분류기를 훈련하여 주어진 쿼리를 온디바이스 모델로 라우팅할지 클라우드 기반 모델로 라우팅할지 빠르게 결정하는 데 사용됩니다.
  * 이 방식은 응답 속도와 정확성 간의 균형을 유지하면서, 리소스를 효율적으로 사용할 수 있게 해줍니다.

### 2.4 **라우팅 아키텍처의 작동 방식 (예: 날씨 앱)**
라우팅 아키텍처는 하이브리드 LLM을 구현하는 강력한 방법입니다. 날씨 앱을 예로 들어 설명하면 다음과 같다:

1. **라우터 훈련**: 먼저 **Mobile BERT**와 같이 모바일 하드웨어에 최적화된 텍스트 분류기인 라우터를 훈련합니다. 이 라우터는 사용자의 쿼리를 '로컬(local)' 또는 '원격(remote)'으로 분류하도록 훈련됩니다. 이 훈련은 Gemma 3 27B 모델을 사용하여 합성된 수천 개의 미리 레이블링된 쿼리를 제공함으로써 수행될 수 있다.
   * 라우터는 실제 사용자 데이터를 직접 사용하지 않고, 합성 데이터 기반으로 학습시켜 프라이버시 이슈를 방지할 수 있습니다.

2. **쿼리 라우팅**: 앱은 라우터가 지정한 레이블을 확인한 후, 쿼리를 온디바이스 모델(예: Gemma 3 1B) 또는 클라우드 모델(예: Gemini 2.5 Pro)로 보낼지 결정합니다.
   * 예를 들어 “오늘 서울 날씨 알려줘”는 로컬에서 처리 가능하지만, “이번 주말에 아이들과 갈만한 서울 근교 나들이 장소 추천해줘”는 클라우드로 전송됩니다.

3. **응답 생성**: 선택된 모델은 RAG(검색 증강 생성) 또는 Google 검색이나 API 호출과 같은 도구를 사용하여 더 정확한 응답을 제공하도록 구성될 수 있습니다.
   * 검색 기반 생성은 최신 정보를 포함한 답변을 생성할 수 있어, 단순 생성 모델보다 훨씬 유용한 결과를 제공합니다.

4. **사용자에게 응답**: 모델이 생성한 응답은 사용자에게 질문에 대한 답으로 보여집니다.
   * 이때 응답 속도, 자연스러움, 정확성 등을 종합적으로 고려하여 최종 사용자 경험을 최적화합니다.

### 2.5 **성공적인 앱을 위한 평가**
라우터와 LLM의 응답 모두에 대한 **평가가 중요**합니다. 앱이 사용 사례에 최적화되었는지 확인하려면 **품질, 지연 시간(latency), 및 레이블링 결과**를 반드시 고려해야 합니다.
* 이를 위해 오프라인 A/B 테스트, 사용자 피드백, 자동화된 응답 평가 지표(BLEU, ROUGE 등)를 함께 활용하는 것이 좋습니다.

이러한 하이브리드 LLM 아키텍처를 통해 **Gemini와 Gemma의 장점을 모두 활용**하여 강력하고 효율적인 AI 솔루션을 구축할 수 있습니다.
* 특히 모바일 환경에서도 고급 AI 기능을 안정적으로 제공할 수 있기 때문에, 차세대 앱 개발의 핵심 전략으로 떠오르고 있습니다.
