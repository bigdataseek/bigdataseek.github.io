---
title: 11차시 2 :StatQuest(Statistics) 2
layout: single
classes: wide
categories:
  - Statistics 
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 31. 파이 차트보다 막대 차트가 더 나은 이유

**1. 핵심 주장: 막대 차트가 파이 차트보다 우수하다**

*   파이 차트를 사용하여 데이터를 표현하려 할 때, 대신 막대 차트를 사용하는 것을 **일반적으로 더 선호한다**고 주장합니다. 그 이유는 막대 차트가 더 쉽게 해석되고 공간 효율적이기 때문입니다.

**2. 막대 차트가 더 나은 구체적인 이유**

*   **유사한 크기의 비교 용이성**: 파이 차트에서는 크기가 비슷한 두 부분이 얼마나 차이 나는지 구별하기 어렵습니다. 예를 들어, Herman이 잠자는 시간과 일하는 시간을 파이 차트로 보면 크기가 매우 비슷해 보입니다. 하지만 막대 차트에서는 Herman이 일하는 시간이 잠자는 시간보다 한 시간 더 길다는 것이 **확연하게 드러납니다**.
*   **정량적 차이 식별의 용이성**: 막대 차트에서는 휴식 시간이 식사 시간보다 한 시간 더 길다는 등의 **정량적인 차이를 쉽게 파악**할 수 있습니다. 파이 차트에서는 이러한 미묘한 차이를 알아채기가 어렵습니다.
*   **여러 데이터 세트 간 비교의 어려움 해소**: 두 개 이상의 파이 차트를 나란히 놓고 데이터를 비교하는 것은 **매우 어렵습니다**. Herman과 Earl의 활동 시간을 비교하는 예시에서, 파이 차트로는 Earl이 Herman보다 더 많이 일하는지, 혹은 얼마나 더 많이 일하는지와 같은 질문에 답하기가 **놀랍도록 어렵다**고 지적합니다.
*   **막대 차트의 명확한 비교**: 반면, 막대 차트를 사용하면 Herman이 Earl보다 30분 더 오래 자고, 40분 더 오래 일하며, Earl은 Herman보다 한 시간 더 오래 먹고 한 시간 더 오래 쉰다는 등의 **구체적인 비교를 훨씬 쉽게** 할 수 있습니다.

결론적으로, 데이터를 시각화할 때는 파이 차트 대신 막대 차트를 사용하는 것을 고려하여, 정보를 더 명확하고 효과적으로 전달하는 것이 좋습니다.

## 32. 박스 플롯(Box Plots)
**박스 플롯**은 때로는 **박스 앤 위스커 플롯(Box and Whisker Plots)**이라고도 불리며, 데이터의 분포를 시각적으로 효과적으로 보여주는 강력한 도구입니다.

1. 핵심 구성 요소는 다음과 같습니다:
*   **중앙선(The line in the middle of the box):** 데이터의 **중앙값(median value)**을 나타냅니다. 중앙값은 전체 데이터의 50%가 이 값보다 크고, 50%가 이 값보다 작다는 것을 의미합니다. 특히, 중앙값은 평균(mean)과 달리 **이상치(outliers)에 덜 영향을 받기 때문에** 데이터에 극단적인 값이 포함되어 있을 때 데이터의 중심 경향을 파악하는 데 매우 유용합니다.
*   **상자(The Box):** 상자 내에는 전체 데이터의 **50%**가 포함됩니다. 중앙값을 기준으로 위쪽에 25%, 아래쪽에 25%의 데이터가 분포해 있습니다. 이는 데이터의 중간 50%가 어디에 집중되어 있는지를 보여주며, 데이터의 산포도를 이해하는 데 도움을 줍니다.
*   **수염(The Whiskers):** 상자에서 뻗어 나온 선들로, 상자 내에 포함되지 않는 나머지 데이터의 범위를 나타냅니다.
*   **이상치(Outliers):** 수염 밖에 점(dots)으로 표시되는 데이터 포인트들은 이상치로 간주됩니다. 박스 플롯을 통해 이상치를 쉽게 식별할 수 있어 데이터 전처리 과정에서 중요한 정보를 제공합니다.

2. **박스 플롯의 유용성:**
*   **데이터 분포 시각화:** 전통적인 막대 그래프(bar plots)가 보여주는 동일한 데이터를 박스 플롯은 데이터의 분산, 중앙값, 이상치 등을 한눈에 파악할 수 있도록 더 풍부하게 보여줍니다.
*   **데이터 밀도 및 신뢰도 파악:** 박스 플롯 위에 원본 데이터를 겹쳐서 표시(overlay)하면, 특정 구간에 얼마나 많은 측정값이 밀집되어 있는지 쉽게 알 수 있습니다. 이는 통계적 분석의 **신뢰도**를 평가하는 데 중요한 단서가 됩니다. 예를 들어, 측정값이 많은 데이터는 통계 결과에 대한 신뢰도가 더 높다고 볼 수 있습니다.
*   **이상치 처리:** AI 모델 학습 전 데이터 전처리 단계에서 이상치를 식별하고 처리하는 것은 모델의 성능에 큰 영향을 미치므로, 박스 플롯은 이 과정에서 필수적인 도구입니다.


## 33. 로그(Logarithms)

### 33.1 **로그란 무엇인가요?**

로그의 가장 기본적인 개념은 **지수를 분리하는 것**입니다. 어떤 숫자를 특정 밑(base)의 거듭제곱으로 나타냈을 때, 로그 함수는 바로 그 **지수(exponent)**를 결과로 돌려줍니다.

예를 들어, 밑이 2인 로그(log base 2)를 사용하여 설명해 보겠습니다.
*   숫자 8은 2의 3승(2³)으로 다시 쓸 수 있습니다. 따라서 log₂8은 지수인 3이 됩니다.
*   숫자 4는 2의 2승(2²)이므로 log₂4는 2입니다.
*   숫자 2는 2의 1승(2¹)이므로 log₂2는 1입니다.
*   숫자 1은 2의 0승(2⁰)이므로 log₂1은 0입니다.
*   1/2는 2의 -1승(2⁻¹)이므로 log₂1/2는 -1입니다.
*   1/4는 2의 -2승(2⁻²)이므로 log₂1/4는 -2입니다.
*   1/8는 2의 -3승(2⁻³)이므로 log₂1/8는 -3입니다.

이처럼 로그는 숫자를 밑의 거듭제곱으로 표현한 후 지수만을 분리해내는 역할을 합니다.



### 33.2 **로그 스케일(Log Scale)의 장점**

1.  **대칭적인 변화율(Fold Change) 시각화**:
    *   로그 스케일은 **변화율(fold change)**을 다룰 때 특히 유용합니다. 예를 들어, 일반 숫자 스케일에서는 숫자 1에서 8까지의 거리와 1에서 1/8까지의 거리가 비대칭적으로 보입니다. 하지만 로그 스케일에서는 **8배 증가(8 fold up)와 8배 감소(8 fold down)**가 0으로부터 동일한 거리로 표현되어 대칭적인 스케일을 제공합니다. 이는 데이터 변화의 실제적인 "크기"를 더 직관적으로 이해할 수 있게 해줍니다.
    *   따라서 AI 모델 학습 시 **특정 피처의 변화율이 중요하거나, 지수적인 증가/감소를 보이는 데이터를 시각화할 때** 로그 스케일을 사용하면 데이터의 패턴을 더 명확하게 파악할 수 있습니다.

2.  **0의 문제 및 처리**:
    *   **로그 0은 존재하지 않습니다**. 어떤 밑이든 0승을 하더라도 0이 될 수 없기 때문입니다.
    *   하지만 통계 및 프로그래밍 언어(예: R 언어)에서는 **log₂0을 음의 무한대(negative infinity)로 정의**하기도 합니다. 이는 2의 마이너스 무한대 승이 상상할 수 있는 가장 작은 양수(거의 0에 가까운)가 되므로 직관적으로 이해될 수 있습니다.

### 33.3 **로그의 활용**

1.  **이상치(Outliers)에 강인한 기하 평균(Geometric Mean)**:
    *   데이터가 매 단계마다 특정 비율로 증가하거나 감소하는 경우(예: 매 주기마다 두 배로 증가하는 qPCR 데이터), **로그 값의 평균**을 사용하는 것이 일반적인 산술 평균보다 더 적합합니다.
    *   로그 값의 평균을 **기하 평균(geometric mean)**이라고 부르는데, 이는 일반적인 평균보다 **이상치의 영향에 덜 민감(less sensitive to outliers)**합니다. AI 모델은 이상치에 의해 성능이 크게 저하될 수 있으므로, 로그 변환 후 기하 평균을 사용하면 **데이터의 중심 경향을 더 견고하게 파악**하고, 이상치에 덜 민감한 모델을 구축하는 데 도움이 될 수 있습니다.

2.  **로그를 이용한 연산 간소화**:
    *   **곱셈은 덧셈으로**: 숫자들을 곱하는 대신, 그 숫자들을 로그로 변환한 후 **지수들을 더함으로써** 곱셈을 수행할 수 있습니다.
    *   **나눗셈은 뺄셈으로**: 숫자들을 나누는 대신, 로그로 변환한 후 **지수들을 뺄셈함으로써** 나눗셈을 수행할 수 있습니다.
    *   이러한 특성은 특히 복잡한 통계 모델이나 기계 학습 알고리즘에서 **연산 비용을 줄이거나 수치적 안정성을 높이는 데** 활용될 수 있습니다 (예: 확률을 다루는 모델에서 로그 확률을 사용).

### 33.4 **로그의 밑(Base) 선택**

로그의 개념은 어떤 밑에도 적용됩니다.
*   **로그 10 (log base 10)**: 데시벨이나 지진 규모처럼 10의 거듭제곱으로 변화하는 데이터를 다룰 때 유용합니다.
*   **자연 로그 (natural log, log base E)**: 수학 상수 E (약 2.7)를 밑으로 하며, 많은 수학 및 통계 공식에서 기본적으로 사용됩니다. R 프로그래밍 언어에서도 `log` 함수를 사용하면 기본적으로 자연 로그를 계산합니다.
*   **데이터에 적합한 임의의 밑**: 데이터가 매 단계마다 3배씩 증가한다면 밑이 3인 로그를 사용할 수 있습니다. 데이터가 매 단계마다 7.5배씩 증가한다면 밑이 7.5인 로그를 사용할 수 있습니다.

결론적으로, 로그는 지수적인 관계를 선형적으로 변환하고, 데이터의 분포를 대칭적으로 파악하며, 이상치에 강인한 통계적 측정치를 제공하고, 복잡한 연산을 단순화하는 등 AI 분야에서 데이터 분석 및 모델링에 필수적인 도구입니다.

## 34. 신뢰 구간(Confidence Intervals)

### **34.1 신뢰 구간이란 무엇인가요?**

신뢰 구간은 데이터가 분포하는 **구간**을 나타내며, 특정 모수(예: 평균)가 이 구간 안에 있을 확률을 제시합니다. 많은 사람들이 신뢰 구간을 오해하지만, **부트스트래핑(Bootstrapping)** 방법을 통해 쉽게 이해할 수 있습니다.

### **34.2 부트스트래핑(Bootstrapping)으로 신뢰 구간 이해하기**

신뢰 구간을 계산하는 여러 방법 중 하나인 부트스트래핑을 통해 개념을 익혀봅시다.

1.  **초기 샘플 데이터 준비**: 예를 들어, 암컷 쥐 12마리의 몸무게를 측정했다고 가정해 봅시다. 이 12개의 측정값은 지구상의 모든 암컷 쥐의 평균(전체 평균)이 아닌, 우리가 샘플링한 쥐들의 평균(표본 평균)을 계산하는 데 사용될 수 있습니다.
2.  **부트스트랩 샘플 생성**:
    *   원본 샘플(12마리 쥐의 몸무게)에서 **무작위로 12개의 몸무게를 선택**합니다. 이때, **중복 선택이 허용됩니다 (sampling with replacement)**. 예를 들어, 특정 쥐의 몸무게가 두 번 선택될 수도 있고, 어떤 쥐의 몸무게는 전혀 선택되지 않을 수도 있습니다.
    *   이렇게 선택된 12개의 몸무게로 새로운 **'부트스트랩 샘플'**을 만듭니다.
3.  **부트스트랩 평균 계산**: 새로 생성된 부트스트랩 샘플의 평균을 계산합니다.
4.  **반복**: 위 1단계와 2단계를 **수없이 많이 반복합니다 (예: 10,000번 이상)**. 이 과정을 통해 우리는 수많은 부트스트랩 평균값들을 얻게 됩니다.

### **34.3 신뢰 구간의 정의**

이렇게 계산된 수많은 부트스트랩 평균값들을 통해 신뢰 구간을 정의할 수 있습니다.

*   **95% 신뢰 구간**: 계산된 부트스트랩 평균들의 **95%가 포함되는 구간**을 의미합니다.
*   **99% 신뢰 구간**: 계산된 부트스트랩 평균들의 **99%가 포함되는 구간**을 의미하며, 95% 신뢰 구간보다 **더 넓은 구간**이 됩니다.

신뢰 구간은 단지 우리가 계산한 많은 평균값 중 특정 비율(예: 95% 또는 99%)이 포함되는 범위일 뿐입니다.

### **34.4 신뢰 구간의 유용성**

신뢰 구간은 시각적인 통계 검정 도구로서 매우 유용합니다.

1.  **시각적인 통계 검정**:
    *   95% 신뢰 구간 **밖에 있는 값**은 전체 평균 중 5% 미만의 확률로 발생하는 값으로 간주됩니다.
    *   이는 곧, 해당 값의 **P-값(P-value)이 0.05 미만**이며, **통계적으로 유의미한 차이**가 있음을 의미합니다.

2.  **특정 값과의 비교**:
    *   우리가 측정한 표본 평균이 전 세계 모든 암컷 쥐의 **참 평균(true mean)**을 추정하는 데 사용될 수 있습니다.
    *   예를 들어, 참 평균이 20보다 작을 확률을 알고 싶을 때, 우리의 95% 신뢰 구간을 그려볼 수 있습니다.
    *   만약 20보다 작은 영역(예: 특정 값 이하의 영역)이 95% 신뢰 구간 **밖에 있다면**, 참 평균이 이 영역에 있을 확률은 0.05 미만으로 **통계적으로 유의미한 차이**가 있다고 결론 내릴 수 있습니다.

3.  **두 샘플 간의 비교**:
    *   **두 샘플의 95% 신뢰 구간이 겹치지 않는다면**, 이 두 샘플의 평균 사이에는 **통계적으로 유의미한 차이가 있다**고 단언할 수 있습니다 (P-값이 0.05 미만). 예를 들어, 암컷 쥐와 수컷 쥐의 몸무게 신뢰 구간이 겹치지 않는다면, 두 성별 간의 몸무게 차이는 유의미하다고 볼 수 있습니다.
    *   **주의**: 만약 신뢰 구간이 **겹친다면**, 두 평균 사이에 유의미한 차이가 없을 수도 있지만, 여전히 **차이가 있을 가능성이 남아 있습니다**. 이 경우에는 추가적인 통계 검정(예: T-검정)을 수행하여 확인해야 합니다. 하지만 신뢰 구간이 겹치지 않는다면, 유의미한 차이가 있음을 확신할 수 있습니다.

AI 모델을 개발하고 평가할 때, 신뢰 구간은 모델의 성능 비교, 특정 특성 값의 유의미성 판단, 그리고 실험 결과의 신뢰도를 시각적으로 빠르게 확인하는 데 큰 도움을 줄 수 있습니다.

## 35. R-제곱(R²): 데이터의 변동성을 설명하는 강력한 지표

### **35.1 R-제곱(R²)이란 무엇인가요?**
*   R-제곱은 상관관계의 지표로, 계산하기 쉽고 직관적으로 해석할 수 있습니다.
*   기존의 상관계수 'R'과 매우 유사하지만, 해석이 더 용이하다는 장점이 있습니다. 예를 들어, R=0.7이 R=0.5보다 얼마나 좋은 상관관계인지 바로 알기 어렵지만, R²=0.7은 R²=0.5보다 1.4배 더 좋다고 직관적으로 이해할 수 있습니다.
*   **핵심 아이디어는 R-제곱이 두 변수 간의 관계에 의해 설명되는 변동성의 비율(%)이라는 것입니다**.

### **35.2 R-제곱(R²)의 직관적인 계산 및 이해**
동영상에서는 쥐의 몸무게와 크기 데이터를 통해 R-제곱의 계산 과정을 직관적으로 설명합니다.

*   **1단계: 평균(Mean)을 이용한 변동성 파악**
    *   먼저 쥐의 몸무게(Y축) 데이터가 있을 때, 단순히 **평균 몸무게**를 선으로 그어 데이터의 변동성을 계산합니다. 이는 각 데이터 포인트와 평균 사이의 제곱 차이의 합으로 구합니다. 이 값은 해당 데이터가 평균으로부터 얼마나 퍼져 있는지를 나타냅니다.
*   **2단계: 회귀선(Blue Line)을 이용한 변동성 파악**
    *   이제 쥐의 크기(X축)를 사용하여 몸무게를 예측하는 **회귀선(파란색 선)**을 데이터에 맞게 그립니다. 이 선이 평균보다 데이터를 더 잘 설명하는지 궁금할 수 있습니다.
    *   이 회귀선을 기준으로 다시 각 데이터 포인트와 회귀선 사이의 제곱 차이의 합을 계산합니다. 이 값은 회귀선으로 설명되지 않는 잔여 변동성을 나타냅니다.
*   **3단계: R-제곱(R²) 계산**
    *   R-제곱의 공식은 다음과 같습니다:
        (평균 주위의 변동성 - 회귀선 주위의 변동성) / 평균 주위의 변동성
    *   예시를 통해 보면:
        *   평균 주위의 변동성 = 32
        *   회귀선 주위의 변동성 = 6 (쥐 크기와 몸무게 관계의 경우)
        *   R² = (32 - 6) / 32 = 26 / 32 = **0.81 또는 81%**
    *   이는 쥐의 크기와 몸무게 관계가 전체 변동성의 **81%**를 설명한다는 의미입니다. 즉, 데이터를 파란색 선으로 모델링함으로써 평균을 사용했을 때보다 변동성이 81% 감소했다는 뜻입니다.

### **35.3 R-제곱(R²) 값의 해석**
*   **R²가 높을수록 (1에 가까울수록)**: 두 변수 간의 관계가 데이터의 변동성을 **더 많이 설명**합니다. 예를 들어, R²가 0.9 (90%)이면, 두 변수 간의 관계가 데이터 변동성의 90%를 설명한다는 강력한 증거입니다.
*   **R²가 낮을수록 (0에 가까울수록)**: 두 변수 간의 관계가 데이터의 변동성을 **거의 설명하지 못한다**는 의미입니다. 예를 들어, 쥐가 바위를 킁킁거리는 시간과 몸무게의 관계에서 R²가 0.06 (6%)이 나왔다면, 이 관계는 데이터 변동성의 6%만을 설명하며, 거의 무의미하다고 볼 수 있습니다.

### **35.4 일반적인 상관계수 'R'과의 관계**
*   **R-제곱(R²)은 단순히 'R'의 제곱입니다**.
*   따라서 누군가 'R' 값을 알려주면, 그 값을 제곱함으로써 'R-제곱'을 쉽게 얻을 수 있고, 데이터 변동성의 몇 퍼센트가 설명되는지 더 잘 이해할 수 있습니다.
    *   예: R = 0.9 → R² = 0.81 (81% 설명)
    *   예: R = 0.5 → R² = 0.25 (25% 설명)
*   R-제곱은 R=0.7이 R=0.5보다 얼마나 더 나은지 명확하게 보여줍니다. R²=0.49는 49%를, R²=0.25는 25%를 설명하므로, 전자가 후자보다 두 배 더 좋다는 것을 알 수 있습니다.

### **35.5 주의사항**
*   R-제곱 값은 항상 양수이므로 **상관관계의 방향(양의 상관관계 또는 음의 상관관계)을 나타내지는 않습니다**. 상관관계의 방향이 명확하지 않을 때는 "두 변수는 R² 값과 함께 양의(또는 음의) 상관관계가 있었습니다"라고 따로 언급해야 합니다.

결론적으로, R-제곱(R²)은 AI 모델이 데이터를 얼마나 잘 설명하고 예측하는지를 이해하는 데 매우 유용한 지표입니다. 모델을 평가하고 개선하는 과정에서 R-제곱을 통해 모델의 설명력을 정량적으로 파악할 수 있을 것입니다.

## 36. 데이터에 선을 맞추는 주요 아이디어: 최소 제곱법과 선형 회귀
AI 모델을 구축하고 데이터를 이해하는 데 있어 **선형 회귀(Linear Regression)**는 기본적이면서도 강력한 도구입니다. 오늘은 데이터에 최적의 선을 맞추는 방법인 **최소 제곱법(Least Squares)**의 주요 아이디어에 대해 알아보겠습니다.

### **36.1 데이터에 선을 맞추는 이유 (Why Fit a Line to Data?)**
우리가 열심히 실험하여 얻은 데이터를 XY 그래프에 플롯했을 때, 이 데이터의 **추세(trend)**를 시각적으로 파악하고 미래 값을 예측하기 위해 선을 추가하고 싶어 합니다. 하지만 어떤 선이 가장 좋은 선일까요? 선을 어떻게 배치하느냐에 따라 데이터에 대한 설명력이 달라집니다.

### **36.2 "적합도(Fit)" 측정의 시작: 평균 Y 값 (Average Y Value)**
*   데이터에 선을 맞추는 방법을 논의하기 위한 좋은 출발점은 **평균 Y 값(average y-value)**을 가로지르는 수평선입니다. 이 선은 데이터의 Y 값 평균을 나타냅니다 (예: 3.5).
*   이 수평선이 데이터에 얼마나 잘 맞는지 측정하려면, 각 데이터 포인트에서 이 선까지의 **거리(distance)**를 계산합니다.
*   만약 데이터 포인트가 선 위에 있으면 거리는 양수, 선 아래에 있으면 음수가 됩니다 (예: B - y1, B - y2 등).

### **36.3 왜 "제곱"을 할까요? (Why Square the Residuals?)**
*   각 데이터 포인트와 선 사이의 거리를 **잔차(Residuals)**라고 합니다. 이 잔차들의 합을 단순히 계산하면 양수와 음수가 서로 상쇄되어 전체 적합도가 실제보다 더 좋아 보이게 할 수 있습니다.
*   이 문제를 해결하기 위해, 초기에는 잔차의 **절대값(absolute value)**을 사용하기도 했지만, 수학적으로 다루기 어려워졌습니다.
*   따라서 각 잔차를 **제곱(square)**하는 방법을 선택했습니다. 제곱을 하면 모든 항이 양수가 되어 상쇄되는 문제를 해결하고, 더 큰 오차에 더 큰 가중치를 부여할 수 있게 됩니다.
*   이렇게 계산된 값은 **제곱 잔차의 합(Sum of Squared Residuals)**이라고 부르며, 이 선이 데이터에 얼마나 잘 맞는지를 측정하는 지표가 됩니다.

### **36.4 최적의 선을 찾는 방법: 최소 제곱법 (Finding the Optimal Line: Least Squares)**
*   우리의 목표는 **제곱 잔차의 합을 최소화(minimize the sum of squared residuals)**하는 선을 찾는 것입니다.
*   선형 회귀에서 선은 **`y = ax + b`**라는 일반적인 방정식으로 표현됩니다. 여기서 `a`는 **기울기(slope)**, `b`는 **Y 절편(Y-intercept)**을 나타냅니다.
*   **최소 제곱법**은 이 `a`와 `b`의 최적 값을 찾는 방법입니다. 이 방법은 수학적으로 **미분(derivative)**을 사용하여 이루어집니다.
    *   제곱 잔차의 합을 나타내는 함수를 상상했을 때 (Y축: 제곱 잔차의 합, X축: 선의 회전 또는 기울기), 이 함수의 기울기가 **0이 되는 지점**이 바로 제곱 잔차의 합이 최소가 되는 지점입니다.
    *   기울기와 절편에 대한 미분을 통해 이 최소값을 찾아냅니다.
*   이러한 복잡한 계산은 보통 **컴퓨터(computer)**가 수행하므로, 직접 미분을 계산하는 방법을 아는 것보다 **개념을 이해하는 것이 훨씬 중요합니다**.

### **36.5 핵심 개념 (Big Important Concepts)**
1.  **관측된 값과 선 사이의 거리의 제곱을 최소화하는 것**.
2.  **미분을 취하고 그 값이 0이 되는 지점을 찾아 최소값을 구하는 것**.

최종적으로 찾아진 선은 실측 데이터와의 제곱 잔차 합을 최소화하여 **최소 제곱(least squares)**을 달성한 선이 됩니다. 예를 들어, `y = 0.77 * x + 0.66`과 같은 방정식으로 표현될 수 있습니다.

이러한 최소 제곱법과 선형 회귀의 개념은 AI 분야에서 예측 모델을 만들고 데이터의 패턴을 이해하는 데 매우 중요한 기초가 됩니다. 이 개념들을 잘 이해하면, 여러분이 만들 AI 모델의 성능을 평가하고 개선하는 데 큰 도움이 될 것입니다.

## 37. Lowess와 Loess: 데이터에 곡선 맞추기 (Curve Fitting)

**Lowess(로워스) 및 Loess(로에스) 스무딩**이라는 기법을 사용하여 데이터에 곡선을 맞추는 방법(AKA Lowess smoothing, AKA Loess smoothing)을 설명합니다. 이는 이전에 다루었던 최소 제곱법(least squares)을 이용해 데이터에 선을 맞추는 것과 유사하지만, 선 대신 곡선을 찾는 데 중점을 둡니다.

### **37.1 핵심 아이디어 및 기본 원리:**
Lowess/Loess 스무딩의 주요 아이디어는 두 가지입니다:
*   **슬라이딩 윈도우(Sliding Window) 사용:** 데이터를 더 작은 "덩어리(blobs)"로 나누기 위해 일종의 슬라이딩 윈도우를 사용합니다.
*   **가중 최소 제곱법(Weighted Least Squares) 적용:** 각 데이터 포인트에서 가중 최소 제곱법을 사용하여 선(또는 포물선)을 맞춥니다.

### **37.2 상세 과정:**
이 과정은 여러 단계를 거쳐 이루어집니다.

*   **윈도우 설정 및 초점점(Focal Point) 지정:**
    *   예를 들어, 윈도우 크기를 5로 설정하고, 첫 번째 점을 윈도우의 **초점점(focal point)**으로 지정합니다.
    *   이 윈도우 내에서 초점점에 가장 가까운 5개의 점을 선택합니다 (X축 거리를 기준으로). Y축 거리는 이 단계에서 고려되지 않습니다.
*   **가중 최소 제곱법 적용 (1차):**
    *   선택된 5개의 점에 **가중 최소 제곱법**을 적용하여 선을 맞춥니다.
    *   이때, **초점점에 가까운 점일수록 더 많은 가중치(영향력)를 갖습니다**. 초점점 자체가 가장 높은 가중치를 가지며, 가장 가까운 점이 그 다음, 가장 먼 점이 가장 적은 가중치를 갖습니다.
    *   이렇게 맞춰진 선을 사용하여 현재 초점점에 해당하는 **곡선의 첫 번째 점**을 정의합니다.
*   **슬라이딩 윈도우 이동 및 반복:**
    *   이제 두 번째 점이 초점점이 되고, 윈도우는 다시 가장 가까운 점들을 포함하도록 이동합니다 (때로는 윈도우가 같은 위치에 머무를 수도 있습니다).
    *   새로운 초점점과 가까운 점들에 다시 가중 최소 제곱법을 적용하고, 곡선의 다음 점을 정의합니다.
    *   이 과정을 모든 데이터 포인트에 대해 반복하여 **예비 곡선(preliminary curve)**을 만듭니다.

### **37.3 곡선 다듬기 및 이상치(Outlier) 처리:**

*   처음에 생성된 점들은 **이상치(outlier)**의 영향을 받을 수 있습니다.
*   이러한 이상치의 영향을 줄이기 위해, **원래의 점과 새로 생성된 곡선 점 사이의 Y축 거리를 기반으로 추가적인 가중치**를 부여합니다.
    *   원래 점과 새로 생성된 점이 Y축으로 가까이 있으면 높은 가중치를 받고, 멀리 떨어져 있으면 낮은 가중치를 받습니다.
*   이제 **X축 거리에 기반한 원래 가중치**와 **Y축 거리에 기반한 새로운 가중치**, 이렇게 두 가지 가중치를 사용하여 전체 과정을 다시 반복합니다.
*   이 과정을 통해 **이상치의 영향이 줄어들고 곡선이 훨씬 더 부드러워집니다**.
*   이 가중치 조정 과정은 원하는 만큼 매끄러운 곡선이 나올 때까지 여러 번 반복될 수 있습니다.

### **37.4 추가 고려 사항:**

*   **선 vs. 포물선(Parabolas) 맞추기:** 각 윈도우 내에서 데이터에 선을 맞출 수도 있고, **포물선**을 맞출 수도 있습니다. 포물선은 복잡한 데이터 세트에서 데이터를 더 잘 표현할 수 있습니다. R의 `lowess` 함수는 선만 맞추지만, `loess` 함수는 선 또는 포물선을 맞출 수 있으며, `loess`의 기본 설정은 포물선입니다.
*   **윈도우 크기:** 윈도우에 포함될 점의 수를 변경하여 윈도우 크기를 조절할 수 있습니다. 보통 전체 데이터 포인트의 **비율**로 지정합니다. 윈도우 크기는 곡선의 부드러움에 영향을 미칩니다.
*   **가중치 함수(Weight Functions):** 가중치를 결정하는 공식은 잘 작동하는 것으로 알려져 있지만, 물리적 또는 생물학적 정당성은 없으며, 다른 가중치 함수를 사용할 수도 있습니다.
*   **신뢰 구간(Confidence Intervals):** R의 `loess` 함수는 곡선 주위에 신뢰 구간을 그릴 수도 있습니다.

이러한 Lowess/Loess 스무딩 기법은 데이터의 전반적인 추세를 시각적으로 파악하고, 불규칙한 노이즈를 제거하여 부드러운 곡선 형태로 데이터를 표현하는 데 유용합니다.

## 38. 선형 회귀(Linear Regression)

선형 회귀는 데이터 간의 관계를 수량화하는 통계 기법으로, 주로 **최소 제곱법(Least Squares)**을 사용하여 데이터에 선을 맞추고, 그 관계의 신뢰도를 **R-제곱(R-squared)**과 **P-값(p-value)**으로 평가합니다.

### **38.1 선형 회귀의 핵심 아이디어: 최소 제곱법을 이용한 선 맞추기**

선형 회귀의 첫 번째 단계는 **최소 제곱법**을 사용하여 데이터에 선을 맞추는 것입니다. 그 과정은 다음과 같습니다:

*   **잔차(Residual) 계산:** 먼저 데이터에 가상의 선을 그립니다. 그리고 각 데이터 포인트에서 이 선까지의 수직 거리(Y축 거리)를 측정하는데, 이를 **잔차(residual)**라고 합니다.
*   **잔차 제곱의 합(Sum of Squared Residuals):** 각 잔차를 제곱한 다음, 모든 잔차 제곱 값을 합산합니다. 이 값이 작을수록 선이 데이터에 더 잘 맞는다는 것을 의미합니다.
*   **최소 제곱 회전 찾기:** 선을 조금씩 회전시키면서 잔차 제곱의 합을 계속 계산합니다. 이 과정을 반복하여 **잔차 제곱의 합이 가장 작은 선**을 찾습니다. 이 선이 바로 최소 제곱법으로 데이터에 맞춰진 최적의 선입니다.
*   **방정식 도출:** 이 최소 제곱 선은 Y축 절편(y-axis intercept)과 기울기(slope)라는 두 가지 매개변수를 추정하여 선의 방정식을 생성합니다. 예를 들어, 쥐의 체중으로 크기를 예측할 때, 기울기가 0이 아니면 체중이 크기 예측에 도움이 된다는 의미입니다.

### **38.2 관계의 강도 측정: R-제곱(R-squared)**

R-제곱은 우리가 찾은 선이 데이터를 얼마나 잘 설명하는지, 즉 예측의 품질을 나타내는 지표입니다.

*   **기본 개념:** R-제곱은 "쥐 크기(Mouse size)의 변동성 중 쥐 체중(Mouse weight)으로 설명될 수 있는 변동성의 양"을 알려줍니다.
*   **계산 방법:**
    1.  **평균 주위 잔차 제곱의 합(SS mean):** 쥐 크기의 평균을 기준으로 각 데이터 포인트까지의 거리(잔차)를 제곱하여 모두 더합니다. 이는 독립 변수를 고려하지 않았을 때의 데이터의 총 변동성을 나타냅니다.
    2.  **적합 선 주위 잔차 제곱의 합(SS fit):** 최소 제곱법으로 맞춘 선을 기준으로 각 데이터 포인트까지의 거리(잔차)를 제곱하여 모두 더합니다. 이는 독립 변수를 고려했을 때 남은 변동성을 나타냅니다.
    3.  **R-제곱 공식:** R-제곱은 (평균 주위 변동성 - 적합 선 주위 변동성) / 평균 주위 변동성 으로 계산됩니다. 또는 (SS mean - SS fit) / SS mean 으로도 계산할 수 있습니다.
*   **해석:**
    *   **예시:** R-제곱이 0.6 (60%)이라는 것은 쥐 체중을 고려했을 때 쥐 크기 변동성의 60%가 감소했거나, 쥐 체중이 쥐 크기 변동성의 60%를 설명한다는 의미입니다.
    *   **R-제곱 = 1 (100%):** 완벽한 예측을 의미하며, 적합 선 주위의 변동성이 0일 때 발생합니다.
    *   **R-제곱 = 0 (0%):** 예측에 전혀 도움이 되지 않음을 의미하며, 적합 선 주위의 변동성이 평균 주위의 변동성과 같을 때 발생합니다.
*   **다중 매개변수:** R-제곱 개념은 어떤 방정식에도 적용될 수 있습니다. 여러 예측 변수(예: 쥐 체중과 꼬리 길이)가 있을 경우, 선 대신 평면(plane)을 맞출 수 있으며, 최소 제곱법은 예측에 도움이 되지 않는 변수(매개변수)의 계수를 0으로 만들어 효과적으로 무시할 수 있습니다.
*   **수정된 R-제곱(Adjusted R-squared):** 매개변수를 많이 추가할수록 우연에 의해 R-제곱 값이 높아질 가능성이 있으므로, 매개변수의 수에 따라 R-제곱 값을 조정하는 수정된 R-제곱 값을 보고하기도 합니다.

### **38.3 관계의 신뢰도 측정: R-제곱에 대한 P-값**

R-제곱 값이 높다고 해서 항상 의미 있는 결과는 아닙니다. 우연히 높은 R-제곱이 나올 수도 있기 때문에, R-제곱 값이 통계적으로 유의미한지 판단하기 위해 **P-값**을 계산해야 합니다. 좋은 결과는 **큰 R-제곱 값과 작은 P-값**을 모두 가져야 합니다.

*   **F 통계량:** R-제곱에 대한 P-값은 **F 통계량**에서 파생됩니다. F는 (체중에 의해 설명되는 쥐 크기의 변동성) / (체중에 의해 설명되지 않는 쥐 크기의 변동성) 으로 정의됩니다.
    *   **분자:** 모델에 의해 설명되는 변동성을 나타냅니다. 이는 R-제곱의 분자와 동일하게, 특정 매개변수(예: 쥐 체중)를 고려함으로써 감소하는 변동성을 의미합니다.
    *   **분모:** 모델에 의해 설명되지 않는 변동성을 나타냅니다. 이는 선을 맞춘 후 남아있는 잔차들의 변동성입니다.
    *   **해석:** 적합이 좋으면 모델이 설명하는 변동성이 크고 설명되지 않는 변동성이 작으므로 F 값은 매우 커집니다.
*   **자유도(Degrees of Freedom):** F 통계량을 계산할 때 **자유도**가 사용되어 잔차 제곱의 합을 분산으로 변환합니다.
    *   **P fit:** 적합 선 방정식의 매개변수 개수 (예: Y절편과 기울기 = 2개).
    *   **P mean:** 평균 선 방정식의 매개변수 개수 (예: Y절편 = 1개).
    *   **분자 자유도:** (P fit - P mean)은 추가 매개변수(예: 쥐 체중)에 의해 설명되는 변동성의 자유도를 나타냅니다.
    *   **분모 자유도:** (데이터 포인트 수 n - P fit)은 적합에 의해 설명되지 않는 변동성의 자유도를 나타냅니다. 매개변수가 많을수록 이를 추정하는 데 더 많은 데이터가 필요합니다.
*   **P-값 계산의 개념:**
    1.  개념적으로 무작위 데이터 세트를 수없이 많이 생성하여 각각에 대한 F 값을 계산하고 히스토그램으로 그립니다.
    2.  우리의 실제 데이터에서 얻은 F 값을 이 히스토그램에 표시합니다.
    3.  **P-값**은 실제 데이터의 F 값보다 더 극단적인 값들의 비율을 의미합니다.
*   **실제 적용:** 실제로는 수많은 무작위 데이터 세트를 생성하는 대신, 자유도에 따라 형태가 결정되는 **표준 F 분포 곡선**을 사용하여 P-값을 계산합니다. 샘플 수가 매개변수 수에 비해 많을수록 P-값은 더 작아지는 경향이 있습니다.

선형 회귀는 데이터 간의 관계를 수량화하고 (R-제곱), 그 관계가 통계적으로 유의미한지 (P-값)를 판단하는 데 필수적인 도구입니다.

## 39. R을 사용하여 선형 회귀(Linear Regression)

### **39.1. 데이터 준비 및 시각화**
*   영상에서는 `weight`와 `size` 두 컬럼을 가진 `Mouse data`라는 **데이터 프레임(data frame)**을 생성합니다.
*   R에서 `Mouse data`를 입력하면 깔끔한 컬럼 형식으로 데이터 프레임이 출력됩니다.
*   `plot` 함수를 사용하여 데이터를 **XY 그래프**에 그릴 수 있습니다.

### **39.2 선형 회귀 모델 생성 (`lm` 함수)**
*   실제 선형 회귀는 `lm` (linear models의 약자) 함수를 사용하여 설정합니다.
*   이 함수에 **수식(formula)**과 **데이터(mouse data)**를 전달합니다.
*   수식을 `size ~ weight`와 같이 지정하면 `size`가 **Y값** (종속 변수)으로, `weight`가 **X값** (독립 변수)으로 간주됩니다.
*   `lm` 함수는 **Y절편(y-intercept)**과 **기울기(slope)**에 대한 **최소 제곱 추정치(least squares estimates)**를 계산합니다.

### **39.3 모델 결과 해석 (`summary` 함수)**
R에서 회귀 분석의 핵심은 `summary` 함수에 있습니다. 이 함수는 다양한 종류의 출력을 생성하며, 그 내용은 다음과 같습니다:

*   **Original Call**: `lm` 함수에 대한 원래 호출을 출력합니다.
*   **Residuals (잔차)**:
    *   잔차는 **데이터 포인트와 적합된 선(fitted line) 사이의 거리**를 나타냅니다.
    *   이상적으로 잔차는 선을 중심으로 **대칭적으로 분포**되어야 합니다.
    *   이는 **최솟값(min value)**과 **최댓값(max value)**이 0으로부터 거의 같은 거리에 있어야 함을 의미합니다.
    *   마찬가지로 **첫 번째 사분위수(1Q)**와 **세 번째 사분위수(3Q)**도 0으로부터 같은 거리에 있어야 하며, **중앙값(median)**은 0에 가까워야 합니다.
*   **Coefficients (계수 추정치)**:
    *   이 섹션은 적합된 선에 대한 **최소 제곱 추정치**를 알려줍니다.
    *   `Intercept` 값은 **Y절편**을, `weight` 값은 **기울기**를 나타냅니다.
    *   각 추정치에 대한 **표준 오차(standard error)**와 **T-값(T value)**이 제공되어 **P-값(P values)**이 어떻게 계산되었는지 보여줍니다.
    *   **P-값**은 Y절편과 기울기에 대한 추정치가 0과 같은지 여부를 **검정(test)**합니다. 만약 0이라면 모델에서 큰 의미가 없다는 뜻입니다.
    *   일반적으로 Y절편에는 관심이 없는 경우가 많으므로 P-값은 중요하지 않을 수 있습니다.
    *   그러나 **`weight`에 대한 P-값은 0.05 미만**이어야 합니다. 이는 통계적으로 **유의미(statistically significant)**하다는 의미입니다.
    *   `weight`에 대한 유의미한 P-값은 **마우스 크기를 신뢰할 수 있게 예측**할 수 있음을 의미합니다.
    *   P-값을 직접 읽을 수 없을 때 오른쪽의 **별표(`*`) 코드**를 통해 P-값의 대략적인 크기를 알 수 있습니다.
*   **Residual Standard Error (잔차 표준 오차)**: F 통계량 방정식의 분모 제곱근을 나타냅니다.
*   **R-squared values (R 제곱 값)**:
    *   **`Multiple R-squared`**는 `weight`가 `size` 변동의 **61%를 설명**할 수 있음을 의미하며, 이는 좋은 결과입니다.
    *   **`Adjusted R-squared`**는 모델의 매개변수 수에 따라 R-제곱 값을 조정한 것입니다.
*   **F-statistic and P-value (F 통계량 및 P-값)**:
    *   이 값은 **R-제곱 값이 유의미한지** 여부를 알려줍니다.
    *   여기서의 **P-값**은 `weight`가 `size`에 대한 **신뢰할 수 있는 추정치를 제공**한다는 것을 의미합니다.

### **39.4 회귀선 추가**
*   마지막으로, 생성된 회귀선을 처음에 그렸던 XY 그래프에 추가하여 시각적으로 확인할 수 있습니다.

R을 활용하여 **선형 회귀 모델을 구축하고 그 결과를 통계적으로 해석하는 데 필요한 핵심 개념과 실제 코드 적용 방법**을 명확하게 제시하고 있습니다. 특히 `summary` 함수의 출력을 이해하는 것은 모델의 **예측력**과 **변수들의 유의미성**을 판단하는 데 매우 중요합니다.

## 40. 다중 회귀(Multiple Regression)

### **40.1 다중 회귀란 무엇인가?**
*   **단순 선형 회귀(Simple Regression)**는 데이터에 **직선(line)을 맞추는 것**입니다.
*   **다중 회귀**는 데이터에 **평면(plane) 또는 고차원 객체(higher dimensional object)를 맞추는 것**입니다.
*   "고차원 객체"라는 용어는 복잡하게 들리지만, 단순히 모델에 **추가적인 데이터(additional data)를 추가**하는 것을 의미합니다.
*   예를 들어, 이전 선형 회귀에서는 마우스 무게만으로 몸 길이를 모델링했지만, 다중 회귀에서는 마우스 무게와 꼬리 길이 **모두를 사용하여 몸 길이를 모델링**할 수 있습니다. 음식 섭취량이나 쳇바퀴 달린 시간 같은 추가 요인들도 '추가 차원' 또는 '추가 데이터'로 간주될 수 있습니다.

### **40.2 R-제곱(R-squared) 계산**
*   R-제곱 계산은 단순 선형 회귀와 다중 회귀 **모두에서 동일**합니다. 적합된 모델 주변의 제곱합(sums of squares around the fit)과 평균 주변의 제곱합(sums of squares around the mean) 값을 사용하여 계산합니다.
*   다만, 다중 회귀의 경우 방정식에 **추가된 매개변수를 보정하기 위해 R-제곱 값을 조정(adjust)합니다**. 이 내용은 선형 회귀에 대한 StatQuest에서 이미 다루었습니다.

### **40.3 P-값(P-value) 계산 (F-통계량)**
*   R-제곱에 대한 F-통계량과 P-값을 계산하는 과정은 기본적으로 동일합니다.
*   **단순 회귀**의 경우 `P fit` 값은 2인데, 이는 최소 제곱(least squares)이 추정해야 하는 매개변수가 두 개이기 때문입니다.
*   **다중 회귀**의 경우, 특정 예시에서 `P fit` 값은 3인데, 이는 최소 제곱이 세 가지 다른 매개변수를 추정해야 했기 때문입니다. 모델에 추가적인 데이터가 추가되면 `P fit` 값은 새로운 방정식의 매개변수 수와 동일하게 변경되어야 합니다.
*   `P mean` 값은 단순 및 다중 회귀 모두에서 1인데, 이는 몸 길이의 평균값 하나만 추정하면 되기 때문입니다.

### **40.4 다중 회귀의 핵심 강점: 모델 비교**
*   이전에는 단순 회귀와 다중 회귀를 각각 평균과 비교했지만, 다중 회귀의 진정한 가치는 **두 모델을 서로 비교**할 수 있다는 점입니다.
*   이것은 예를 들어 꼬리 길이 데이터를 수집하는 시간과 노력이 **가치가 있는지 여부**를 알려줍니다. 즉, 꼬리 길이를 포함하지 않은 모델(단순 회귀)과 꼬리 길이를 포함한 모델(다중 회귀)을 비교하는 것입니다.
*   모델 비교를 위한 F-값 계산은 이전과 동일하지만, 이번에는 **'평균' 관련 내용을 '단순 회귀' 관련 내용으로 대체**합니다.
    *   평균 주변의 제곱합 대신 **단순 회귀 주변의 제곱합**을 사용하고.
    *   `P mean` 대신 **`P simple` (단순 회귀의 매개변수 수, 즉 2)**을 사용합니다.
    *   그리고 다중 회귀 주변의 제곱합과 다중 회귀 방정식의 매개변수 수를 사용합니다.

### **40.5 결과 해석**
*   단순 회귀와 다중 회귀 간의 **R-제곱 값의 차이가 크고 P-값이 작으면**, 꼬리 길이를 모델에 추가하는 것이 **가치 있다(worth the trouble)**는 의미입니다.

다중 회귀의 개념을 이해하고, 단순 회귀와의 차이점을 파악하며, 특히 모델을 비교하여 추가적인 변수가 모델의 예측력을 얼마나 향상시키는지 평가하는 방법을 배우는 데 매우 유용합니다. 

## 41. R을 사용하여 다중 회귀(Multiple Regression)

### 41.1 단순 회귀(Simple Regression)

단순 회귀는 하나의 독립 변수를 사용하여 종속 변수를 예측하는 모델입니다. 이 영상에서는 **몸무게(weight)를 사용하여 크기(size)를 예측**하는 예시를 사용합니다.

*   **1단계: 데이터 시각화 (Plot Your Data)**
    *   **항상 데이터를 시각화하는 것이 중요합니다.** 이는 선형 회귀가 적절한지 평가하는 데 도움이 됩니다.
    *   X축에 'weight', Y축에 'size'를 지정하여 플롯합니다.
    *   데이터에서 'size'와 'weight' 사이에 관계가 보이면 회귀 분석이 타당하다는 것을 의미합니다.
*   **2단계: 선형 모델 적합 (Fit a Linear Model)**
    *   R에서 **`lm()` 함수(linear model의 약자)**를 사용하여 데이터에 선을 맞춥니다.
    *   방정식은 'size ~ weight'로 지정합니다. 여기서 `~`는 'predicted by'를 의미합니다.
    *   R은 기본적으로 y-절편과 기울기 항을 추가합니다.
    *   R은 **최소 제곱법(least-squares)**을 사용하여 제곱 잔차를 최소화하는 y-절편과 기울기 값을 찾습니다.
*   **3단계: 결과 요약 확인 (Summary of Results)**
    *   `lm()` 함수의 결과를 `simple.regression` 변수에 저장한 후, **`summary()` 함수**를 사용하여 회귀 결과를 요약합니다.
    *   **가장 중요한 정보는 R-제곱(R-squared)과 p-값(p-value)입니다**.
        *   **R-제곱은 0.613**으로, 'weight'가 'size'를 예측하는 데 꽤 좋은 성능을 보임을 나타냅니다.
        *   **p-값은 0.012**로, 통계적으로 유의미한 예측 관계임을 보여줍니다.
        *   단순 회귀에서는 `multiple r-squared` 또는 `r-squared` 값을 사용하며, `adjusted R squared`는 복잡한 모델에서 사용됩니다.
*   **4단계: 회귀선 추가 (Add Regression Line)**
    *   `abline()` 함수를 사용하여 그래프에 최소 제곱 적합 선을 추가합니다.

### 41.2 다중 회귀(Multiple Regression)

다중 회귀는 둘 이상의 독립 변수를 사용하여 종속 변수를 예측하는 모델입니다. 이 영상에서는 **몸무게(weight)와 꼬리 길이(tail)를 사용하여 크기(size)를 예측**하는 예시를 사용합니다.

*   **1단계: 데이터 시각화 (Plot Your Data)**
    *   이번에는 R이 모든 데이터 열('size', 'weight', 'tail')을 서로 비교하여 플롯합니다.
    *   이를 통해 다중 회귀가 이 데이터에 적합한지 결정하는 데 필요한 모든 플롯을 얻을 수 있습니다.
    *   관찰 결과:
        *   'weight'와 'tail' 모두 'size'와 상관관계가 있습니다. 이는 좋은 예측 변수임을 의미합니다.
        *   **'weight'와 'tail' 또한 서로 상관관계가 있습니다.** 이는 두 변수가 유사한 정보를 제공하며, 모델에 두 변수 모두 필요하지 않을 수도 있음을 시사합니다.
*   **2단계: 선형 모델 적합 (Fit a Linear Model)**
    *   `lm()` 함수를 다시 사용합니다.
    *   방정식은 'size ~ weight + tail'로 지정합니다. 여기서 `+` 기호는 두 변수를 모두 사용함을 의미합니다.
    *   R은 기본적으로 y-절편과 두 개의 기울기 항(weight, tail 각각)을 추가합니다.
*   **3단계: 결과 요약 확인 (Summary of Results)**
    *   `summary()` 함수를 사용하여 결과를 요약합니다.
    *   다중 회귀에서는 **`adjusted r-squared` 값에 더 큰 관심을 둡니다**.
    *   `r-squared`, `adjusted r-squared`, `p-value`가 양호한지 확인합니다.
    *   **개별 예측 변수의 유의미성 비교:**
        *   'weight'와 'tail'을 모두 사용하여 'size'를 예측하는 모델과 **'tail'만 사용하여 'size'를 예측하는 모델을 비교**하는 줄이 있습니다.
        *   이 경우 **p-값은 0.65로, 'weight'와 'tail'을 모두 사용하는 것이 'tail'만 사용하는 것보다 유의미하게 더 낫지 않다는 것을 의미합니다**. 즉, 'tail'이 이미 충분한 정보를 제공하므로 'weight'를 추가하는 것이 예측력을 크게 향상시키지 않는다는 것입니다.
        *   'weight'와 'tail'을 모두 사용하여 'size'를 예측하는 모델과 **'weight'만 사용하여 'size'를 예측하는 모델을 비교**하는 줄도 있습니다.
        *   이 경우 **p-값은 0.015로, 'weight'와 'tail'을 모두 사용하는 것이 'weight'만 사용하는 것보다 유의미하게 더 낫다는 것을 의미합니다**. 즉, 'weight'만으로는 부족하고 'tail'을 추가하는 것이 예측력을 유의미하게 높인다는 것입니다.

### 41.3 시사점

*   'weight'와 'tail'을 모두 사용하여 'size'를 예측하는 것은 좋지만, **시간을 절약하고 싶다면 굳이 쥐의 몸무게를 측정하지 않고 꼬리 길이만으로 'size'를 예측해도 충분할 수 있습니다.** 왜냐하면 'weight'와 'tail'을 모두 사용하는 모델이 'tail'만 사용하는 모델보다 통계적으로 유의미하게 더 좋지는 않기 때문입니다.
*   이 동영상은 **R을 활용한 선형 회귀 분석의 기본부터 심화까지 실용적인 방법을 제공**합니다. 데이터 시각화, `lm()` 함수를 이용한 모델 적합, `summary()` 함수를 통한 결과 해석은 AI 및 머신러닝 모델을 구축하는 데 필수적인 기초 지식입니다. 특히 **`R-squared`, `adjusted R-squared`, `p-value`의 의미를 정확히 이해하고 다양한 모델 간의 비교를 통해 최적의 예측 변수를 선택하는 능력**은 데이터 과학자로서 중요한 역량입니다.

## 42. 선형 회귀(linear regression) 기술을 사용하여 t-검정과 분산 분석

### 42.1 선형 회귀(Linear Regression) 간략 복습

지난번 StatQuest에서는 쥐의 몸무게(mouse weight)를 사용하여 쥐의 크기(mouse size)를 예측하는 방법을 다뤘습니다. 이때 우리는 두 가지를 알고자 했습니다:
*   **R-제곱(R-squared)**: 쥐의 몸무게가 쥐의 크기를 예측하는 데 얼마나 유용한지.
*   **P-값(P-value)**: 예측 관계가 우연에 의한 것인지 여부.

### 42.2 t-검정(t-test)에 선형 회귀 기법 적용하기

t-검정의 목표는 **두 평균을 비교하고 통계적으로 유의미하게 다른지 확인**하는 것입니다. 이 동영상에서는 **대조군 쥐(control mice)와 돌연변이 쥐(mutant mice) 간의 유전자 발현을 비교**하는 예시를 사용합니다.

**t-검정 수행 단계 (선형 회귀 기법 활용):**

1.  **전체 평균 찾기 (Ignore X-axis and find overall mean)**:
    *   먼저 X축을 무시하고 Y축(유전자 발현)에 초점을 맞춰 전체 데이터의 평균을 찾습니다.
2.  **평균 주위 잔차 제곱합 계산 (Calculate Sum of Squared Residuals around the mean, SSmean)**:
    *   각 데이터 포인트와 전체 평균(선) 사이의 거리를 잔차(residuals)라고 하며, 이 잔차들을 제곱하여 합산합니다.
3.  **데이터에 선 적합 (Fit a line to the data)**:
    *   이제 X축을 고려합니다. t-검정에서는 각 그룹(대조군, 돌연변이군)의 평균이 해당 그룹 데이터에 대한 **최소 제곱 적합(least squares fit)**이 됩니다.
    *   대조군 데이터에는 대조군의 평균(예: 2.2)을 나타내는 수평선(y = 2.2)을 적합하고, 돌연변이군 데이터에는 돌연변이군의 평균(예: 3.6)을 나타내는 수평선(y = 3.6)을 적합합니다.
    *   **두 개의 선을 하나의 방정식으로 결합**: 컴퓨터가 자동으로 계산할 수 있도록 두 그룹의 적합선을 하나의 방정식으로 결합하는 방법이 필요합니다. 이는 **설계 행렬(design Matrix)**이라는 개념을 사용합니다.
        *   설계 행렬은 0과 1을 사용하여 각 평균을 "켜고 끄는" 스위치 역할을 합니다. 예를 들어, 대조군 데이터 포인트에 대해서는 대조군 평균에 1을 곱하고 돌연변이군 평균에 0을 곱하며, 돌연변이군 데이터 포인트에 대해서는 그 반대로 합니다.
        *   이 설계 행렬과 추상화된 방정식을 결합하여 데이터에 대한 적합을 나타냅니다.
4.  **적합된 선 주위 잔차 제곱합 계산 (Calculate Sum of Squares of the Residuals around the fitted lines, SSfit)**:
    *   각 데이터 포인트와 해당 그룹의 적합된 선(그룹 평균) 사이의 잔차를 제곱하여 합산합니다.
5.  **F 및 P-값 계산 (Calculate F and P-value)**:
    *   계산된 SSmean, SSfit 값과 함께 **Pmean**(전체 평균 방정식의 매개변수 수, 여기서는 1개) 및 **Pfit**(적합된 선 방정식의 매개변수 수, 여기서는 2개 – 대조군 평균, 돌연변이군 평균)를 F 통계량 공식에 대입하여 F 값을 계산합니다.
    *   F 값을 통해 P-값을 얻습니다.

### 42.3 분산 분석(ANOVA)에 선형 회귀 기법 적용하기

분산 분석(ANOVA)은 **여러 범주(카테고리) 간의 평균이 모두 동일한지 여부를 검정**하는 데 사용됩니다. 이 동영상에서는 대조군, 돌연변이군 외에도 특이한 식단을 먹는 대조군 및 돌연변이군, 그리고 이형접합체 쥐(heterozygote mice) 등 **다섯 가지 범주**를 비교하는 예시를 사용합니다.

**ANOVA 수행 단계 (선형 회귀 기법 활용):**

1.  **전체 평균 주위 잔차 제곱합 계산 (SSmean)**:
    *   모든 범주를 아우르는 전체 평균 값을 계산하고, 각 데이터 포인트와 이 전체 평균 사이의 잔차를 제곱하여 합산합니다. 이 전체 평균은 하나의 매개변수(Pmean = 1)를 가집니다.
2.  **적합된 선 주위 잔차 제곱합 계산 (SSfit)**:
    *   각 범주에 대해 개별적인 평균(적합된 선)을 계산합니다. 이 적합된 선 방정식은 각 범주의 평균마다 하나의 매개변수를 가지므로, 다섯 가지 범주에는 다섯 개의 매개변수(Pfit = 5)가 있습니다.
    *   각 범주에 대한 설계 행렬은 각 범주마다 하나의 열을 가집니다.
3.  **F 및 P-값 계산 (Calculate F and P-value)**:
    *   SSmean, SSfit, Pmean, Pfit 값을 F 통계량 공식에 대입하여 F 값을 계산하고, 이를 통해 P-값을 얻습니다.

### 42.4 설계 행렬(Design Matrix)의 중요성 및 추가 정보

*   이 동영상에서 보여준 설계 행렬은 t-검정 및 ANOVA에 사용되는 **표준 설계 행렬과는 다를 수 있지만**, 동일한 결과를 얻을 수 있습니다. 더 일반적인 설계 행렬과 복잡한 설계는 다음 StatQuest에서 다룰 예정입니다.
*   **설계 행렬은 컴퓨터가 수많은 최소 제곱 기반 문제를 새로운 방법을 만들 필요 없이 유연하게 해결**할 수 있도록 하는 핵심 개념입니다.

### 42.5 시사점

**통계 모델링의 근본적인 통찰력**을 제공합니다.
*   **선형 회귀가 단순히 예측 모델이 아니라, t-검정이나 ANOVA와 같은 가설 검정에도 사용될 수 있는 강력한 일반 선형 모델의 일부**임을 이해하는 것이 중요합니다.
*   **설계 행렬**은 범주형 데이터를 선형 모델에 통합하는 핵심적인 방법이며, 이는 **원-핫 인코딩(one-hot encoding)**과 같은 AI/머신러닝 기법의 기반 개념과 연결될 수 있습니다.
*   P-값, R-제곱, 잔차 제곱합, 매개변수 수와 같은 개념을 이해하는 것은 **모델의 성능을 평가하고 해석하며, 복잡한 데이터 관계를 이해하는 데 필수적인 역량**입니다. 이 지식은 향후 더 복잡한 머신러닝 모델(예: 로지스틱 회귀, 신경망)을 이해하고 구축하는 데 강력한 기초가 될 것입니다.

## 43. 일반 선형 모델과 설계 행렬

### **43.1 설계 행렬이란 무엇인가요?**
설계 행렬은 통계 모델의 방정식을 구성하는 데 사용되는 숫자들의 행렬입니다. 이 행렬의 각 열은 방정식의 특정 항(term)에 해당하며, 각 행은 개별 데이터 포인트를 나타냅니다. 설계 행렬의 숫자는 방정식의 해당 항을 '활성화'하거나 '비활성화'하거나 '조절'하는 역할을 합니다.

### **43.2 설계 행렬의 작동 방식:**
*   **활성화/비활성화 (1과 0):** 일반적으로 1은 해당 항을 활성화하여 방정식에 포함하고, 0은 해당 항을 비활성화하여 제외합니다. 예를 들어, t-검정에서는 대조군 데이터에 대해 평균 대조군 값에 대한 항을 켜고(1), 돌연변이군 데이터와의 차이에 대한 항을 끕니다(0). 반면, 돌연변이군 데이터는 두 항 모두를 켭니다(1).
*   **조절 (다른 숫자):** 설계 행렬은 1과 0뿐만 아니라 다른 숫자도 포함할 수 있습니다. 선형 회귀(linear regression)의 경우, 첫 번째 열은 y-절편을 활성화하는 1로 채워지고, 두 번째 열은 각 데이터 포인트의 x-축 위치를 포함하여 기울기(slope) 항을 **조절(scaling)**합니다. 이 숫자들을 y-절편 및 기울기 값과 곱하면 최소 제곱 적합선(least squares fit line)상의 특정 점을 얻을 수 있습니다.

### **43.3 설계 행렬의 활용 사례:**

*   **T-검정 및 ANOVA:** 범주형(categorical) 데이터가 있을 때 특정 방정식의 부분을 켜거나 끄는 데 이상적입니다. 동영상에서는 t-검정을 위한 표준 설계 행렬을 보여주며, 이는 대조군 평균과 돌연변이군과 대조군 간의 차이 항을 사용하여 돌연변이군 데이터에 대한 오프셋(offset)을 제공합니다. 두 가지 다른 방정식과 설계 행렬이 동일한 잔차(residuals)와 F 값, P 값을 산출할 수 있음을 강조합니다.
*   **선형 회귀(Linear Regression):** x-축 위치와 같은 연속형(continuous) 데이터를 사용하여 기울기 항을 조절함으로써 데이터 포인트에 가장 잘 맞는 선을 모델링합니다.
*   **복합 모델 (t-검정 + 회귀):** 여러 유형의 데이터를 동시에 분석하기 위해 설계 행렬을 사용할 수 있습니다. 예를 들어, 쥐의 체중과 크기 관계를 분석할 때, 일반 쥐와 돌연변이 쥐를 비교하는 경우를 들 수 있습니다.
    *   이 모델은 일반 쥐의 y-절편, 돌연변이 쥐 오프셋, 그리고 (두 유형의 쥐에서 동일한) 기울기에 대한 항을 포함합니다.
    *   설계 행렬은 첫 번째 열에 y-절편을 위한 1, 두 번째 열에 돌연변이 오프셋을 켜거나 끄는 1 또는 0, 마지막 열에 체중 데이터(x-좌표)를 포함합니다.
    *   이를 통해 각 쥐 유형에 대한 별도의 선을 모델링하고 통계적으로 비교할 수 있습니다.
*   **배치 효과(Batch Effect) 처리:** 여러 실험실에서 얻은 데이터처럼 측정값이 전반적으로 다를 때, 배치 효과를 보상하기 위해 설계 행렬에 추가 항을 포함할 수 있습니다. 예를 들어, 특정 실험실의 데이터에 대한 오프셋 항을 추가하여 데이터를 결합하여 분석할 수 있습니다.

### **43.4 가설 검정 및 모델 비교:**
설계 행렬을 통해 복잡한("fancy") 모델과 단순한("simple") 모델의 적합도를 비교하여 **P-값**을 계산할 수 있습니다.
*   모델에서 계산된 잔차(residuals) 제곱합을 사용하여 F 통계량을 계산합니다.
*   P-값은 복잡한 모델이 단순한 모델보다 데이터를 예측하는 데 **통계적으로 유의미하게 더 나은지**를 알려줍니다.
*   어떤 단순 모델과 비교할지는 해결하려는 질문에 따라 달라집니다. 예를 들어, 체중과 쥐 유형을 모두 고려하는 모델이 체중만 고려하는 모델보다 나은지, 또는 쥐 유형만 고려하는 모델보다 나은지 비교할 수 있습니다.

### **43.5시사점:**
설계 행렬은 선형 모델의 유연성과 강력함을 보여주는 핵심 개념입니다. AI 분야에서 데이터를 모델링하고 예측하는 데 있어 **선형 회귀, ANOVA, t-검정**과 같은 기본 통계 방법은 여전히 매우 중요합니다. 설계 행렬을 이해하면 다음과 같은 이점이 있습니다.
*   **데이터의 관계 이해:** 여러 변수가 결과에 어떻게 영향을 미치는지 명확하게 파악할 수 있습니다.
*   **모델 설계:** 특정 가설을 검정하거나 데이터의 복잡한 패턴을 포착하기 위해 맞춤형 모델을 구축하는 데 필요한 지식을 제공합니다.
*   **해석 가능한 AI:** 설계 행렬은 모델의 각 구성 요소가 예측에 기여하는 방식을 시각적으로, 그리고 수치적으로 이해하는 데 도움을 주어 **모델의 해석 가능성(interpretability)**을 높일 수 있습니다.
*   **기본 통계 개념 강화:** AI 모델 학습에서 자주 접하게 되는 통계적 추론(statistical inference)의 기초를 다지는 데 필수적입니다.

결론적으로, 설계 행렬은 단순히 1과 0의 집합이 아니라, 우리가 원하는 어떤 숫자라도 방정식에 한 행씩 대입할 수 있는 유연한 도구이며, 이를 통해 복잡한 데이터 관계를 모델링하고 통계적 유의성을 평가할 수 있습니다.

## 44. 분포에서 샘플(표본) 추출

### **44.1 분포(Distribution)란 무엇인가요?**
분포는 측정값의 **확률**을 시각적으로 나타낸 것입니다. 동영상에서는 사람들의 키를 예로 들어, 히스토그램이나 부드러운 곡선으로 표현됩니다.
*   **높은 부분:** 해당 측정값이 나올 확률이 높다는 것을 의미합니다 (예: 대부분의 사람들이 5피트 7인치와 6피트 사이의 키를 가짐).
*   **낮은 부분:** 해당 측정값이 나올 확률이 낮다는 것을 의미합니다 (예: 4.5피트 이하 또는 6.5피트 이상인 키는 드뭄).

### **44.2 분포에서 샘플(표본)을 추출한다는 것은 무엇인가요?**
이는 **히스토그램이나 곡선이 나타내는 확률을 기반으로 컴퓨터가 무작위 숫자를 생성하는 것**을 의미합니다.
*   컴퓨터는 확률이 높은 구간(예: 분포의 중간)에서 값을 뽑을 가능성이 더 높습니다.
*   하지만 때로는 확률이 낮은 구간(예: 분포의 가장자리)에서 값을 뽑을 수도 있습니다.

### **44.3 왜 분포에서 샘플(표본)을 추출해야 할까요? (AI 학습자를 위한 중요성)**
분포에서 샘플을 추출하는 가장 큰 이유는 **통계를 연구하고 탐구하기 위함**입니다.
*   **통계 테스트 시뮬레이션:** 컴퓨터는 수많은 샘플을 빠르게 생성할 수 있으며, 이 샘플들을 실제 데이터를 사용하는 것처럼 통계 테스트에 적용해 볼 수 있습니다.
*   **예상과 실제 비교:** 우리는 원래 분포가 어떤 것인지 알고 있기 때문에, 시뮬레이션을 통해 얻은 결과가 우리의 예상과 얼마나 일치하는지 비교할 수 있습니다.
*   **모델 이해 및 검증:** AI 모델은 종종 특정 분포를 가정하거나, 예측 결과가 특정 분포를 따르도록 학습됩니다. 샘플링을 통해 모델의 동작을 이해하고, 다양한 시나리오에서 모델이 어떻게 반응하는지 가상으로 테스트해 볼 수 있습니다.

### **44.4 t-검정 시뮬레이션 예시:**
동영상은 t-검정을 예로 들어 설명합니다.
*   **같은 분포에서 샘플 추출:** 만약 동일한 하나의 분포에서 N=3인 두 개의 샘플을 뽑아서 t-검정을 수행한다면, 두 샘플 간에 유의미한 차이가 없을 것이므로 **큰 P-값**이 나올 것입니다.
*   **다른 분포에서 샘플 추출:** 만약 두 개의 분리된(다른) 분포에서 샘플을 뽑아서 t-검정을 수행한다면, 두 샘플 간에 차이가 있을 가능성이 높으므로 **작은 P-값**이 나올 것입니다.
*   **샘플 크기 결정:** 이 과정을 수없이 반복하면, t-검정이 얼마나 자주 정확하게 작동하고 작은 P-값을 주는지 파악할 수 있습니다. 이는 실제로 실험을 하기 전에 **"샘플 크기를 늘려야 할까?"**와 같은 중요한 결정을 내리는 데 도움을 줍니다.

분포에서 샘플을 추출하는 것은 컴퓨터가 특정 분포의 확률을 반영하는 무작위 숫자 묶음을 생성하도록 하는 강력한 방법입니다. 이를 통해 **실제로 데이터를 수집하고 실험하지 않고도 통계 테스트와 모델이 할 수 있는 것이 무엇인지 미리 파악**할 수 있습니다.

## 45. Bam!!! Clearly Explained!!!

## 46. 유의미성 임계값 (Thresholds for Significance)

사람들이 자주 묻는 "어떤 유의미성 임계값이 좋은가?"라는 질문에 답하고 있습니다. 특히 0.05라는 숫자가 항상 최적의 임계값인지, 예외는 없는지에 대해 다룹니다.

### **46.1 임계값 0.05의 의미와 사용**

*   **기원:** 0.05라는 임계값은 **상대적으로 임의적으로 선택**되었으며, 생물학적 또는 자연적인 최적의 이유가 있는 것은 아닙니다.
*   **일반적인 사용:** 과학 분야의 출판물 등에서 오랫동안 널리 사용되어 왔으며, **대부분의 경우 적절하게 작동**합니다.
*   **의미:** 0.05는 5%의 경우 잘못된 결론을 내릴 수 있지만, 95%의 경우 올바른 결론을 내린다는 것을 의미하며, 이는 **대부분의 생물학 과학에서 비용 효율적인 임계값**으로 간주됩니다.
*   **게시를 위한 기준:** 데이터를 출판할 경우, **거의 항상 0.05가 좋은 임계값**입니다. 편집자와 심사위원들이 이 기준을 기대하기 때문입니다. 0.05보다 낮은 p-값을 얻을 수 있다면 더욱 좋습니다.

### **46.2 0.05 외의 예외 상황 및 고려사항**

0.05가 일반적인 기준이지만, 항상 고수해야 하는 것은 아닙니다. 다음과 같은 경우들을 고려해야 합니다.

*   **탐색적 데이터 분석 (Exploratory Data Analysis)**
    *   만약 데이터를 **출판할 목적이 아니라** 인터넷에서 다운로드한 제한된 데이터셋을 가지고 가설을 탐색하거나, 나중에 **다른 방법으로 검증할 새로운 가설을 생성하는 경우**라면, p-값이 0.05보다 약간 높더라도 괜찮습니다.
    *   이러한 탐색 단계에서는 **아주 작은 p-값이 필요하지 않습니다**. 대략 작기만 해도 충분하며, 0.05에 얽매일 필요가 없습니다.

*   **효과 크기 (Effect Size)의 중요성 (R-squared 예시)**
    *   **아무리 작은 p-값을 얻더라도 효과 크기가 좋지 않다면 의미가 없을 수 있습니다**.
    *   **R-squared**와 같은 효과 크기 지표는 연구 중인 요인이 데이터 변동의 얼마나 많은 부분을 설명하는지를 나타냅니다. 예를 들어, R-squared가 10%라면 데이터 변동의 10%만 설명한다는 의미로, 이는 그리 크지 않은 효과입니다.
    *   **작은 p-값과 더불어 좋은 효과 크기** (예: 좋은 R-squared)를 확보하는 것이 중요합니다. p-값이 아무리 0에 가깝더라도 R-squared가 "형편없다면" 그 결과는 크게 중요하지 않습니다.

*   **비범한 주장은 비범한 데이터가 필요하다 (Extraordinary Claims Need Extraordinary Data)**
    *   매우 **놀랍거나 비범한 주장**을 하려면 (예: 뉴욕 상공에 UFO가 날아다닌다는 증거를 발견했다면), **매우 매우 작은 p-값이 필요**합니다.
    *   이러한 주장에는 0.05보다 약간 낮은 p-값(예: 0.047)만으로는 충분하지 않습니다. 사람들에게 주장을 믿게 하려면 **믿을 수 없을 정도로 작은 p-값**을 제시해야 합니다.

### **46.3 요약:**

*   데이터를 출판하거나 모델의 성능을 공식적으로 보고할 때는 일반적으로 0.05의 유의미성 임계값을 사용하고, 더 낮은 p-값을 목표로 하세요.
*   **모델 탐색, 초기 가설 검증, 또는 새로운 아이디어를 발견하는 단계에서는 p-값이 0.05보다 약간 높더라도 너무 걱정하지 마세요.** 이때는 데이터에서 흥미로운 것을 찾는 것이 더 중요합니다.
*   **p-값만 보지 말고, 항상 **효과 크기(Effect Size)**를 함께 고려하세요.** AI 모델이 아무리 통계적으로 유의미한 결과를 보여주더라도, 그 효과가 실제 문제의 변동을 충분히 설명하지 못한다면 실용적인 가치가 떨어질 수 있습니다.
*   **만약 여러분의 AI 모델이 기존의 상식을 뒤엎는 **비범한 주장**을 한다면, 이를 뒷받침할 수 있는 **압도적으로 작은 p-값과 강력한 증거**를 제시해야 합니다**.

## 47. 어떤 T-검정을 사용해야 하는가?
T-검정은 주로 두 그룹 간의 평균 차이가 통계적으로 유의미한지 확인하는 데 사용됩니다.

### **47.1 T-검정의 주요 범주: 짝지은(Paired) vs. 짝지어지지 않은(Unpaired)**

*   **짝지은(Paired) T-검정:**
    *   **언제 사용?** 동일한 테스트 대상으로부터 **"전후(before and after)" 측정값**이 있을 때 유용합니다.
    *   **예시:** 혈압약의 효과를 측정하기 위해 한 그룹의 사람들에게 약을 주기 전 혈압을 측정하고, 약을 복용한 후 다시 혈압을 측정하는 경우. 각 개인마다 '복용 전'과 '복용 후'라는 한 쌍의 측정값을 가지게 됩니다.
    *   **특징:** 각 대상에 대해 두 개의 측정값이 짝을 이루는 데이터에 사용됩니다.

*   **짝지어지지 않은(Unpaired) T-검정:**
    *   **언제 사용?** 두 개의 **서로 다른 그룹**에서 측정값을 얻을 때 사용됩니다.
    *   **예시:** 그룹 A 사람들의 키를 측정하고, 그룹 B 사람들의 키를 측정하여 두 그룹의 키를 비교하는 경우. 각 그룹에 속한 사람이 서로 겹치지 않습니다.
    *   **하위 범주:** 짝지어지지 않은 T-검정은 두 가지 하위 범주로 나뉩니다.
        *   **등분산 가정(Assumes Equal Variation):** 각 그룹 내의 분산(변동)이 동일하다고 가정하는 경우. 예를 들어, 그룹 A의 키 측정값 주변의 변동이 그룹 B의 키 측정값 주변의 변동과 정확히 같다고 가정합니다.
        *   **등분산 비가정(Does Not Assume Equal Variation):** 각 그룹 내의 분산이 다를 수 있다고 가정하는 경우.
        *   **권장 사항:** 일반적으로 **등분산을 가정하지 않는 검정(does not assume equal variation)**을 선택하는 것이 좋습니다. 그 이유는 이 검정이 **약간 더 보수적(more conservative)**이기 때문입니다. 만약 데이터가 더 보수적인 이 T-검정을 통과한다면, 여러분의 데이터가 **"매우 확실하다(rock solid)"**는 것을 알 수 있습니다.

### **47.2 꼬리(Tail) 유형: 단측(One-tailed) vs. 양측(Two-tailed)**

*   **양측(Two-tailed) T-검정:**
    *   **언제 사용?** 그룹 A가 그룹 B보다 유의미하게 **높은지 또는 낮은지** **모두** 테스트합니다.
    *   **특징:** 데이터에 대해 어떠한 **선입견(preconceived notion)**도 가지지 않습니다. 데이터가 스스로 말하도록 내버려 둡니다.
    *   **권장 사항:** 항상 **양측 T-검정**을 사용하는 것을 추천합니다. 이는 **더 보수적**이며, 데이터가 스스로 이야기하도록 하고, 만약 데이터가 이 검정을 통과하면 **"매우 확실하다(rock solid)"**는 것을 알 수 있기 때문입니다.

*   **단측(One-tailed) T-검정:**
    *   **언제 사용?** 한 그룹이 다른 그룹보다 **특정 방향(예: 반드시 더 높을 것이다)**으로 유의미하게 차이 날 것이라는 **가정**이 있을 때 사용됩니다.
    *   **특징:** 양측 검정보다 **덜 보수적(less conservative)**입니다. 학술 저널에서는 일반적으로 권장되지 않는 방식입니다.

### 47.3 **요약:**

T-검정은 AI 모델의 성능을 비교하거나, A/B 테스트 결과를 분석하거나, 특정 피처(feature)의 영향력을 평가할 때 매우 유용할 수 있는 통계 도구입니다. 이 동영상에서 제시하는 핵심 권장 사항은 다음과 같습니다.

*   **동일한 대상의 전후 데이터를 비교할 때는 **짝지은(paired) T-검정**을 사용하세요**.
*   **서로 다른 두 그룹을 비교할 때는 **짝지어지지 않은(unpaired) T-검정**을 사용하세요**.
    *   이때, **등분산을 가정하지 않는(does not assume equal variance) 버전**을 선택하는 것이 좋습니다. 이는 더 보수적이며 결과의 신뢰도를 높여줍니다.
*   **어떤 T-검정을 사용하든, 항상 **양측(two-tailed) T-검정**을 사용하세요.** 이는 데이터가 스스로 말하도록 하고, 더 보수적인 접근 방식으로 결과의 견고함을 보장합니다.

## 48. 단측 검정(One-Tailed Test) vs. 양측 검정(Two-Tailed Test)
**단측 p-값(one-tailed p-value)과 양측 p-값(two-tailed p-value) 중 어떤 것을 사용해야 하는지**에 대한 명확한 설명을 제공합니다.

### **48.1 핵심 개념 소개: 신약 치료제 임상시험 예시**
*   새로운 암 치료제가 표준 치료제보다 더 나은 결과를 보여줄 것이라는 가설을 세우고 소규모 임상시험을 진행했다고 가정해 봅시다.
*   데이터를 분석한 결과, 새로운 치료제가 더 낫다는 시사점을 얻었지만, 결과에 약간의 모호성이 있었습니다.
*   이때 통계 분석을 통해 **단측 검정은 0.03의 p-값**을, **양측 검정은 0.06의 p-값**을 나타냈습니다. 일반적으로 유의성 판단 기준인 0.05와 비교했을 때, 단측 검정 결과는 유의하지만 양측 검정 결과는 유의하지 않습니다.

### **48.2 단측 p-값(One-Tailed P-Value)과 양측 p-값(Two-Tailed P-Value)의 정의**
*   **단측 p-값**: 새로운 치료법이 표준 치료법보다 **더 낫다**는 가설(또는 더 나쁘다는 가설)을 검정합니다. 즉, 한쪽 방향의 변화에만 관심이 있을 때 사용됩니다.
*   **양측 p-값**: 새로운 치료법이 표준 치료법보다 **더 낫거나, 더 나쁘거나, 유의미한 차이가 없다**는 가설을 검정합니다. 즉, 양쪽 방향의 모든 가능한 변화를 고려합니다.

### **48.3 p-값 크기의 차이 및 선택의 중요성**
*   단측 p-값이 양측 p-값보다 더 작은 값을 가지는 경향이 있습니다. 이는 단측 검정이 '더 나쁘다' 또는 '유의미한 차이가 없다'는 경우를 구분하지 않고 한 방향으로만 초점을 맞추기 때문입니다.
*   만약 새로운 치료법이 표준 치료법보다 **더 나쁠 수도 있다는 가능성**을 항상 고려해야 하므로, 우리는 **양측 p-값을 사용해야 합니다**.

### **48.4 통계적 좋은 관행: 'p-해킹(P-Hacking)' 피하기**
*   **가장 중요한 원칙**: 어떤 통계 검정을 사용할지, 즉 **단측 또는 양측 p-값 중 어떤 것을 사용할지 결정하는 것은 실험을 수행하기 전에 이루어져야 합니다.** 데이터를 먼저 보고 결과가 좋아 보이는 방향으로 검정 방식을 바꾸는 것을 **'p-해킹'**이라고 합니다.
*   **p-해킹의 문제점**: 이는 잘못된 결과(false positive)를 보고할 확률을 크게 증가시킵니다.
    *   동일한 분포에서 추출한 두 샘플에 대해 양측 T-검정을 10,000번 수행했을 때, p-값이 0.05 미만인 거짓 양성(false positive)은 약 5% (500회) 발생할 것으로 예상됩니다.
    *   하지만 만약 데이터가 특정 방향으로 좋게 보일 때 (예: 한 샘플의 값이 다른 샘플보다 일관되게 작을 때) **단측 검정으로 전환하면 거짓 양성 발생 확률이 5%에서 8%로 증가**하는 것이 시뮬레이션을 통해 확인되었습니다. 이는 유의성 임계값을 0.05로 유지하더라도 발생합니다.
*   따라서, 유의성 여부를 결정하기 위해 데이터를 본 후에 단측 p-값을 사용하기로 결정해서는 안 됩니다.

### **48.5 결론 및 권장 사항**
*   실험을 시작하기 전에 무엇을 배우고 싶은지 명확히 해야 합니다. 암 치료제 예시처럼, 개선뿐만 아니라 악화 가능성도 항상 고려해야 합니다.
*   모든 데이터 분석에서 양측 또는 단측 p-값 중 선택의 여지가 있다면, **항상 '양측 p-값'을 선택해야 합니다.** 우리는 이야기의 한 면뿐만 아니라 양면을 모두 알고 싶기 때문입니다.
*   **예외**: 모든 통계 검정에 단측/양측 선택지가 있는 것은 아닙니다. 만약 선택지가 없다면 걱정할 필요가 없습니다.

AI 모델의 성능 평가나 실험 결과를 분석할 때, 특정 가설(예: 새 모델 A가 기존 모델 B보다 성능이 좋다)을 검정하게 됩니다. 이때 통계적 유의성을 평가할 때 단측/양측 검정 선택이 중요합니다. 단순히 더 낮은 p-값을 얻기 위해 단측 검정을 선택하는 것은 'p-해킹'으로 이어질 수 있으며, 이는 모델의 실제 효과를 잘못 판단하게 할 위험이 있습니다. **항상 양쪽 가능성(더 좋다/더 나쁘다/차이 없다)을 염두에 두고 양측 검정을 기본으로 사용하는 것이 견고하고 신뢰할 수 있는 통계적 결론을 내리는 데 중요합니다.**

## 49. 퀀타일(Quantiles)과 퍼센타일(Percentiles)

### **49.1 퀀타일(Quantile)의 엄격한 정의**
*   **퀀타일은 데이터를 동일한 수의 데이터 포인트(개수)를 포함하는 그룹으로 나누는 기준선**을 의미합니다.
*   **중앙값(Median)**: 가장 대표적인 퀀타일로, 데이터를 절반으로 나눕니다. 50%의 데이터 포인트는 중앙값보다 높고, 50%는 중앙값보다 낮습니다.
    *   예를 들어, 15개 유전자 발현 측정값에서 중앙값은 4.5이며, 이를 **0.5 퀀타일 또는 50% 퀀타일**이라고 부릅니다.

### **49.2 퀀타일의 추가 예시**
*   데이터를 네 개의 동일한 크기의 그룹으로 나눌 때 사용되는 선들도 퀀타일입니다.
    *   **0.25 퀀타일 (또는 25% 퀀타일)**: 전체 데이터 포인트의 25%가 이 값보다 작습니다.
    *   **0.75 퀀타일 (또는 75% 퀀타일)**: 전체 데이터 포인트의 75%가 이 값보다 작습니다.

### **49.3 퍼센타일(Percentile)의 정의와 실제 사용**
*   **기술적인 정의**: 퍼센타일은 데이터를 **100개의 동일한 크기의 그룹으로 나누는 퀀타일**입니다.
*   **실제 사용의 유연성**: 하지만 실제로는 용어가 훨씬 유연하게 사용됩니다.
    *   데이터 세트가 100개 그룹으로 나눌 만큼 크지 않더라도, 중앙값(50% 퀀타일)을 **"50번째 퍼센타일(50th percentile)"**이라고 부르고, 75% 퀀타일을 **"75번째 퍼센타일(75th percentile)"**이라고 부르기도 합니다.
*   **개별 데이터 포인트에 대한 적용**: 종종 퀀타일과 퍼센타일은 각 데이터 포인트를 자체 그룹으로 나눌 때 사용되기도 합니다.
    *   예를 들어, 15개의 데이터 포인트 중 가장 작은 값은 "0% 퀀타일" 또는 "0번째 퍼센타일"입니다.
    *   어떤 값보다 작은 데이터 포인트의 수를 전체 데이터 포인트 수로 나누어 해당 값의 퀀타일 또는 퍼센타일을 계산할 수 있습니다.

### **49.4 퀀타일/퍼센타일 계산 방법의 다양성**
*   퀀타일과 퍼센타일을 계산하는 방법은 여러 가지가 있습니다.
*   **R의 `quantile` 함수만 해도 9가지의 다른 계산 방식**을 제공하며, 각 방식마다 결과가 약간씩 다를 수 있습니다.
*   **데이터 세트 크기의 영향**:
    *   **데이터 세트가 작을 때**: 계산 방법에 따라 퀀타일 값이 크게 달라질 수 있으므로, 퀀타일에 너무 많은 비중을 두지 않는 것이 좋습니다.
    *   **데이터 세트가 클 때**: 모든 계산 방법이 상당히 유사한 결과를 제공합니다.

퀀타일과 퍼센타일은 데이터 분포를 파악하고 이상치를 탐지하거나 데이터를 정규화하는 데 사용될 수 있는 기본적인 통계 도구입니다. 예를 들어, 특성 엔지니어링 시 데이터를 특정 퀀타일 기준으로 나누거나, 모델의 예측 오차 분포를 분석할 때 이 개념들이 활용될 수 있습니다. 다양한 계산 방식과 데이터 크기에 따른 결과의 변동성을 이해하는 것은 정확하고 신뢰할 수 있는 데이터 분석을 수행하는 데 필수적입니다.

## 50. 퀀타일-퀀타일 플롯(QQ 플롯)

데이터가 특정 분포(예: 정규 분포)를 따르는지, 또는 두 데이터 세트의 분포가 서로 유사한지 시각적으로 확인하는 데 사용되는 **QQ 플롯**을 설명합니다. 

### **50.1 QQ 플롯의 목적**
*   **데이터가 정규 분포를 따르는지 여부 확인**.
*   **어떤 이론적 분포가 수집된 데이터와 가장 잘 맞는지 비교**.
*   **두 개의 다른 데이터 세트가 유사한 분포를 가지는지 비교**.

### **50.2 QQ 플롯 생성 과정: 데이터와 정규 분포 비교 예시**
1.  **데이터의 퀀타일 생성**:
    *   수집된 각 데이터 포인트에 고유한 퀀타일(예: 15개의 유전자 발현 측정값)을 할당합니다.
    *   이는 데이터를 동일한 크기의 그룹으로 나누는 기준선들을 만드는 것과 같습니다.
2.  **이론적 정규 분포의 퀀타일 생성**:
    *   어떤 정규 분포를 선택하고, 데이터에서 생성한 것과 **동일한 수의 퀀타일**을 이 정규 분포에서도 생성합니다.
    *   정규 분포에서 '동일한 크기의 그룹'이란 각 그룹 내에서 값이 관찰될 **확률이 동일하다**는 의미입니다.
    *   이는 분포의 양 끝단에 있는 그룹은 관찰 확률이 낮으므로 더 넓게, 중앙에 있는 그룹은 관찰 확률이 높으므로 더 좁게 형성됨을 의미합니다.
3.  **QQ 그래프 그리기**:
    *   **x축**: 정규 분포에서 생성된 퀀타일.
    *   **y축**: 수집된 데이터 세트에서 생성된 퀀타일.
    *   **점 플로팅**: 가장 작은 퀀타일끼리, 두 번째로 작은 퀀타일끼리... 순서대로 대응되는 퀀타일 값을 찾아 점을 찍습니다. 예를 들어, 데이터의 가장 작은 퀀타일(0.6)과 정규 분포의 가장 작은 퀀타일(-1.5)이 만나는 지점에 점을 찍는 식입니다. 이 과정을 모든 퀀타일에 대해 반복합니다.
4.  **해석**:
    *   모든 점이 **직선에 잘 맞으면**, 수집된 데이터가 **정규 분포를 따른다**고 볼 수 있습니다.
    *   직선에 잘 맞지 않으면, 데이터가 정규 분포를 따르지 않는다는 의미이며, 다른 분포와 비교해 볼 필요가 있습니다.

### **50.3 다른 분포와의 비교**
*   데이터를 균등 분포(uniform distribution)와 비교하는 경우에도 과정은 동일합니다.
*   QQ 플롯의 점들이 균등 분포의 직선에 더 가깝게 나타나면, 균등 분포가 데이터에 더 적합하다는 것을 의미합니다.
*   이러한 방식으로 QQ 플롯은 데이터에 가장 잘 맞는 분포를 찾는 데 도움을 줍니다.

### **50.4 두 데이터 세트 간의 비교**
*   두 개의 데이터 세트(예: 원본 데이터 세트와 다른 작은 데이터 세트)를 비교할 수도 있습니다.
*   이때는 한 데이터 세트의 퀀타일이 y축에, 다른 데이터 세트의 퀀타일이 x축에 위치하며, 동일한 방식으로 점을 찍습니다.
*   점이 직선에 잘 맞으면 두 데이터 세트의 분포가 서로 유사하다는 것을 나타냅니다.
*   퀀타일 수가 적으면 (데이터 세트가 작으면) 분포의 유사성에 대한 명확한 결론을 내리기 어렵습니다.

AI 모델을 구축하고 분석하는 과정에서 데이터의 분포를 이해하는 것은 매우 중요합니다.
*   **전처리**: 많은 머신러닝 모델은 입력 데이터가 정규 분포를 따를 때 더 잘 작동합니다. QQ 플롯을 통해 데이터가 정규 분포에서 얼마나 벗어나는지 확인하고, 필요하다면 정규화(Normalization)나 변환(Transformation)과 같은 전처리 기법을 적용할 수 있습니다.
*   **모델 선택 및 가정 검증**: 선형 회귀와 같은 일부 통계 모델은 잔차(residuals)가 정규 분포를 따른다는 가정을 합니다. QQ 플롯은 이러한 모델 가정을 시각적으로 검증하는 데 유용합니다.
*   **이상치 탐지**: QQ 플롯에서 직선에서 크게 벗어나는 점들은 이상치(outliers)일 가능성이 있습니다.
*   **특성 엔지니어링**: 데이터 특성(feature)들의 분포를 이해함으로써, 모델 성능을 향상시키는 데 필요한 새로운 특성을 생성하는 데 도움이 될 수 있습니다.
QQ 플롯은 데이터의 잠재적인 분포를 직관적으로 파악할 수 있게 하여, 데이터 기반 의사결정과 모델 개발에 필수적인 통찰력을 제공합니다.

## 51. Quantile Normalization(퀀타일 정규화)

**퀀타일 정규화(Quantile Normalization)**는 다양한 실험에서 발생할 수 있는 **기술적인 차이(technical differences)**를 보정하기 위해 사용되는 데이터 정규화(normalization) 방법입니다. 이는 생물학적 요인과는 무관하게 발생하는 측정 오류나 시스템적 편향을 제거하여, 서로 다른 실험 간의 데이터를 공정하게 비교할 수 있도록 돕습니다.

### **51.1 왜 필요한가?**
마이크로어레이(microarray) 실험을 예로 들면, 유전자의 활성도를 측정할 때 사용되는 광원(light bulb)의 밝기가 실험마다 다르면 모든 측정값이 밝거나 어둡게 나올 수 있습니다. 이러한 **전반적인 강도(overall intensities)의 차이**는 데이터 분석에 혼란을 줄 수 있으며, 퀀타일 정규화는 이러한 **기술적 인공물(technical artifact)**을 보정합니다. 마이크로어레이 데이터 외에도 이 방법은 **다양한 유형의 다른 데이터에도 적용될 수 있습니다**.

### **51.2 어떻게 작동하는가?**

퀀타일 정규화 과정은 다음과 같습니다:

1.  **데이터 준비**: 먼저, 각 색상이 서로 다른 유전자를 나타내는 원본 데이터를 준비합니다. 이 데이터에서 y축의 값은 마이크로어레이에서 해당 유전자가 가진 강도(intensity)를 나타냅니다. 각 샘플은 다른 평균값을 가질 수 있으며, 이는 전반적인 빛의 강도 차이를 보정해야 함을 시사합니다.

2.  **최고 발현 유전자 처리**:
    *   각 샘플에서 **가장 높은 발현값을 가진 유전자**에 집중합니다.
    *   이 유전자들의 발현값들의 **평균값(mean value)**을 계산합니다.
    *   이 계산된 평균값을 새로운 플롯으로 확장하여, 가장 높은 발현값을 가진 유전자들의 **정규화된 퀀타일 값**으로 할당합니다.

3.  **다음 발현 유전자 처리**:
    *   다음으로 **두 번째로 높은 발현값을 가진 유전자**들에 집중하고.
    *   이들의 **평균값**을 계산하여.
    *   새로운 정규화된 값으로 할당합니다.

4.  **반복**: 이 과정을 **가장 낮은 발현값을 가진 유전자**까지 모든 유전자에 대해 반복합니다.

### **51.3 결과**:

*   퀀타일 정규화 후, **각 샘플의 값들은 동일하게 됩니다**.
*   하지만 **원래 유전자들의 순서(original gene orders)는 보존됩니다**. 즉, 원본 데이터에서 가장 낮은 발현값을 가졌던 유전자는 정규화된 데이터에서도 가장 낮은 값을 유지합니다.
*   이 방법을 **퀀타일 정규화라고 부르는 이유는 정규화된 데이터 세트가 **동일한 사분위수(identical quartiles)**를 가지기 때문입니다.

퀀타일 정규화는 데이터의 기술적 편향을 제거하여 실제 생물학적 또는 내재적 패턴을 더 명확하게 드러내게 하는 강력한 도구입니다. 마이크로어레이 데이터에 대한 설명이 제공되었지만, 이 기술은 **다른 많은 유형의 데이터에도 적용될 수 있으므로**, AI 및 데이터 과학 분야에서 **데이터 전처리(data preprocessing)**의 중요한 기법으로 이해하고 활용할 수 있습니다. 데이터를 분석하기 전에 정규화를 통해 데이터 품질을 향상시키는 것은 모델의 성능과 해석 가능성에 큰 영향을 미칠 수 있습니다.

## 52. 확률(Probability)과 가능도(Likelihood)의 차이점

### **52.1 확률(Probability)**

*   **정의**: 확률은 **고정된 분포(fixed distribution)**에서 특정 데이터 범위(또는 사건)가 나타날 **영역(area under the curve)**을 나타냅니다.
*   **예시**: 생쥐의 몸무게 분포를 예로 들어 설명합니다. 평균 32그램, 표준편차 2.5그램인 정규 분포가 있다고 가정합니다. **32그램에서 34그램 사이의 생쥐를 무작위로 선택할 확률**은 이 분포 곡선 아래의 해당 구간의 면적입니다. 이 면적이 0.29라면, 무작위로 선택된 생쥐가 32~34그램일 확률은 29%입니다.
*   **수학적 표기**: **`P(데이터 | 분포)`**, 즉 **분포가 주어졌을 때 데이터가 나타날 확률**로 표현됩니다. 여기서 분포의 특성(예: 평균, 표준편차)은 고정되어 있고, 우리가 관심 있는 데이터의 범위(예: 32~34그램)를 변경하여 확률을 계산합니다.

**핵심**: 확률은 **고정된 분포**에 대해 **데이터의 특정 구간이 발생할 가능성**을 측정하며, 이는 곡선 아래의 **면적**으로 나타납니다.

### **52.2 가능도(Likelihood)**

*   **정의**: 가능도는 **고정된 관측 데이터(fixed data point)**가 주어졌을 때, **다양한 분포(movable distributions)** 중 어떤 분포가 이 데이터를 생성했을 가능성이 가장 높은지를 나타내는 **Y축 값(y-axis value)**입니다.
*   **예시**: 우리가 이미 34그램짜리 생쥐 한 마리의 몸무게를 측정했다고 가정합니다. 이때 **평균이 32그램이고 표준편차가 2.5그램인 분포가 이 34그램 생쥐를 생성했을 가능도**는 해당 분포 곡선에서 34그램 지점의 Y축 값입니다. 이 값은 0.12가 될 수 있습니다.
    *   만약 분포를 이동시켜 평균이 34그램이 되면, 이 34그램 생쥐의 가능도 값은 0.21로 더 높아집니다. 이는 평균 34그램인 분포가 34그램 생쥐를 생성했을 가능성이 더 높다는 것을 의미합니다.
*   **수학적 표기**: **`L(분포 | 데이터)`**, 즉 **데이터가 주어졌을 때 이 데이터를 생성했을 분포의 가능도**로 표현됩니다. 여기서 관측된 데이터(예: 34그램)는 고정되어 있고, 우리는 분포의 모양과 위치(예: 평균, 표준편차)를 변경하면서 해당 데이터 지점의 Y축 값을 평가합니다.

**핵심**: 가능도는 **고정된 데이터**가 주어졌을 때, **다양한 분포 중에서 어떤 분포가 이 데이터를 설명하는 데 가장 적합한지**를 측정하며, 이는 곡선 위의 **Y축 값**으로 나타납니다.

### **52.3 시사점**:

확률과 가능도의 차이를 이해하는 것은 AI 및 통계 모델링에서 매우 중요합니다.

*   **확률**: 특정 데이터가 발생할 가능성을 이해하는 데 사용됩니다. 예를 들어, 분류 모델에서 특정 입력이 주어졌을 때 각 클래스에 속할 **확률**을 예측할 수 있습니다.
*   **가능도**: **최대 가능도 추정(Maximum Likelihood Estimation, MLE)**과 같은 중요한 통계적 방법을 이해하는 데 핵심적인 개념입니다. MLE는 주어진 데이터를 가장 잘 설명하는 모델의 매개변수(예: 분포의 평균, 표준편차)를 찾는 데 사용됩니다. 많은 AI 모델(예: 로지스틱 회귀, 신경망)은 손실 함수(loss function)의 설계에 가능도 개념을 활용하여 모델의 매개변수를 최적화합니다.

이 두 개념을 명확히 구분함으로써, 여러분은 통계적 추론의 기초를 튼튼히 하고, 다양한 AI 모델의 작동 원리를 더 깊이 이해하며, 데이터에 기반한 현명한 결정을 내릴 수 있을 것입니다.

## 53. 최대 우도(Maximum Likelihood)
**데이터 분포를 맞추기 위한 최적의 방법을 찾는 것**이 최대 우도의 핵심 목적임을 명확히 설명합니다.

### **53.1 데이터 분포를 맞추는 이유**
*   데이터 분포를 맞추는 것은 **작업하기가 쉽고 더 일반적**이기 때문입니다.
*   이는 **같은 종류의 모든 실험에 적용**될 수 있습니다.
*   다양한 데이터 타입에 따라 정규 분포, 지수 분포, 감마 분포 등 여러 분포가 존재합니다.

### **53.2 쥐 몸무게 예시로 본 정규 분포**
*   예시로 쥐들의 몸무게를 측정할 때, 이 몸무게가 **정규 분포를 따른다고 가정**할 수 있습니다.
*   **정규 분포의 특징**:
    *   **대부분의 관측치(쥐 몸무게)가 평균에 가깝습니다**.
    *   관측치가 **평균을 중심으로 대칭**이며, 한쪽으로 심하게 치우치지 않습니다.
*   정규 분포는 '스키니', '중간', '큰 뼈대' 등 **다양한 형태와 크기**로 나타날 수 있습니다.

### **53.3 최대 우도 추정 과정 (평균과 표준편차 찾기)**
최대 우도 추정법의 목표는 주어진 데이터에 가장 잘 맞는 분포의 **모수(parameter)**(예: 평균, 표준편차)를 찾는 것입니다.

*   **평균 찾기**:
    *   먼저 임의의 정규 분포를 가정하고, 그 분포의 평균이 실제 측정된 쥐 몸무게의 평균과 다를 때, **대부분의 측정값이 분포의 평균과 떨어져 있다면 이 몸무게들의 확률 또는 우도(likelihood)는 낮습니다**.
    *   분포를 움직여 **분포의 평균이 쥐 몸무게의 평균과 같아지도록 하면**, 이 몸무게들의 확률 또는 우도가 **비교적 높아집니다**.
    *   이 과정을 통해 가능한 모든 평균 위치를 시도하여, **관측된 몸무게의 우도를 최대화하는 평균 위치**를 찾습니다. 이 지점의 평균이 바로 **최대 우도 추정법으로 찾은 평균**입니다. 특히 정규 분포의 경우, 이 추정된 평균은 데이터의 평균과 같습니다.
*   **표준편차 찾기**: 평균을 찾는 과정과 유사하게, **표준편차를 기준으로 데이터의 우도를 표시**하여, 측정된 몸무게의 우도를 최대화하는 표준편차를 찾을 수 있습니다.

**결론적으로, 최대 우도 추정법(Maximum Likelihood Estimation)은 관측치에 대한 우도를 최대화하는 평균, 표준편차 또는 그 밖의 모수를 찾는 방법**입니다.

### **53.4 용어 설명: 확률과 우도**
*   **일상 대화에서는 확률과 우도가 같은 의미**로 사용되지만.
*   **통계 분야에서 '우도(likelihood)'**는 오늘 다룬 이 특별한 상황, 즉 **관측치에 대한 평균이나 표준편차와 같은 모수의 최적 값을 찾는 상황**과 관련이 있습니다. 이는 데이터를 특정 분포에 부합하게 하는 방법으로 이해할 수 있습니다.

## 54. 지수 분포(Exponential Distribution)에서 최대 우도 추정법

우리가 이전에 쥐 몸무게 예시를 통해 정규 분포의 평균과 표준편차를 찾는 일반적인 최대 우도 개념을 살펴보았듯이, 이번에는 **사건 발생 간격 시간**을 모델링하는 지수 분포의 핵심 모수인 **람다(lambda)** 값을 찾는 방법을 이해하는 것이 목표입니다.

### 54.1 **지수 분포(Exponential Distribution)란 무엇인가요?**

 지수 분포는 **사건들 사이의 시간 간격**을 모델링하는 통계 분포입니다. 예를 들어, 다음 문자 메시지를 받을 때까지 얼마나 기다려야 할지, 또는 이 동영상을 다음 사람이 볼 때까지 얼마나 시간이 걸릴지 등을 모델링할 수 있습니다.

*   **그래프**: x축은 사건 발생 간의 시간 간격을 나타내고, y축은 곡선 아래의 총 면적이 1이 되도록 스케일링됩니다.
*   **모수(Parameter) 람다(λ)**: 지수 분포의 핵심 모수는 **람다(λ)**라고 불리는 **비율 모수(rate parameter)**입니다. 람다는 사건이 얼마나 빨리 발생하는지에 비례합니다.
    *   **람다 = 1**일 때: 평균적으로 1초마다 사건이 발생합니다 (예: 1초에 한 번 동영상이 시청됨).
    *   **람다 = 2**일 때: 평균적으로 1초에 두 번 사건이 발생합니다 (예: 1초에 두 번 동영상이 시청됨).
    *   **람다 = 0.5**일 때: 평균적으로 2초에 한 번 사건이 발생합니다 (예: 2초에 한 번 동영상이 시청됨).

### **54.2 최대 우도 추정(MLE)의 목표: 최적의 람다(λ) 찾기**

 최대 우도 추정의 목표는 주어진 측정값들(예: 동영상 시청 간격 시간 데이터)을 바탕으로 **람다(λ)의 최적 값**을 찾는 것입니다. 즉, 우리가 수집한 데이터에 가장 잘 맞는 지수 분포의 람다 값을 찾아내는 것이죠.

### **54.3 람다(λ)에 대한 우도(Likelihood) 계산**

 우리가 동영상 시청 간격 시간 데이터를 여러 개 수집했다고 가정해 봅시다 (X₁은 첫 번째와 두 번째 시청 사이의 시간, X₂는 두 번째와 세 번째 시청 사이의 시간 등).

1.  **개별 측정값에 대한 우도**:
    *   람다(λ)가 주어졌을 때, 첫 번째 측정값 X₁에 대한 우도 함수는 `L(λ | X₁) = λ * e^(-λ * X₁)` 입니다. 이는 지수 분포 곡선에 X₁ 값을 대입한 것과 같습니다.
    *   두 번째 측정값 X₂에 대한 우도도 마찬가지로 `L(λ | X₂) = λ * e^(-λ * X₂)` 입니다.
    *   그래프에서 X₁이나 X₂가 특정 위치에 있을 때, 그에 해당하는 y축 값이 바로 람다의 우도가 됩니다.

2.  **모든 측정값에 대한 결합 우도**:
    *   **모든 개별 측정값 X₁, X₂, ..., Xn 에 대한 람다의 우도를 계산하려면, 이 모든 개별 우도 함수를 서로 곱해야 합니다**.
    *   이를 통해 `L(λ | X₁, ..., Xn) = (λ * e^(-λ * X₁)) * (λ * e^(-λ * X₂)) * ... * (λ * e^(-λ * Xn))` 라는 긴 식이 됩니다.
    *   이 식을 정리하면 `L(λ | X₁, ..., Xn) = λ^n * e^(-λ * (X₁ + X₂ + ... + Xn))` 로 간결해집니다.

### **54.4 최대 우도 추정(MLE)으로 람다(λ) 구하기**

 람다의 최적 값을 찾기 위해 우리는 이 우도 함수를 **최대화**해야 합니다. 즉, **우도 함수가 가장 높은 값을 가질 때의 람다 값을 찾아야 합니다.** 이는 함수의 기울기가 0이 되는 지점을 찾는 것으로, 미분(derivative)을 통해 가능합니다.

1.  **로그 우도(Log-Likelihood) 사용**:
    *   **지수 함수가 포함된 함수의 미분은 로그를 취하면 훨씬 쉬워집니다**. 그 이유는 **함수의 도함수가 0이 되는 지점과 함수의 로그의 도함수가 0이 되는 지점이 같기 때문입니다**.
    *   로그를 취하면 곱셈이 덧셈으로 바뀌고, 지수가 앞으로 내려오게 되어 계산이 단순해집니다.
    *   따라서 `log(L) = log(λ^n * e^(-λ * ΣXᵢ))` 는 `log(L) = n * log(λ) - λ * ΣXᵢ` 로 변환됩니다. (여기서 ΣXᵢ는 모든 X 값들의 합을 의미합니다.)

2.  **로그 우도 함수 미분**:
    *   `log(L)` 함수를 람다(λ)에 대해 미분합니다.
    *   `d(log(L))/d(λ) = n/λ - ΣXᵢ`

3.  **미분 값을 0으로 설정하고 람다(λ)에 대해 풀기**:
    *   `n/λ - ΣXᵢ = 0`
    *   `n/λ = ΣXᵢ`
    *   **λ = n / ΣXᵢ**

 이것이 바로 **지수 분포의 람다에 대한 최대 우도 추정치(Maximum Likelihood Estimate for lambda)**입니다! 이제 데이터(n개의 측정값)만 있으면 이 공식을 통해 최적의 람다 값을 쉽게 구할 수 있습니다.

### **54.5 실제 예시**

 예를 들어, 동영상 시청 간격 시간이 다음과 같다고 가정해 봅시다:
*   X₁ = 2초
*   X₂ = 2.5초
*   X₃ = 1.5초
*   총 데이터 개수 `n = 3`
*   모든 X 값의 합 `ΣXᵢ = 2 + 2.5 + 1.5 = 6`

 이 값들을 최대 우도 추정 공식에 대입하면:
*   `λ = n / ΣXᵢ = 3 / 6 = 0.5`

따라서 이 데이터에 대한 **최대 우도 추정치 람다(λ)는 0.5**가 됩니다. 이 람다 값을 사용하여 데이터를 가장 잘 나타내는 지수 분포를 그릴 수 있습니다.

**요약하자면,** AI를 배우는 학생으로서 지수 분포의 최대 우도 추정법을 이해하는 것은 특정 유형의 이벤트 발생 시간을 예측하거나 모델링하는 데 매우 중요합니다. 이 방법은 관측된 데이터에 가장 잘 맞는 분포 모수(여기서는 람다)를 수학적으로 찾아내는 강력한 도구입니다.

## 55. 이항 분포의 최대 우도
이항 분포(binomial distribution)의 맥락에서 **최대 우도 추정(Maximum Likelihood Estimation, MLE) 개념**을 명확하게 설명합니다. 동영상은 오렌지 환타 선호도에 대한 질문을 예시로 들어, 주어진 관측 데이터를 기반으로 특정 사건의 발생 확률 P를 어떻게 가장 합리적으로 추정하는지 보여줍니다. 

### **55.1 핵심 개념**

*   **이항 분포**: 동영상에서는 X를 오렌지 환타를 선호하는 사람들의 수(예: 4명), n을 질문한 총 인원(예: 7명), P를 무작위로 오렌지 환타를 선택할 확률(예: 0.5)로 정의합니다. 이항 분포 방정식은 이러한 X, n, P가 주어졌을 때 특정 사건(예: 7명 중 4명이 오렌지 환타 선호)이 발생할 확률을 계산하는 데 사용됩니다.

*   **확률 (Probability) vs. 우도 (Likelihood)**:
    *   **확률**: n과 P가 주어졌을 때 X(특정 사건)가 발생할 가능성을 나타냅니다.
    *   **우도**: **관측된 데이터(n과 X)가 고정된 상태**에서, **확률 P의 특정 값**이 그 관측 데이터를 얼마나 잘 설명하는지(즉, 그 데이터가 나타날 가능성이 높은 P 값은 무엇인지)를 나타냅니다. 우리는 P의 다양한 값을 대입하여 그에 따른 우도를 계산할 수 있습니다.
    *   예를 들어, 7명 중 4명이 오렌지 환타를 선호할 때, P=0.5의 우도는 0.273, P=0.25의 우도는 0.058, P=0.57의 우도는 0.294입니다.

### **55.2 최대 우도 추정 (MLE) 과정**

**최대 우도**는 P에 대한 다양한 값을 그래프로 그렸을 때 **가장 높은 지점(봉우리)**을 의미합니다. 이 봉우리에서 곡선의 기울기는 0이 됩니다. P에 대한 최대 우도 추정량(Maximum Likelihood Estimate)을 찾기 위해 다음 단계를 따릅니다.

1.  **우도 함수 설정**: 이항 분포의 확률 함수를 P에 대한 우도 함수로 재구성합니다. 이때 n과 X는 관측된 고정 값으로 간주됩니다.
2.  **로그 우도 함수 사용**: 원래 우도 함수는 곱셈과 지수 항이 많아 미분하기 복잡합니다. 하지만 **로그 우도 함수**는 원래 우도 함수와 P에 대해 **동일한 값에서 최대값에 도달**하며, 로그가 곱셈을 덧셈으로, 지수를 곱셈으로 바꾸어 미분을 훨씬 쉽게 만듭니다.
3.  **미분 및 최대값 찾기**:
    *   로그 우도 함수를 P에 대해 미분합니다.
    *   미분된 함수(기울기)를 0으로 설정합니다.
    *   이 방정식을 풀어 P 값을 찾습니다. 이 P 값이 바로 최대 우도 추정량입니다.

### **55.3 일반 공식 유도 및 결론**

동영상은 n=7, X=4와 같은 특정 데이터로 과정을 보여준 후, 일반적인 **n개의 시행에서 X번의 성공**이 있을 때 P에 대한 최대 우도 추정량을 유도합니다.

*   로그 우도 함수를 P에 대해 미분하고 0으로 설정하는 과정을 거치면, 최종적으로 **P = X / n**이라는 간단한 공식을 얻게 됩니다.
*   이는 직관적으로 '성공 횟수를 총 시행 횟수로 나눈 평균' 또는 '비율'과 같다는 것을 수학적으로 증명한 것입니다. 이 결과는 매우 직관적이지만, 수학적 증명으로 그 타당성을 확보합니다.

### **55.4 시사점**

이항 분포의 맥락에서 최대 우도 추정의 개념과 계산 과정을 명확하게 설명합니다. AI를 배우는 학생들에게는 **데이터로부터 모델의 매개변수(여기서는 P)를 어떻게 가장 합리적으로 추정할 수 있는지**에 대한 기본적인 이해를 제공하며, 이러한 통계적 추정 방법이 기계 학습 모델의 학습 과정에서 어떻게 활용될 수 있는지에 대한 중요한 기초 지식을 제공합니다. 예를 들어, 분류 문제에서 특정 클래스에 속할 확률을 추정하는 데 유사한 원리가 적용될 수 있습니다. 이항 분포의 MLE는 직관적인 X/n 결과를 수학적으로 뒷받침한다는 점에서 의미가 큽니다.

## 56. 정규 분포의 최대 우도

### **56.1 정규 분포의 핵심 매개변수**

정규 분포 곡선은 두 가지 매개변수에 의해 정의됩니다:
*   **μ (뮤) - 평균(Mean)**: 분포의 중심 위치를 결정합니다. μ 값이 작아지면 분포의 평균이 왼쪽으로 이동하고, μ 값이 커지면 평균이 오른쪽으로 이동합니다.
*   **σ (시그마) - 표준 편차(Standard Deviation)**: 분포의 폭을 결정합니다. σ 값이 커지면 정규 곡선은 더 짧고 넓어지며, σ 값이 작아지면 더 높고 좁아집니다.

### **56.2 우도(Likelihood)의 이해**

우도는 **관측된 데이터가 고정된 상태에서, 특정 분포 매개변수(예: μ와 σ)가 그 데이터를 얼마나 잘 설명하는지**를 나타내는 척도입니다.

*   **단일 측정값의 우도**: 예를 들어, 32그램인 생쥐 한 마리(X)의 데이터가 있을 때, 우리는 다양한 μ와 σ 값을 가진 정규 분포를 가정하고, 이 분포가 32그램이라는 측정값을 얼마나 잘 설명하는지(즉, 그 데이터가 나타날 가능성이 높은 μ와 σ 값은 무엇인지) 우도로 계산할 수 있습니다.
    *   μ=28, σ=2인 분포의 경우, 32g 데이터에 대한 우도는 0.03입니다.
    *   μ=30, σ=2인 분포의 경우, 32g 데이터에 대한 우도는 0.12입니다.
*   **여러 측정값의 우도**: 측정값들이 **독립적**일 경우, 전체 데이터 세트(예: 32g인 X1과 34g인 X2)에 대한 정규 분포의 우도는 각 개별 측정값에 대한 우도를 **모두 곱한 값**입니다. 즉, `Likelihood(X1, X2) = Likelihood(X1) * Likelihood(X2)` 입니다. n개의 데이터 포인트가 있다면, n개의 개별 우도 함수를 모두 곱하게 됩니다.
*   **최대 우도**: 다양한 μ 또는 σ 값을 대입하여 우도를 계산하고 그래프로 그렸을 때, **가장 높은 지점(봉우리)**이 나타납니다. 이 지점에서 우도 곡선의 기울기는 0이 됩니다.

### **56.3 최대 우도 추정(MLE) 과정**

MLE의 목표는 우도 함수를 최대화하는 μ와 σ 값을 찾는 것입니다.

1.  **우도 함수 설정**: n개의 독립적인 측정값에 대한 전체 우도 함수는 각 개별 측정값에 대한 우도 함수의 곱으로 표현됩니다.
2.  **로그 우도 함수 사용**: 우도 함수는 곱셈과 지수 항이 많아 미분하기 복잡합니다. 따라서 **로그를 취하여 로그 우도 함수**를 만듭니다. 로그는 곱셈을 덧셈으로 변환하여 미분을 훨씬 쉽게 만듭니다. 중요한 점은, **로그 우도 함수와 원래 우도 함수는 μ와 σ에 대해 동일한 값에서 최대값에 도달**한다는 것입니다.
3.  **미분 및 최대값 찾기**:
    *   **μ에 대한 MLE 찾기**: σ를 상수로 간주하고, 로그 우도 함수를 μ에 대해 미분합니다. 미분된 함수(기울기)를 **0으로 설정**하고 μ에 대해 풀면, μ의 최대 우도 추정량을 찾을 수 있습니다.
    *   **σ에 대한 MLE 찾기**: μ를 상수로 간주하고, 로그 우도 함수를 σ에 대해 미분합니다. 미분된 함수(기울기)를 **0으로 설정**하고 σ에 대해 풀면, σ의 최대 우도 추정량을 찾을 수 있습니다.

### **56.4 최대 우도 추정량 (MLEs) 결과**

복잡한 미분 및 대수학적 과정을 거쳐 다음과 같은 결론에 도달합니다:

*   **μ에 대한 최대 우도 추정량은 데이터의 평균(Mean)입니다**. 즉, 정규 분포의 중심은 데이터의 평균에 위치해야 가장 높은 우도를 가집니다.
*   **σ에 대한 최대 우도 추정량은 데이터의 표준 편차(Standard Deviation)입니다**. 즉, 정규 분포의 폭은 데이터의 표준 편차에 의해 결정되어야 가장 높은 우도를 가집니다.

이러한 결과는 직관적으로 명백해 보일 수 있지만, 이 동영상은 **수학적으로 우리의 직관이 옳다는 것을 증명**합니다.

## 57. Odds (오즈)와 Log(Odds)

### **57.1 Odds(오즈)란 무엇인가?**

*   **정의**: 오즈는 **어떤 사건이 발생할 확률과 발생하지 않을 확률의 비율**입니다. 즉, '성공 횟수' 대 '실패 횟수'의 비율로 표현됩니다.
*   **예시**:
    *   팀이 이길 오즈가 1 대 4인 경우, 5경기 중 1승 4패를 의미하며, 분수로는 1/4 또는 0.25가 됩니다.
    *   팀이 이길 오즈가 5 대 3인 경우, 8경기 중 5승 3패를 의미하며, 분수로는 5/3 또는 약 1.7이 됩니다.
*   **확률과의 차이점**: 오즈는 **확률과 다릅니다**.
    *   **오즈**: 어떤 사건이 발생하는 것(예: 팀이 이기는 것)과 발생하지 않는 것(예: 팀이 지는 것)의 비율입니다.
    *   **확률**: 어떤 사건이 발생하는 것(예: 팀이 이기는 것)과 **발생할 수 있는 모든 경우의 수**(예: 팀이 이기거나 지는 것)의 비율입니다.
    *   예를 들어, 5승 3패의 경우 오즈는 5/3이지만, 이길 확률은 5/8입니다.
*   **계산 방법**: 오즈는 원시 카운트(예: 5승 3패)로 계산되거나, **확률로부터 계산될 수 있습니다**.
    *   확률 P가 주어졌을 때, 오즈는 `P / (1 - P)` 공식으로 계산됩니다. `1 - P`는 사건이 발생하지 않을 확률을 나타냅니다.

### **57.2 Log(Odds)(로그 오즈)의 필요성 및 이점**

*   **오즈 값의 비대칭성 문제**:
    *   오즈가 팀에게 불리할수록(예: 1 대 4, 1 대 8), 오즈 값은 0과 1 사이에 존재하며 0에 가까워집니다.
    *   오즈가 팀에게 유리할수록(예: 4 대 3, 8 대 3), 오즈 값은 1부터 무한대까지 증가합니다.
    *   이러한 **비대칭성**은 '불리한 오즈'(0과 1 사이)와 '유리한 오즈'(1과 무한대 사이)를 비교하고 해석하기 어렵게 만듭니다. 예를 들어, 1 대 6으로 불리한 오즈(0.17)와 6 대 1로 유리한 오즈(6)는 크기에서 큰 차이가 있는 것처럼 보입니다.
*   **로그 오즈의 해결책**: 오즈에 로그를 취하면 이 **비대칭성 문제가 해결되고 모든 것이 대칭적으로 변합니다**.
    *   예를 들어, 1 대 6의 오즈(0.17)에 로그를 취하면 약 -1.79가 되고, 6 대 1의 오즈(6)에 로그를 취하면 약 1.79가 됩니다.
    *   이처럼 원점(0)으로부터의 거리가 동일해져 **비교와 해석이 훨씬 쉬워집니다**.

### **57.3 시사점**
*   **로지스틱 회귀의 기초**: 확률의 비율에 로그를 취한 것을 **로짓 함수(logit function)**라고 부르며, 이는 **로지스틱 회귀(Logistic Regression)의 기초**를 형성합니다. 로지스틱 회귀는 이진 분류 문제(예: 예/아니오, 승리/패배)에서 널리 사용되는 중요한 기계 학습 알고리즘입니다.
*   **통계 문제 해결**: 로그 오즈는 대칭적이며 해석하기 쉽기 때문에, **특정 통계 문제, 특히 승패, 예/아니오, 참/거짓과 같은 이진 결과에 대한 확률을 결정하는 문제**를 해결하는 데 유용합니다. 심지어 무작위 숫자의 쌍으로 로그 오즈를 계산하고 히스토그램을 그리면 정규 분포 형태를 띠는 것을 볼 수 있습니다.

요약하자면, 오즈는 사건 발생과 미발생의 비율이며, 로그 오즈는 오즈의 비대칭성을 해결하여 데이터를 통계적으로 다루고 해석하는 데 유용한 도구입니다. 특히 로지스틱 회귀와 같은 AI 모델의 핵심 개념이다.

## 58. 오즈비(Odds Ratios) 및 로그 오즈비(Log Odds Ratios)

### 58.1  **오즈(Odds)의 개념**
오즈는 어떤 사건이 발생할 확률과 발생하지 않을 확률의 비율입니다. 예를 들어, 팀이 이길 오즈는 '이길 경우' 대 '지지 않을 경우'의 비율로 나타낼 수 있습니다.

### 58.2  **오즈비(Odds Ratio)란 무엇인가?**
사람들이 "오즈비"라고 말할 때는 단순히 오즈가 아니라 **오즈들의 비율**을 의미합니다. 즉, 어떤 한 오즈를 다른 오즈로 나눈 값입니다.
*   오즈비 계산 결과, 분모가 분자보다 크면 오즈비는 0에서 1 사이의 값을 가지며, 분자가 분모보다 크면 1에서 무한대까지의 값을 가집니다.

### 58.3  **로그 오즈비(Log Odds Ratio)의 필요성**
오즈비에 로그를 취하면 값이 **대칭적**으로 변하여 해석이 용이해집니다. 예를 들어, 특정 오즈비가 0.17일 때 로그 오즈비는 -1.79이고, 반대 오즈비(1/0.17)에 대한 로그 오즈비는 +1.79가 됩니다.

### 58.4  **오즈비의 실제 적용 및 해석**
오즈비는 두 가지 요인 사이의 **관계**를 파악하는 데 사용됩니다. 예를 들어, 특정 **유전자 변이(mutated gene)**와 **암** 발생 사이의 관계를 파악할 수 있습니다.
*   예시: 유전자 변이가 있는 사람이 암에 걸릴 오즈가 변이가 없는 사람이 암에 걸릴 오즈보다 **6.88배 더 높다**는 결과를 오즈비를 통해 얻을 수 있습니다. 이 경우 로그 오즈비는 1.93입니다.
*   오즈비와 로그 오즈비는 **R-제곱(R-squared)**과 유사하게 두 가지 요인 간의 관계를 나타내며, 그 값의 크기는 **효과 크기(effect size)**에 해당합니다. 값이 클수록 한 요인이 다른 요인의 좋은 예측 변수임을 의미합니다.

### 58.5  **통계적 유의성(Statistical Significance) 확인**
오즈비와 로그 오즈비에서 얻은 관계가 통계적으로 유의미한지 확인해야 합니다. 이를 위한 세 가지 주요 방법이 있습니다:
*   **피셔의 정확 검정(Fisher's Exact Test)**: M&M 비유를 사용하여 특정 조합의 M&M을 뽑을 확률을 계산하는 방식과 유사하게 p-값을 계산합니다.
*   **카이제곱 검정(Chi-square Test)**: 관측된 값과 **기대되는 값**을 비교하여 유전자 변이와 암 사이에 관계가 없다고 가정했을 때의 p-값을 계산합니다.
*   **왈드 검정(Wald Test)**: 로지스틱 회귀 분석에서 오즈비의 유의성을 결정하고 **신뢰 구간(confidence interval)**을 계산하는 데 일반적으로 사용됩니다. 로그 오즈비가 **정규 분포**를 따른다는 사실을 활용하며, 관측된 로그 오즈비가 0(관계가 없다고 가정했을 때의 중심)으로부터 얼마나 많은 표준편차 떨어져 있는지를 계산하여 p-값을 도출합니다.

### 58.6  **어떤 방법을 사용해야 할까?**
이러한 통계 검정 방법 중 어느 것이 가장 좋다는 **일반적인 합의는 없습니다**. 사람들은 종종 피셔의 정확 검정이나 카이제곱 검정으로 p-값을 계산하고 왈드 검정으로 신뢰 구간을 계산하거나, 왈드 검정 하나로 모든 것을 처리하기도 합니다. 특정 분야에서 가장 널리 받아들여지는 방법을 확인하는 것이 중요합니다. 영상의 저자는 여러 테스트를 모두 통과한 경우 더 안심할 수 있다고 언급합니다.

**요약하자면,** 오즈비는 두 오즈의 비율을 통해 두 가지 요인(예: 유전자 변이와 질병) 사이의 **관계 강도**를 알려줍니다. 로그 오즈비는 이러한 관계를 대칭적으로 해석할 수 있게 해줍니다. 이 관계가 통계적으로 의미 있는지 판단하기 위해 **피셔의 정확 검정, 카이제곱 검정 또는 왈드 검정**과 같은 통계적 유의성 검정을 사용합니다. 

## 59. 세 가지 핵심 교훈

### 59.1  **핵심 아이디어에 집중하라 (Focus on the main ideas)**
*   **교훈:** 통계학을 처음 배울 때 조쉬 스타머는 t-테스트, ANOVA, 선형 회귀와 같은 다양한 통계 방법들에 대해 개별적인 공식을 암기해야만 했습니다. 하지만 그의 아버지는 이 모든 복잡해 보이는 공식들이 사실은 **"F = 설명된 분산(explained variance) / 설명되지 않은 분산(unexplained variance)"**이라는 하나의 비교적 간단한 핵심 아이디어와 연결되어 있다는 것을 보여주었습니다. 이 하나의 개념으로 t-테스트, ANOVA, 회귀 분석을 모두 동일하게 쉽게 수행할 수 있습니다.
*   **AI 학습자에 대한 시사점:** AI 분야는 방대한 알고리즘과 모델로 가득 차 있습니다. 각 개별 모델의 세부 사항에 매몰되기보다는, **그들이 공유하는 근본적인 원리나 핵심 아이디어(예: 최적화, 손실 함수, 경사 하강법)에 집중**하는 것이 중요합니다. 이를 통해 다양한 알고리즘을 더 깊이 이해하고 새로운 문제에 적용할 수 있는 통찰력을 얻을 수 있습니다.

### 59.2  **항상 더 나은 방법이 있다 (There's always a better way)**
*   **교훈:** 조쉬 스타머의 아버지는 "항상 더 나은 방법이 있다"는 문구가 적힌 포스터를 통해 그에게 영감을 주었습니다. 이 정신은 조쉬 스타머가 다른 데이터 과학 채널들과 다르게 StatQuest를 만들고, 새로운 주제를 배울 때 항상 더 나은 설명 방식을 찾도록 격려했습니다.
*   **AI 학습자에 대한 시사점:** AI 학습 과정에서 특정 개념이나 알고리즘을 이해하는 데 어려움을 겪을 때, 단순히 포기하거나 한 가지 설명 방식에만 갇히지 마십시오. **다양한 자료와 관점을 탐색하며 자신에게 가장 잘 맞는 '더 나은 방법'을 찾아내는 유연한 사고방식**이 중요합니다. 또한, 문제를 해결하는 데 있어 기존 방식에만 얽매이지 않고 창의적인 접근 방식을 모색하는 데 도움이 됩니다.

### 59.3  **추측하지 말고, 계산하라 (Don't guess, do the math)**
*   **교훈:** 어린 시절 조쉬 스타머가 수학 숙제를 할 때, 답이 명확해 보여 단계를 건너뛰고 추측하려 했을 때 그의 아버지는 "추측하지 말고, 계산하라"고 조언했습니다. 이 경험을 통해 그는 아무리 답이 명확해 보여도 모든 단계를 직접 계산해야만 정확한 이해에 도달하고 실수를 피할 수 있다는 것을 배웠습니다. 이는 그가 새로운 알고리즘을 배울 때 모든 단계를 직접 풀어보면서 새로운 통찰력을 발견하는 데 큰 도움이 되었습니다(예: 신경망 시리즈).
*   **AI 학습자에 대한 시사점:** AI 알고리즘, 특히 복잡한 머신러닝 및 딥러닝 모델을 배울 때는 단순히 공식이나 개념을 표면적으로 이해하는 것을 넘어, **수학적 배경과 각 단계의 작동 원리를 직접 손으로 풀어보고 이해하려는 노력**이 필수적입니다. 이 과정에서 미처 예상치 못했던 깊은 통찰력을 얻을 수 있으며, 이는 모델의 동작을 더 잘 이해하고 디버깅하며 개선하는 데 결정적인 역할을 합니다.

## 60. 표본 분산을 계산할 때 n-1로 나누어야 하는 이유

### 60.1  **모집단 매개변수와 표본 추정치**
*   **모집단 평균 (µ)**: 전체 모집단의 측정값을 모두 더하여 측정값의 개수로 나눈 값입니다.
*   **모집단 분산**: 데이터와 모집단 평균 간의 제곱 거리의 평균입니다.
*   실제로는 모든 모집단의 데이터를 측정하기 어렵기 때문에, 우리는 **모집단 매개변수를 추정**합니다.
*   **추정된 모집단 평균 (x̄)**: 표본의 측정값을 모두 더하여 표본의 개수로 나눈 값, 즉 표본 평균입니다.
*   **추정된 모집단 분산**: 표본의 데이터를 사용하여 계산하며, 이 때 **n-1로 나눕니다**.

### 60.2  **N으로 나눌 때의 문제점: 과소평가 (Underestimation)**
*   만약 **표본 분산을 계산할 때 n으로 나누게 되면, 모집단 분산을 일관되게 과소평가**하게 됩니다.
*   이는 **데이터와 표본 평균(x̄) 간의 차이가 데이터와 모집단 평균(µ) 간의 차이보다 더 작아지는 경향이 있기 때문**입니다.
*   동영상에서는 여러 예시를 통해 특정 지점(V)을 기준으로 분산을 계산했을 때, **항상 표본 평균(x̄)을 기준으로 계산한 분산이 가장 작게 나타난다**는 것을 보여줍니다. 즉, 표본 평균은 주어진 데이터에 대해 분산을 최소화하는 값입니다.

### 60.3  **수학적 증명**
*   동영상은 미분법을 사용하여 이 현상을 수학적으로 증명합니다.
*   데이터와 미지의 값(V) 간의 제곱 차이의 평균에 대한 공식을 세우고, V에 대해 미분하여 기울기가 0이 되는 지점(즉, 분산이 최소가 되는 지점)을 찾습니다.
*   이 과정을 통해 **분산을 최소화하는 V 값은 항상 표본 평균(x̄)이라는 것을 증명**합니다.
*   이는 표본 평균을 사용하여 분산을 계산하면, 다른 어떤 값(모집단 평균 포함)을 사용할 때보다 항상 분산 값이 작아진다는 것을 의미합니다.

### 60.4  **N-1로 나누는 이유**
*   **n-1로 나누는 것은 우리가 모집단 평균 대신 표본 평균을 사용하여 차이를 계산하는 사실을 보정하기 위한 것**입니다.
*   이렇게 n-1로 나누면, 표본 분산이 모집단 분산을 더 정확하게 추정할 수 있게 됩니다.
*   (참고: 왜 n-1이 정확히 사용되는지에 대한 더 깊은 설명은 '기댓값(expected values)'에 대한 다른 StatQuest 동영상에서 다룬다고 언급됩니다).

### 60.5  **제곱 차이를 사용하는 이유**
*   차이의 절댓값 대신 **제곱 차이를 사용하는 것은 미분 계산을 통해 최소값을 찾는 것이 훨씬 쉽기 때문**입니다. 절댓값을 사용하면 최소값 지점에서 "날카로운 각"이 생겨 미분이 불가능해지기 때문에, 수학적으로 다루기 어렵습니다.

데이터 과학 및 머신러닝 모델을 다룰 때, 데이터의 변동성(분산)을 정확하게 이해하고 추정하는 것은 매우 중요합니다. 특히 제한된 표본 데이터로 모집단을 추론해야 하는 경우, **표본 분산을 계산할 때 n-1로 나누는 것이 모집단 분산을 더 정확하게 추정하는 표준 방법**임을 기억해야 합니다. 이를 통해 모델의 성능을 평가하거나 통계적 유의성을 검정할 때 올바른 결론을 내릴 수 있습니다.