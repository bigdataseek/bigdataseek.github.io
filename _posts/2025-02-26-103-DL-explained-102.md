---
title: 3차시 3:Deep Learning Explained 2
layout: single
classes: wide
categories:
  - 딥러닝
  - DeepLearning
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---

## 12. 기울기 클리핑
- 출처:[Gradient Clipping for Neural Networks](https://www.youtube.com/watch?v=KrQp1TxTCUY&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=12)

심층 신경망에서 불안정한 기울기(unstable gradients)는 주요 문제 중 하나이며, 특히 순환 신경망(RNN)에서는 배치 정규화(batch normalization) 구현이 까다롭습니다. 이러한 문제, 특히 **기울기 폭주(exploding gradients) 문제**를 해결하기 위해 기울기 클리핑(gradient clipping)이 사용됩니다.

### 12.1   **기울기 클리핑이란?**
- 기울기 클리핑은 **기울기를 특정 임계값 내로 제한(클리핑)**하는 간단한 방법입니다. 예를 들어, 기울기를 -1과 1 사이로 제한하고 싶다면, 계산된 기울기 값이 이 범위를 벗어날 경우 -1 또는 1로 잘라냅니다.

### 12.2   **주요 접근 방식 및 그 문제점**
*   **값에 의한 클리핑(Clipping by value)**: 
    -   지정된 범위(예: -1과 1)를 벗어나는 개별 기울기 값을 해당 범위 내로 조정합니다.
    *   **단점**: 이 방법은 **기울기 벡터의 방향을 변경**시킬 수 있습니다. 원본 벡터의 일부 값만 클리핑하면 벡터의 전체 방향이 원래와 완전히 달라질 수 있습니다.
*   **노름(norm)에 의한 클리핑(Clipping by norm)**: 
    - 개별적인 클리핑 대신, **기울기 벡터 내의 모든 값을 비례적으로 낮춰** 원하는 범위 안에 들게 하여 기울기 벡터의 방향을 유지합니다.
    *   **단점**: 이 방법은 일부 기울기 값이 **매우 작아져서 네트워크 매개변수 업데이트에 효과적이지 않을 수 있다**는 문제가 있습니다.

### 12.3   **적용 및 중요성**
*   기울기 클리핑에는 **확고한 규칙이 없으므로**, 사용자는 값에 의한 클리핑과 노름에 의한 클리핑을 모두 시도해보고 더 나은 결과를 제공하는 것을 찾아야 합니다. 또한, 다양한 임계값도 시도해봐야 합니다.
*   결론적으로 기울기 클리핑은 **기울기 폭주 문제를 해결하는 매우 효과적인 방법**이며, Keras 딥러닝 라이브러리를 사용하여 **구현하기도 매우 간단합니다**.

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# 간단한 모델 정의
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(100,)),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# 옵티마이저에 기울기 클리핑 적용 (clipnorm 또는 clipvalue 사용)
optimizer = keras.optimizers.Adam(
    learning_rate=0.001,
    clipnorm=1.0   # L2 노름 기준으로 기울기 크기를 1.0 이하로 제한
    # clipvalue=0.5 # 절대값 기준으로 기울기 크기를 제한하고 싶을 경우
)

# 모델 컴파일
model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# 더미 데이터 생성
import numpy as np
x_train = np.random.rand(1000, 100)
y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000,)), num_classes=10)

# 학습
model.fit(x_train, y_train, epochs=5, batch_size=32)

```

## 13. 자연어 처리(NLP)
- 출처:[What is Natural Language Processing?](https://www.youtube.com/watch?v=ZifynN2oyhs&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=13)


### 13.1   **자연어 처리(NLP)란 무엇인가?**
*   **정의**: NLP는 자연어로 된 언어, 주로 텍스트에서 의미를 파악하려고 노력하는 분야입니다.
*   **엘런 튜링의 관점**: 1950년대 엘런 튜링은 기계가 인간과 원활하게 대화할 수 있다면, 그것이 바로 기계의 지능에 대한 의문을 멈출 때라고 주장했으며, NLP는 이러한 목표를 달성하는 데 큰 기여를 했습니다.
*   **하위 목표 및 기술**: NLP는 다양한 기술과 하위 목표를 포함합니다.
    *   **토큰화(Tokenization)**: 
        -   텍스트를 더 작은 단위인 토큰(단어, 하위 단어, 문자 등)으로 분리하는 방법입니다.
    *   **어간 추출(Stemming) 또는 표제어 추출(Lemmatization)**: 
        -   단어를 그 뿌리 형태로 줄이는 것을 목표로 하며, 접근 방식에 약간의 차이가 있습니다.
    *   **단어 의미 중의성 해소(Word Sense Disambiguation)**: 
        -   문맥을 사용하여 단어가 문장에서 어떤 의미로 사용되는지 이해하는 기술입니다.
    *   **개체명 인식(Named Entity Recognition)**: 
        -   주어진 문장이나 텍스트에서 언급된 고유한 개체를 이해하는 데 도움을 줍니다.
    *   **텍스트 분류(Text Classification)**: 
        -   텍스트를 관련 그룹으로 분류하는 고수준 작업입니다.
    *   **기계 번역(Machine Translation)**: 
        -   한 언어의 텍스트를 다른 언어로 번역하는 목표입니다.
    *   **콘텐츠 조정(Content Moderation)**: 
        -   특정 그룹에 부적절한 내용을 필터링하는 목표입니다 (예: 아동 도서에서 비속어 제거).
    *   **주제 감지(Topic Detection)**: 
        -   주어진 텍스트에서 어떤 주제가 언급되고 있는지 이해하는 것입니다.
    *   **감성 분석(Sentiment Analysis)**: 
        -   주어진 문장이나 텍스트가 긍정적인지 또는 부정적인 감성을 가지고 있는지 이해하는 목표입니다.
    *   **요약(Summarization)**: 
        -   긴 텍스트를 한두 문장으로 요약하는 것입니다.

### 13.2   **왜 NLP에 관심을 갖는가?**
*   **방대한 텍스트 데이터**: 세상에는 사용 가능한 텍스트 및 서면 형식의 데이터가 너무 많아서 이를 활용하고 싶어 합니다.
*   **구조화되지 않은 데이터 활용**: 기존의 구조화된 데이터 소스만으로는 충분하지 않거나, 아예 구조화된 정보가 없는 경우 비정형 소스(예: 텍스트)에서 정보를 추출해야 합니다.
*   **인간의 의사소통 방식**: 대화는 인간에게 가장 쉬운 형태의 의사소통 방식이며, 컴퓨터가 자연어를 이해하고 응답하도록 하여 마찰 없는 의사소통을 하려는 목표입니다.

### 13.3   **컴퓨터가 자연어를 다루기 어려운 이유**
*   언어는 인간에게 자연스럽게 느껴지지만, **풍자, 숨겨진 의미, 관용구** 등 복잡한 요소들이 많습니다.
*   언어는 **끊임없이 진화**하며, 새로운 표현들이 계속 생겨납니다.
*   하나의 문장이나 단어에도 **여러 가지 의미**가 있을 수 있으며, 문맥에 따라 그 의미가 바뀝니다.
*   이러한 요소들은 컴퓨터가 언어를 이해하기 위해 알아야 할 것들이며, 언어가 바뀌면 이 모든 것을 처음부터 다시 시작해야 하므로 컴퓨터에게는 매우 어려운 작업입니다.

### 13.4   **NLP 기술의 발전 과정**
*   **2000년대 이전**: 언어를 해독하고 이해하기 위해 **논리 및 언어학 기반 시스템**을 만드는 데 중점을 두었습니다. 연구자들은 언어 작동 규칙을 작성했지만, 언어에는 너무 많은 예외와 특수한 경우(corner cases)가 있어 규칙 기반 시스템만으로는 한계가 있었습니다.
*   **2000년대**: 연구자들은 언어를 이해하고 해독하기 위해 **기계 학습 기반 및 통계적 기술**을 사용하기 시작했습니다. 이는 개선이었지만, 여전히 언어를 완전히 이해하고 원활하게 작동하지 못했습니다.
*   **지난 10년간**: **신경망(neural networks)**의 광범위한 채택과 함께 기계가 언어를 이해하고 활용하는 방식에 상당한 발전이 있었습니다. **컨볼루션 신경망(CNN), 순환 신경망(RNN)** 및 그 변형들이 NLP 분야에서 사용되기 시작했습니다.
*   **최근 몇 년간**: **어텐션 메커니즘을 가진 트랜스포머(Transformers) 아키텍처**를 사용하면서 가장 큰 발전이 이루어졌습니다.

### 13.5   **NLP 프로젝트 시작 방법**
*   정확하고 맞춤화된 NLP 모델을 처음부터 구축하는 것은 많은 전문 지식과 시간을 필요로 합니다.
*   **산업에서는 이미 만들어진 공개 모델을 미세 조정(fine-tuning)하거나 NLP 서비스를 제공하는 API를 사용하는 경우가 많습니다**.
*   **라이브러리**: NLTK, spaCy, TextBlob, Gensim, Stanford CoreNLP와 같은 많은 라이브러리가 NLP 프로젝트 구축에 도움을 줄 수 있습니다.
*   **공개 모델 미세 조정**: Hugging Face 라이브러리를 활용할 수 있습니다.
*   **NLP 서비스**: IBM, Amazon, Google과 같은 대기업들은 클라우드 컴퓨팅 서비스의 일부로 NLP 기능을 제공하며, MonkeyLearn, Aylien 같은 다른 회사들도 NLP 서비스를 제공합니다.
*   **오디오 데이터 처리**: AssemblyAI와 같은 회사들은 주제 감지, 개체 감지, 자동 요약, 콘텐츠 조정 등 오디오 인텔리전스 기능을 API를 통해 제공합니다.

### 13.6   **NLP의 미래**
*   한 가지 학설은 **언어학(linguistics)과 딥러닝 기술의 결합**이 NLP의 미래가 될 것이라고 주장합니다.
*   동시에, 매년 이전 모델을 뛰어넘는 **매우 큰 모델들이 개발**되고 있으며, 이러한 추세는 계속될 것으로 예측됩니다.

## 14. 트랜스포머(Transformer)
- 출처:[Transformers for beginners ](https://www.youtube.com/watch?v=_UVfwBqcnbM&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=14)

<div style="text-align: center;">
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Transformer%2C_full_architecture.png/720px-Transformer%2C_full_architecture.png" style="background-color: white; padding: 20px;">
</div >


### **14.1 트랜스포머의 등장 및 필요성**
*   트랜스포머는 몇 년 전 등장하여 NLP(자연어 처리) 분야를 빠르게 장악했습니다. Hugging Face와 같은 라이브러리나 BERT, GPT-3와 같은 구현체 덕분에 널리 사용되고 있습니다.
*   이전에는 RNN(순환 신경망)을 텍스트나 시퀀스 데이터에 사용했지만, 긴 문장에서 앞부분을 잊어버리는 경향이 있었고, 순환적 특성 때문에 병렬 처리가 불가능했습니다.
*   LSTM(장단기 기억 신경망)은 정보를 더 오래 기억했지만 훈련 시간이 매우 길었습니다.
*   **트랜스포머는 오직 어텐션(Attention) 메커니즘에만 의존하며, 순환 구조가 전혀 없어 병렬 처리가 가능하여 더 빠릅니다**.

### **14.2 어텐션(Attention)이란?**
*   어텐션은 모델이 문장, 이미지 또는 모든 종류의 입력에서 **중요한 부분에 주의를 기울이는 능력**입니다.
*   번역 예시에서, 어텐션 모델은 단어 순서가 프랑스어에서 뒤바뀌면 역방향으로 주의를 기울이기도 합니다.
*   이미지 분류에서는 모델이 개의 코나 귀 등 어떤 부분에 주목하는지 확인할 수 있습니다.

### **14.3 트랜스포머 네트워크의 아키텍처**
*   트랜스포머는 크게 **인코더(Encoder)와 디코더(Decoder) 부분으로 구성**됩니다.
*   실제로는 **6개의 인코더와 6개의 디코더**를 가지고 있습니다.
*   **각 인코더**는 하나의 **셀프-어텐션(Self-Attention) 레이어**와 하나의 **피드-포워드(Feed-Forward) 신경망 레이어**를 가집니다.
*   **각 디코더**는 두 개의 **셀프-어텐션 레이어**와 하나의 **피드-포워드 신경망 레이어**를 가집니다.
*   **병렬 처리**는 문장의 모든 단어를 네트워크(특히 인코더의 첫 번째 셀프-어텐션 서브 레이어)에 동시에 입력함으로써 이루어집니다.
*   셀프-어텐션 서브 레이어에서는 문장 내 모든 단어가 다른 모든 단어와 비교되어 단어 간의 정보 교환이 일어납니다. 피드-포워드 신경망에서는 단어들이 개별적으로 처리되어 정보 교환이 없습니다.

### **14.4 입력 및 출력 처리**
*   **임베딩(Embeddings)**: 인코더 또는 디코더로 들어가는 원시 입력은 임베딩됩니다. 임베딩은 단어를 N-길이 벡터로 표현하는 방법입니다 (원본 논문에서는 512 길이 벡터 사용).
*   **위치 인코딩(Positional Encodings)**: 트랜스포머는 순환 구조가 없으므로, 모델이 단어의 순서나 위치를 이해할 수 없습니다. 이를 위해 단어 임베딩에 **위치 인코딩을 추가**하여 단어의 문장 내 위치 정보를 주입합니다.
    *   원본 논문에서는 **고정된 위치 인코딩**을 사용하는데, 이는 훈련 세트에서 본 적 없는 문장 길이도 처리할 수 있는 장점이 있습니다.
    *   정수를 할당하는 방식은 문제가 있어, 대신 **다양한 주파수의 사인(sine) 및 코사인(cosine) 함수를 사용**.
    *   이렇게 생성된 위치 인코딩은 각 위치마다 고유하며, 두 단어 사이의 차이를 항상 알 수 있게 해줍니다.
    *   단어 임베딩과 위치 인코딩을 함께 더한 후 인코더에 공급합니다.
*   **출력**: 디코더의 끝에는 **선형(Linear) 레이어와 소프트맥스(Softmax) 레이어**가 있습니다. 디코더의 출력이 우리가 이해할 수 있는 형태로 변환되며, 이는 어휘에 있는 단어 수만큼의 길이를 가진 벡터로, 각 셀은 다음 단어가 될 가능성을 나타냅니다.

### **14.5 핵심 구성 요소 및 개선점**
*   **정규화 레이어(Normalization Layers)**: 셀프-어텐션 레이어와 피드-포워드 신경망 사이에 **추가 및 정규화(Add & Normalize) 레이어**가 있습니다. 이는 서브 레이어의 출력을 정규화하며, **레이어 정규화(Layer Normalization)** 기술을 사용합니다.
*   **스킵 연결(Skip Connections)**: 서브 레이어를 우회하여 정보를 정규화 레이어로 직접 보내는 화살표가 있습니다. 이는 모델이 정보를 잊지 않고 중요한 정보를 네트워크의 더 깊은 곳으로 전달하는 데 도움이 됩니다.

### **14.6 트랜스포머의 두 가지 핵심 아이디어 (Noble Ideas)**
트랜스포머의 성능을 결정한 두 가지 주요 아이디어는 **위치 인코딩**과 **멀티-헤드 어텐션(Multi-Headed Attention)**입니다.

*   **멀티-헤드 어텐션**
    *   두 가지 유형: 일반 멀티-헤드 어텐션과 마스크드(Masked) 멀티-헤드 어텐션이 있습니다. 인코더와 디코더에서 사용되며, 마스크드는 특정 단어 이전의 단어만 비교하는 차이가 있습니다.
    *   **스케일드 닷 프로덕트 어텐션(Scaled Dot Product Attention)**이 사용됩니다.
    *   **Query(질의), Key(키), Value(값) 행렬/벡터**: 입력 임베딩 벡터를 무작위로 초기화되고 훈련 중에 학습되는 Query, Key, Value 행렬과 곱하여 각 단어에 대한 Query, Key, Value 벡터를 얻습니다.
    *   **점수 계산**: 각 단어의 Query 벡터와 다른 모든 단어의 Key 벡터의 **내적(dot product)을 계산**하여 해당 단어가 다른 단어에 대해 얼마나 관련이 있는지 점수를 매깁니다.
    *   **정규화**: 이 점수들을 8(Query, Key, Value 벡터 길이 64의 제곱근)로 나눈 후 **소프트맥스 레이어**를 통과시켜 모든 단어에 대한 점수 합이 1이 되도록 정규화합니다. 이 결과는 일종의 가중치 역할을 합니다.
    *   **출력 계산**: 이 가중치를 모든 단어의 Value 벡터에 곱한 다음, 모든 가중치가 적용된 Value 벡터를 합산하여 해당 단어에 대한 셀프-어텐션 레이어의 출력을 생성합니다.
    *   **멀티-헤드 효과**: 이 과정을 **여러 번(예: 8번)** 반복하며, 각각 다른 Query, Key, Value 행렬을 사용합니다. 이 결과들을 **연결(concatenate)한 후 또 다른 학습 가능한 가중치 행렬과 곱하여 하나의 최종 어텐션 레이어 출력**을 만듭니다.
    *   이러한 방식을 통해 모델은 문장 내에서 하나의 단어가 아닌 **여러 다른 단어에 주의를 기울일 수 있으며**, 다양한 길이의 문장을 원활하게 처리할 수 있습니다.

### **14.7 트랜스포머의 전체 작동 방식**
1.  입력 단어들을 임베딩하고 위치 인코딩을 추가합니다.
2.  이것을 **6개 층의 인코더**를 통과시킵니다.
3.  인코더의 출력은 **6개 층의 디코더 모두**에 전달됩니다 (특히 디코더의 두 번째 멀티-헤드 어텐션 서브 레이어에).
4.  디코더의 첫 번째 마스크드 멀티-헤드 어텐션 서브 레이어는 **이전 시점의 디코더 출력**을 입력으로 받습니다.
5.  디코더들은 함께 작동하여 출력 벡터를 생성합니다.
6.  이 출력 벡터는 선형 변환(로짓 벡터 생성)을 거쳐 소프트맥스 레이어를 통해 **각 단어가 다음 단어가 될 확률**을 얻습니다.

결론적으로, 트랜스포머는 인코더와 디코더, 그리고 특히 **위치 인코딩과 멀티-헤드 어텐션 레이어라는 두 가지 독창적인 아이디어**를 통해 작동하는 모델입니다.

## 15. 전이 학습(Transfer Learning)
- 출처:[What is Transfer Learning?](https://www.youtube.com/watch?v=DyPW-994t7w&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=15)


### 15.1   **전이 학습이란 무엇인가?**
*   전이 학습은 특정 작업에 대해 이미 개발된 모델의 전부 또는 일부를 가져와 새로운 작업에 해당 학습 내용을 사용하는 기술입니다.
*   이미 훈련된 기본 모델을 **사전 훈련 모델(pre-trained model)**이라고 부르며, 이를 새로운 작업에 맞게 조정하는 과정을 **미세 조정(fine-tuning)**이라고 합니다.
*   예를 들어, 이미지에서 물체(마이크, 테이블, 컴퓨터 등)를 감지하도록 훈련된 대규모 모델을 꽃 이미지를 특정 종으로 분류하도록 미세 조정할 수 있습니다.

### 15.2   **전이 학습은 어떻게 작동하는가?**
*   **방법 1: 출력 계층만 변경**: 
    -   사전 훈련 모델의 출력 계층만 제거하고 자신만의 출력 계층을 추가하는 방식입니다.
*   **방법 2: 더 많은 계층 변경**: 
    -   모델의 더 많은 계층을 제거하고 더 많은 계층과 출력 계층을 추가하는 방식입니다. 제거하거나 추가하는 계층의 수는 두 작업의 유사성에 따라 달라집니다.
*   계층을 제거하거나 추가한 후에는 특정 데이터셋으로 모델을 **미세 조정(훈련)**해야 합니다.
*   예를 들어, 사람 얼굴 인식을 위해 훈련된 CNN 모델에서 깊은 계층은 고수준 특징(눈, 코, 귀 등)을 인식할 수 있는데, 개 얼굴 인식을 위해 이 모델을 사용할 경우, 사람 얼굴에 특화된 마지막 계층들은 제거하고, 개 얼굴 인식을 위한 새로운 계층들을 추가한 후 개 이미지로 훈련합니다. 이렇게 하면 처음부터 모델을 만들 필요 없이 기존의 정보(선, 원 등 저수준 특징 인식)를 활용하여 시간을 절약할 수 있습니다.

### 15.3   **왜 전이 학습을 사용하는가?**
*   **데이터 부족 문제 해결**: 전이 학습은 주로 **데이터 부족 문제**를 해결하기 위해 사용됩니다. 복잡한 아키텍처는 많은 훈련 데이터가 필요하지만, 만약 특정 작업에 대한 데이터가 부족하다면 (예: 5천 장의 개 얼굴 사진만 있는 경우), 유사한 작업을 통해 학습된 지식을 활용하여 모델이 "선두를 차지"하게 할 수 있습니다.
*   **시간 절약**: 일반화된 모델이 구축되면 많은 사람들이 이를 자신들의 특정 작업에 맞게 미세 조정할 수 있어 **시간을 절약**할 수 있습니다.
*   **탄소 발자국 감소**: 일반화된 모델을 공유하고 재사용함으로써 모델 훈련에 필요한 자원과 시간을 줄여 **탄소 발자국을 낮추는 데 기여**합니다.

### 15.4   **전이 학습 시작하기**
*   **1단계: 사전 훈련 모델 찾기**
    -   TensorFlow Hub, Keras Applications, PyTorch pre-trained models와 같은 딥러닝 프레임워크 내에 내장된 모델이나 Hugging Face, Model Zoo와 같은 웹사이트/API에서 사전 훈련 모델을 찾을 수 있습니다. GitHub에서 모델을 다운로드할 경우 라이선스를 확인해야 합니다.
*   **2단계: 데이터셋 준비**
    -   모델에 맞게 데이터를 준비해야 합니다. 예시에서는 Keras Applications에서 제공하는 Xception 모델을 사용하며, 이 모델은 224x224 픽셀 이미지를 필요로 하므로, `tf_flowers` 데이터셋의 이미지를 리사이즈하고 `preprocess_input` 함수로 전처리합니다. 또한 데이터셋이 훈련/테스트/검증으로 분리되어 있지 않다면 직접 분할해야 합니다.
*   **3단계: 모델 구성**
    *   사전 훈련된 Xception 모델을 가져오되, 분류를 위한 최상위 계층(평균 풀링 계층 및 출력 계층)은 포함하지 않습니다.
    *   자신만의 새로운 평균 풀링 계층과 출력 계층을 생성하고, 이를 기본 모델과 결합하여 미세 조정할 모델을 만듭니다.
*   **4단계: 계층 동결(Freezing)**
    -   미세 조정 훈련을 시작하기 전에 기본 모델의 계층들을 **동결(freeze)**합니다. 이는 미세 조정 과정에서 이 계층들의 파라미터가 업데이트되지 않도록 설정하는 것을 의미합니다. 새로 생성된 최상위 계층의 파라미터만 업데이트됩니다.
*   **5단계: 컴파일 및 훈련(초기)**
    -   모델을 컴파일하고 미세 조정을 시작합니다. 새로 생성된 계층들이 빠르게 학습하도록 비교적 높은 학습률(learning rate)을 사용하고, 소수의 에포크(epochs) 동안 훈련합니다.
*   **6단계: 계층 동결 해제 및 훈련(전체)** 
    -   초기 미세 조정 후, 필요하다면 기본 모델의 계층들을 **동결 해제(unfreeze)**하고 모델을 다시 컴파일하여 전체 모델을 낮은 학습률로 훈련합니다. 이는 기본 모델의 파라미터를 크게 변경하지 않으면서 성능을 더욱 향상시키는 데 도움이 됩니다.
*   GPU 런타임을 사용하면 훈련 시간을 크게 절약할 수 있습니다.

```python
import tensorflow_datasets as tfds
import tensorflow as tf

#데이터 불러오기
dataset, info = tfds.load("tf_flowers",
                          as_supervised=True,
                          with_info=True)

dataset_size = info.splits["train"].num_examples
class_names = info.features["label"].names
n_classes = info. features["label"].num_classes

#데이터셋 나누기
test_set_raw, valid_set_raw, train_set_raw
            = tfds.load("tf_flowers",
                        split=["train[:10%]",
                               "train[10%:25%]",
                               "train[25%:]"],
                        as_supervised=True)
#데이터 전처리 함수
def preprocess(image,label):
    # Xception 모델 입력 크기 **224x224**로 변환
    # `preprocess_input`: 모델이 학습된 방식에 맞게 픽셀값을 조정
    resized_image = tf.image.resize(image, [224,224])
    final_image = tf.keras.applications.xception.preprocess_input(resized_image)
    return final_image, label

# 데이터 파이프라인 구성
# `batch(32)` → 미니배치 학습
# `prefetch(1)` → 다음 배치 1개 미리 준비
batch_size = 32
train_set = train_set_raw.shuffle(1000)
train_set = train_set.map(preprocess).batch(batch_size).prefetch(1)
valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)
test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)

#사전학습 모델 불러오기
# **Xception**: 이미지넷으로 학습된 고성능 CNN 모델
# `include_top=False` → 마지막 분류층 제외 (우리 데이터에 맞게 새로 붙이기 위해)
base_model = tf.keras.applications.xception.Xception(weights="imagenet",
                                            include_top=False)

# 새로운 출력층 추가
# **GlobalAveragePooling2D**: 특징맵을 평균값으로 변환 → 파라미터 수 감소, 과적합 방지
# **Dense(n\_classes, softmax)**: 꽃 종류별 확률 예측

avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)
output = tf.keras.layers.Dense(n_classes, activation="softmax")(avg)

model = tf.keras.Model(inputs=base_model.input, outputs=output)

# 1단계 학습 (Feature Extraction)
# **기존 가중치 고정** → 새로운 출력층만 학습
for layer in base_model.layers:
  layer.trainable = False

# **SGD + Momentum** 사용
optimizer = tf.keras.optimizers.SGD(lr=0.2,
                                    momentum=0.9,
                                    decay=0.01)

model.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])

# **5 Epoch** 동안 빠른 수렴
history = model.fit(train_set,
                    epochs=5,
                    validation_data=valid_set)

#2단계 학습 (Fine-Tuning)
# 이제 **기존 모델 가중치도 미세 조정**

for layer in base_model.layers:
  layer.trainable = True

# 학습률을 줄여서 (`lr=0.01`) 조금씩 업데이트
optimizer = tf.keras.optimizers.SGD(lr=0.01,
                                    momentum=0.9,
                                    decay=0.001)

model.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])

# **10 Epoch** 동안 전체 모델 재학습
history = model.fit(train_set,
                    epochs=10,
                    validation_data=valid_set)
```

### 15.5 전이학습 흐름 정리
1. **사전학습 모델 불러오기** (include\_top=False)
2. **새로운 출력층 추가**
3. **기존 가중치 고정 후** 새 층만 학습 (Feature Extraction 단계)
4. **전체 가중치 미세 조정** (Fine-Tuning 단계)


## 16. 워드 임베딩(Word Embeddings)
- 출처:[A Complete Overview of Word Embeddings](https://www.youtube.com/watch?v=5MaWmXwxFNQ&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=16)

- **워드 임베딩이 필요한 이유**
    *   NLP(자연어 처리) 모델은 텍스트를 직접 처리할 수 없으며, 숫자로 표현된 데이터를 필요로 합니다.

### 16.1 **기존 텍스트 표현 방법 및 한계**
워드 임베딩 외에도 텍스트를 숫자로 표현하는 여러 방법이 있습니다:
*   **원-핫 인코딩(One-hot encoding)**:
    *   어휘에 있는 단어 수만큼 긴 벡터를 생성하며, 특정 단어에 해당하는 셀만 1이고 나머지는 0입니다.
    *   **매우 희소한(sparse) 벡터**를 만들고 공간 효율성이 떨어집니다.
*   **카운트 기반(Count-based) 표현 기법**:
    *   전체 문장을 하나의 벡터로 압축하려 합니다.
    *   **단어 가방(Bag of Words)**: 
        -   문장에서 단어의 순서를 고려하지 않고 각 단어가 나타나는 횟수를 세어 벡터를 만듭니다.
    *   **N-그램(N-gram)**: 
        -   단일 단어가 아닌 n개의 단어 그룹의 발생 횟수를 셉니다.
    *   **TF-IDF(Term Frequency-Inverse Document Frequency)**: 
        -   특정 단어가 문서에 얼마나 자주 나타나는지(TF)와 훈련 데이터의 다른 문서에서 얼마나 자주 나타나는지(IDF)를 추적하여, 흔한 단어와 특정 문서에 중요한 단어를 구별합니다.
*   **기존 방법의 한계점**:
    *   **맥락(context)을 고려하지 않습니다**.
    *   훈련 예시에서 보지 못한 단어를 처리할 수 없습니다.
    *   생성된 임베딩이 희소하여 공간 효율성이 떨어집니다.

### 16.2 **워드 임베딩이란?**
*   **목표**: 
    -   단어를 **밀집(dense) 벡터**로 표현하면서 **유사한 단어들이 임베딩 공간에서 서로 가깝게** 위치하도록 하는 것
*   **밀집 벡터**: 
    -   대부분의 값이 0이 아닌 벡터를 의미하며, 일반적으로 어휘의 단어 수보다 차원(dimensions)이 적습니다.
*   **유사한 단어**: 
    -   대부분 유사하거나 같은 맥락에서 사용되는 단어들을 의미합니다. 예를 들어 'tea'와 'coffee'는 'breakfast', 'drink', 'enjoy'와 같은 단어 주변에서 자주 사용되므로 유사하게 간주됩니다.
*   **임베딩 공간(Embedding space)**: 임베딩된 데이터가 존재하는 공간입니다.
    *   두 데이터 포인트 간의 **거리**는 그 유사성을 나타냅니다.
    *   3차원 이상의 공간에서는 시각화하기 어렵지만, 코사인 유사도(cosine similarity)와 같은 방법을 사용하여 두 벡터 간의 거리를 계산할 수 있습니다.
    *   성공적인 워드 임베딩은 유사한 단어들을 서로 가깝게 배치합니다 (예: king, queen, sovereign, kingdom 또는 cat, dog, pet, bird, animal).
    *   일부 경우, 단어 간의 상대적인 거리가 문맥적 정보를 나타내기도 합니다. 예를 들어, 'king' 벡터에서 'man' 벡터를 빼고 'woman' 벡터를 더하면 'queen' 벡터가 되는 것처럼 남성-여성 관계가 유지됩니다.

### 16.3 **워드 임베딩 생성 방법**
워드 임베딩은 방대한 텍스트 코퍼스(말뭉치)로부터 학습됩니다.
*   **사용자 정의 임베딩 계층(Custom embedding layer)**:
    *   모델의 핵심 부분에 텍스트를 공급하기 전에 임베딩 계층을 두어 무작위 가중치로 초기화하고 실제 모델 훈련 중에 단어를 최적으로 표현하는 방법을 학습하게 합니다.
    *   장점: 
        -   특정 사용 사례 및 데이터셋에 특화된 임베딩을 얻을 수 있습니다.
    *   단점: 
        -   성공적인 임베딩 계층을 위해서는 **많은 양의 훈련 데이터와 긴 훈련 시간**이 필요합니다. (예: 트랜스포머(Transformer) 아키텍처).
*   **Word2Vec**:
    *   원-핫 인코딩된 단어 버전을 사용하고 문장의 맥락을 활용하여 임베딩을 생성합니다.
    *   **CBOW(Continuous Bag of Words)**: 
        -   주변 단어를 입력으로 받아 가운데 단어를 예측합니다.
    *   **Skip-gram**: 
        -   가운데 단어를 입력으로 받아 주변 단어를 예측합니다.
    *   단일 은닉 계층을 가진 신경망을 사용하여 학습하며, 은닉 계층의 뉴런 수는 임베딩의 크기가 됩니다.
*   **GloVe (Global Vectors)**:
    *   Word2Vec의 확장으로, 단어의 지역적 의존성뿐만 아니라 전체 문장의 **전역적 맥락(global context)**도 고려합니다.
    *   단어의 **동시 발생(co-occurrence) 메트릭스**를 고려하여 학습합니다.
*   **FastText**:
    *   Word2Vec 알고리즘의 확장입니다.
    *   단어 전체가 아닌 **하위 단어(subwords, 문자 N-그램)**를 사용하여 모델을 훈련합니다.
    *   **장점**: **희귀 단어나 훈련 데이터에서 보지 못한 단어**를 잘 처리합니다. 독일어나 터키어와 같이 형태론적으로 풍부한 언어에도 효과적입니다.
*   **ELMo (Embeddings from Language Models)**:
    *   워드 임베딩 분야의 최신 혁신 중 하나입니다.
    *   단어의 임베딩이 **문맥에 따라 동적으로 생성**됩니다.
    *   양방향 LSTM(bi-directional LSTM) 모델에서 파생되며, 문장의 다음 단어와 이전 단어를 예측하는 언어 모델링 작업을 통해 훈련됩니다.
    *   **동음이의어(homonyms)를 구별**할 수 있습니다 (예: 'fair'가 '공정한' 또는 '박람회'로 사용될 때 다른 임베딩을 가짐).
    *   첫 번째 계층이 단어 대신 문자를 처리하기 때문에 **오타나 오기된 단어를 잘 처리**합니다.

### 16.4 **워드 임베딩 사용 방법**
프로젝트에서 워드 임베딩을 사용하는 두 가지 주요 방법이 있습니다:
*   **자신만의 워드 임베딩을 처음부터 만들기**:
    *   알고리즘 및 모델을 제공하는 라이브러리를 사용하여 구현할 수 있습니다.
    *   많은 훈련 데이터와 긴 시간이 필요하지만, 특정 사용 사례와 데이터셋에 매우 적합한 임베딩을 얻는다.
*   **사전 훈련된(pre-trained) 워드 임베딩 사용하기**:
    *   라이브러리(예: **Gensim**)나 연구 그룹에서 공개적으로 제공하는 사전 훈련된 임베딩을 사용할 수 있습니다.
    *   **장점**: 많은 시간과 노력을 절약할 수 있습니다.
    *   **활용 방법**:
        *   **정적으로 사용(statically)**: 훈련 중에 업데이트하지 않고 플러그 앤 플레이 방식으로 사용합니다.
        *   **미세 조정(fine-tune)**: 훈련 과정의 일부로 포함하여 모델 훈련과 함께 임베딩을 조정합니다.
    *   Gensim 라이브러리는 FastText, GloVe, Word2Vec 등 다양한 사전 훈련된 워드 임베딩을 제공하며, 이들은 트위터, 위키피디아, 구글 뉴스 등 다양한 데이터셋으로 훈련되었습니다.

### 16.5 **워드 임베딩 탐색 예시 (Gensim 라이브러리)**
*   **가장 가까운 단어 찾기**: 
    -   'tea'에 가장 가까운 단어로 Word2Vec은 'T's' 등을, GloVe는 'coffee', 'milk', 'wine', 'cream' 등을, FastText는 'tea bags', 'babies' 등을 제시하며 GloVe의 결과가 더 정확했습니다.
*   **두 단어 간의 거리 확인**: 
    -   'tea'와 'coffee'의 거리는 0.43인 반면, 'tea'와 'pea'의 거리는 0.7이었습니다. 이는 유사한 단어가 더 가깝고, 유사하지 않은 단어는 철자가 비슷하더라도 더 멀리 떨어져 있다는 워드 임베딩의 목표와 일치합니다.
*   **유추(Analogy) 작업**: 
    -   'king'에서 'man'을 빼고 'woman'을 더하면 가장 가까운 단어로 'queen'이 나옵니다. 또한, 'restaurant'에서 'dinner'를 빼고 'cocktail'을 더했을 때 Word2Vec은 'eatery', 'bartender'를, GloVe는 'parasol', 'espresso', 'brewery'를, FastText는 'bar', 'restaurant bar', 'nightclub' 등을 제시했습니다. 일반적으로 FastText가 이러한 유추 작업에서 더 나은 성능을 보이는 경향이 있습니다.
*   이러한 탐색은 워드 임베딩의 작동 방식을 이해하는 데 도움이 되며, 실제 프로젝트에서는 워드 임베딩이 일반적으로 더 큰 코어 모델과 결합되어 NLP 작업(예: 감성 분석)에 사용됩니다.

## 17. LLM이 다음 단어를 어떻게 선택하는지에 대한 이해
- 출처:[How Language Models Choose the Next Word](https://www.youtube.com/watch?v=vQbSBdJ1Irw&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=17)

### 17.1   **LLM의 두 가지 단계**:
*   **모델링 단계(학습)**: 
    -   LLM이 훈련 중에 어떻게 학습하는지에 초점을 맞춥니다. 이 단계에서 LLM은 텍스트 시퀀스에서 다음 단어의 확률을 추정하는 목적 함수를 최적화하며, 이는 언어의 통계 모델을 생성합니다. 지난 몇 년간 LLM 개발의 돌파구는 방대한 계산량과 훈련 데이터를 통해 이 간단한 목적 함수를 최적화하는 것이 인간 언어를 효과적으로 모델링한다는 경험적 발견이었습니다.
*   **디코딩 단계(텍스트 생성)**: 
    -   LLM이 텍스트를 생성하는 방법입니다. 훈련된 LLM은 텍스트 생성을 위해 내부 통계 언어 모델을 활용하는 다양한 알고리즘 전략을 사용할 수 있는 수학적 함수를 제공합니다. 디코딩의 목표는 주어진 입력 토큰 시퀀스(S)에 대해 문맥적으로 일관성 있는 추가 토큰(N)을 선택하여 완성된 시퀀스(S')를 형성하는 것입니다.

### 17.2   **디코딩 전략의 중요성**:
*   최근 Frontier LLM의 등장과 함께 개발자들은 모델 성능 비교에 집중했지만, LLM이 텍스트를 생성하는 방식을 결정하는 알고리즘인 **디코딩 전략은 주목받지 못했습니다**.
*   프롬프트 엔지니어링과 같은 다른 영역에 연구가 집중되었지만, 토큰 선택 알고리즘에 대한 심층적인 실험은 대체로 간과되었습니다.
*   디코딩 전략은 모델이 어떻게 진화하거나 훈련 패러다임이 변하더라도 언어 모델 기반 텍스트 생성의 핵심 구성 요소로 남을 것입니다.
*   특정 디코딩 전략의 선택은 **과제별 성능(예: 창의적 작업 대 예측 가능한 출력)과 추론 속도** 측면에서 모델의 품질에 영향을 미칠 수 있습니다.

### 17.3   **디코딩의 일반적인 절차**:
1.  현재 컨텍스트 시퀀스(S)에서 모델의 분포를 기반으로 토큰(X)을 샘플링합니다.
2.  컨텍스트 시퀀스를 S' (S 뒤에 X가 오는)로 업데이트합니다.
3.  모델이 특수 종료 토큰을 예측할 때까지 1단계와 2단계를 반복합니다.
    *   이 프레임워크 내에서 1단계에서 토큰 X를 샘플링하는 방식이 다양한 디코딩 전략을 구별합니다.

### 17.4   **결정론적 디코딩 전략**: 
동일한 입력이 항상 동일한 출력을 생성하며 무작위성이 없습니다.
*   **탐욕적 탐색(Greedy Search)**:
    *   가장 간단한 디코딩 전략입니다.
    *   각 단계에서 컨텍스트 시퀀스(S)에 주어진 **가장 가능성이 높은 토큰(X)**을 선택합니다.
    *   **현재 대화형 LLM이 텍스트를 생성하는 방식이 아닙니다**.
    *   각 개별 단계에서 가장 가능성 높은 토큰을 선택하지만, 반드시 전반적으로 가장 가능성 높은 시퀀스를 생성하는 것은 아닙니다. 덜 가능성 있는 토큰을 선택하는 것이 다음 단계에서 전반적으로 더 가능성 높은 시퀀스로 이어질 수 있습니다.
    *   확률 트리의 단일 분기만 탐색하며, 실제 가장 가능성 높은 시퀀스를 찾으려면 모든 가능한 토큰 조합의 전체 트리를 탐색해야 하는데, 이는 현실적인 텍스트 길이에서는 계산적으로 불가능합니다.
    *   계산 효율성에도 불구하고, 탐욕적 탐색은 일반적으로 **고품질 텍스트를 생성하지 못하며 개방형 생성 설정에서 일반적이고 단조로운 텍스트를 출력**하는 것으로 경험적으로 밝혀졌습니다.
*   **빔 탐색(Beam Search)**:
    *   탐욕적 탐색의 자연스러운 일반화이며, 확률 트리의 여러 분기를 탐색하는 방법입니다.
    *   다음 토큰 중 가장 높은 확률을 가진 하나만 선택하는 대신, 각 시간 단계에서 **K개의 가장 가능성이 높은 시퀀스 빔**을 유지합니다 (K는 빔 너비).
    *   더 많은 계산이 필요하여 탐욕적 탐색보다 느릴 수 있지만, 빔 크기에 따라 **더 높은 품질의 텍스트**를 생성합니다.
    *   여러 후보 시퀀스를 유지함으로써 빔 탐색은 확률 트리에서 앞을 내다볼 수 있으며, 탐욕적 탐색이 놓칠 수 있는 더 높은 확률의 시퀀스를 잠재적으로 찾을 수 있습니다.
    *   그러나 빔 탐색은 **개방형 텍스트 생성 작업에서 종종 부족합니다**. 사람의 선호도를 포함하는 질적 연구와 인간이 생성한 텍스트와 빔 탐색을 비교하는 양적 분석 모두 빔 탐색 출력에서 **퇴화 패턴**을 드러냈습니다.
    *   빔 탐색 시퀀스는 일반적으로 높은 가능성 점수를 가지지만, 인간이 작성한 텍스트의 특징인 **자연스러운 분산과 다양성이 부족**합니다. 이는 구문 반복 및 덜 다양한 어휘 사용 경향으로 나타날 수 있습니다.

### 17.5   **무작위성의 중요성**:
*   생성된 텍스트에 더 많은 분산과 예측 불가능성을 도입하는 동시에 전반적인 일관성과 높은 가능성을 유지하는 방법은 **디코딩 프로세스에 어느 정도의 무작위성을 도입하는 것**입니다.
*   이는 확률적 방법을 통한 텍스트 생성으로 이어집니다.
*   모델 훈련 후 텍스트 생성 또는 디코딩에서 **무작위성은 LLM이 가능한 텍스트 시퀀스의 무한한 공간을 효과적인 방식으로 탐색**할 수 있도록 하는 데 필수적입니다.

## 18. LLM(대규모 언어 모델) 텍스트 생성의 기본 원리
- 출처:[The Fundamentals of LLM Text Generation](https://www.youtube.com/watch?v=a-6hVvU1WMk&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=18)

### 18.1   **LLM 텍스트 생성의 기본 원리**:
*   LLM은 다음 단어를 예측하도록 훈련되지만, 실제로 텍스트를 생성할 때는 단순히 가장 있을 법한 단어를 선택하는 것이 아니라 **특정 수준의 무작위성**을 활용합니다.
*   무작위성은 LLM이 가능한 텍스트 시퀀스의 무한한 공간을 효과적으로 탐색하고, **인간이 말하는 것처럼 덜 예측 가능하고 더 자연스러운 텍스트를 생성**할 수 있게 해줍니다.
*   무작위성이 없으면 모델은 반복적이고 로봇 같은 느낌을 주며, 너무 많은 무작위성은 텍스트를 예측 불가능하고 무의미하게 만들 수 있습니다.

### 18.2   **확률적 디코딩 전략 (Stochastic Decoding Strategies)**:
*   LLM은 텍스트 생성 과정에 무작위성을 도입하기 위해 다양한 알고리즘을 사용하며, 가장 기본적인 전략은 **Top-K, Top-P, 온도 샘플링(Temperature Sampling)**입니다.

*   **Top-K 샘플링**:
    *   작동 방식: **가장 확률이 높은 상위 K개의 토큰 중에서 다음 토큰을 무작위로 샘플링**합니다.
    *   문제점: K값이 고정되어 있어, 확률 분포의 형태에 따라 문제가 발생할 수 있습니다.
        *   '꼬리가 긴' 분포(분포가 넓게 퍼져 있을 때): 
            -   잠재적으로 흥미로운 많은 토큰을 임의로 잘라내어 텍스트의 다양성을 제한할 수 있습니다.
        *   '피크가 있는' 분포(분포가 특정 토큰에 집중되어 있을 때): 
            -   K값이 너무 크면 불필요한 토큰이 포함되거나, K값이 작으면 동일하게 확률 높은 토큰이 제외될 수 있습니다.

*   **Top-P 샘플링 (Nucleus Sampling)**:
    *   작동 방식: 
        -   **누적 확률이 특정 임계값 P(예: 0.7)를 충족하거나 초과하는 최소한의 토큰들을 선택하고, 이 '핵심(nucleus)'에서 다음 토큰을 샘플링**합니다.
    *   장점: 
        -   Top-K와 달리 고정된 토큰 수가 아니라 **각 단계의 확률 분포에 따라 동적으로 선택 범위가 조정**됩니다. 평평한 분포에서는 더 많은 토큰 풀에서 샘플링하고, 피크가 있는 분포에서는 더 적은 토큰 풀에서 샘플링합니다.
    *   제한: 
        -   생성 과정의 무작위성 양을 직접적으로 제어하지는 않습니다.

*   **온도 샘플링 (Temperature Sampling)**:
    *   작동 방식: 확률 분포의 '날카로움'을 직접 조정합니다. 이는 소프트맥스(softmax) 함수에 온도 매개변수 T를 도입하여 이루어집니다.
    *   T의 영향:
        *   **T가 1 미만**: 
            -   확률 분포가 가장 있을 법한 토큰 주변에 **더 집중**됩니다. T가 0에 가까워질수록 샘플링은 **탐욕적 탐색(greedy search)에 가까워져 결정론적**이 됩니다.
        *   **T가 1**: 
            -   원래의 확률 분포가 **그대로 유지**됩니다.
        *   **T가 1 초과**: 
            -   확률이 **평평해져 무작위성이 증가**합니다. T가 매우 높은 값으로 증가하면 분포는 균일 분포에 가까워져 사실상 무작위 단어 생성기와 같아집니다.
    *   조합: 종종 Top-P와 결합되어 모델 응답의 보수성 또는 창의성을 더 정밀하게 제어할 수 있습니다.
    *   궁극적 목표: 이러한 확률적 디코딩 전략들은 예측 불가능성을 도입하지만, 수학적으로는 여전히 **높은 확률을 가진 텍스트 시퀀스의 가능성을 극대화**하는 것을 목표로 합니다.

### 18.3   **정보 이론 기반의 '전형적 샘플링(Typical Sampling)'**:
*   새로운 접근 방식: 단순한 확률 최대화 외에 **언어의 정보 내용** 수준에서 예측 가능성과 놀라움 사이의 균형을 찾는 대안적인 접근법입니다.
*   핵심 아이디어: 자연어가 정보 전송 과정이라면, 효과적인 의사소통을 위해 이 정보가 **최적의 속도**로 텍스트 시퀀스에 인코딩되어야 한다고 가정합니다.
*   두 가지 원칙:
    1.  **문장을 짧고 정보 밀도 있게 유지**: 주어진 메시지에서 전달되는 정보의 양을 최대화합니다.
    2.  **높은 정보 밀도의 순간 피하기**: 청중이 처리하기 너무 어렵거나 놀라운 시퀀스를 피합니다.
*   작동 방식: 디코딩 중 **평균 엔트로피(정보 내용의 수학적 측정)**를 가진 텍스트를 목표로 하여, 흥미로우면서도 독자를 압도하지 않을 정도로 정보량이 적절한 텍스트를 생성합니다.
*   결과: 추상적 요약 및 스토리 생성과 같은 NLP 작업에서 Top-K 및 Top-P 샘플링에 비해 성능을 향상시키고 **반복을 줄이는 데 효과적**임을 보여주었습니다.

### 18.4   **LLM의 '발현 능력(Emergent Abilities)'**:
*   LLM이 특정 규모에 도달할 때, 번역, 요약, 코드 완성 등 **훈련받지 않은 작업을 갑자기 수행할 수 있게 되는 능력**

## 19. 스페큘레이티브 샘플링(Speculative Sampling)
- 출처:[What is Speculative Sampling?](https://www.youtube.com/watch?v=2ouqE9g6oeM&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=19)

### 19.1   **추론 속도 향상의 중요성**: 
- 언어 모델(LLM)이 강력해지면서 텍스트 생성 속도(추론 속도)를 높이는 것이 중요하며, 이는 워크플로우 자동화 및 에이전트 AI 시스템과 같은 AI 애플리케이션의 확장에 필수적입니다. 추론 효율성 개선은 비용과 에너지 발자국을 줄일 뿐만 아니라, 현재 도달하기 어려운 새로운 애플리케이션과 기능을 가능하게 할 수 있습니다.

### 19.2   **스페큘레이티브 샘플링(Speculative Sampling)이란?**:
*   Google Research와 DeepMind 팀이 독자적으로 개발한 최신 디코딩 전략입니다.
*   LLM 텍스트 생성 속도를 **2~3배 향상**시키며, **출력 품질에는 변화가 없습니다**.
*   기존에는 단일 토큰 생성을 위해 모델 호출(포워드 패스)이 한 번 필요했으며, 이는 트랜스포머 모델의 크기에 비례하여 시간이 오래 걸렸습니다. 스페큘레이티브 샘플링은 단일 모델 패스에서 여러 토큰을 생성하여 이 문제를 해결합니다.

### 19.3   **작동 방식: 두 모델의 협업**:
스페큘레이티브 샘플링은 기존의 단일 모델 대신 두 개의 모델을 동시에 사용하여 토큰 생성 루프를 재설계합니다.

1.  **1단계: 드래프트 모델(Draft Model) 생성**:
    *   더 작고 빠른 "드래프트 모델"이 주어진 문맥 시퀀스에서 고정된 수의 K개(예: 5개) 토큰을 생성합니다.
    *   이 과정에서 드래프트 모델은 K번의 패스를 수행하며, 생성된 토큰과 해당 토큰의 다음 토큰 확률 분포를 메모리에 캐시합니다.

2.  **2단계: 타겟 모델(Target Model) 검증**:
    *   원본의 더 큰 "타겟 모델"이 검증자로 사용됩니다.
    *   단일 포워드 패스로 전체 시퀀스(접두사 시퀀스 + K개의 드래프트 토큰)를 평가합니다.
    *   이 과정에서 타겟 모델은 접두사 시퀀스에 대한 하나와 각 토큰 위치에 대한 하나씩, 총 (K+1)개의 다음 토큰 분포를 캐시합니다.
    *   **핵심 원리**: 트랜스포머 모델의 어텐션 메커니즘은 전체 시퀀스에 대해 행렬 연산을 사용하여 모든 토큰의 어텐션 가중치를 동시에 계산하고 캐시할 수 있으므로, 타겟 모델이 단일 패스 내에서 각 토큰 위치를 동시에 처리할 수 있게 합니다.

3.  **3단계: 수정(Correction) - Rejection Sampling**:
    *   "리젝션 샘플링(rejection sampling)"이라는 기법을 사용하여 드래프트 토큰이 타겟 모델과 드래프트 모델의 확률 비율에 따라 순차적으로 승인되거나 거부됩니다.
    *   만약 토큰이 거부되면, 프로세스는 중단되고 다음 토큰은 타겟 모델의 예측에서 파생된 조정된 분포에서 샘플링됩니다.

### 19.4   **속도 향상 원리**:
*   **최상의 경우**: K개(예: 5개)의 드래프트 토큰이 모두 타겟 모델에 의해 유효하다고 받아들여지면, 타겟 모델 패스당 5개의 토큰이 생성됩니다. 드래프트 모델의 계산 비용은 타겟 모델에 비해 무시할 수 있으므로, 약 5배의 속도 향상을 가져옵니다.
*   **최악의 경우**: 첫 번째 토큰이 거부되더라도 타겟 모델의 분포를 사용하여 한 개의 토큰을 생성할 수 있습니다. 이 경우 비용은 기본(1X)과 거의 동일합니다.
*   **일반적인 예상 경우**: 최상과 최악의 중간 지점에서 2~3배의 평균 속도 향상이 발생합니다.

### 19.5   **출력 품질 보존**:
*   수정 단계에서 타겟 모델은 자체 예측이 드래프트 모델과 일치하는 경우에만 토큰을 승인하고 그렇지 않으면 거부합니다.
*   이것은 최종 생성된 시퀀스가 타겟 모델의 분포와 일치하도록 보장, 따라서 **출력 품질에 영향을 미치지 않습니다**.
*   이 기술은 드래프트 모델이 사용하는 디코딩 전략(예: 온도 샘플링, 일반 샘플링 등)과 **다른 디코딩 전략과도 함께 작동**할 수 있습니다.

요약하자면, 스페큘레이티브 샘플링은 작고 빠른 드래프트 모델로 초안을 생성하고, 원본의 큰 타겟 모델로 이 초안을 효율적으로 검증 및 수정함으로써 LLM의 추론 속도를 획기적으로 높이는 기법입니다.

## 20. 신경망 오디오 압축
- 출처:[Neural Audio Compression ](https://www.youtube.com/watch?v=4FFFDMDo6SM&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=20)

### 20.1 오디오 압축
*   **오디오 압축의 중요성**: 현재 인터넷 트래픽의 80% 이상이 스트리밍 오디오 및 비디오에서 발생하므로, 효율적인 압축이 매우 중요합니다.
*   **기존 압축 방식의 한계**: MP3와 같은 전통적인 압축 방식은 사람이 듣지 못하는 부분을 제거하지만, **만능 방식(one-size-fits-all approach)**이라 음질 손상이 발생할 수 있으며, 특정 유형의 소리(예: 음악)에 최적화되어 있어 다른 소리에는 효과적이지 않을 수 있습니다.

### 20.2   **신경망 압축의 등장**:
*   신경망 기반 방법은 **다양한 오디오 유형에 동적으로 적응**하여 품질을 보존하는 유연성을 제공합니다.
*   고정된 규칙 대신 데이터로부터 직접 패턴을 학습하여 사람의 인식에 가장 중요한 것을 보존하는 방식으로 오디오를 압축합니다.
*   **반복적인 노이즈 제거(iterative noising)** 또는 **확산 모델(diffusion models)**과 결합될 때 원본에 매우 충실하게 데이터를 재현할 수 있어, **새롭고 유연한 거의 무손실 압축 기술**로 이어질 수 있습니다.

### 20.3   **신경망 오디오 압축의 작동 방식**:
1.  **인코더 신경망**: 
    -   오디오 파형을 **벡터 시퀀스**로 변환합니다. 이 벡터들은 소리의 필수적인 특징을 포착합니다.
2.  **벡터 양자화(Vector Quantization, VQ)**: 
    -   인코더가 생성하는 연속적인 벡터를 효율적으로 전송하기 위해, 미리 정의된 유한한 참조 벡터 세트인 **코드북(codebook)**에서 가장 가까운 벡터로 대체합니다. 즉, 원본 고차원 벡터 대신 코드북에서 가장 일치하는 벡터의 **단일 정수 인덱스**만 전송합니다.
3.  **VQ의 문제점**: 
    -   비트 전송률(bit rate)이 높아질수록(예: 1kbps에서 3kbps로) 필요한 코드북의 크기가 기하급수적으로 커져 **실제로 불가능**해집니다 (예: 수십억 개 이상의 벡터 필요).

### 20.4   **잔차 벡터 양자화(Residual Vector Quantization, RVQ)의 해결책**:
*   RVQ는 압축을 **여러 단계(multi-layered approach)**로 나누어 이 문제를 해결합니다.
*   첫 번째 계층은 적당한 해상도로 벡터를 양자화하고, **각 후속 계층은 이전 계층의 '잔차 오류'(residual error), 즉 벡터 차이를 처리**합니다.
*   이러한 다중 계층 양자화 프로세스를 통해 필요한 **코드북 크기를 획기적으로 줄일 수 있습니다** (예: 5개 계층만으로 코드북 크기를 10억 개 이상에서 320개로 감소).
*   각 계층이 신호의 일부만 처리하면 되므로 전체 프로세스가 단일 코드북으로 모든 것을 포착하려는 것보다 훨씬 효율적입니다.

### 20.5   **RVQ의 적용 및 영향**:
*   RVQ를 기반으로 한 신경망 압축 방식은 Spotify의 고품질 음악 스트리밍, Google의 SoundStream 및 Meta의 EnCodec과 같은 **최첨단 AI 모델의 핵심 기술**입니다.
*   텍스트-음성 변환(text-to-speech) 및 텍스트-음악 생성기(text-to-music generators)에도 필수적으로 사용
*   궁극적으로 MP3와 같은 전통적인 코덱을 대체하고 **비디오 및 오디오 압축의 미래를 재편**할 것으로 예상됩니다.