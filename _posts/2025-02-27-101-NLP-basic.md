---
title: 11차시 1:NLP (기초)
layout: single
classes: wide
categories:
  - NLP
toc: true # 이 포스트에서 목차를 활성화
toc_sticky: true # 목차를 고정할지 여부 (선택 사항)
---




## 1. **NLP의 기본개념**

### 1.1 **NLP란**
NLP는 **텍스트 데이터** 또는 **음성 데이터**를 분석하고 이해하여, 컴퓨터가 인간의 언어와 상호작용할 수 있도록 하는 기술입니다. 주요 작업에는 다음과 같은 것들이 포함됩니다:
- **텍스트 분류**: 텍스트를 특정 범주로 분류 (예: 스팸 메일 필터링)
- **감정 분석**: 텍스트의 감정을 분석 (예: 긍정/부정 판단)
- **기계 번역**: 한 언어에서 다른 언어로 텍스트 번역 (예: Google 번역)
- **질문 응답**: 질문에 대한 답변 생성 (예: 챗봇)
- **텍스트 요약**: 긴 텍스트를 짧게 요약
- **개체명 인식**: 텍스트에서 사람, 장소, 조직 등을 식별
- **텍스트 생성**: 주어진 입력을 기반으로 새로운 텍스트 생성 (예: GPT 모델)

### 1.2 NLP의 필요성

- **데이터의 폭발적 증가**
    - 인터넷, 소셜 미디어, 메시지 플랫폼 등에서 텍스트 데이터가 급증하고 있습니다.
    - 이러한 데이터를 효과적으로 분석하고 활용하려면 NLP 기술이 필수적입니다.

- **자동화와 효율성**
    - NLP를 통해 반복적이고 시간이 많이 걸리는 작업을 자동화할 수 있습니다.
    - 예: 고객 문의 자동 응답, 문서 요약, 리포트 생성 등

- **사용자 경험 향상**
    - 챗봇, 음성 보조, 추천 시스템 등은 NLP를 통해 더 자연스럽고 직관적인 사용자 경험을 제공합니다.
    - 예: Amazon Alexa, Google Assistant

- **의사결정 지원**
    - 텍스트 데이터에서 유용한 정보를 추출하여 비즈니스, 의료, 금융 등 다양한 분야에서 의사결정을 지원합니다.
    - 예: 증권 리포트 분석, 환자 기록 분석

- **언어 장벽 해소**
    - 기계 번역 기술은 언어 간 소통의 장벽을 낮추고, 글로벌 협력을 촉진합니다.
    - 예: Google 번역, DeepL

### 1.3 NLP의 활용 분야
- **고객 서비스**: 챗봇, 자동 응답 시스템.
- **의료**: 환자 기록 분석, 질병 진단 지원.
- **금융**: 시장 분석, 감정 분석을 통한 주가 예측.
- **교육**: 자동 채점, 맞춤형 학습 콘텐츠 제공.
- **엔터테인먼트**: 영화 추천, 게임 내 대화 시스템.

### 1.4 NLP의 주요 개념
- **토큰화(Tokenization)**: 텍스트를 단어 또는 서브워드 단위로 분리.
- **임베딩(Embedding)**: 단어를 벡터 형태로 변환 (예: Word2Vec, GloVe).
- **시퀀스 모델링**: RNN, LSTM, GRU 등을 사용하여 시퀀스 데이터 처리.
- **트랜스포머(Transformer)**: Self-Attention 메커니즘을 사용한 고급 모델 (예: BERT, GPT).
- **전처리(Preprocessing)**: 텍스트 정규화, 불용어 제거, 어간 추출 등.

### 1.5 **NLP의 주요 흐름**
- **텍스트 수집**: 웹 크롤링, API, 데이터베이스 등에서 텍스트 데이터 수집
- **텍스트 전처리**: 토큰화, 정제, 정규화, 불용어 제거
- **특성 추출**: Bag of Words, TF-IDF, Word Embedding 등을 이용해 텍스트 데이터를 수치 데이터로 변환
- **모델 구축**: 분류, 군집화, 감성 분석, 개체명 인식 등 목적에 맞는 모델 구축
- **모델 평가 및 개선**: 정확도, 재현율, F1 스코어 등을 통한 모델 평가 및 개선

## 2. NLP 주요기술
### 2.1 **텍스트 전처리**
1. **토큰화(Tokenization)**: 텍스트를 단어, 문장 등의 의미 있는 단위로 분리

    ```python
    import nltk
    from nltk.tokenize import word_tokenize, sent_tokenize

    nltk.download('punkt')  # 토큰화를 위한 데이터 다운로드

    text = "ChatGPT is amazing! It helps with natural language processing."

    # 단어 토큰화
    word_tokens = word_tokenize(text)
    print("Word Tokenization:", word_tokens)

    # 문장 토큰화
    sent_tokens = sent_tokenize(text)
    print("Sentence Tokenization:", sent_tokens)
    ```

    🔹 **출력 예시:**  
    ```
    Word Tokenization: ['ChatGPT', 'is', 'amazing', '!', 'It', 'helps', 'with', 'natural', 'language', 'processing', '.']
    Sentence Tokenization: ['ChatGPT is amazing!', 'It helps with natural language processing.']
    ```


2. **정제(Cleaning)**: 불필요한 문자, 기호 제거

    ```python
    import re

    text = "This is an Example! NLP is AWESOME!!!"
    lower_text = text.lower()

    # 특수문자 제거
    clean_text = re.sub(r'[^a-zA-Z0-9\s]', '', lower_text)
    print("Cleaned Text:", clean_text)
    ```

    🔹 **출력 예시:**  
    ```
    Cleaned Text: this is an example nlp is awesome
    ```



3. **정규화(Normalization)**: 대소문자 통일, 어근 추출(Stemming), 표제어 추출(Lemmatization)

    - 어근 추출(Stemming) 
        - 어근 추출은 단어의 어간(Stem)만 남기고 접사(Suffix)를 제거하는 기법입니다.
        - 대표적인 방식으로 Porter Stemmer와 Lancaster Stemmer

        ```python
        from nltk.stem import PorterStemmer, LancasterStemmer

        stemmer1 = PorterStemmer()
        stemmer2 = LancasterStemmer()

        words = ["running", "flies", "happily", "studies", "better"]

        print("Porter Stemmer 결과:")
        print([stemmer1.stem(word) for word in words])

        print("\nLancaster Stemmer 결과:")
        print([stemmer2.stem(word) for word in words])

        ```
    - 표제어 추출(Lemmatization):
        - 표제어 추출은 단어의 원형(기본 사전형, Lemma) 을 찾는 과정입니다.
        - 이 방식은 문맥과 품사 정보를 반영하여 더 정확한 결과를 제공

        ```python
        import spacy

        nlp = spacy.load("en_core_web_sm")

        text = "running flies happily studies better"
        doc = nlp(text)

        print("표제어 추출 결과:")
        for token in doc:
            print(token.text, "→", token.lemma_)
        ```

    - 한국어: 어근 추출보다 형태소 분석을 활용한 원형 복원이 더 효과적

        ```python
        from konlpy.tag import Okt

        okt = Okt()
        text = "달려가는 강아지가 귀엽습니다."

        # 형태소 분석 후 원형 출력
        morphs = okt.pos(text, stem=True)
        print(morphs)

        ```

4. **불용어 제거(Stopwords Removal)**: 'the', 'a', 'is'와 같은 분석에 큰 의미가 없는 단어 제거

- 불필요한 단어(예: "is", "an", "the")를 제거하여 핵심 단어만 남깁니다.  
    ```python
    from nltk.corpus import stopwords

    nltk.download('stopwords')

    words = ["this", "is", "an", "example", "of", "stopword", "removal"]
    filtered_words = [word for word in words if word not in stopwords.words('english')]

    print("Filtered Words:", filtered_words)
    ```

    🔹 **출력 예시:**  
    ```
    Filtered Words: ['example', 'stopword', 'removal']
    ```

### 2.2 **형태소 분석**

1. **형태소 분석(Morphological Analysis) 실습**  
- 형태소 분석을 통해 단어의 어근, 품사를 확인합니다.  

    ```python
    import spacy

    nlp = spacy.load("en_core_web_sm")
    text = "Running faster is good for your health."

    # 형태소 분석 수행
    doc = nlp(text)
    for token in doc:
        print(token.text, "→", token.lemma_, "/", token.pos_)
    ```

    🔹 **출력 예시:**  
    ```
    Running → run / VERB
    faster → fast / ADV
    is → be / AUX
    good → good / ADJ
    for → for / ADP
    your → your / PRON
    health → health / NOUN
    . → . / PUNCT
    ```

2. **한국어 형태소 분석 (KoNLPy 활용)**

    ```python
    from konlpy.tag import Okt

    okt = Okt()
    text = "자연어 처리는 재미있습니다."

    # 형태소 분석
    morphs = okt.morphs(text)
    print("형태소:", morphs)

    # 품사 태깅
    pos_tags = okt.pos(text)
    print("품사 태깅:", pos_tags)
    ```

    🔹 **출력 예시:**  
    ```
    형태소: ['자연어', '처리', '는', '재미있', '습니다', '.']
    품사 태깅: [('자연어', 'Noun'), ('처리', 'Noun'), ('는', 'Josa'), ('재미있', 'Adjective'), ('습니다', 'Eomi'), ('.', 'Punctuation')]
    ```


### 2.3 텍스트 표현
1. **Bag of Words**: 단어의 등장 빈도를 벡터로 표현
    ```python
    from sklearn.feature_extraction.text import CountVectorizer

    # 문서 리스트
    documents = [
        "I love NLP",
        "NLP is fascinating",
        "I enjoy learning NLP"
    ]

    # CountVectorizer를 사용하여 Bag of Words 생성
    vectorizer = CountVectorizer()
    bow_matrix = vectorizer.fit_transform(documents)

    # 결과 출력
    print("단어 목록:", vectorizer.get_feature_names_out())
    print("Bag of Words 행렬:\n", bow_matrix.toarray())
    ```

    🔹 **출력 예시:**  
    ```
    단어 목록: ['enjoy' 'fascinating' 'is' 'learning' 'love' 'nlp']
    Bag of Words 행렬:
    [[0 0 0 0 1 1]
    [0 1 1 0 0 1]
    [1 0 0 1 0 1]]
    ```


2. **TF-IDF**: 단어 빈도와 문서 빈도의 역수를 곱한 값으로 중요도 표현

    ```python
    from sklearn.feature_extraction.text import TfidfVectorizer

    # 문서 리스트
    documents = [
        "I love NLP",
        "NLP is fascinating",
        "I enjoy learning NLP"
    ]

    # TfidfVectorizer를 사용하여 TF-IDF 행렬 생성
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(documents)

    # 결과 출력
    print("단어 목록:", vectorizer.get_feature_names_out())
    print("TF-IDF 행렬:\n", tfidf_matrix.toarray())
    ```

    🔹 **출력 예시:** 
    ```
    단어 목록: ['enjoy' 'fascinating' 'is' 'learning' 'love' 'nlp']
    TF-IDF 행렬:
    [[0.         0.         0.         0.         0.70710678 0.70710678]
    [0.         0.70710678 0.70710678 0.         0.         0.70710678]
    [0.57735027 0.         0.         0.57735027 0.         0.57735027]]
    ```

3. **Word Embedding**: Word2Vec, GloVe, FastText 등을 이용한 단어의 의미적 표현
    - 단어를 벡터 공간에 표현하여 단어의 의미적 관계를 파악할 수 있게 합니다.
    - Word2Vec 사용
    
    ```python
    from gensim.models import Word2Vec

    # 문장 리스트
    sentences = [
        ["I", "love", "NLP"],
        ["NLP", "is", "fascinating"],
        ["I", "enjoy", "learning", "NLP"]
    ]

    # Word2Vec 모델 학습
    model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=1)

    # 단어 "NLP"의 벡터 표현
    nlp_vector = model.wv["NLP"]
    print("'NLP'의 벡터 표현:", nlp_vector)

    # 가장 유사한 단어 찾기
    similar_words = model.wv.most_similar("NLP")
    print("'NLP'와 가장 유사한 단어:", similar_words)
    ```

    🔹 **출력 예시:** 
    ```
    'NLP'의 벡터 표현: [ 0.12345678 -0.23456789  0.34567891 ... ]
    'NLP'와 가장 유사한 단어: [('love', 0.98765432), ('fascinating', 0.87654321), ...]
    ```

## 3. 언어 모델
### 3.1 **통계적 언어 모델**: N-gram 모델
- N-gram 모델은 이전 N-1개의 단어를 기반으로 다음 단어를 예측하는 통계적 모델입니다.

    ```python
    from nltk import ngrams
    from collections import defaultdict, Counter

    # 샘플 문장
    sentence = "I love natural language processing"

    # 2-gram 모델 생성
    n = 2
    bigrams = list(ngrams(sentence.split(), n))

    # 빈도수 계산
    bigram_freq = Counter(bigrams)

    # 결과 출력
    for bigram, freq in bigram_freq.items():
        print(f"{bigram}: {freq}")
    ```

    🔹 **출력 예시:**  
    ```
    ('I', 'love'): 1
    ('love', 'natural'): 1
    ('natural', 'language'): 1
    ('language', 'processing'): 1
    ```

### 3.2 **신경망 기반 언어 모델**: RNN, LSTM, GRU
- RNN, LSTM, GRU는 시퀀스 데이터를 처리하는 데 사용되는 신경망 모델입니다.

    ```python
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    import numpy as np
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Embedding, SimpleRNN, Dense

    # 샘플 텍스트 데이터
    texts = [
        "I love natural language processing",
        "NLP is a fascinating field",
        "I enjoy learning new things"
    ]

    # 토크나이저 객체 생성 및 단어 사전 구축
    tokenizer = Tokenizer(num_words=10000)
    tokenizer.fit_on_texts(texts)

    # 텍스트를 단어 인덱스 시퀀스로 변환
    sequences = tokenizer.texts_to_sequences(texts)

    # 입력 데이터 (단어 인덱스 시퀀스)
    input_sequences = np.array(sequences)

    # 타겟 데이터 (다음 단어 예측을 위한 레이블)
    target_words = np.array([5, 10, 14])  # 각 시퀀스의 다음 단어

    # RNN 모델 정의
    vocab_size = 10000
    embedding_dim = 128
    rnn_units = 64

    model = Sequential([
        Embedding(vocab_size, embedding_dim, input_length=5),
        SimpleRNN(rnn_units),
        Dense(vocab_size, activation='softmax')
    ])

    # 모델 컴파일
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

    # 모델 학습
    model.fit(input_sequences, target_words, epochs=10, batch_size=1)

    # 새로운 텍스트 데이터
    new_text = "I enjoy learning"

    # 새로운 텍스트를 단어 인덱스 시퀀스로 변환
    new_sequence = tokenizer.texts_to_sequences([new_text])

    # 패딩 추가 (모델 입력 길이에 맞추기)
    new_sequence = pad_sequences(new_sequence, maxlen=5)

    # 다음 단어 예측
    predictions = model.predict(new_sequence)

    # 예측 결과에서 가장 높은 확률을 가진 단어 인덱스 찾기
    predicted_word_index = np.argmax(predictions, axis=-1)

    # 단어 인덱스를 단어로 변환
    predicted_word = tokenizer.index_word.get(predicted_word_index[0], "UNK")

    print("새로운 입력 시퀀스:", new_sequence)
    print("예측된 다음 단어:", predicted_word)
    ```

### 3.3 **트랜스포머 기반 모델**: BERT, GPT, T5
- 트랜스포머 기반 모델은 self-attention 메커니즘을 사용하여 시퀀스 데이터를 처리합니다.

1. **BERT를 사용한 텍스트 분류**

    ```python
    from transformers import BertTokenizer, TFBertForSequenceClassification
    import tensorflow as tf

    # BERT 토크나이저 로드
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    # 샘플 텍스트
    text = "I love natural language processing"

    # 토크나이징
    inputs = tokenizer(text, return_tensors='tf', truncation=True, padding=True)

    # BERT 모델 로드
    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')

    # 예측
    outputs = model(inputs)
    logits = outputs.logits

    # 결과 출력
    print(logits)
    ```

2. **GPT를 사용한 텍스트 생성**
    ```python
    from transformers import GPT2Tokenizer, TFGPT2LMHeadModel

    # GPT-2 토크나이저 로드
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

    # GPT-2 모델 로드
    model = TFGPT2LMHeadModel.from_pretrained('gpt2')

    # 샘플 텍스트
    text = "Once upon a time"

    # 토크나이징
    inputs = tokenizer.encode(text, return_tensors='tf')

    # 텍스트 생성
    outputs = model.generate(inputs, max_length=50, num_return_sequences=1)

    # 결과 출력
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(generated_text)
    ```

3. **T5를 사용한 텍스트 요약**
    ```python
    from transformers import T5Tokenizer, TFT5ForConditionalGeneration

    # T5 토크나이저 로드
    tokenizer = T5Tokenizer.from_pretrained('t5-small')

    # T5 모델 로드
    model = TFT5ForConditionalGeneration.from_pretrained('t5-small')

    # 샘플 텍스트
    text = "Natural language processing is a field of artificial intelligence."

    # 토크나이징
    inputs = tokenizer.encode("summarize: " + text, return_tensors='tf', max_length=512, truncation=True)

    # 요약 생성
    outputs = model.generate(inputs, max_length=50, num_beams=4, early_stopping=True)

    # 결과 출력
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(summary)
    ```




